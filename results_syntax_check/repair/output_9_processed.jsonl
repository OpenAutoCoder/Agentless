{"model_name_or_path": "agentless", "instance_id": "astropy__astropy-13579", "model_patch": "diff --git a/astropy/wcs/wcsapi/low_level_api.py b/astropy/wcs/wcsapi/low_level_api.py\nindex 0025ee0..9191289 100644\n--- a/astropy/wcs/wcsapi/low_level_api.py\n+++ b/astropy/wcs/wcsapi/low_level_api.py\n@@ -87,7 +87,8 @@ class BaseLowLevelWCS(metaclass=abc.ABCMeta):\n         method returns a single scalar or array, otherwise a tuple of scalars or\n         arrays is returned.\n         \"\"\"\n-        return self.pixel_to_world_values(*index_arrays[::-1])\n+        # Ensure correct order of indices for pixel to world conversion\n+        return self.pixel_to_world_values(*index_arrays)\n \n     @abc.abstractmethod\n     def world_to_pixel_values(self, *world_arrays):\n", "raw_model_patch": "diff --git a/astropy/wcs/wcsapi/low_level_api.py b/astropy/wcs/wcsapi/low_level_api.py\nindex 0025ee0..9191289 100644\n--- a/astropy/wcs/wcsapi/low_level_api.py\n+++ b/astropy/wcs/wcsapi/low_level_api.py\n@@ -87,7 +87,8 @@ class BaseLowLevelWCS(metaclass=abc.ABCMeta):\n         method returns a single scalar or array, otherwise a tuple of scalars or\n         arrays is returned.\n         \"\"\"\n-        return self.pixel_to_world_values(*index_arrays[::-1])\n+        # Ensure correct order of indices for pixel to world conversion\n+        return self.pixel_to_world_values(*index_arrays)\n \n     @abc.abstractmethod\n     def world_to_pixel_values(self, *world_arrays):\n", "original_file_content": "import os\nimport abc\n\nimport numpy as np\n\n__all__ = ['BaseLowLevelWCS', 'validate_physical_types']\n\n\nclass BaseLowLevelWCS(metaclass=abc.ABCMeta):\n    \"\"\"\n    Abstract base class for the low-level WCS interface.\n\n    This is described in `APE 14: A shared Python interface for World Coordinate\n    Systems <https://doi.org/10.5281/zenodo.1188875>`_.\n    \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def pixel_n_dim(self):\n        \"\"\"\n        The number of axes in the pixel coordinate system.\n        \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def world_n_dim(self):\n        \"\"\"\n        The number of axes in the world coordinate system.\n        \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def world_axis_physical_types(self):\n        \"\"\"\n        An iterable of strings describing the physical type for each world axis.\n\n        These should be names from the VO UCD1+ controlled Vocabulary\n        (http://www.ivoa.net/documents/latest/UCDlist.html). If no matching UCD\n        type exists, this can instead be ``\"custom:xxx\"``, where ``xxx`` is an\n        arbitrary string.  Alternatively, if the physical type is\n        unknown/undefined, an element can be `None`.\n        \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def world_axis_units(self):\n        \"\"\"\n        An iterable of strings given the units of the world coordinates for each\n        axis.\n\n        The strings should follow the `IVOA VOUnit standard\n        <http://ivoa.net/documents/VOUnits/>`_ (though as noted in the VOUnit\n        specification document, units that do not follow this standard are still\n        allowed, but just not recommended).\n        \"\"\"\n\n    @abc.abstractmethod\n    def pixel_to_world_values(self, *pixel_arrays):\n        \"\"\"\n        Convert pixel coordinates to world coordinates.\n\n        This method takes `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_n_dim` scalars or arrays as\n        input, and pixel coordinates should be zero-based. Returns\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_n_dim` scalars or arrays in units given by\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_axis_units`. Note that pixel coordinates are\n        assumed to be 0 at the center of the first pixel in each dimension. If a\n        pixel is in a region where the WCS is not defined, NaN can be returned.\n        The coordinates should be specified in the ``(x, y)`` order, where for\n        an image, ``x`` is the horizontal coordinate and ``y`` is the vertical\n        coordinate.\n\n        If `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_n_dim` is ``1``, this\n        method returns a single scalar or array, otherwise a tuple of scalars or\n        arrays is returned.\n        \"\"\"\n\n    def array_index_to_world_values(self, *index_arrays):\n        \"\"\"\n        Convert array indices to world coordinates.\n\n        This is the same as `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_to_world_values` except that\n        the indices should be given in ``(i, j)`` order, where for an image\n        ``i`` is the row and ``j`` is the column (i.e. the opposite order to\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_to_world_values`).\n\n        If `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_n_dim` is ``1``, this\n        method returns a single scalar or array, otherwise a tuple of scalars or\n        arrays is returned.\n        \"\"\"\n        return self.pixel_to_world_values(*index_arrays[::-1])\n\n    @abc.abstractmethod\n    def world_to_pixel_values(self, *world_arrays):\n        \"\"\"\n        Convert world coordinates to pixel coordinates.\n\n        This method takes `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_n_dim` scalars or arrays as\n        input in units given by `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_axis_units`. Returns\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_n_dim` scalars or arrays. Note that pixel\n        coordinates are assumed to be 0 at the center of the first pixel in each\n        dimension. If a world coordinate does not have a matching pixel\n        coordinate, NaN can be returned.  The coordinates should be returned in\n        the ``(x, y)`` order, where for an image, ``x`` is the horizontal\n        coordinate and ``y`` is the vertical coordinate.\n\n        If `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_n_dim` is ``1``, this\n        method returns a single scalar or array, otherwise a tuple of scalars or\n        arrays is returned.\n        \"\"\"\n\n    def world_to_array_index_values(self, *world_arrays):\n        \"\"\"\n        Convert world coordinates to array indices.\n\n        This is the same as `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_to_pixel_values` except that\n        the indices should be returned in ``(i, j)`` order, where for an image\n        ``i`` is the row and ``j`` is the column (i.e. the opposite order to\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_to_world_values`). The indices should be\n        returned as rounded integers.\n\n        If `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_n_dim` is ``1``, this\n        method returns a single scalar or array, otherwise a tuple of scalars or\n        arrays is returned.\n        \"\"\"\n        pixel_arrays = self.world_to_pixel_values(*world_arrays)\n        if self.pixel_n_dim == 1:\n            pixel_arrays = (pixel_arrays,)\n        else:\n            pixel_arrays = pixel_arrays[::-1]\n        array_indices = tuple(np.asarray(np.floor(pixel + 0.5), dtype=np.int_) for pixel in pixel_arrays)\n        return array_indices[0] if self.pixel_n_dim == 1 else array_indices\n\n    @property\n    @abc.abstractmethod\n    def world_axis_object_components(self):\n        \"\"\"\n        A list with `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_n_dim` elements giving information\n        on constructing high-level objects for the world coordinates.\n\n        Each element of the list is a tuple with three items:\n\n        * The first is a name for the world object this world array\n          corresponds to, which *must* match the string names used in\n          `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_axis_object_classes`. Note that names might\n          appear twice because two world arrays might correspond to a single\n          world object (e.g. a celestial coordinate might have both \u201cra\u201d and\n          \u201cdec\u201d arrays, which correspond to a single sky coordinate object).\n\n        * The second element is either a string keyword argument name or a\n          positional index for the corresponding class from\n          `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_axis_object_classes`.\n\n        * The third argument is a string giving the name of the property\n          to access on the corresponding class from\n          `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_axis_object_classes` in\n          order to get numerical values. Alternatively, this argument can be a\n          callable Python object that takes a high-level coordinate object and\n          returns the numerical values suitable for passing to the low-level\n          WCS transformation methods.\n\n        See the document\n        `APE 14: A shared Python interface for World Coordinate Systems\n        <https://doi.org/10.5281/zenodo.1188875>`_ for examples.\n        \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def world_axis_object_classes(self):\n        \"\"\"\n        A dictionary giving information on constructing high-level objects for\n        the world coordinates.\n\n        Each key of the dictionary is a string key from\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_axis_object_components`, and each value is a\n        tuple with three elements or four elements:\n\n        * The first element of the tuple must be a class or a string specifying\n          the fully-qualified name of a class, which will specify the actual\n          Python object to be created.\n\n        * The second element, should be a tuple specifying the positional\n          arguments required to initialize the class. If\n          `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_axis_object_components` specifies that the\n          world coordinates should be passed as a positional argument, this this\n          tuple should include `None` placeholders for the world coordinates.\n\n        * The third tuple element must be a dictionary with the keyword\n          arguments required to initialize the class.\n\n        * Optionally, for advanced use cases, the fourth element (if present)\n          should be a callable Python object that gets called instead of the\n          class and gets passed the positional and keyword arguments. It should\n          return an object of the type of the first element in the tuple.\n\n        Note that we don't require the classes to be Astropy classes since there\n        is no guarantee that Astropy will have all the classes to represent all\n        kinds of world coordinates. Furthermore, we recommend that the output be\n        kept as human-readable as possible.\n\n        The classes used here should have the ability to do conversions by\n        passing an instance as the first argument to the same class with\n        different arguments (e.g. ``Time(Time(...), scale='tai')``). This is\n        a requirement for the implementation of the high-level interface.\n\n        The second and third tuple elements for each value of this dictionary\n        can in turn contain either instances of classes, or if necessary can\n        contain serialized versions that should take the same form as the main\n        classes described above (a tuple with three elements with the fully\n        qualified name of the class, then the positional arguments and the\n        keyword arguments). For low-level API objects implemented in Python, we\n        recommend simply returning the actual objects (not the serialized form)\n        for optimal performance. Implementations should either always or never\n        use serialized classes to represent Python objects, and should indicate\n        which of these they follow using the\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.serialized_classes` attribute.\n\n        See the document\n        `APE 14: A shared Python interface for World Coordinate Systems\n        <https://doi.org/10.5281/zenodo.1188875>`_ for examples .\n        \"\"\"\n\n    # The following three properties have default fallback implementations, so\n    # they are not abstract.\n\n    @property\n    def array_shape(self):\n        \"\"\"\n        The shape of the data that the WCS applies to as a tuple of length\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_n_dim` in ``(row, column)``\n        order (the convention for arrays in Python).\n\n        If the WCS is valid in the context of a dataset with a particular\n        shape, then this property can be used to store the shape of the\n        data. This can be used for example if implementing slicing of WCS\n        objects. This is an optional property, and it should return `None`\n        if a shape is not known or relevant.\n        \"\"\"\n        if self.pixel_shape is None:\n            return None\n        else:\n            return self.pixel_shape[::-1]\n\n    @property\n    def pixel_shape(self):\n        \"\"\"\n        The shape of the data that the WCS applies to as a tuple of length\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_n_dim` in ``(x, y)``\n        order (where for an image, ``x`` is the horizontal coordinate and ``y``\n        is the vertical coordinate).\n\n        If the WCS is valid in the context of a dataset with a particular\n        shape, then this property can be used to store the shape of the\n        data. This can be used for example if implementing slicing of WCS\n        objects. This is an optional property, and it should return `None`\n        if a shape is not known or relevant.\n\n        If you are interested in getting a shape that is comparable to that of\n        a Numpy array, you should use\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.array_shape` instead.\n        \"\"\"\n        return None\n\n    @property\n    def pixel_bounds(self):\n        \"\"\"\n        The bounds (in pixel coordinates) inside which the WCS is defined,\n        as a list with `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_n_dim`\n        ``(min, max)`` tuples.\n\n        The bounds should be given in ``[(xmin, xmax), (ymin, ymax)]``\n        order. WCS solutions are sometimes only guaranteed to be accurate\n        within a certain range of pixel values, for example when defining a\n        WCS that includes fitted distortions. This is an optional property,\n        and it should return `None` if a shape is not known or relevant.\n        \"\"\"\n        return None\n\n    @property\n    def pixel_axis_names(self):\n        \"\"\"\n        An iterable of strings describing the name for each pixel axis.\n\n        If an axis does not have a name, an empty string should be returned\n        (this is the default behavior for all axes if a subclass does not\n        override this property). Note that these names are just for display\n        purposes and are not standardized.\n        \"\"\"\n        return [''] * self.pixel_n_dim\n\n    @property\n    def world_axis_names(self):\n        \"\"\"\n        An iterable of strings describing the name for each world axis.\n\n        If an axis does not have a name, an empty string should be returned\n        (this is the default behavior for all axes if a subclass does not\n        override this property). Note that these names are just for display\n        purposes and are not standardized. For standardized axis types, see\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.world_axis_physical_types`.\n        \"\"\"\n        return [''] * self.world_n_dim\n\n    @property\n    def axis_correlation_matrix(self):\n        \"\"\"\n        Returns an (`~astropy.wcs.wcsapi.BaseLowLevelWCS.world_n_dim`,\n        `~astropy.wcs.wcsapi.BaseLowLevelWCS.pixel_n_dim`) matrix that\n        indicates using booleans whether a given world coordinate depends on a\n        given pixel coordinate.\n\n        This defaults to a matrix where all elements are `True` in the absence\n        of any further information. For completely independent axes, the\n        diagonal would be `True` and all other entries `False`.\n        \"\"\"\n        return np.ones((self.world_n_dim, self.pixel_n_dim), dtype=bool)\n\n    @property\n    def serialized_classes(self):\n        \"\"\"\n        Indicates whether Python objects are given in serialized form or as\n        actual Python objects.\n        \"\"\"\n        return False\n\n    def _as_mpl_axes(self):\n        \"\"\"\n        Compatibility hook for Matplotlib and WCSAxes. With this method, one can\n        do::\n\n            from astropy.wcs import WCS\n            import matplotlib.pyplot as plt\n            wcs = WCS('filename.fits')\n            fig = plt.figure()\n            ax = fig.add_axes([0.15, 0.1, 0.8, 0.8], projection=wcs)\n            ...\n\n        and this will generate a plot with the correct WCS coordinates on the\n        axes.\n        \"\"\"\n        from astropy.visualization.wcsaxes import WCSAxes\n        return WCSAxes, {'wcs': self}\n\n\nUCDS_FILE = os.path.join(os.path.dirname(__file__), 'data', 'ucds.txt')\nwith open(UCDS_FILE) as f:\n    VALID_UCDS = {x.strip() for x in f.read().splitlines()[1:]}\n\n\ndef validate_physical_types(physical_types):\n    \"\"\"\n    Validate a list of physical types against the UCD1+ standard\n    \"\"\"\n    for physical_type in physical_types:\n        if (physical_type is not None and\n            physical_type not in VALID_UCDS and\n                not physical_type.startswith('custom:')):\n            raise ValueError(\n                f\"'{physical_type}' is not a valid IOVA UCD1+ physical type. \"\n                \"It must be a string specified in the list (http://www.ivoa.net/documents/latest/UCDlist.html) \"\n                \"or if no matching type exists it can be any string prepended with 'custom:'.\"\n            )"}
{"model_name_or_path": "agentless", "instance_id": "astropy__astropy-14096", "model_patch": "diff --git a/astropy/coordinates/sky_coordinate.py b/astropy/coordinates/sky_coordinate.py\nindex f158027..b34027d 100644\n--- a/astropy/coordinates/sky_coordinate.py\n+++ b/astropy/coordinates/sky_coordinate.py\n@@ -896,7 +896,7 @@ class SkyCoord(ShapedLikeNDArray):\n \n         # Fail\n         raise AttributeError(\n-            f\"'{self.__class__.__name__}' object has no attribute '{attr}'\"\n+            f\"'{self.__class__.__name__}' object has no attribute '{attr}' (accessed from '{self.__class__.__name__}.{self._last_accessed_property}')\"\n         )\n \n     def __setattr__(self, attr, val):\n", "raw_model_patch": "diff --git a/astropy/coordinates/sky_coordinate.py b/astropy/coordinates/sky_coordinate.py\nindex f158027..b34027d 100644\n--- a/astropy/coordinates/sky_coordinate.py\n+++ b/astropy/coordinates/sky_coordinate.py\n@@ -896,7 +896,7 @@ class SkyCoord(ShapedLikeNDArray):\n \n         # Fail\n         raise AttributeError(\n-            f\"'{self.__class__.__name__}' object has no attribute '{attr}'\"\n+            f\"'{self.__class__.__name__}' object has no attribute '{attr}' (accessed from '{self.__class__.__name__}.{self._last_accessed_property}')\"\n         )\n \n     def __setattr__(self, attr, val):\n", "original_file_content": "import copy\nimport operator\nimport re\nimport warnings\n\nimport erfa\nimport numpy as np\n\nfrom astropy import units as u\nfrom astropy.constants import c as speed_of_light\nfrom astropy.table import QTable\nfrom astropy.time import Time\nfrom astropy.utils import ShapedLikeNDArray\nfrom astropy.utils.data_info import MixinInfo\nfrom astropy.utils.exceptions import AstropyUserWarning\n\nfrom .angles import Angle\nfrom .baseframe import BaseCoordinateFrame, GenericFrame, frame_transform_graph\nfrom .distances import Distance\nfrom .representation import (\n    RadialDifferential,\n    SphericalDifferential,\n    SphericalRepresentation,\n    UnitSphericalCosLatDifferential,\n    UnitSphericalDifferential,\n    UnitSphericalRepresentation,\n)\nfrom .sky_coordinate_parsers import (\n    _get_frame_class,\n    _get_frame_without_data,\n    _parse_coordinate_data,\n)\n\n__all__ = [\"SkyCoord\", \"SkyCoordInfo\"]\n\n\nclass SkyCoordInfo(MixinInfo):\n    \"\"\"\n    Container for meta information like name, description, format.  This is\n    required when the object is used as a mixin column within a table, but can\n    be used as a general way to store meta information.\n    \"\"\"\n\n    attrs_from_parent = {\"unit\"}  # Unit is read-only\n    _supports_indexing = False\n\n    @staticmethod\n    def default_format(val):\n        repr_data = val.info._repr_data\n        formats = [\"{0.\" + compname + \".value:}\" for compname in repr_data.components]\n        return \",\".join(formats).format(repr_data)\n\n    @property\n    def unit(self):\n        repr_data = self._repr_data\n        unit = \",\".join(\n            str(getattr(repr_data, comp).unit) or \"None\"\n            for comp in repr_data.components\n        )\n        return unit\n\n    @property\n    def _repr_data(self):\n        if self._parent is None:\n            return None\n\n        sc = self._parent\n        if issubclass(sc.representation_type, SphericalRepresentation) and isinstance(\n            sc.data, UnitSphericalRepresentation\n        ):\n            repr_data = sc.represent_as(sc.data.__class__, in_frame_units=True)\n        else:\n            repr_data = sc.represent_as(sc.representation_type, in_frame_units=True)\n        return repr_data\n\n    def _represent_as_dict(self):\n        sc = self._parent\n        attrs = list(sc.representation_component_names)\n\n        # Don't output distance unless it's actually distance.\n        if isinstance(sc.data, UnitSphericalRepresentation):\n            attrs = attrs[:-1]\n\n        diff = sc.data.differentials.get(\"s\")\n        if diff is not None:\n            diff_attrs = list(sc.get_representation_component_names(\"s\"))\n            # Don't output proper motions if they haven't been specified.\n            if isinstance(diff, RadialDifferential):\n                diff_attrs = diff_attrs[2:]\n            # Don't output radial velocity unless it's actually velocity.\n            elif isinstance(\n                diff, (UnitSphericalDifferential, UnitSphericalCosLatDifferential)\n            ):\n                diff_attrs = diff_attrs[:-1]\n            attrs.extend(diff_attrs)\n\n        attrs.extend(frame_transform_graph.frame_attributes.keys())\n\n        out = super()._represent_as_dict(attrs)\n\n        out[\"representation_type\"] = sc.representation_type.get_name()\n        out[\"frame\"] = sc.frame.name\n        # Note that sc.info.unit is a fake composite unit (e.g. 'deg,deg,None'\n        # or None,None,m) and is not stored.  The individual attributes have\n        # units.\n\n        return out\n\n    def new_like(self, skycoords, length, metadata_conflicts=\"warn\", name=None):\n        \"\"\"\n        Return a new SkyCoord instance which is consistent with the input\n        SkyCoord objects ``skycoords`` and has ``length`` rows.  Being\n        \"consistent\" is defined as being able to set an item from one to each of\n        the rest without any exception being raised.\n\n        This is intended for creating a new SkyCoord instance whose elements can\n        be set in-place for table operations like join or vstack.  This is used\n        when a SkyCoord object is used as a mixin column in an astropy Table.\n\n        The data values are not predictable and it is expected that the consumer\n        of the object will fill in all values.\n\n        Parameters\n        ----------\n        skycoords : list\n            List of input SkyCoord objects\n        length : int\n            Length of the output skycoord object\n        metadata_conflicts : str ('warn'|'error'|'silent')\n            How to handle metadata conflicts\n        name : str\n            Output name (sets output skycoord.info.name)\n\n        Returns\n        -------\n        skycoord : SkyCoord (or subclass)\n            Instance of this class consistent with ``skycoords``\n\n        \"\"\"\n        # Get merged info attributes like shape, dtype, format, description, etc.\n        attrs = self.merge_cols_attributes(\n            skycoords, metadata_conflicts, name, (\"meta\", \"description\")\n        )\n        skycoord0 = skycoords[0]\n\n        # Make a new SkyCoord object with the desired length and attributes\n        # by using the _apply / __getitem__ machinery to effectively return\n        # skycoord0[[0, 0, ..., 0, 0]]. This will have the all the right frame\n        # attributes with the right shape.\n        indexes = np.zeros(length, dtype=np.int64)\n        out = skycoord0[indexes]\n\n        # Use __setitem__ machinery to check for consistency of all skycoords\n        for skycoord in skycoords[1:]:\n            try:\n                out[0] = skycoord[0]\n            except Exception as err:\n                raise ValueError(\"Input skycoords are inconsistent.\") from err\n\n        # Set (merged) info attributes\n        for attr in (\"name\", \"meta\", \"description\"):\n            if attr in attrs:\n                setattr(out.info, attr, attrs[attr])\n\n        return out\n\n\nclass SkyCoord(ShapedLikeNDArray):\n    \"\"\"High-level object providing a flexible interface for celestial coordinate\n    representation, manipulation, and transformation between systems.\n\n    The `SkyCoord` class accepts a wide variety of inputs for initialization. At\n    a minimum these must provide one or more celestial coordinate values with\n    unambiguous units.  Inputs may be scalars or lists/tuples/arrays, yielding\n    scalar or array coordinates (can be checked via ``SkyCoord.isscalar``).\n    Typically one also specifies the coordinate frame, though this is not\n    required. The general pattern for spherical representations is::\n\n      SkyCoord(COORD, [FRAME], keyword_args ...)\n      SkyCoord(LON, LAT, [FRAME], keyword_args ...)\n      SkyCoord(LON, LAT, [DISTANCE], frame=FRAME, unit=UNIT, keyword_args ...)\n      SkyCoord([FRAME], <lon_attr>=LON, <lat_attr>=LAT, keyword_args ...)\n\n    It is also possible to input coordinate values in other representations\n    such as cartesian or cylindrical.  In this case one includes the keyword\n    argument ``representation_type='cartesian'`` (for example) along with data\n    in ``x``, ``y``, and ``z``.\n\n    See also: https://docs.astropy.org/en/stable/coordinates/\n\n    Examples\n    --------\n    The examples below illustrate common ways of initializing a `SkyCoord`\n    object.  For a complete description of the allowed syntax see the\n    full coordinates documentation.  First some imports::\n\n      >>> from astropy.coordinates import SkyCoord  # High-level coordinates\n      >>> from astropy.coordinates import ICRS, Galactic, FK4, FK5  # Low-level frames\n      >>> from astropy.coordinates import Angle, Latitude, Longitude  # Angles\n      >>> import astropy.units as u\n\n    The coordinate values and frame specification can now be provided using\n    positional and keyword arguments::\n\n      >>> c = SkyCoord(10, 20, unit=\"deg\")  # defaults to ICRS frame\n      >>> c = SkyCoord([1, 2, 3], [-30, 45, 8], frame=\"icrs\", unit=\"deg\")  # 3 coords\n\n      >>> coords = [\"1:12:43.2 +31:12:43\", \"1 12 43.2 +31 12 43\"]\n      >>> c = SkyCoord(coords, frame=FK4, unit=(u.hourangle, u.deg), obstime=\"J1992.21\")\n\n      >>> c = SkyCoord(\"1h12m43.2s +1d12m43s\", frame=Galactic)  # Units from string\n      >>> c = SkyCoord(frame=\"galactic\", l=\"1h12m43.2s\", b=\"+1d12m43s\")\n\n      >>> ra = Longitude([1, 2, 3], unit=u.deg)  # Could also use Angle\n      >>> dec = np.array([4.5, 5.2, 6.3]) * u.deg  # Astropy Quantity\n      >>> c = SkyCoord(ra, dec, frame='icrs')\n      >>> c = SkyCoord(frame=ICRS, ra=ra, dec=dec, obstime='2001-01-02T12:34:56')\n\n      >>> c = FK4(1 * u.deg, 2 * u.deg)  # Uses defaults for obstime, equinox\n      >>> c = SkyCoord(c, obstime='J2010.11', equinox='B1965')  # Override defaults\n\n      >>> c = SkyCoord(w=0, u=1, v=2, unit='kpc', frame='galactic',\n      ...              representation_type='cartesian')\n\n      >>> c = SkyCoord([ICRS(ra=1*u.deg, dec=2*u.deg), ICRS(ra=3*u.deg, dec=4*u.deg)])\n\n    Velocity components (proper motions or radial velocities) can also be\n    provided in a similar manner::\n\n      >>> c = SkyCoord(ra=1*u.deg, dec=2*u.deg, radial_velocity=10*u.km/u.s)\n\n      >>> c = SkyCoord(ra=1*u.deg, dec=2*u.deg, pm_ra_cosdec=2*u.mas/u.yr, pm_dec=1*u.mas/u.yr)\n\n    As shown, the frame can be a `~astropy.coordinates.BaseCoordinateFrame`\n    class or the corresponding string alias.  The frame classes that are built in\n    to astropy are `ICRS`, `FK5`, `FK4`, `FK4NoETerms`, and `Galactic`.\n    The string aliases are simply lower-case versions of the class name, and\n    allow for creating a `SkyCoord` object and transforming frames without\n    explicitly importing the frame classes.\n\n    Parameters\n    ----------\n    frame : `~astropy.coordinates.BaseCoordinateFrame` class or string, optional\n        Type of coordinate frame this `SkyCoord` should represent. Defaults to\n        to ICRS if not given or given as None.\n    unit : `~astropy.units.Unit`, string, or tuple of :class:`~astropy.units.Unit` or str, optional\n        Units for supplied coordinate values.\n        If only one unit is supplied then it applies to all values.\n        Note that passing only one unit might lead to unit conversion errors\n        if the coordinate values are expected to have mixed physical meanings\n        (e.g., angles and distances).\n    obstime : time-like, optional\n        Time(s) of observation.\n    equinox : time-like, optional\n        Coordinate frame equinox time.\n    representation_type : str or Representation class\n        Specifies the representation, e.g. 'spherical', 'cartesian', or\n        'cylindrical'.  This affects the positional args and other keyword args\n        which must correspond to the given representation.\n    copy : bool, optional\n        If `True` (default), a copy of any coordinate data is made.  This\n        argument can only be passed in as a keyword argument.\n    **keyword_args\n        Other keyword arguments as applicable for user-defined coordinate frames.\n        Common options include:\n\n        ra, dec : angle-like, optional\n            RA and Dec for frames where ``ra`` and ``dec`` are keys in the\n            frame's ``representation_component_names``, including `ICRS`,\n            `FK5`, `FK4`, and `FK4NoETerms`.\n        pm_ra_cosdec, pm_dec  : `~astropy.units.Quantity` ['angular speed'], optional\n            Proper motion components, in angle per time units.\n        l, b : angle-like, optional\n            Galactic ``l`` and ``b`` for for frames where ``l`` and ``b`` are\n            keys in the frame's ``representation_component_names``, including\n            the `Galactic` frame.\n        pm_l_cosb, pm_b : `~astropy.units.Quantity` ['angular speed'], optional\n            Proper motion components in the `Galactic` frame, in angle per time\n            units.\n        x, y, z : float or `~astropy.units.Quantity` ['length'], optional\n            Cartesian coordinates values\n        u, v, w : float or `~astropy.units.Quantity` ['length'], optional\n            Cartesian coordinates values for the Galactic frame.\n        radial_velocity : `~astropy.units.Quantity` ['speed'], optional\n            The component of the velocity along the line-of-sight (i.e., the\n            radial direction), in velocity units.\n    \"\"\"\n\n    # Declare that SkyCoord can be used as a Table column by defining the\n    # info property.\n    info = SkyCoordInfo()\n\n    def __init__(self, *args, copy=True, **kwargs):\n        # these are frame attributes set on this SkyCoord but *not* a part of\n        # the frame object this SkyCoord contains\n        self._extra_frameattr_names = set()\n\n        # If all that is passed in is a frame instance that already has data,\n        # we should bypass all of the parsing and logic below. This is here\n        # to make this the fastest way to create a SkyCoord instance. Many of\n        # the classmethods implemented for performance enhancements will use\n        # this as the initialization path\n        if (\n            len(args) == 1\n            and len(kwargs) == 0\n            and isinstance(args[0], (BaseCoordinateFrame, SkyCoord))\n        ):\n            coords = args[0]\n            if isinstance(coords, SkyCoord):\n                self._extra_frameattr_names = coords._extra_frameattr_names\n                self.info = coords.info\n\n                # Copy over any extra frame attributes\n                for attr_name in self._extra_frameattr_names:\n                    # Setting it will also validate it.\n                    setattr(self, attr_name, getattr(coords, attr_name))\n\n                coords = coords.frame\n\n            if not coords.has_data:\n                raise ValueError(\n                    \"Cannot initialize from a coordinate frame \"\n                    \"instance without coordinate data\"\n                )\n\n            if copy:\n                self._sky_coord_frame = coords.copy()\n            else:\n                self._sky_coord_frame = coords\n\n        else:\n            # Get the frame instance without coordinate data but with all frame\n            # attributes set - these could either have been passed in with the\n            # frame as an instance, or passed in as kwargs here\n            frame_cls, frame_kwargs = _get_frame_without_data(args, kwargs)\n\n            # Parse the args and kwargs to assemble a sanitized and validated\n            # kwargs dict for initializing attributes for this object and for\n            # creating the internal self._sky_coord_frame object\n            args = list(args)  # Make it mutable\n            skycoord_kwargs, components, info = _parse_coordinate_data(\n                frame_cls(**frame_kwargs), args, kwargs\n            )\n\n            # In the above two parsing functions, these kwargs were identified\n            # as valid frame attributes for *some* frame, but not the frame that\n            # this SkyCoord will have. We keep these attributes as special\n            # skycoord frame attributes:\n            for attr in skycoord_kwargs:\n                # Setting it will also validate it.\n                setattr(self, attr, skycoord_kwargs[attr])\n\n            if info is not None:\n                self.info = info\n\n            # Finally make the internal coordinate object.\n            frame_kwargs.update(components)\n            self._sky_coord_frame = frame_cls(copy=copy, **frame_kwargs)\n\n            if not self._sky_coord_frame.has_data:\n                raise ValueError(\"Cannot create a SkyCoord without data\")\n\n    @property\n    def frame(self):\n        return self._sky_coord_frame\n\n    @property\n    def representation_type(self):\n        return self.frame.representation_type\n\n    @representation_type.setter\n    def representation_type(self, value):\n        self.frame.representation_type = value\n\n    # TODO: remove these in future\n    @property\n    def representation(self):\n        return self.frame.representation\n\n    @representation.setter\n    def representation(self, value):\n        self.frame.representation = value\n\n    @property\n    def shape(self):\n        return self.frame.shape\n\n    def __eq__(self, value):\n        \"\"\"Equality operator for SkyCoord\n\n        This implements strict equality and requires that the frames are\n        equivalent, extra frame attributes are equivalent, and that the\n        representation data are exactly equal.\n        \"\"\"\n\n        if isinstance(value, BaseCoordinateFrame):\n            if value._data is None:\n                raise ValueError(\"Can only compare SkyCoord to Frame with data\")\n\n            return self.frame == value\n\n        if not isinstance(value, SkyCoord):\n            return NotImplemented\n\n        # Make sure that any extra frame attribute names are equivalent.\n        for attr in self._extra_frameattr_names | value._extra_frameattr_names:\n            if not self.frame._frameattr_equiv(\n                getattr(self, attr), getattr(value, attr)\n            ):\n                raise ValueError(\n                    f\"cannot compare: extra frame attribute '{attr}' is not equivalent\"\n                    \" (perhaps compare the frames directly to avoid this exception)\"\n                )\n\n        return self._sky_coord_frame == value._sky_coord_frame\n\n    def __ne__(self, value):\n        return np.logical_not(self == value)\n\n    def _apply(self, method, *args, **kwargs):\n        \"\"\"Create a new instance, applying a method to the underlying data.\n\n        In typical usage, the method is any of the shape-changing methods for\n        `~numpy.ndarray` (``reshape``, ``swapaxes``, etc.), as well as those\n        picking particular elements (``__getitem__``, ``take``, etc.), which\n        are all defined in `~astropy.utils.shapes.ShapedLikeNDArray`. It will be\n        applied to the underlying arrays in the representation (e.g., ``x``,\n        ``y``, and ``z`` for `~astropy.coordinates.CartesianRepresentation`),\n        as well as to any frame attributes that have a shape, with the results\n        used to create a new instance.\n\n        Internally, it is also used to apply functions to the above parts\n        (in particular, `~numpy.broadcast_to`).\n\n        Parameters\n        ----------\n        method : str or callable\n            If str, it is the name of a method that is applied to the internal\n            ``components``. If callable, the function is applied.\n        *args\n            Any positional arguments for ``method``.\n        **kwargs : dict\n            Any keyword arguments for ``method``.\n        \"\"\"\n\n        def apply_method(value):\n            if isinstance(value, ShapedLikeNDArray):\n                return value._apply(method, *args, **kwargs)\n            else:\n                if callable(method):\n                    return method(value, *args, **kwargs)\n                else:\n                    return getattr(value, method)(*args, **kwargs)\n\n        # create a new but empty instance, and copy over stuff\n        new = super().__new__(self.__class__)\n        new._sky_coord_frame = self._sky_coord_frame._apply(method, *args, **kwargs)\n        new._extra_frameattr_names = self._extra_frameattr_names.copy()\n        for attr in self._extra_frameattr_names:\n            value = getattr(self, attr)\n            if getattr(value, \"shape\", ()):\n                value = apply_method(value)\n            elif method == \"copy\" or method == \"flatten\":\n                # flatten should copy also for a single element array, but\n                # we cannot use it directly for array scalars, since it\n                # always returns a one-dimensional array. So, just copy.\n                value = copy.copy(value)\n            setattr(new, \"_\" + attr, value)\n\n        # Copy other 'info' attr only if it has actually been defined.\n        # See PR #3898 for further explanation and justification, along\n        # with Quantity.__array_finalize__\n        if \"info\" in self.__dict__:\n            new.info = self.info\n\n        return new\n\n    def __setitem__(self, item, value):\n        \"\"\"Implement self[item] = value for SkyCoord\n\n        The right hand ``value`` must be strictly consistent with self:\n        - Identical class\n        - Equivalent frames\n        - Identical representation_types\n        - Identical representation differentials keys\n        - Identical frame attributes\n        - Identical \"extra\" frame attributes (e.g. obstime for an ICRS coord)\n\n        With these caveats the setitem ends up as effectively a setitem on\n        the representation data.\n\n          self.frame.data[item] = value.frame.data\n        \"\"\"\n        if self.__class__ is not value.__class__:\n            raise TypeError(\n                \"can only set from object of same class: \"\n                f\"{self.__class__.__name__} vs. {value.__class__.__name__}\"\n            )\n\n        # Make sure that any extra frame attribute names are equivalent.\n        for attr in self._extra_frameattr_names | value._extra_frameattr_names:\n            if not self.frame._frameattr_equiv(\n                getattr(self, attr), getattr(value, attr)\n            ):\n                raise ValueError(f\"attribute {attr} is not equivalent\")\n\n        # Set the frame values.  This checks frame equivalence and also clears\n        # the cache to ensure that the object is not in an inconsistent state.\n        self._sky_coord_frame[item] = value._sky_coord_frame\n\n    def insert(self, obj, values, axis=0):\n        \"\"\"\n        Insert coordinate values before the given indices in the object and\n        return a new Frame object.\n\n        The values to be inserted must conform to the rules for in-place setting\n        of ``SkyCoord`` objects.\n\n        The API signature matches the ``np.insert`` API, but is more limited.\n        The specification of insert index ``obj`` must be a single integer,\n        and the ``axis`` must be ``0`` for simple insertion before the index.\n\n        Parameters\n        ----------\n        obj : int\n            Integer index before which ``values`` is inserted.\n        values : array-like\n            Value(s) to insert.  If the type of ``values`` is different\n            from that of quantity, ``values`` is converted to the matching type.\n        axis : int, optional\n            Axis along which to insert ``values``.  Default is 0, which is the\n            only allowed value and will insert a row.\n\n        Returns\n        -------\n        out : `~astropy.coordinates.SkyCoord` instance\n            New coordinate object with inserted value(s)\n\n        \"\"\"\n        # Validate inputs: obj arg is integer, axis=0, self is not a scalar, and\n        # input index is in bounds.\n        try:\n            idx0 = operator.index(obj)\n        except TypeError:\n            raise TypeError(\"obj arg must be an integer\")\n\n        if axis != 0:\n            raise ValueError(\"axis must be 0\")\n\n        if not self.shape:\n            raise TypeError(\n                f\"cannot insert into scalar {self.__class__.__name__} object\"\n            )\n\n        if abs(idx0) > len(self):\n            raise IndexError(\n                f\"index {idx0} is out of bounds for axis 0 with size {len(self)}\"\n            )\n\n        # Turn negative index into positive\n        if idx0 < 0:\n            idx0 = len(self) + idx0\n\n        n_values = len(values) if values.shape else 1\n\n        # Finally make the new object with the correct length and set values for the\n        # three sections, before insert, the insert, and after the insert.\n        out = self.__class__.info.new_like(\n            [self], len(self) + n_values, name=self.info.name\n        )\n\n        # Set the output values. This is where validation of `values` takes place to ensure\n        # that it can indeed be inserted.\n        out[:idx0] = self[:idx0]\n        out[idx0 : idx0 + n_values] = values\n        out[idx0 + n_values :] = self[idx0:]\n\n        return out\n\n    def is_transformable_to(self, new_frame):\n        \"\"\"\n        Determines if this coordinate frame can be transformed to another\n        given frame.\n\n        Parameters\n        ----------\n        new_frame : frame class, frame object, or str\n            The proposed frame to transform into.\n\n        Returns\n        -------\n        transformable : bool or str\n            `True` if this can be transformed to ``new_frame``, `False` if\n            not, or the string 'same' if ``new_frame`` is the same system as\n            this object but no transformation is defined.\n\n        Notes\n        -----\n        A return value of 'same' means the transformation will work, but it will\n        just give back a copy of this object.  The intended usage is::\n\n            if coord.is_transformable_to(some_unknown_frame):\n                coord2 = coord.transform_to(some_unknown_frame)\n\n        This will work even if ``some_unknown_frame``  turns out to be the same\n        frame class as ``coord``.  This is intended for cases where the frame\n        is the same regardless of the frame attributes (e.g. ICRS), but be\n        aware that it *might* also indicate that someone forgot to define the\n        transformation between two objects of the same frame class but with\n        different attributes.\n        \"\"\"\n        # TODO! like matplotlib, do string overrides for modified methods\n        new_frame = (\n            _get_frame_class(new_frame) if isinstance(new_frame, str) else new_frame\n        )\n        return self.frame.is_transformable_to(new_frame)\n\n    def transform_to(self, frame, merge_attributes=True):\n        \"\"\"Transform this coordinate to a new frame.\n\n        The precise frame transformed to depends on ``merge_attributes``.\n        If `False`, the destination frame is used exactly as passed in.\n        But this is often not quite what one wants.  E.g., suppose one wants to\n        transform an ICRS coordinate that has an obstime attribute to FK4; in\n        this case, one likely would want to use this information. Thus, the\n        default for ``merge_attributes`` is `True`, in which the precedence is\n        as follows: (1) explicitly set (i.e., non-default) values in the\n        destination frame; (2) explicitly set values in the source; (3) default\n        value in the destination frame.\n\n        Note that in either case, any explicitly set attributes on the source\n        `SkyCoord` that are not part of the destination frame's definition are\n        kept (stored on the resulting `SkyCoord`), and thus one can round-trip\n        (e.g., from FK4 to ICRS to FK4 without losing obstime).\n\n        Parameters\n        ----------\n        frame : str, `BaseCoordinateFrame` class or instance, or `SkyCoord` instance\n            The frame to transform this coordinate into.  If a `SkyCoord`, the\n            underlying frame is extracted, and all other information ignored.\n        merge_attributes : bool, optional\n            Whether the default attributes in the destination frame are allowed\n            to be overridden by explicitly set attributes in the source\n            (see note above; default: `True`).\n\n        Returns\n        -------\n        coord : `SkyCoord`\n            A new object with this coordinate represented in the `frame` frame.\n\n        Raises\n        ------\n        ValueError\n            If there is no possible transformation route.\n\n        \"\"\"\n        from astropy.coordinates.errors import ConvertError\n\n        frame_kwargs = {}\n\n        # Frame name (string) or frame class?  Coerce into an instance.\n        try:\n            frame = _get_frame_class(frame)()\n        except Exception:\n            pass\n\n        if isinstance(frame, SkyCoord):\n            frame = frame.frame  # Change to underlying coord frame instance\n\n        if isinstance(frame, BaseCoordinateFrame):\n            new_frame_cls = frame.__class__\n            # Get frame attributes, allowing defaults to be overridden by\n            # explicitly set attributes of the source if ``merge_attributes``.\n            for attr in frame_transform_graph.frame_attributes:\n                self_val = getattr(self, attr, None)\n                frame_val = getattr(frame, attr, None)\n                if frame_val is not None and not (\n                    merge_attributes and frame.is_frame_attr_default(attr)\n                ):\n                    frame_kwargs[attr] = frame_val\n                elif self_val is not None and not self.is_frame_attr_default(attr):\n                    frame_kwargs[attr] = self_val\n                elif frame_val is not None:\n                    frame_kwargs[attr] = frame_val\n        else:\n            raise ValueError(\n                \"Transform `frame` must be a frame name, class, or instance\"\n            )\n\n        # Get the composite transform to the new frame\n        trans = frame_transform_graph.get_transform(self.frame.__class__, new_frame_cls)\n        if trans is None:\n            raise ConvertError(\n                f\"Cannot transform from {self.frame.__class__} to {new_frame_cls}\"\n            )\n\n        # Make a generic frame which will accept all the frame kwargs that\n        # are provided and allow for transforming through intermediate frames\n        # which may require one or more of those kwargs.\n        generic_frame = GenericFrame(frame_kwargs)\n\n        # Do the transformation, returning a coordinate frame of the desired\n        # final type (not generic).\n        new_coord = trans(self.frame, generic_frame)\n\n        # Finally make the new SkyCoord object from the `new_coord` and\n        # remaining frame_kwargs that are not frame_attributes in `new_coord`.\n        for attr in set(new_coord.frame_attributes) & set(frame_kwargs.keys()):\n            frame_kwargs.pop(attr)\n\n        # Always remove the origin frame attribute, as that attribute only makes\n        # sense with a SkyOffsetFrame (in which case it will be stored on the frame).\n        # See gh-11277.\n        # TODO: Should it be a property of the frame attribute that it can\n        # or cannot be stored on a SkyCoord?\n        frame_kwargs.pop(\"origin\", None)\n\n        return self.__class__(new_coord, **frame_kwargs)\n\n    def apply_space_motion(self, new_obstime=None, dt=None):\n        \"\"\"\n        Compute the position of the source represented by this coordinate object\n        to a new time using the velocities stored in this object and assuming\n        linear space motion (including relativistic corrections). This is\n        sometimes referred to as an \"epoch transformation.\"\n\n        The initial time before the evolution is taken from the ``obstime``\n        attribute of this coordinate.  Note that this method currently does not\n        support evolving coordinates where the *frame* has an ``obstime`` frame\n        attribute, so the ``obstime`` is only used for storing the before and\n        after times, not actually as an attribute of the frame. Alternatively,\n        if ``dt`` is given, an ``obstime`` need not be provided at all.\n\n        Parameters\n        ----------\n        new_obstime : `~astropy.time.Time`, optional\n            The time at which to evolve the position to. Requires that the\n            ``obstime`` attribute be present on this frame.\n        dt : `~astropy.units.Quantity`, `~astropy.time.TimeDelta`, optional\n            An amount of time to evolve the position of the source. Cannot be\n            given at the same time as ``new_obstime``.\n\n        Returns\n        -------\n        new_coord : `SkyCoord`\n            A new coordinate object with the evolved location of this coordinate\n            at the new time.  ``obstime`` will be set on this object to the new\n            time only if ``self`` also has ``obstime``.\n        \"\"\"\n        from .builtin_frames.icrs import ICRS\n\n        if (new_obstime is None) == (dt is None):\n            raise ValueError(\n                \"You must specify one of `new_obstime` or `dt`, but not both.\"\n            )\n\n        # Validate that we have velocity info\n        if \"s\" not in self.frame.data.differentials:\n            raise ValueError(\"SkyCoord requires velocity data to evolve the position.\")\n\n        if \"obstime\" in self.frame.frame_attributes:\n            raise NotImplementedError(\n                \"Updating the coordinates in a frame with explicit time dependence is\"\n                \" currently not supported. If you would like this functionality, please\"\n                \" open an issue on github:\\nhttps://github.com/astropy/astropy\"\n            )\n\n        if new_obstime is not None and self.obstime is None:\n            # If no obstime is already on this object, raise an error if a new\n            # obstime is passed: we need to know the time / epoch at which the\n            # the position / velocity were measured initially\n            raise ValueError(\n                \"This object has no associated `obstime`. apply_space_motion() must\"\n                \" receive a time difference, `dt`, and not a new obstime.\"\n            )\n\n        # Compute t1 and t2, the times used in the starpm call, which *only*\n        # uses them to compute a delta-time\n        t1 = self.obstime\n        if dt is None:\n            # self.obstime is not None and new_obstime is not None b/c of above\n            # checks\n            t2 = new_obstime\n        else:\n            # new_obstime is definitely None b/c of the above checks\n            if t1 is None:\n                # MAGIC NUMBER: if the current SkyCoord object has no obstime,\n                # assume J2000 to do the dt offset. This is not actually used\n                # for anything except a delta-t in starpm, so it's OK that it's\n                # not necessarily the \"real\" obstime\n                t1 = Time(\"J2000\")\n                new_obstime = None  # we don't actually know the initial obstime\n                t2 = t1 + dt\n            else:\n                t2 = t1 + dt\n                new_obstime = t2\n        # starpm wants tdb time\n        t1 = t1.tdb\n        t2 = t2.tdb\n\n        # proper motion in RA should not include the cos(dec) term, see the\n        # erfa function eraStarpv, comment (4).  So we convert to the regular\n        # spherical differentials.\n        icrsrep = self.icrs.represent_as(SphericalRepresentation, SphericalDifferential)\n        icrsvel = icrsrep.differentials[\"s\"]\n\n        parallax_zero = False\n        try:\n            plx = icrsrep.distance.to_value(u.arcsecond, u.parallax())\n        except u.UnitConversionError:  # No distance: set to 0 by convention\n            plx = 0.0\n            parallax_zero = True\n\n        try:\n            rv = icrsvel.d_distance.to_value(u.km / u.s)\n        except u.UnitConversionError:  # No RV\n            rv = 0.0\n\n        starpm = erfa.pmsafe(\n            icrsrep.lon.radian,\n            icrsrep.lat.radian,\n            icrsvel.d_lon.to_value(u.radian / u.yr),\n            icrsvel.d_lat.to_value(u.radian / u.yr),\n            plx,\n            rv,\n            t1.jd1,\n            t1.jd2,\n            t2.jd1,\n            t2.jd2,\n        )\n\n        if parallax_zero:\n            new_distance = None\n        else:\n            new_distance = Distance(parallax=starpm[4] << u.arcsec)\n\n        icrs2 = ICRS(\n            ra=u.Quantity(starpm[0], u.radian, copy=False),\n            dec=u.Quantity(starpm[1], u.radian, copy=False),\n            pm_ra=u.Quantity(starpm[2], u.radian / u.yr, copy=False),\n            pm_dec=u.Quantity(starpm[3], u.radian / u.yr, copy=False),\n            distance=new_distance,\n            radial_velocity=u.Quantity(starpm[5], u.km / u.s, copy=False),\n            differential_type=SphericalDifferential,\n        )\n\n        # Update the obstime of the returned SkyCoord, and need to carry along\n        # the frame attributes\n        frattrs = {\n            attrnm: getattr(self, attrnm) for attrnm in self._extra_frameattr_names\n        }\n        frattrs[\"obstime\"] = new_obstime\n        result = self.__class__(icrs2, **frattrs).transform_to(self.frame)\n\n        # Without this the output might not have the right differential type.\n        # Not sure if this fixes the problem or just hides it.  See #11932\n        result.differential_type = self.differential_type\n\n        return result\n\n    def _is_name(self, string):\n        \"\"\"\n        Returns whether a string is one of the aliases for the frame.\n        \"\"\"\n        return self.frame.name == string or (\n            isinstance(self.frame.name, list) and string in self.frame.name\n        )\n\n    def __getattr__(self, attr):\n        \"\"\"\n        Overrides getattr to return coordinates that this can be transformed\n        to, based on the alias attr in the primary transform graph.\n        \"\"\"\n        if \"_sky_coord_frame\" in self.__dict__:\n            if self._is_name(attr):\n                return self  # Should this be a deepcopy of self?\n\n            # Anything in the set of all possible frame_attr_names is handled\n            # here. If the attr is relevant for the current frame then delegate\n            # to self.frame otherwise get it from self._<attr>.\n            if attr in frame_transform_graph.frame_attributes:\n                if attr in self.frame.frame_attributes:\n                    return getattr(self.frame, attr)\n                else:\n                    return getattr(self, \"_\" + attr, None)\n\n            # Some attributes might not fall in the above category but still\n            # are available through self._sky_coord_frame.\n            if not attr.startswith(\"_\") and hasattr(self._sky_coord_frame, attr):\n                return getattr(self._sky_coord_frame, attr)\n\n            # Try to interpret as a new frame for transforming.\n            frame_cls = frame_transform_graph.lookup_name(attr)\n            if frame_cls is not None and self.frame.is_transformable_to(frame_cls):\n                return self.transform_to(attr)\n\n        # Fail\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{attr}'\"\n        )\n\n    def __setattr__(self, attr, val):\n        # This is to make anything available through __getattr__ immutable\n        if \"_sky_coord_frame\" in self.__dict__:\n            if self._is_name(attr):\n                raise AttributeError(f\"'{attr}' is immutable\")\n\n            if not attr.startswith(\"_\") and hasattr(self._sky_coord_frame, attr):\n                setattr(self._sky_coord_frame, attr, val)\n                return\n\n            frame_cls = frame_transform_graph.lookup_name(attr)\n            if frame_cls is not None and self.frame.is_transformable_to(frame_cls):\n                raise AttributeError(f\"'{attr}' is immutable\")\n\n        if attr in frame_transform_graph.frame_attributes:\n            # All possible frame attributes can be set, but only via a private\n            # variable.  See __getattr__ above.\n            super().__setattr__(\"_\" + attr, val)\n            # Validate it\n            frame_transform_graph.frame_attributes[attr].__get__(self)\n            # And add to set of extra attributes\n            self._extra_frameattr_names |= {attr}\n\n        else:\n            # Otherwise, do the standard Python attribute setting\n            super().__setattr__(attr, val)\n\n    def __delattr__(self, attr):\n        # mirror __setattr__ above\n        if \"_sky_coord_frame\" in self.__dict__:\n            if self._is_name(attr):\n                raise AttributeError(f\"'{attr}' is immutable\")\n\n            if not attr.startswith(\"_\") and hasattr(self._sky_coord_frame, attr):\n                delattr(self._sky_coord_frame, attr)\n                return\n\n            frame_cls = frame_transform_graph.lookup_name(attr)\n            if frame_cls is not None and self.frame.is_transformable_to(frame_cls):\n                raise AttributeError(f\"'{attr}' is immutable\")\n\n        if attr in frame_transform_graph.frame_attributes:\n            # All possible frame attributes can be deleted, but need to remove\n            # the corresponding private variable.  See __getattr__ above.\n            super().__delattr__(\"_\" + attr)\n            # Also remove it from the set of extra attributes\n            self._extra_frameattr_names -= {attr}\n\n        else:\n            # Otherwise, do the standard Python attribute setting\n            super().__delattr__(attr)\n\n    def __dir__(self):\n        \"\"\"\n        Override the builtin `dir` behavior to include:\n        - Transforms available by aliases\n        - Attribute / methods of the underlying self.frame object\n        \"\"\"\n        dir_values = set(super().__dir__())\n\n        # determine the aliases that this can be transformed to.\n        for name in frame_transform_graph.get_names():\n            frame_cls = frame_transform_graph.lookup_name(name)\n            if self.frame.is_transformable_to(frame_cls):\n                dir_values.add(name)\n\n        # Add public attributes of self.frame\n        dir_values.update(\n            {attr for attr in dir(self.frame) if not attr.startswith(\"_\")}\n        )\n\n        # Add all possible frame attributes\n        dir_values.update(frame_transform_graph.frame_attributes.keys())\n\n        return sorted(dir_values)\n\n    def __repr__(self):\n        clsnm = self.__class__.__name__\n        coonm = self.frame.__class__.__name__\n        frameattrs = self.frame._frame_attrs_repr()\n        if frameattrs:\n            frameattrs = \": \" + frameattrs\n\n        data = self.frame._data_repr()\n        if data:\n            data = \": \" + data\n\n        return f\"<{clsnm} ({coonm}{frameattrs}){data}>\"\n\n    def to_string(self, style=\"decimal\", **kwargs):\n        \"\"\"\n        A string representation of the coordinates.\n\n        The default styles definitions are::\n\n          'decimal': 'lat': {'decimal': True, 'unit': \"deg\"}\n                     'lon': {'decimal': True, 'unit': \"deg\"}\n          'dms': 'lat': {'unit': \"deg\"}\n                 'lon': {'unit': \"deg\"}\n          'hmsdms': 'lat': {'alwayssign': True, 'pad': True, 'unit': \"deg\"}\n                    'lon': {'pad': True, 'unit': \"hour\"}\n\n        See :meth:`~astropy.coordinates.Angle.to_string` for details and\n        keyword arguments (the two angles forming the coordinates are are\n        both :class:`~astropy.coordinates.Angle` instances). Keyword\n        arguments have precedence over the style defaults and are passed\n        to :meth:`~astropy.coordinates.Angle.to_string`.\n\n        Parameters\n        ----------\n        style : {'hmsdms', 'dms', 'decimal'}\n            The formatting specification to use. These encode the three most\n            common ways to represent coordinates. The default is `decimal`.\n        **kwargs\n            Keyword args passed to :meth:`~astropy.coordinates.Angle.to_string`.\n        \"\"\"\n\n        sph_coord = self.frame.represent_as(SphericalRepresentation)\n\n        styles = {\n            \"hmsdms\": {\n                \"lonargs\": {\"unit\": u.hour, \"pad\": True},\n                \"latargs\": {\"unit\": u.degree, \"pad\": True, \"alwayssign\": True},\n            },\n            \"dms\": {\"lonargs\": {\"unit\": u.degree}, \"latargs\": {\"unit\": u.degree}},\n            \"decimal\": {\n                \"lonargs\": {\"unit\": u.degree, \"decimal\": True},\n                \"latargs\": {\"unit\": u.degree, \"decimal\": True},\n            },\n        }\n\n        lonargs = {}\n        latargs = {}\n\n        if style in styles:\n            lonargs.update(styles[style][\"lonargs\"])\n            latargs.update(styles[style][\"latargs\"])\n        else:\n            raise ValueError(f\"Invalid style.  Valid options are: {','.join(styles)}\")\n\n        lonargs.update(kwargs)\n        latargs.update(kwargs)\n\n        if np.isscalar(sph_coord.lon.value):\n            coord_string = (\n                f\"{sph_coord.lon.to_string(**lonargs)}\"\n                f\" {sph_coord.lat.to_string(**latargs)}\"\n            )\n        else:\n            coord_string = []\n            for lonangle, latangle in zip(sph_coord.lon.ravel(), sph_coord.lat.ravel()):\n                coord_string += [\n                    f\"{lonangle.to_string(**lonargs)} {latangle.to_string(**latargs)}\"\n                ]\n            if len(sph_coord.shape) > 1:\n                coord_string = np.array(coord_string).reshape(sph_coord.shape)\n\n        return coord_string\n\n    def to_table(self):\n        \"\"\"\n        Convert this |SkyCoord| to a |QTable|.\n\n        Any attributes that have the same length as the |SkyCoord| will be\n        converted to columns of the |QTable|. All other attributes will be\n        recorded as metadata.\n\n        Returns\n        -------\n        `~astropy.table.QTable`\n            A |QTable| containing the data of this |SkyCoord|.\n\n        Examples\n        --------\n        >>> sc = SkyCoord(ra=[40, 70]*u.deg, dec=[0, -20]*u.deg,\n        ...               obstime=Time([2000, 2010], format='jyear'))\n        >>> t =  sc.to_table()\n        >>> t\n        <QTable length=2>\n           ra     dec   obstime\n          deg     deg\n        float64 float64   Time\n        ------- ------- -------\n           40.0     0.0  2000.0\n           70.0   -20.0  2010.0\n        >>> t.meta\n        {'representation_type': 'spherical', 'frame': 'icrs'}\n        \"\"\"\n        self_as_dict = self.info._represent_as_dict()\n        tabledata = {}\n        metadata = {}\n        # Record attributes that have the same length as self as columns in the\n        # table, and the other attributes as table metadata.  This matches\n        # table.serialize._represent_mixin_as_column().\n        for key, value in self_as_dict.items():\n            if getattr(value, \"shape\", ())[:1] == (len(self),):\n                tabledata[key] = value\n            else:\n                metadata[key] = value\n        return QTable(tabledata, meta=metadata)\n\n    def is_equivalent_frame(self, other):\n        \"\"\"\n        Checks if this object's frame as the same as that of the ``other``\n        object.\n\n        To be the same frame, two objects must be the same frame class and have\n        the same frame attributes. For two `SkyCoord` objects, *all* of the\n        frame attributes have to match, not just those relevant for the object's\n        frame.\n\n        Parameters\n        ----------\n        other : SkyCoord or BaseCoordinateFrame\n            The other object to check.\n\n        Returns\n        -------\n        isequiv : bool\n            True if the frames are the same, False if not.\n\n        Raises\n        ------\n        TypeError\n            If ``other`` isn't a `SkyCoord` or a `BaseCoordinateFrame` or subclass.\n        \"\"\"\n        if isinstance(other, BaseCoordinateFrame):\n            return self.frame.is_equivalent_frame(other)\n        elif isinstance(other, SkyCoord):\n            if other.frame.name != self.frame.name:\n                return False\n\n            for fattrnm in frame_transform_graph.frame_attributes:\n                if not BaseCoordinateFrame._frameattr_equiv(\n                    getattr(self, fattrnm), getattr(other, fattrnm)\n                ):\n                    return False\n            return True\n        else:\n            # not a BaseCoordinateFrame nor a SkyCoord object\n            raise TypeError(\n                \"Tried to do is_equivalent_frame on something that isn't frame-like\"\n            )\n\n    # High-level convenience methods\n    def separation(self, other):\n        \"\"\"\n        Computes on-sky separation between this coordinate and another.\n\n        .. note::\n\n            If the ``other`` coordinate object is in a different frame, it is\n            first transformed to the frame of this object. This can lead to\n            unintuitive behavior if not accounted for. Particularly of note is\n            that ``self.separation(other)`` and ``other.separation(self)`` may\n            not give the same answer in this case.\n\n        For more on how to use this (and related) functionality, see the\n        examples in :doc:`astropy:/coordinates/matchsep`.\n\n        Parameters\n        ----------\n        other : `~astropy.coordinates.SkyCoord` or `~astropy.coordinates.BaseCoordinateFrame`\n            The coordinate to get the separation to.\n\n        Returns\n        -------\n        sep : `~astropy.coordinates.Angle`\n            The on-sky separation between this and the ``other`` coordinate.\n\n        Notes\n        -----\n        The separation is calculated using the Vincenty formula, which\n        is stable at all locations, including poles and antipodes [1]_.\n\n        .. [1] https://en.wikipedia.org/wiki/Great-circle_distance\n\n        \"\"\"\n        from . import Angle\n        from .angle_utilities import angular_separation\n\n        if not self.is_equivalent_frame(other):\n            try:\n                kwargs = (\n                    {\"merge_attributes\": False} if isinstance(other, SkyCoord) else {}\n                )\n                other = other.transform_to(self, **kwargs)\n            except TypeError:\n                raise TypeError(\n                    \"Can only get separation to another SkyCoord \"\n                    \"or a coordinate frame with data\"\n                )\n\n        lon1 = self.spherical.lon\n        lat1 = self.spherical.lat\n        lon2 = other.spherical.lon\n        lat2 = other.spherical.lat\n\n        # Get the separation as a Quantity, convert to Angle in degrees\n        sep = angular_separation(lon1, lat1, lon2, lat2)\n        return Angle(sep, unit=u.degree)\n\n    def separation_3d(self, other):\n        \"\"\"\n        Computes three dimensional separation between this coordinate\n        and another.\n\n        For more on how to use this (and related) functionality, see the\n        examples in :doc:`astropy:/coordinates/matchsep`.\n\n        Parameters\n        ----------\n        other : `~astropy.coordinates.SkyCoord` or `~astropy.coordinates.BaseCoordinateFrame`\n            The coordinate to get the separation to.\n\n        Returns\n        -------\n        sep : `~astropy.coordinates.Distance`\n            The real-space distance between these two coordinates.\n\n        Raises\n        ------\n        ValueError\n            If this or the other coordinate do not have distances.\n        \"\"\"\n        if not self.is_equivalent_frame(other):\n            try:\n                kwargs = (\n                    {\"merge_attributes\": False} if isinstance(other, SkyCoord) else {}\n                )\n                other = other.transform_to(self, **kwargs)\n            except TypeError:\n                raise TypeError(\n                    \"Can only get separation to another SkyCoord \"\n                    \"or a coordinate frame with data\"\n                )\n\n        if issubclass(self.data.__class__, UnitSphericalRepresentation):\n            raise ValueError(\n                \"This object does not have a distance; cannot compute 3d separation.\"\n            )\n        if issubclass(other.data.__class__, UnitSphericalRepresentation):\n            raise ValueError(\n                \"The other object does not have a distance; \"\n                \"cannot compute 3d separation.\"\n            )\n\n        c1 = self.cartesian.without_differentials()\n        c2 = other.cartesian.without_differentials()\n        return Distance((c1 - c2).norm())\n\n    def spherical_offsets_to(self, tocoord):\n        r\"\"\"\n        Computes angular offsets to go *from* this coordinate *to* another.\n\n        Parameters\n        ----------\n        tocoord : `~astropy.coordinates.BaseCoordinateFrame`\n            The coordinate to find the offset to.\n\n        Returns\n        -------\n        lon_offset : `~astropy.coordinates.Angle`\n            The angular offset in the longitude direction. The definition of\n            \"longitude\" depends on this coordinate's frame (e.g., RA for\n            equatorial coordinates).\n        lat_offset : `~astropy.coordinates.Angle`\n            The angular offset in the latitude direction. The definition of\n            \"latitude\" depends on this coordinate's frame (e.g., Dec for\n            equatorial coordinates).\n\n        Raises\n        ------\n        ValueError\n            If the ``tocoord`` is not in the same frame as this one. This is\n            different from the behavior of the `separation`/`separation_3d`\n            methods because the offset components depend critically on the\n            specific choice of frame.\n\n        Notes\n        -----\n        This uses the sky offset frame machinery, and hence will produce a new\n        sky offset frame if one does not already exist for this object's frame\n        class.\n\n        See Also\n        --------\n        separation :\n            for the *total* angular offset (not broken out into components).\n        position_angle :\n            for the direction of the offset.\n\n        \"\"\"\n        if not self.is_equivalent_frame(tocoord):\n            raise ValueError(\n                \"Tried to use spherical_offsets_to with two non-matching frames!\"\n            )\n\n        aframe = self.skyoffset_frame()\n        acoord = tocoord.transform_to(aframe)\n\n        dlon = acoord.spherical.lon.view(Angle)\n        dlat = acoord.spherical.lat.view(Angle)\n        return dlon, dlat\n\n    def spherical_offsets_by(self, d_lon, d_lat):\n        \"\"\"\n        Computes the coordinate that is a specified pair of angular offsets away\n        from this coordinate.\n\n        Parameters\n        ----------\n        d_lon : angle-like\n            The angular offset in the longitude direction. The definition of\n            \"longitude\" depends on this coordinate's frame (e.g., RA for\n            equatorial coordinates).\n        d_lat : angle-like\n            The angular offset in the latitude direction. The definition of\n            \"latitude\" depends on this coordinate's frame (e.g., Dec for\n            equatorial coordinates).\n\n        Returns\n        -------\n        newcoord : `~astropy.coordinates.SkyCoord`\n            The coordinates for the location that corresponds to offsetting by\n            ``d_lat`` in the latitude direction and ``d_lon`` in the longitude\n            direction.\n\n        Notes\n        -----\n        This internally uses `~astropy.coordinates.SkyOffsetFrame` to do the\n        transformation. For a more complete set of transform offsets, use\n        `~astropy.coordinates.SkyOffsetFrame` or `~astropy.wcs.WCS` manually.\n        This specific method can be reproduced by doing\n        ``SkyCoord(SkyOffsetFrame(d_lon, d_lat, origin=self.frame).transform_to(self))``.\n\n        See Also\n        --------\n        spherical_offsets_to : compute the angular offsets to another coordinate\n        directional_offset_by : offset a coordinate by an angle in a direction\n        \"\"\"\n        from .builtin_frames.skyoffset import SkyOffsetFrame\n\n        return self.__class__(\n            SkyOffsetFrame(d_lon, d_lat, origin=self.frame).transform_to(self)\n        )\n\n    def directional_offset_by(self, position_angle, separation):\n        \"\"\"\n        Computes coordinates at the given offset from this coordinate.\n\n        Parameters\n        ----------\n        position_angle : `~astropy.coordinates.Angle`\n            position_angle of offset\n        separation : `~astropy.coordinates.Angle`\n            offset angular separation\n\n        Returns\n        -------\n        newpoints : `~astropy.coordinates.SkyCoord`\n            The coordinates for the location that corresponds to offsetting by\n            the given `position_angle` and `separation`.\n\n        Notes\n        -----\n        Returned SkyCoord frame retains only the frame attributes that are for\n        the resulting frame type.  (e.g. if the input frame is\n        `~astropy.coordinates.ICRS`, an ``equinox`` value will be retained, but\n        an ``obstime`` will not.)\n\n        For a more complete set of transform offsets, use `~astropy.wcs.WCS`.\n        `~astropy.coordinates.SkyCoord.skyoffset_frame()` can also be used to\n        create a spherical frame with (lat=0, lon=0) at a reference point,\n        approximating an xy cartesian system for small offsets. This method\n        is distinct in that it is accurate on the sphere.\n\n        See Also\n        --------\n        position_angle : inverse operation for the ``position_angle`` component\n        separation : inverse operation for the ``separation`` component\n\n        \"\"\"\n        from . import angle_utilities\n\n        slat = self.represent_as(UnitSphericalRepresentation).lat\n        slon = self.represent_as(UnitSphericalRepresentation).lon\n\n        newlon, newlat = angle_utilities.offset_by(\n            lon=slon, lat=slat, posang=position_angle, distance=separation\n        )\n\n        return SkyCoord(newlon, newlat, frame=self.frame)\n\n    def match_to_catalog_sky(self, catalogcoord, nthneighbor=1):\n        \"\"\"\n        Finds the nearest on-sky matches of this coordinate in a set of\n        catalog coordinates.\n\n        For more on how to use this (and related) functionality, see the\n        examples in :doc:`astropy:/coordinates/matchsep`.\n\n        Parameters\n        ----------\n        catalogcoord : `~astropy.coordinates.SkyCoord` or `~astropy.coordinates.BaseCoordinateFrame`\n            The base catalog in which to search for matches. Typically this\n            will be a coordinate object that is an array (i.e.,\n            ``catalogcoord.isscalar == False``)\n        nthneighbor : int, optional\n            Which closest neighbor to search for.  Typically ``1`` is\n            desired here, as that is correct for matching one set of\n            coordinates to another. The next likely use case is ``2``,\n            for matching a coordinate catalog against *itself* (``1``\n            is inappropriate because each point will find itself as the\n            closest match).\n\n        Returns\n        -------\n        idx : int array\n            Indices into ``catalogcoord`` to get the matched points for\n            each of this object's coordinates. Shape matches this\n            object.\n        sep2d : `~astropy.coordinates.Angle`\n            The on-sky separation between the closest match for each\n            element in this object in ``catalogcoord``. Shape matches\n            this object.\n        dist3d : `~astropy.units.Quantity` ['length']\n            The 3D distance between the closest match for each element\n            in this object in ``catalogcoord``. Shape matches this\n            object. Unless both this and ``catalogcoord`` have associated\n            distances, this quantity assumes that all sources are at a\n            distance of 1 (dimensionless).\n\n        Notes\n        -----\n        This method requires `SciPy <https://www.scipy.org/>`_ to be\n        installed or it will fail.\n\n        See Also\n        --------\n        astropy.coordinates.match_coordinates_sky\n        SkyCoord.match_to_catalog_3d\n        \"\"\"\n        from .matching import match_coordinates_sky\n\n        if not (\n            isinstance(catalogcoord, (SkyCoord, BaseCoordinateFrame))\n            and catalogcoord.has_data\n        ):\n            raise TypeError(\n                \"Can only get separation to another SkyCoord or a \"\n                \"coordinate frame with data\"\n            )\n\n        res = match_coordinates_sky(\n            self, catalogcoord, nthneighbor=nthneighbor, storekdtree=\"_kdtree_sky\"\n        )\n        return res\n\n    def match_to_catalog_3d(self, catalogcoord, nthneighbor=1):\n        \"\"\"\n        Finds the nearest 3-dimensional matches of this coordinate to a set\n        of catalog coordinates.\n\n        This finds the 3-dimensional closest neighbor, which is only different\n        from the on-sky distance if ``distance`` is set in this object or the\n        ``catalogcoord`` object.\n\n        For more on how to use this (and related) functionality, see the\n        examples in :doc:`astropy:/coordinates/matchsep`.\n\n        Parameters\n        ----------\n        catalogcoord : `~astropy.coordinates.SkyCoord` or `~astropy.coordinates.BaseCoordinateFrame`\n            The base catalog in which to search for matches. Typically this\n            will be a coordinate object that is an array (i.e.,\n            ``catalogcoord.isscalar == False``)\n        nthneighbor : int, optional\n            Which closest neighbor to search for.  Typically ``1`` is\n            desired here, as that is correct for matching one set of\n            coordinates to another.  The next likely use case is\n            ``2``, for matching a coordinate catalog against *itself*\n            (``1`` is inappropriate because each point will find\n            itself as the closest match).\n\n        Returns\n        -------\n        idx : int array\n            Indices into ``catalogcoord`` to get the matched points for\n            each of this object's coordinates. Shape matches this\n            object.\n        sep2d : `~astropy.coordinates.Angle`\n            The on-sky separation between the closest match for each\n            element in this object in ``catalogcoord``. Shape matches\n            this object.\n        dist3d : `~astropy.units.Quantity` ['length']\n            The 3D distance between the closest match for each element\n            in this object in ``catalogcoord``. Shape matches this\n            object.\n\n        Notes\n        -----\n        This method requires `SciPy <https://www.scipy.org/>`_ to be\n        installed or it will fail.\n\n        See Also\n        --------\n        astropy.coordinates.match_coordinates_3d\n        SkyCoord.match_to_catalog_sky\n        \"\"\"\n        from .matching import match_coordinates_3d\n\n        if not (\n            isinstance(catalogcoord, (SkyCoord, BaseCoordinateFrame))\n            and catalogcoord.has_data\n        ):\n            raise TypeError(\n                \"Can only get separation to another SkyCoord or a \"\n                \"coordinate frame with data\"\n            )\n\n        res = match_coordinates_3d(\n            self, catalogcoord, nthneighbor=nthneighbor, storekdtree=\"_kdtree_3d\"\n        )\n\n        return res\n\n    def search_around_sky(self, searcharoundcoords, seplimit):\n        \"\"\"\n        Searches for all coordinates in this object around a supplied set of\n        points within a given on-sky separation.\n\n        This is intended for use on `~astropy.coordinates.SkyCoord` objects\n        with coordinate arrays, rather than a scalar coordinate.  For a scalar\n        coordinate, it is better to use\n        `~astropy.coordinates.SkyCoord.separation`.\n\n        For more on how to use this (and related) functionality, see the\n        examples in :doc:`astropy:/coordinates/matchsep`.\n\n        Parameters\n        ----------\n        searcharoundcoords : coordinate-like\n            The coordinates to search around to try to find matching points in\n            this `SkyCoord`. This should be an object with array coordinates,\n            not a scalar coordinate object.\n        seplimit : `~astropy.units.Quantity` ['angle']\n            The on-sky separation to search within.\n\n        Returns\n        -------\n        idxsearcharound : int array\n            Indices into ``searcharoundcoords`` that match the\n            corresponding elements of ``idxself``. Shape matches\n            ``idxself``.\n        idxself : int array\n            Indices into ``self`` that match the\n            corresponding elements of ``idxsearcharound``. Shape matches\n            ``idxsearcharound``.\n        sep2d : `~astropy.coordinates.Angle`\n            The on-sky separation between the coordinates. Shape matches\n            ``idxsearcharound`` and ``idxself``.\n        dist3d : `~astropy.units.Quantity` ['length']\n            The 3D distance between the coordinates. Shape matches\n            ``idxsearcharound`` and ``idxself``.\n\n        Notes\n        -----\n        This method requires `SciPy <https://www.scipy.org/>`_ to be\n        installed or it will fail.\n\n        In the current implementation, the return values are always sorted in\n        the same order as the ``searcharoundcoords`` (so ``idxsearcharound`` is\n        in ascending order).  This is considered an implementation detail,\n        though, so it could change in a future release.\n\n        See Also\n        --------\n        astropy.coordinates.search_around_sky\n        SkyCoord.search_around_3d\n        \"\"\"\n        from .matching import search_around_sky\n\n        return search_around_sky(\n            searcharoundcoords, self, seplimit, storekdtree=\"_kdtree_sky\"\n        )\n\n    def search_around_3d(self, searcharoundcoords, distlimit):\n        \"\"\"\n        Searches for all coordinates in this object around a supplied set of\n        points within a given 3D radius.\n\n        This is intended for use on `~astropy.coordinates.SkyCoord` objects\n        with coordinate arrays, rather than a scalar coordinate.  For a scalar\n        coordinate, it is better to use\n        `~astropy.coordinates.SkyCoord.separation_3d`.\n\n        For more on how to use this (and related) functionality, see the\n        examples in :doc:`astropy:/coordinates/matchsep`.\n\n        Parameters\n        ----------\n        searcharoundcoords : `~astropy.coordinates.SkyCoord` or `~astropy.coordinates.BaseCoordinateFrame`\n            The coordinates to search around to try to find matching points in\n            this `SkyCoord`. This should be an object with array coordinates,\n            not a scalar coordinate object.\n        distlimit : `~astropy.units.Quantity` ['length']\n            The physical radius to search within.\n\n        Returns\n        -------\n        idxsearcharound : int array\n            Indices into ``searcharoundcoords`` that match the\n            corresponding elements of ``idxself``. Shape matches\n            ``idxself``.\n        idxself : int array\n            Indices into ``self`` that match the\n            corresponding elements of ``idxsearcharound``. Shape matches\n            ``idxsearcharound``.\n        sep2d : `~astropy.coordinates.Angle`\n            The on-sky separation between the coordinates. Shape matches\n            ``idxsearcharound`` and ``idxself``.\n        dist3d : `~astropy.units.Quantity` ['length']\n            The 3D distance between the coordinates. Shape matches\n            ``idxsearcharound`` and ``idxself``.\n\n        Notes\n        -----\n        This method requires `SciPy <https://www.scipy.org/>`_ to be\n        installed or it will fail.\n\n        In the current implementation, the return values are always sorted in\n        the same order as the ``searcharoundcoords`` (so ``idxsearcharound`` is\n        in ascending order).  This is considered an implementation detail,\n        though, so it could change in a future release.\n\n        See Also\n        --------\n        astropy.coordinates.search_around_3d\n        SkyCoord.search_around_sky\n        \"\"\"\n        from .matching import search_around_3d\n\n        return search_around_3d(\n            searcharoundcoords, self, distlimit, storekdtree=\"_kdtree_3d\"\n        )\n\n    def position_angle(self, other):\n        \"\"\"\n        Computes the on-sky position angle (East of North) between this\n        `SkyCoord` and another.\n\n        Parameters\n        ----------\n        other : `SkyCoord`\n            The other coordinate to compute the position angle to.  It is\n            treated as the \"head\" of the vector of the position angle.\n\n        Returns\n        -------\n        pa : `~astropy.coordinates.Angle`\n            The (positive) position angle of the vector pointing from ``self``\n            to ``other``.  If either ``self`` or ``other`` contain arrays, this\n            will be an array following the appropriate `numpy` broadcasting\n            rules.\n\n        Examples\n        --------\n        >>> c1 = SkyCoord(0*u.deg, 0*u.deg)\n        >>> c2 = SkyCoord(1*u.deg, 0*u.deg)\n        >>> c1.position_angle(c2).degree\n        90.0\n        >>> c3 = SkyCoord(1*u.deg, 1*u.deg)\n        >>> c1.position_angle(c3).degree  # doctest: +FLOAT_CMP\n        44.995636455344844\n        \"\"\"\n        from . import angle_utilities\n\n        if not self.is_equivalent_frame(other):\n            try:\n                other = other.transform_to(self, merge_attributes=False)\n            except TypeError:\n                raise TypeError(\n                    \"Can only get position_angle to another \"\n                    \"SkyCoord or a coordinate frame with data\"\n                )\n\n        slat = self.represent_as(UnitSphericalRepresentation).lat\n        slon = self.represent_as(UnitSphericalRepresentation).lon\n        olat = other.represent_as(UnitSphericalRepresentation).lat\n        olon = other.represent_as(UnitSphericalRepresentation).lon\n\n        return angle_utilities.position_angle(slon, slat, olon, olat)\n\n    def skyoffset_frame(self, rotation=None):\n        \"\"\"\n        Returns the sky offset frame with this `SkyCoord` at the origin.\n\n        Returns\n        -------\n        astrframe : `~astropy.coordinates.SkyOffsetFrame`\n            A sky offset frame of the same type as this `SkyCoord` (e.g., if\n            this object has an ICRS coordinate, the resulting frame is\n            SkyOffsetICRS, with the origin set to this object)\n        rotation : angle-like\n            The final rotation of the frame about the ``origin``. The sign of\n            the rotation is the left-hand rule. That is, an object at a\n            particular position angle in the un-rotated system will be sent to\n            the positive latitude (z) direction in the final frame.\n        \"\"\"\n        from .builtin_frames.skyoffset import SkyOffsetFrame\n\n        return SkyOffsetFrame(origin=self, rotation=rotation)\n\n    def get_constellation(self, short_name=False, constellation_list=\"iau\"):\n        \"\"\"\n        Determines the constellation(s) of the coordinates this `SkyCoord`\n        contains.\n\n        Parameters\n        ----------\n        short_name : bool\n            If True, the returned names are the IAU-sanctioned abbreviated\n            names.  Otherwise, full names for the constellations are used.\n        constellation_list : str\n            The set of constellations to use.  Currently only ``'iau'`` is\n            supported, meaning the 88 \"modern\" constellations endorsed by the IAU.\n\n        Returns\n        -------\n        constellation : str or string array\n            If this is a scalar coordinate, returns the name of the\n            constellation.  If it is an array `SkyCoord`, it returns an array of\n            names.\n\n        Notes\n        -----\n        To determine which constellation a point on the sky is in, this first\n        precesses to B1875, and then uses the Delporte boundaries of the 88\n        modern constellations, as tabulated by\n        `Roman 1987 <http://cdsarc.u-strasbg.fr/viz-bin/Cat?VI/42>`_.\n\n        See Also\n        --------\n        astropy.coordinates.get_constellation\n        \"\"\"\n        from .funcs import get_constellation\n\n        # because of issue #7028, the conversion to a PrecessedGeocentric\n        # system fails in some cases.  Work around is to  drop the velocities.\n        # they are not needed here since only position information is used\n        extra_frameattrs = {nm: getattr(self, nm) for nm in self._extra_frameattr_names}\n        novel = SkyCoord(\n            self.realize_frame(self.data.without_differentials()), **extra_frameattrs\n        )\n        return get_constellation(novel, short_name, constellation_list)\n\n        # the simpler version below can be used when gh-issue #7028 is resolved\n        # return get_constellation(self, short_name, constellation_list)\n\n    # WCS pixel to/from sky conversions\n    def to_pixel(self, wcs, origin=0, mode=\"all\"):\n        \"\"\"\n        Convert this coordinate to pixel coordinates using a `~astropy.wcs.WCS`\n        object.\n\n        Parameters\n        ----------\n        wcs : `~astropy.wcs.WCS`\n            The WCS to use for convert\n        origin : int\n            Whether to return 0 or 1-based pixel coordinates.\n        mode : 'all' or 'wcs'\n            Whether to do the transformation including distortions (``'all'``) or\n            only including only the core WCS transformation (``'wcs'``).\n\n        Returns\n        -------\n        xp, yp : `numpy.ndarray`\n            The pixel coordinates\n\n        See Also\n        --------\n        astropy.wcs.utils.skycoord_to_pixel : the implementation of this method\n        \"\"\"\n        from astropy.wcs.utils import skycoord_to_pixel\n\n        return skycoord_to_pixel(self, wcs=wcs, origin=origin, mode=mode)\n\n    @classmethod\n    def from_pixel(cls, xp, yp, wcs, origin=0, mode=\"all\"):\n        \"\"\"\n        Create a new `SkyCoord` from pixel coordinates using an\n        `~astropy.wcs.WCS` object.\n\n        Parameters\n        ----------\n        xp, yp : float or ndarray\n            The coordinates to convert.\n        wcs : `~astropy.wcs.WCS`\n            The WCS to use for convert\n        origin : int\n            Whether to return 0 or 1-based pixel coordinates.\n        mode : 'all' or 'wcs'\n            Whether to do the transformation including distortions (``'all'``) or\n            only including only the core WCS transformation (``'wcs'``).\n\n        Returns\n        -------\n        coord : `~astropy.coordinates.SkyCoord`\n            A new object with sky coordinates corresponding to the input ``xp``\n            and ``yp``.\n\n        See Also\n        --------\n        to_pixel : to do the inverse operation\n        astropy.wcs.utils.pixel_to_skycoord : the implementation of this method\n        \"\"\"\n        from astropy.wcs.utils import pixel_to_skycoord\n\n        return pixel_to_skycoord(xp, yp, wcs=wcs, origin=origin, mode=mode, cls=cls)\n\n    def contained_by(self, wcs, image=None, **kwargs):\n        \"\"\"\n        Determines if the SkyCoord is contained in the given wcs footprint.\n\n        Parameters\n        ----------\n        wcs : `~astropy.wcs.WCS`\n            The coordinate to check if it is within the wcs coordinate.\n        image : array\n            Optional.  The image associated with the wcs object that the cooordinate\n            is being checked against. If not given the naxis keywords will be used\n            to determine if the coordinate falls within the wcs footprint.\n        **kwargs\n            Additional arguments to pass to `~astropy.coordinates.SkyCoord.to_pixel`\n\n        Returns\n        -------\n        response : bool\n            True means the WCS footprint contains the coordinate, False means it does not.\n        \"\"\"\n\n        if image is not None:\n            ymax, xmax = image.shape\n        else:\n            xmax, ymax = wcs._naxis\n\n        import warnings\n\n        with warnings.catch_warnings():\n            #  Suppress warnings since they just mean we didn't find the coordinate\n            warnings.simplefilter(\"ignore\")\n            try:\n                x, y = self.to_pixel(wcs, **kwargs)\n            except Exception:\n                return False\n\n        return (x < xmax) & (x > 0) & (y < ymax) & (y > 0)\n\n    def radial_velocity_correction(\n        self, kind=\"barycentric\", obstime=None, location=None\n    ):\n        \"\"\"\n        Compute the correction required to convert a radial velocity at a given\n        time and place on the Earth's Surface to a barycentric or heliocentric\n        velocity.\n\n        Parameters\n        ----------\n        kind : str\n            The kind of velocity correction.  Must be 'barycentric' or\n            'heliocentric'.\n        obstime : `~astropy.time.Time` or None, optional\n            The time at which to compute the correction.  If `None`, the\n            ``obstime`` frame attribute on the `SkyCoord` will be used.\n        location : `~astropy.coordinates.EarthLocation` or None, optional\n            The observer location at which to compute the correction.  If\n            `None`, the  ``location`` frame attribute on the passed-in\n            ``obstime`` will be used, and if that is None, the ``location``\n            frame attribute on the `SkyCoord` will be used.\n\n        Raises\n        ------\n        ValueError\n            If either ``obstime`` or ``location`` are passed in (not ``None``)\n            when the frame attribute is already set on this `SkyCoord`.\n        TypeError\n            If ``obstime`` or ``location`` aren't provided, either as arguments\n            or as frame attributes.\n\n        Returns\n        -------\n        vcorr : `~astropy.units.Quantity` ['speed']\n            The  correction with a positive sign.  I.e., *add* this\n            to an observed radial velocity to get the barycentric (or\n            heliocentric) velocity. If m/s precision or better is needed,\n            see the notes below.\n\n        Notes\n        -----\n        The barycentric correction is calculated to higher precision than the\n        heliocentric correction and includes additional physics (e.g time dilation).\n        Use barycentric corrections if m/s precision is required.\n\n        The algorithm here is sufficient to perform corrections at the mm/s level, but\n        care is needed in application. The barycentric correction returned uses the optical\n        approximation v = z * c. Strictly speaking, the barycentric correction is\n        multiplicative and should be applied as::\n\n          >>> from astropy.time import Time\n          >>> from astropy.coordinates import SkyCoord, EarthLocation\n          >>> from astropy.constants import c\n          >>> t = Time(56370.5, format='mjd', scale='utc')\n          >>> loc = EarthLocation('149d33m00.5s','-30d18m46.385s',236.87*u.m)\n          >>> sc = SkyCoord(1*u.deg, 2*u.deg)\n          >>> vcorr = sc.radial_velocity_correction(kind='barycentric', obstime=t, location=loc)  # doctest: +REMOTE_DATA\n          >>> rv = rv + vcorr + rv * vcorr / c  # doctest: +SKIP\n\n        Also note that this method returns the correction velocity in the so-called\n        *optical convention*::\n\n          >>> vcorr = zb * c  # doctest: +SKIP\n\n        where ``zb`` is the barycentric correction redshift as defined in section 3\n        of Wright & Eastman (2014). The application formula given above follows from their\n        equation (11) under assumption that the radial velocity ``rv`` has also been defined\n        using the same optical convention. Note, this can be regarded as a matter of\n        velocity definition and does not by itself imply any loss of accuracy, provided\n        sufficient care has been taken during interpretation of the results. If you need\n        the barycentric correction expressed as the full relativistic velocity (e.g., to provide\n        it as the input to another software which performs the application), the\n        following recipe can be used::\n\n          >>> zb = vcorr / c  # doctest: +REMOTE_DATA\n          >>> zb_plus_one_squared = (zb + 1) ** 2  # doctest: +REMOTE_DATA\n          >>> vcorr_rel = c * (zb_plus_one_squared - 1) / (zb_plus_one_squared + 1)  # doctest: +REMOTE_DATA\n\n        or alternatively using just equivalencies::\n\n          >>> vcorr_rel = vcorr.to(u.Hz, u.doppler_optical(1*u.Hz)).to(vcorr.unit, u.doppler_relativistic(1*u.Hz))  # doctest: +REMOTE_DATA\n\n        See also `~astropy.units.equivalencies.doppler_optical`,\n        `~astropy.units.equivalencies.doppler_radio`, and\n        `~astropy.units.equivalencies.doppler_relativistic` for more information on\n        the velocity conventions.\n\n        The default is for this method to use the builtin ephemeris for\n        computing the sun and earth location.  Other ephemerides can be chosen\n        by setting the `~astropy.coordinates.solar_system_ephemeris` variable,\n        either directly or via ``with`` statement.  For example, to use the JPL\n        ephemeris, do::\n\n          >>> from astropy.coordinates import solar_system_ephemeris\n          >>> sc = SkyCoord(1*u.deg, 2*u.deg)\n          >>> with solar_system_ephemeris.set('jpl'):  # doctest: +REMOTE_DATA\n          ...     rv += sc.radial_velocity_correction(obstime=t, location=loc)  # doctest: +SKIP\n\n        \"\"\"\n        # has to be here to prevent circular imports\n        from .solar_system import get_body_barycentric_posvel\n\n        # location validation\n        timeloc = getattr(obstime, \"location\", None)\n        if location is None:\n            if self.location is not None:\n                location = self.location\n                if timeloc is not None:\n                    raise ValueError(\n                        \"`location` cannot be in both the passed-in `obstime` and this\"\n                        \" `SkyCoord` because it is ambiguous which is meant for the\"\n                        \" radial_velocity_correction.\"\n                    )\n            elif timeloc is not None:\n                location = timeloc\n            else:\n                raise TypeError(\n                    \"Must provide a `location` to radial_velocity_correction, either as\"\n                    \" a SkyCoord frame attribute, as an attribute on the passed in\"\n                    \" `obstime`, or in the method call.\"\n                )\n\n        elif self.location is not None or timeloc is not None:\n            raise ValueError(\n                \"Cannot compute radial velocity correction if `location` argument is\"\n                \" passed in and there is also a  `location` attribute on this SkyCoord\"\n                \" or the passed-in `obstime`.\"\n            )\n\n        # obstime validation\n        coo_at_rv_obstime = self  # assume we need no space motion for now\n        if obstime is None:\n            obstime = self.obstime\n            if obstime is None:\n                raise TypeError(\n                    \"Must provide an `obstime` to radial_velocity_correction, either as\"\n                    \" a SkyCoord frame attribute or in the method call.\"\n                )\n        elif self.obstime is not None and self.frame.data.differentials:\n            # we do need space motion after all\n            coo_at_rv_obstime = self.apply_space_motion(obstime)\n        elif self.obstime is None:\n            # warn the user if the object has differentials set\n            if \"s\" in self.data.differentials:\n                warnings.warn(\n                    \"SkyCoord has space motion, and therefore the specified \"\n                    \"position of the SkyCoord may not be the same as \"\n                    \"the `obstime` for the radial velocity measurement. \"\n                    \"This may affect the rv correction at the order of km/s\"\n                    \"for very high proper motions sources. If you wish to \"\n                    \"apply space motion of the SkyCoord to correct for this\"\n                    \"the `obstime` attribute of the SkyCoord must be set\",\n                    AstropyUserWarning,\n                )\n\n        pos_earth, v_earth = get_body_barycentric_posvel(\"earth\", obstime)\n        if kind == \"barycentric\":\n            v_origin_to_earth = v_earth\n        elif kind == \"heliocentric\":\n            v_sun = get_body_barycentric_posvel(\"sun\", obstime)[1]\n            v_origin_to_earth = v_earth - v_sun\n        else:\n            raise ValueError(\n                \"`kind` argument to radial_velocity_correction must \"\n                f\"be 'barycentric' or 'heliocentric', but got '{kind}'\"\n            )\n\n        gcrs_p, gcrs_v = location.get_gcrs_posvel(obstime)\n        # transforming to GCRS is not the correct thing to do here, since we don't want to\n        # include aberration (or light deflection)? Instead, only apply parallax if necessary\n        icrs_cart = coo_at_rv_obstime.icrs.cartesian\n        icrs_cart_novel = icrs_cart.without_differentials()\n        if self.data.__class__ is UnitSphericalRepresentation:\n            targcart = icrs_cart_novel\n        else:\n            # skycoord has distances so apply parallax\n            obs_icrs_cart = pos_earth + gcrs_p\n            targcart = icrs_cart_novel - obs_icrs_cart\n            targcart /= targcart.norm()\n\n        if kind == \"barycentric\":\n            beta_obs = (v_origin_to_earth + gcrs_v) / speed_of_light\n            gamma_obs = 1 / np.sqrt(1 - beta_obs.norm() ** 2)\n            gr = location.gravitational_redshift(obstime)\n            # barycentric redshift according to eq 28 in Wright & Eastmann (2014),\n            # neglecting Shapiro delay and effects of the star's own motion\n            zb = gamma_obs * (1 + beta_obs.dot(targcart)) / (1 + gr / speed_of_light)\n            # try and get terms corresponding to stellar motion.\n            if icrs_cart.differentials:\n                try:\n                    ro = self.icrs.cartesian\n                    beta_star = ro.differentials[\"s\"].to_cartesian() / speed_of_light\n                    # ICRS unit vector at coordinate epoch\n                    ro = ro.without_differentials()\n                    ro /= ro.norm()\n                    zb *= (1 + beta_star.dot(ro)) / (1 + beta_star.dot(targcart))\n                except u.UnitConversionError:\n                    warnings.warn(\n                        \"SkyCoord contains some velocity information, but not enough to\"\n                        \" calculate the full space motion of the source, and so this\"\n                        \" has been ignored for the purposes of calculating the radial\"\n                        \" velocity correction. This can lead to errors on the order of\"\n                        \" metres/second.\",\n                        AstropyUserWarning,\n                    )\n\n            zb = zb - 1\n            return zb * speed_of_light\n        else:\n            # do a simpler correction ignoring time dilation and gravitational redshift\n            # this is adequate since Heliocentric corrections shouldn't be used if\n            # cm/s precision is required.\n            return targcart.dot(v_origin_to_earth + gcrs_v)\n\n    # Table interactions\n    @classmethod\n    def guess_from_table(cls, table, **coord_kwargs):\n        r\"\"\"\n        A convenience method to create and return a new `SkyCoord` from the data\n        in an astropy Table.\n\n        This method matches table columns that start with the case-insensitive\n        names of the the components of the requested frames (including\n        differentials), if they are also followed by a non-alphanumeric\n        character. It will also match columns that *end* with the component name\n        if a non-alphanumeric character is *before* it.\n\n        For example, the first rule means columns with names like\n        ``'RA[J2000]'`` or ``'ra'`` will be interpreted as ``ra`` attributes for\n        `~astropy.coordinates.ICRS` frames, but ``'RAJ2000'`` or ``'radius'``\n        are *not*. Similarly, the second rule applied to the\n        `~astropy.coordinates.Galactic` frame means that a column named\n        ``'gal_l'`` will be used as the the ``l`` component, but ``gall`` or\n        ``'fill'`` will not.\n\n        The definition of alphanumeric here is based on Unicode's definition\n        of alphanumeric, except without ``_`` (which is normally considered\n        alphanumeric).  So for ASCII, this means the non-alphanumeric characters\n        are ``<space>_!\"#$%&'()*+,-./\\:;<=>?@[]^`{|}~``).\n\n        Parameters\n        ----------\n        table : `~astropy.table.Table` or subclass\n            The table to load data from.\n        **coord_kwargs\n            Any additional keyword arguments are passed directly to this class's\n            constructor.\n\n        Returns\n        -------\n        newsc : `~astropy.coordinates.SkyCoord` or subclass\n            The new `SkyCoord` (or subclass) object.\n\n        Raises\n        ------\n        ValueError\n            If more than one match is found in the table for a component,\n            unless the additional matches are also valid frame component names.\n            If a \"coord_kwargs\" is provided for a value also found in the table.\n\n        \"\"\"\n        _frame_cls, _frame_kwargs = _get_frame_without_data([], coord_kwargs)\n        frame = _frame_cls(**_frame_kwargs)\n        coord_kwargs[\"frame\"] = coord_kwargs.get(\"frame\", frame)\n\n        representation_component_names = set(\n            frame.get_representation_component_names()\n        ).union(set(frame.get_representation_component_names(\"s\")))\n\n        comp_kwargs = {}\n        for comp_name in representation_component_names:\n            # this matches things like 'ra[...]'' but *not* 'rad'.\n            # note that the \"_\" must be in there explicitly, because\n            # \"alphanumeric\" usually includes underscores.\n            starts_with_comp = comp_name + r\"(\\W|\\b|_)\"\n            # this part matches stuff like 'center_ra', but *not*\n            # 'aura'\n            ends_with_comp = r\".*(\\W|\\b|_)\" + comp_name + r\"\\b\"\n            # the final regex ORs together the two patterns\n            rex = re.compile(\n                rf\"({starts_with_comp})|({ends_with_comp})\", re.IGNORECASE | re.UNICODE\n            )\n\n            # find all matches\n            matches = {col_name for col_name in table.colnames if rex.match(col_name)}\n\n            # now need to select among matches, also making sure we don't have\n            # an exact match with another component\n            if len(matches) == 0:  # no matches\n                continue\n            elif len(matches) == 1:  # only one match\n                col_name = matches.pop()\n            else:  # more than 1 match\n                # try to sieve out other components\n                matches -= representation_component_names - {comp_name}\n                # if there's only one remaining match, it worked.\n                if len(matches) == 1:\n                    col_name = matches.pop()\n                else:\n                    raise ValueError(\n                        f'Found at least two matches for component \"{comp_name}\":'\n                        f' \"{matches}\". Cannot guess coordinates from a table with this'\n                        \" ambiguity.\"\n                    )\n\n            comp_kwargs[comp_name] = table[col_name]\n\n        for k, v in comp_kwargs.items():\n            if k in coord_kwargs:\n                raise ValueError(\n                    f'Found column \"{v.name}\" in table, but it was already provided as'\n                    ' \"{k}\" keyword to guess_from_table function.'\n                )\n            else:\n                coord_kwargs[k] = v\n\n        return cls(**coord_kwargs)\n\n    # Name resolve\n    @classmethod\n    def from_name(cls, name, frame=\"icrs\", parse=False, cache=True):\n        \"\"\"\n        Given a name, query the CDS name resolver to attempt to retrieve\n        coordinate information for that object. The search database, sesame\n        url, and  query timeout can be set through configuration items in\n        ``astropy.coordinates.name_resolve`` -- see docstring for\n        `~astropy.coordinates.get_icrs_coordinates` for more\n        information.\n\n        Parameters\n        ----------\n        name : str\n            The name of the object to get coordinates for, e.g. ``'M42'``.\n        frame : str or `BaseCoordinateFrame` class or instance\n            The frame to transform the object to.\n        parse : bool\n            Whether to attempt extracting the coordinates from the name by\n            parsing with a regex. For objects catalog names that have\n            J-coordinates embedded in their names, e.g.,\n            'CRTS SSS100805 J194428-420209', this may be much faster than a\n            Sesame query for the same object name. The coordinates extracted\n            in this way may differ from the database coordinates by a few\n            deci-arcseconds, so only use this option if you do not need\n            sub-arcsecond accuracy for coordinates.\n        cache : bool, optional\n            Determines whether to cache the results or not. To update or\n            overwrite an existing value, pass ``cache='update'``.\n\n        Returns\n        -------\n        coord : SkyCoord\n            Instance of the SkyCoord class.\n        \"\"\"\n\n        from .name_resolve import get_icrs_coordinates\n\n        icrs_coord = get_icrs_coordinates(name, parse, cache=cache)\n        icrs_sky_coord = cls(icrs_coord)\n        if frame in (\"icrs\", icrs_coord.__class__):\n            return icrs_sky_coord\n        else:\n            return icrs_sky_coord.transform_to(frame)"}
{"model_name_or_path": "agentless", "instance_id": "django__django-11141", "model_patch": "diff --git a/django/db/migrations/loader.py b/django/db/migrations/loader.py\nindex ab7f543..6e4ab7a 100644\n--- a/django/db/migrations/loader.py\n+++ b/django/db/migrations/loader.py\n@@ -85,10 +85,7 @@ class MigrationLoader:\n                 raise\n             else:\n                 # Empty directories are namespaces.\n-                # getattr() needed on PY36 and older (replace w/attribute access).\n-                if getattr(module, '__file__', None) is None:\n-                    self.unmigrated_apps.add(app_config.label)\n-                    continue\n+                # This check is no longer needed for namespace packages.\n                 # Module is not a package (e.g. migrations.py).\n                 if not hasattr(module, '__path__'):\n                     self.unmigrated_apps.add(app_config.label)\n", "raw_model_patch": "diff --git a/django/db/migrations/loader.py b/django/db/migrations/loader.py\nindex ab7f543..6e4ab7a 100644\n--- a/django/db/migrations/loader.py\n+++ b/django/db/migrations/loader.py\n@@ -85,10 +85,7 @@ class MigrationLoader:\n                 raise\n             else:\n                 # Empty directories are namespaces.\n-                # getattr() needed on PY36 and older (replace w/attribute access).\n-                if getattr(module, '__file__', None) is None:\n-                    self.unmigrated_apps.add(app_config.label)\n-                    continue\n+                # This check is no longer needed for namespace packages.\n                 # Module is not a package (e.g. migrations.py).\n                 if not hasattr(module, '__path__'):\n                     self.unmigrated_apps.add(app_config.label)\n", "original_file_content": "import pkgutil\nimport sys\nfrom importlib import import_module, reload\n\nfrom django.apps import apps\nfrom django.conf import settings\nfrom django.db.migrations.graph import MigrationGraph\nfrom django.db.migrations.recorder import MigrationRecorder\n\nfrom .exceptions import (\n    AmbiguityError, BadMigrationError, InconsistentMigrationHistory,\n    NodeNotFoundError,\n)\n\nMIGRATIONS_MODULE_NAME = 'migrations'\n\n\nclass MigrationLoader:\n    \"\"\"\n    Load migration files from disk and their status from the database.\n\n    Migration files are expected to live in the \"migrations\" directory of\n    an app. Their names are entirely unimportant from a code perspective,\n    but will probably follow the 1234_name.py convention.\n\n    On initialization, this class will scan those directories, and open and\n    read the Python files, looking for a class called Migration, which should\n    inherit from django.db.migrations.Migration. See\n    django.db.migrations.migration for what that looks like.\n\n    Some migrations will be marked as \"replacing\" another set of migrations.\n    These are loaded into a separate set of migrations away from the main ones.\n    If all the migrations they replace are either unapplied or missing from\n    disk, then they are injected into the main set, replacing the named migrations.\n    Any dependency pointers to the replaced migrations are re-pointed to the\n    new migration.\n\n    This does mean that this class MUST also talk to the database as well as\n    to disk, but this is probably fine. We're already not just operating\n    in memory.\n    \"\"\"\n\n    def __init__(self, connection, load=True, ignore_no_migrations=False):\n        self.connection = connection\n        self.disk_migrations = None\n        self.applied_migrations = None\n        self.ignore_no_migrations = ignore_no_migrations\n        if load:\n            self.build_graph()\n\n    @classmethod\n    def migrations_module(cls, app_label):\n        \"\"\"\n        Return the path to the migrations module for the specified app_label\n        and a boolean indicating if the module is specified in\n        settings.MIGRATION_MODULE.\n        \"\"\"\n        if app_label in settings.MIGRATION_MODULES:\n            return settings.MIGRATION_MODULES[app_label], True\n        else:\n            app_package_name = apps.get_app_config(app_label).name\n            return '%s.%s' % (app_package_name, MIGRATIONS_MODULE_NAME), False\n\n    def load_disk(self):\n        \"\"\"Load the migrations from all INSTALLED_APPS from disk.\"\"\"\n        self.disk_migrations = {}\n        self.unmigrated_apps = set()\n        self.migrated_apps = set()\n        for app_config in apps.get_app_configs():\n            # Get the migrations module directory\n            module_name, explicit = self.migrations_module(app_config.label)\n            if module_name is None:\n                self.unmigrated_apps.add(app_config.label)\n                continue\n            was_loaded = module_name in sys.modules\n            try:\n                module = import_module(module_name)\n            except ImportError as e:\n                # I hate doing this, but I don't want to squash other import errors.\n                # Might be better to try a directory check directly.\n                if ((explicit and self.ignore_no_migrations) or (\n                        not explicit and \"No module named\" in str(e) and MIGRATIONS_MODULE_NAME in str(e))):\n                    self.unmigrated_apps.add(app_config.label)\n                    continue\n                raise\n            else:\n                # Empty directories are namespaces.\n                # getattr() needed on PY36 and older (replace w/attribute access).\n                if getattr(module, '__file__', None) is None:\n                    self.unmigrated_apps.add(app_config.label)\n                    continue\n                # Module is not a package (e.g. migrations.py).\n                if not hasattr(module, '__path__'):\n                    self.unmigrated_apps.add(app_config.label)\n                    continue\n                # Force a reload if it's already loaded (tests need this)\n                if was_loaded:\n                    reload(module)\n            self.migrated_apps.add(app_config.label)\n            migration_names = {\n                name for _, name, is_pkg in pkgutil.iter_modules(module.__path__)\n                if not is_pkg and name[0] not in '_~'\n            }\n            # Load migrations\n            for migration_name in migration_names:\n                migration_path = '%s.%s' % (module_name, migration_name)\n                try:\n                    migration_module = import_module(migration_path)\n                except ImportError as e:\n                    if 'bad magic number' in str(e):\n                        raise ImportError(\n                            \"Couldn't import %r as it appears to be a stale \"\n                            \".pyc file.\" % migration_path\n                        ) from e\n                    else:\n                        raise\n                if not hasattr(migration_module, \"Migration\"):\n                    raise BadMigrationError(\n                        \"Migration %s in app %s has no Migration class\" % (migration_name, app_config.label)\n                    )\n                self.disk_migrations[app_config.label, migration_name] = migration_module.Migration(\n                    migration_name,\n                    app_config.label,\n                )\n\n    def get_migration(self, app_label, name_prefix):\n        \"\"\"Return the named migration or raise NodeNotFoundError.\"\"\"\n        return self.graph.nodes[app_label, name_prefix]\n\n    def get_migration_by_prefix(self, app_label, name_prefix):\n        \"\"\"\n        Return the migration(s) which match the given app label and name_prefix.\n        \"\"\"\n        # Do the search\n        results = []\n        for migration_app_label, migration_name in self.disk_migrations:\n            if migration_app_label == app_label and migration_name.startswith(name_prefix):\n                results.append((migration_app_label, migration_name))\n        if len(results) > 1:\n            raise AmbiguityError(\n                \"There is more than one migration for '%s' with the prefix '%s'\" % (app_label, name_prefix)\n            )\n        elif not results:\n            raise KeyError(\"There no migrations for '%s' with the prefix '%s'\" % (app_label, name_prefix))\n        else:\n            return self.disk_migrations[results[0]]\n\n    def check_key(self, key, current_app):\n        if (key[1] != \"__first__\" and key[1] != \"__latest__\") or key in self.graph:\n            return key\n        # Special-case __first__, which means \"the first migration\" for\n        # migrated apps, and is ignored for unmigrated apps. It allows\n        # makemigrations to declare dependencies on apps before they even have\n        # migrations.\n        if key[0] == current_app:\n            # Ignore __first__ references to the same app (#22325)\n            return\n        if key[0] in self.unmigrated_apps:\n            # This app isn't migrated, but something depends on it.\n            # The models will get auto-added into the state, though\n            # so we're fine.\n            return\n        if key[0] in self.migrated_apps:\n            try:\n                if key[1] == \"__first__\":\n                    return self.graph.root_nodes(key[0])[0]\n                else:  # \"__latest__\"\n                    return self.graph.leaf_nodes(key[0])[0]\n            except IndexError:\n                if self.ignore_no_migrations:\n                    return None\n                else:\n                    raise ValueError(\"Dependency on app with no migrations: %s\" % key[0])\n        raise ValueError(\"Dependency on unknown app: %s\" % key[0])\n\n    def add_internal_dependencies(self, key, migration):\n        \"\"\"\n        Internal dependencies need to be added first to ensure `__first__`\n        dependencies find the correct root node.\n        \"\"\"\n        for parent in migration.dependencies:\n            # Ignore __first__ references to the same app.\n            if parent[0] == key[0] and parent[1] != '__first__':\n                self.graph.add_dependency(migration, key, parent, skip_validation=True)\n\n    def add_external_dependencies(self, key, migration):\n        for parent in migration.dependencies:\n            # Skip internal dependencies\n            if key[0] == parent[0]:\n                continue\n            parent = self.check_key(parent, key[0])\n            if parent is not None:\n                self.graph.add_dependency(migration, key, parent, skip_validation=True)\n        for child in migration.run_before:\n            child = self.check_key(child, key[0])\n            if child is not None:\n                self.graph.add_dependency(migration, child, key, skip_validation=True)\n\n    def build_graph(self):\n        \"\"\"\n        Build a migration dependency graph using both the disk and database.\n        You'll need to rebuild the graph if you apply migrations. This isn't\n        usually a problem as generally migration stuff runs in a one-shot process.\n        \"\"\"\n        # Load disk data\n        self.load_disk()\n        # Load database data\n        if self.connection is None:\n            self.applied_migrations = {}\n        else:\n            recorder = MigrationRecorder(self.connection)\n            self.applied_migrations = recorder.applied_migrations()\n        # To start, populate the migration graph with nodes for ALL migrations\n        # and their dependencies. Also make note of replacing migrations at this step.\n        self.graph = MigrationGraph()\n        self.replacements = {}\n        for key, migration in self.disk_migrations.items():\n            self.graph.add_node(key, migration)\n            # Replacing migrations.\n            if migration.replaces:\n                self.replacements[key] = migration\n        for key, migration in self.disk_migrations.items():\n            # Internal (same app) dependencies.\n            self.add_internal_dependencies(key, migration)\n        # Add external dependencies now that the internal ones have been resolved.\n        for key, migration in self.disk_migrations.items():\n            self.add_external_dependencies(key, migration)\n        # Carry out replacements where possible.\n        for key, migration in self.replacements.items():\n            # Get applied status of each of this migration's replacement targets.\n            applied_statuses = [(target in self.applied_migrations) for target in migration.replaces]\n            # Ensure the replacing migration is only marked as applied if all of\n            # its replacement targets are.\n            if all(applied_statuses):\n                self.applied_migrations[key] = migration\n            else:\n                self.applied_migrations.pop(key, None)\n            # A replacing migration can be used if either all or none of its\n            # replacement targets have been applied.\n            if all(applied_statuses) or (not any(applied_statuses)):\n                self.graph.remove_replaced_nodes(key, migration.replaces)\n            else:\n                # This replacing migration cannot be used because it is partially applied.\n                # Remove it from the graph and remap dependencies to it (#25945).\n                self.graph.remove_replacement_node(key, migration.replaces)\n        # Ensure the graph is consistent.\n        try:\n            self.graph.validate_consistency()\n        except NodeNotFoundError as exc:\n            # Check if the missing node could have been replaced by any squash\n            # migration but wasn't because the squash migration was partially\n            # applied before. In that case raise a more understandable exception\n            # (#23556).\n            # Get reverse replacements.\n            reverse_replacements = {}\n            for key, migration in self.replacements.items():\n                for replaced in migration.replaces:\n                    reverse_replacements.setdefault(replaced, set()).add(key)\n            # Try to reraise exception with more detail.\n            if exc.node in reverse_replacements:\n                candidates = reverse_replacements.get(exc.node, set())\n                is_replaced = any(candidate in self.graph.nodes for candidate in candidates)\n                if not is_replaced:\n                    tries = ', '.join('%s.%s' % c for c in candidates)\n                    raise NodeNotFoundError(\n                        \"Migration {0} depends on nonexistent node ('{1}', '{2}'). \"\n                        \"Django tried to replace migration {1}.{2} with any of [{3}] \"\n                        \"but wasn't able to because some of the replaced migrations \"\n                        \"are already applied.\".format(\n                            exc.origin, exc.node[0], exc.node[1], tries\n                        ),\n                        exc.node\n                    ) from exc\n            raise exc\n        self.graph.ensure_not_cyclic()\n\n    def check_consistent_history(self, connection):\n        \"\"\"\n        Raise InconsistentMigrationHistory if any applied migrations have\n        unapplied dependencies.\n        \"\"\"\n        recorder = MigrationRecorder(connection)\n        applied = recorder.applied_migrations()\n        for migration in applied:\n            # If the migration is unknown, skip it.\n            if migration not in self.graph.nodes:\n                continue\n            for parent in self.graph.node_map[migration].parents:\n                if parent not in applied:\n                    # Skip unapplied squashed migrations that have all of their\n                    # `replaces` applied.\n                    if parent in self.replacements:\n                        if all(m in applied for m in self.replacements[parent].replaces):\n                            continue\n                    raise InconsistentMigrationHistory(\n                        \"Migration {}.{} is applied before its dependency \"\n                        \"{}.{} on database '{}'.\".format(\n                            migration[0], migration[1], parent[0], parent[1],\n                            connection.alias,\n                        )\n                    )\n\n    def detect_conflicts(self):\n        \"\"\"\n        Look through the loaded graph and detect any conflicts - apps\n        with more than one leaf migration. Return a dict of the app labels\n        that conflict with the migration names that conflict.\n        \"\"\"\n        seen_apps = {}\n        conflicting_apps = set()\n        for app_label, migration_name in self.graph.leaf_nodes():\n            if app_label in seen_apps:\n                conflicting_apps.add(app_label)\n            seen_apps.setdefault(app_label, set()).add(migration_name)\n        return {app_label: seen_apps[app_label] for app_label in conflicting_apps}\n\n    def project_state(self, nodes=None, at_end=True):\n        \"\"\"\n        Return a ProjectState object representing the most recent state\n        that the loaded migrations represent.\n\n        See graph.make_state() for the meaning of \"nodes\" and \"at_end\".\n        \"\"\"\n        return self.graph.make_state(nodes=nodes, at_end=at_end, real_apps=list(self.unmigrated_apps))"}
{"model_name_or_path": "agentless", "instance_id": "django__django-11206", "model_patch": "diff --git a/django/utils/numberformat.py b/django/utils/numberformat.py\nindex d5baf57..f95c61b 100644\n--- a/django/utils/numberformat.py\n+++ b/django/utils/numberformat.py\n@@ -40,6 +40,8 @@ def format(number, decimal_sep, decimal_pos=None, grouping=0, thousand_sep='',\n             )\n             return '{}e{}'.format(coefficient, exponent)\n         else:\n+            if abs(number) < Decimal(10) ** Decimal(-decimal_pos):\n+                return '0.' + '0' * decimal_pos\n             str_number = '{:f}'.format(number)\n     else:\n         str_number = str(number)\n", "raw_model_patch": "diff --git a/django/utils/numberformat.py b/django/utils/numberformat.py\nindex d5baf57..f95c61b 100644\n--- a/django/utils/numberformat.py\n+++ b/django/utils/numberformat.py\n@@ -40,6 +40,8 @@ def format(number, decimal_sep, decimal_pos=None, grouping=0, thousand_sep='',\n             )\n             return '{}e{}'.format(coefficient, exponent)\n         else:\n+            if abs(number) < Decimal(10) ** Decimal(-decimal_pos):\n+                return '0.' + '0' * decimal_pos\n             str_number = '{:f}'.format(number)\n     else:\n         str_number = str(number)\n", "original_file_content": "from decimal import Decimal\n\nfrom django.conf import settings\nfrom django.utils.safestring import mark_safe\n\n\ndef format(number, decimal_sep, decimal_pos=None, grouping=0, thousand_sep='',\n           force_grouping=False, use_l10n=None):\n    \"\"\"\n    Get a number (as a number or string), and return it as a string,\n    using formats defined as arguments:\n\n    * decimal_sep: Decimal separator symbol (for example \".\")\n    * decimal_pos: Number of decimal positions\n    * grouping: Number of digits in every group limited by thousand separator.\n        For non-uniform digit grouping, it can be a sequence with the number\n        of digit group sizes following the format used by the Python locale\n        module in locale.localeconv() LC_NUMERIC grouping (e.g. (3, 2, 0)).\n    * thousand_sep: Thousand separator symbol (for example \",\")\n    \"\"\"\n    use_grouping = (use_l10n or (use_l10n is None and settings.USE_L10N)) and settings.USE_THOUSAND_SEPARATOR\n    use_grouping = use_grouping or force_grouping\n    use_grouping = use_grouping and grouping != 0\n    # Make the common case fast\n    if isinstance(number, int) and not use_grouping and not decimal_pos:\n        return mark_safe(number)\n    # sign\n    sign = ''\n    if isinstance(number, Decimal):\n        # Format values with more than 200 digits (an arbitrary cutoff) using\n        # scientific notation to avoid high memory usage in {:f}'.format().\n        _, digits, exponent = number.as_tuple()\n        if abs(exponent) + len(digits) > 200:\n            number = '{:e}'.format(number)\n            coefficient, exponent = number.split('e')\n            # Format the coefficient.\n            coefficient = format(\n                coefficient, decimal_sep, decimal_pos, grouping,\n                thousand_sep, force_grouping, use_l10n,\n            )\n            return '{}e{}'.format(coefficient, exponent)\n        else:\n            str_number = '{:f}'.format(number)\n    else:\n        str_number = str(number)\n    if str_number[0] == '-':\n        sign = '-'\n        str_number = str_number[1:]\n    # decimal part\n    if '.' in str_number:\n        int_part, dec_part = str_number.split('.')\n        if decimal_pos is not None:\n            dec_part = dec_part[:decimal_pos]\n    else:\n        int_part, dec_part = str_number, ''\n    if decimal_pos is not None:\n        dec_part = dec_part + ('0' * (decimal_pos - len(dec_part)))\n    dec_part = dec_part and decimal_sep + dec_part\n    # grouping\n    if use_grouping:\n        try:\n            # if grouping is a sequence\n            intervals = list(grouping)\n        except TypeError:\n            # grouping is a single value\n            intervals = [grouping, 0]\n        active_interval = intervals.pop(0)\n        int_part_gd = ''\n        cnt = 0\n        for digit in int_part[::-1]:\n            if cnt and cnt == active_interval:\n                if intervals:\n                    active_interval = intervals.pop(0) or active_interval\n                int_part_gd += thousand_sep[::-1]\n                cnt = 0\n            int_part_gd += digit\n            cnt += 1\n        int_part = int_part_gd[::-1]\n    return sign + int_part + dec_part"}
{"model_name_or_path": "agentless", "instance_id": "django__django-11292", "model_patch": "diff --git a/django/core/management/__init__.py b/django/core/management/__init__.py\nindex 4403daf..f08affa 100644\n--- a/django/core/management/__init__.py\n+++ b/django/core/management/__init__.py\n@@ -142,7 +142,10 @@ def call_command(command_name, *args, **options):\n         )\n     # Move positional args out of options to mimic legacy optparse\n     args = defaults.pop('args', ())\n-    if 'skip_checks' not in options:\n+    # Ensure that skip_checks can be passed as a command line argument\n+    if 'skip_checks' in options:\n+        defaults['skip_checks'] = options['skip_checks']\n+    else:\n         defaults['skip_checks'] = True\n \n     return command.execute(*args, **defaults)\n", "raw_model_patch": "diff --git a/django/core/management/__init__.py b/django/core/management/__init__.py\nindex 4403daf..f08affa 100644\n--- a/django/core/management/__init__.py\n+++ b/django/core/management/__init__.py\n@@ -142,7 +142,10 @@ def call_command(command_name, *args, **options):\n         )\n     # Move positional args out of options to mimic legacy optparse\n     args = defaults.pop('args', ())\n-    if 'skip_checks' not in options:\n+    # Ensure that skip_checks can be passed as a command line argument\n+    if 'skip_checks' in options:\n+        defaults['skip_checks'] = options['skip_checks']\n+    else:\n         defaults['skip_checks'] = True\n \n     return command.execute(*args, **defaults)\n", "original_file_content": "import functools\nimport os\nimport pkgutil\nimport sys\nfrom collections import defaultdict\nfrom difflib import get_close_matches\nfrom importlib import import_module\n\nimport django\nfrom django.apps import apps\nfrom django.conf import settings\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.core.management.base import (\n    BaseCommand, CommandError, CommandParser, handle_default_options,\n)\nfrom django.core.management.color import color_style\nfrom django.utils import autoreload\n\n\ndef find_commands(management_dir):\n    \"\"\"\n    Given a path to a management directory, return a list of all the command\n    names that are available.\n    \"\"\"\n    command_dir = os.path.join(management_dir, 'commands')\n    return [name for _, name, is_pkg in pkgutil.iter_modules([command_dir])\n            if not is_pkg and not name.startswith('_')]\n\n\ndef load_command_class(app_name, name):\n    \"\"\"\n    Given a command name and an application name, return the Command\n    class instance. Allow all errors raised by the import process\n    (ImportError, AttributeError) to propagate.\n    \"\"\"\n    module = import_module('%s.management.commands.%s' % (app_name, name))\n    return module.Command()\n\n\n@functools.lru_cache(maxsize=None)\ndef get_commands():\n    \"\"\"\n    Return a dictionary mapping command names to their callback applications.\n\n    Look for a management.commands package in django.core, and in each\n    installed application -- if a commands package exists, register all\n    commands in that package.\n\n    Core commands are always included. If a settings module has been\n    specified, also include user-defined commands.\n\n    The dictionary is in the format {command_name: app_name}. Key-value\n    pairs from this dictionary can then be used in calls to\n    load_command_class(app_name, command_name)\n\n    If a specific version of a command must be loaded (e.g., with the\n    startapp command), the instantiated module can be placed in the\n    dictionary in place of the application name.\n\n    The dictionary is cached on the first call and reused on subsequent\n    calls.\n    \"\"\"\n    commands = {name: 'django.core' for name in find_commands(__path__[0])}\n\n    if not settings.configured:\n        return commands\n\n    for app_config in reversed(list(apps.get_app_configs())):\n        path = os.path.join(app_config.path, 'management')\n        commands.update({name: app_config.name for name in find_commands(path)})\n\n    return commands\n\n\ndef call_command(command_name, *args, **options):\n    \"\"\"\n    Call the given command, with the given options and args/kwargs.\n\n    This is the primary API you should use for calling specific commands.\n\n    `command_name` may be a string or a command object. Using a string is\n    preferred unless the command object is required for further processing or\n    testing.\n\n    Some examples:\n        call_command('migrate')\n        call_command('shell', plain=True)\n        call_command('sqlmigrate', 'myapp')\n\n        from django.core.management.commands import flush\n        cmd = flush.Command()\n        call_command(cmd, verbosity=0, interactive=False)\n        # Do something with cmd ...\n    \"\"\"\n    if isinstance(command_name, BaseCommand):\n        # Command object passed in.\n        command = command_name\n        command_name = command.__class__.__module__.split('.')[-1]\n    else:\n        # Load the command object by name.\n        try:\n            app_name = get_commands()[command_name]\n        except KeyError:\n            raise CommandError(\"Unknown command: %r\" % command_name)\n\n        if isinstance(app_name, BaseCommand):\n            # If the command is already loaded, use it directly.\n            command = app_name\n        else:\n            command = load_command_class(app_name, command_name)\n\n    # Simulate argument parsing to get the option defaults (see #10080 for details).\n    parser = command.create_parser('', command_name)\n    # Use the `dest` option name from the parser option\n    opt_mapping = {\n        min(s_opt.option_strings).lstrip('-').replace('-', '_'): s_opt.dest\n        for s_opt in parser._actions if s_opt.option_strings\n    }\n    arg_options = {opt_mapping.get(key, key): value for key, value in options.items()}\n    parse_args = [str(a) for a in args]\n    # Any required arguments which are passed in via **options must be passed\n    # to parse_args().\n    parse_args += [\n        '{}={}'.format(min(opt.option_strings), arg_options[opt.dest])\n        for opt in parser._actions if opt.required and opt.dest in options\n    ]\n    defaults = parser.parse_args(args=parse_args)\n    defaults = dict(defaults._get_kwargs(), **arg_options)\n    # Raise an error if any unknown options were passed.\n    stealth_options = set(command.base_stealth_options + command.stealth_options)\n    dest_parameters = {action.dest for action in parser._actions}\n    valid_options = (dest_parameters | stealth_options).union(opt_mapping)\n    unknown_options = set(options) - valid_options\n    if unknown_options:\n        raise TypeError(\n            \"Unknown option(s) for %s command: %s. \"\n            \"Valid options are: %s.\" % (\n                command_name,\n                ', '.join(sorted(unknown_options)),\n                ', '.join(sorted(valid_options)),\n            )\n        )\n    # Move positional args out of options to mimic legacy optparse\n    args = defaults.pop('args', ())\n    if 'skip_checks' not in options:\n        defaults['skip_checks'] = True\n\n    return command.execute(*args, **defaults)\n\n\nclass ManagementUtility:\n    \"\"\"\n    Encapsulate the logic of the django-admin and manage.py utilities.\n    \"\"\"\n    def __init__(self, argv=None):\n        self.argv = argv or sys.argv[:]\n        self.prog_name = os.path.basename(self.argv[0])\n        if self.prog_name == '__main__.py':\n            self.prog_name = 'python -m django'\n        self.settings_exception = None\n\n    def main_help_text(self, commands_only=False):\n        \"\"\"Return the script's main help text, as a string.\"\"\"\n        if commands_only:\n            usage = sorted(get_commands())\n        else:\n            usage = [\n                \"\",\n                \"Type '%s help <subcommand>' for help on a specific subcommand.\" % self.prog_name,\n                \"\",\n                \"Available subcommands:\",\n            ]\n            commands_dict = defaultdict(lambda: [])\n            for name, app in get_commands().items():\n                if app == 'django.core':\n                    app = 'django'\n                else:\n                    app = app.rpartition('.')[-1]\n                commands_dict[app].append(name)\n            style = color_style()\n            for app in sorted(commands_dict):\n                usage.append(\"\")\n                usage.append(style.NOTICE(\"[%s]\" % app))\n                for name in sorted(commands_dict[app]):\n                    usage.append(\"    %s\" % name)\n            # Output an extra note if settings are not properly configured\n            if self.settings_exception is not None:\n                usage.append(style.NOTICE(\n                    \"Note that only Django core commands are listed \"\n                    \"as settings are not properly configured (error: %s).\"\n                    % self.settings_exception))\n\n        return '\\n'.join(usage)\n\n    def fetch_command(self, subcommand):\n        \"\"\"\n        Try to fetch the given subcommand, printing a message with the\n        appropriate command called from the command line (usually\n        \"django-admin\" or \"manage.py\") if it can't be found.\n        \"\"\"\n        # Get commands outside of try block to prevent swallowing exceptions\n        commands = get_commands()\n        try:\n            app_name = commands[subcommand]\n        except KeyError:\n            if os.environ.get('DJANGO_SETTINGS_MODULE'):\n                # If `subcommand` is missing due to misconfigured settings, the\n                # following line will retrigger an ImproperlyConfigured exception\n                # (get_commands() swallows the original one) so the user is\n                # informed about it.\n                settings.INSTALLED_APPS\n            else:\n                sys.stderr.write(\"No Django settings specified.\\n\")\n            possible_matches = get_close_matches(subcommand, commands)\n            sys.stderr.write('Unknown command: %r' % subcommand)\n            if possible_matches:\n                sys.stderr.write('. Did you mean %s?' % possible_matches[0])\n            sys.stderr.write(\"\\nType '%s help' for usage.\\n\" % self.prog_name)\n            sys.exit(1)\n        if isinstance(app_name, BaseCommand):\n            # If the command is already loaded, use it directly.\n            klass = app_name\n        else:\n            klass = load_command_class(app_name, subcommand)\n        return klass\n\n    def autocomplete(self):\n        \"\"\"\n        Output completion suggestions for BASH.\n\n        The output of this function is passed to BASH's `COMREPLY` variable and\n        treated as completion suggestions. `COMREPLY` expects a space\n        separated string as the result.\n\n        The `COMP_WORDS` and `COMP_CWORD` BASH environment variables are used\n        to get information about the cli input. Please refer to the BASH\n        man-page for more information about this variables.\n\n        Subcommand options are saved as pairs. A pair consists of\n        the long option string (e.g. '--exclude') and a boolean\n        value indicating if the option requires arguments. When printing to\n        stdout, an equal sign is appended to options which require arguments.\n\n        Note: If debugging this function, it is recommended to write the debug\n        output in a separate file. Otherwise the debug output will be treated\n        and formatted as potential completion suggestions.\n        \"\"\"\n        # Don't complete if user hasn't sourced bash_completion file.\n        if 'DJANGO_AUTO_COMPLETE' not in os.environ:\n            return\n\n        cwords = os.environ['COMP_WORDS'].split()[1:]\n        cword = int(os.environ['COMP_CWORD'])\n\n        try:\n            curr = cwords[cword - 1]\n        except IndexError:\n            curr = ''\n\n        subcommands = [*get_commands(), 'help']\n        options = [('--help', False)]\n\n        # subcommand\n        if cword == 1:\n            print(' '.join(sorted(filter(lambda x: x.startswith(curr), subcommands))))\n        # subcommand options\n        # special case: the 'help' subcommand has no options\n        elif cwords[0] in subcommands and cwords[0] != 'help':\n            subcommand_cls = self.fetch_command(cwords[0])\n            # special case: add the names of installed apps to options\n            if cwords[0] in ('dumpdata', 'sqlmigrate', 'sqlsequencereset', 'test'):\n                try:\n                    app_configs = apps.get_app_configs()\n                    # Get the last part of the dotted path as the app name.\n                    options.extend((app_config.label, 0) for app_config in app_configs)\n                except ImportError:\n                    # Fail silently if DJANGO_SETTINGS_MODULE isn't set. The\n                    # user will find out once they execute the command.\n                    pass\n            parser = subcommand_cls.create_parser('', cwords[0])\n            options.extend(\n                (min(s_opt.option_strings), s_opt.nargs != 0)\n                for s_opt in parser._actions if s_opt.option_strings\n            )\n            # filter out previously specified options from available options\n            prev_opts = {x.split('=')[0] for x in cwords[1:cword - 1]}\n            options = (opt for opt in options if opt[0] not in prev_opts)\n\n            # filter options by current input\n            options = sorted((k, v) for k, v in options if k.startswith(curr))\n            for opt_label, require_arg in options:\n                # append '=' to options which require args\n                if require_arg:\n                    opt_label += '='\n                print(opt_label)\n        # Exit code of the bash completion function is never passed back to\n        # the user, so it's safe to always exit with 0.\n        # For more details see #25420.\n        sys.exit(0)\n\n    def execute(self):\n        \"\"\"\n        Given the command-line arguments, figure out which subcommand is being\n        run, create a parser appropriate to that command, and run it.\n        \"\"\"\n        try:\n            subcommand = self.argv[1]\n        except IndexError:\n            subcommand = 'help'  # Display help if no arguments were given.\n\n        # Preprocess options to extract --settings and --pythonpath.\n        # These options could affect the commands that are available, so they\n        # must be processed early.\n        parser = CommandParser(usage='%(prog)s subcommand [options] [args]', add_help=False, allow_abbrev=False)\n        parser.add_argument('--settings')\n        parser.add_argument('--pythonpath')\n        parser.add_argument('args', nargs='*')  # catch-all\n        try:\n            options, args = parser.parse_known_args(self.argv[2:])\n            handle_default_options(options)\n        except CommandError:\n            pass  # Ignore any option errors at this point.\n\n        try:\n            settings.INSTALLED_APPS\n        except ImproperlyConfigured as exc:\n            self.settings_exception = exc\n        except ImportError as exc:\n            self.settings_exception = exc\n\n        if settings.configured:\n            # Start the auto-reloading dev server even if the code is broken.\n            # The hardcoded condition is a code smell but we can't rely on a\n            # flag on the command class because we haven't located it yet.\n            if subcommand == 'runserver' and '--noreload' not in self.argv:\n                try:\n                    autoreload.check_errors(django.setup)()\n                except Exception:\n                    # The exception will be raised later in the child process\n                    # started by the autoreloader. Pretend it didn't happen by\n                    # loading an empty list of applications.\n                    apps.all_models = defaultdict(dict)\n                    apps.app_configs = {}\n                    apps.apps_ready = apps.models_ready = apps.ready = True\n\n                    # Remove options not compatible with the built-in runserver\n                    # (e.g. options for the contrib.staticfiles' runserver).\n                    # Changes here require manually testing as described in\n                    # #27522.\n                    _parser = self.fetch_command('runserver').create_parser('django', 'runserver')\n                    _options, _args = _parser.parse_known_args(self.argv[2:])\n                    for _arg in _args:\n                        self.argv.remove(_arg)\n\n            # In all other cases, django.setup() is required to succeed.\n            else:\n                django.setup()\n\n        self.autocomplete()\n\n        if subcommand == 'help':\n            if '--commands' in args:\n                sys.stdout.write(self.main_help_text(commands_only=True) + '\\n')\n            elif not options.args:\n                sys.stdout.write(self.main_help_text() + '\\n')\n            else:\n                self.fetch_command(options.args[0]).print_help(self.prog_name, options.args[0])\n        # Special-cases: We want 'django-admin --version' and\n        # 'django-admin --help' to work, for backwards compatibility.\n        elif subcommand == 'version' or self.argv[1:] == ['--version']:\n            sys.stdout.write(django.get_version() + '\\n')\n        elif self.argv[1:] in (['--help'], ['-h']):\n            sys.stdout.write(self.main_help_text() + '\\n')\n        else:\n            self.fetch_command(subcommand).run_from_argv(self.argv)\n\n\ndef execute_from_command_line(argv=None):\n    \"\"\"Run a ManagementUtility.\"\"\"\n    utility = ManagementUtility(argv)\n    utility.execute()"}
{"model_name_or_path": "agentless", "instance_id": "django__django-11790", "model_patch": "diff --git a/django/contrib/auth/forms.py b/django/contrib/auth/forms.py\nindex 4d607f6..2e4c4f4 100644\n--- a/django/contrib/auth/forms.py\n+++ b/django/contrib/auth/forms.py\n@@ -192,6 +192,7 @@ class AuthenticationForm(forms.Form):\n         # Set the max length and label for the \"username\" field.\n         self.username_field = UserModel._meta.get_field(UserModel.USERNAME_FIELD)\n         self.fields['username'].max_length = self.username_field.max_length or 254\n+        self.fields['username'].widget.attrs['maxlength'] = self.username_field.max_length or 254\n         if self.fields['username'].label is None:\n             self.fields['username'].label = capfirst(self.username_field.verbose_name)\n \n", "raw_model_patch": "diff --git a/django/contrib/auth/forms.py b/django/contrib/auth/forms.py\nindex 4d607f6..2e4c4f4 100644\n--- a/django/contrib/auth/forms.py\n+++ b/django/contrib/auth/forms.py\n@@ -192,6 +192,7 @@ class AuthenticationForm(forms.Form):\n         # Set the max length and label for the \"username\" field.\n         self.username_field = UserModel._meta.get_field(UserModel.USERNAME_FIELD)\n         self.fields['username'].max_length = self.username_field.max_length or 254\n+        self.fields['username'].widget.attrs['maxlength'] = self.username_field.max_length or 254\n         if self.fields['username'].label is None:\n             self.fields['username'].label = capfirst(self.username_field.verbose_name)\n \n", "original_file_content": "import unicodedata\n\nfrom django import forms\nfrom django.contrib.auth import (\n    authenticate, get_user_model, password_validation,\n)\nfrom django.contrib.auth.hashers import (\n    UNUSABLE_PASSWORD_PREFIX, identify_hasher,\n)\nfrom django.contrib.auth.models import User\nfrom django.contrib.auth.tokens import default_token_generator\nfrom django.contrib.sites.shortcuts import get_current_site\nfrom django.core.mail import EmailMultiAlternatives\nfrom django.template import loader\nfrom django.utils.encoding import force_bytes\nfrom django.utils.http import urlsafe_base64_encode\nfrom django.utils.text import capfirst\nfrom django.utils.translation import gettext, gettext_lazy as _\n\nUserModel = get_user_model()\n\n\nclass ReadOnlyPasswordHashWidget(forms.Widget):\n    template_name = 'auth/widgets/read_only_password_hash.html'\n    read_only = True\n\n    def get_context(self, name, value, attrs):\n        context = super().get_context(name, value, attrs)\n        summary = []\n        if not value or value.startswith(UNUSABLE_PASSWORD_PREFIX):\n            summary.append({'label': gettext(\"No password set.\")})\n        else:\n            try:\n                hasher = identify_hasher(value)\n            except ValueError:\n                summary.append({'label': gettext(\"Invalid password format or unknown hashing algorithm.\")})\n            else:\n                for key, value_ in hasher.safe_summary(value).items():\n                    summary.append({'label': gettext(key), 'value': value_})\n        context['summary'] = summary\n        return context\n\n\nclass ReadOnlyPasswordHashField(forms.Field):\n    widget = ReadOnlyPasswordHashWidget\n\n    def __init__(self, *args, **kwargs):\n        kwargs.setdefault(\"required\", False)\n        super().__init__(*args, **kwargs)\n\n    def bound_data(self, data, initial):\n        # Always return initial because the widget doesn't\n        # render an input field.\n        return initial\n\n    def has_changed(self, initial, data):\n        return False\n\n\nclass UsernameField(forms.CharField):\n    def to_python(self, value):\n        return unicodedata.normalize('NFKC', super().to_python(value))\n\n    def widget_attrs(self, widget):\n        return {\n            **super().widget_attrs(widget),\n            'autocapitalize': 'none',\n            'autocomplete': 'username',\n        }\n\n\nclass UserCreationForm(forms.ModelForm):\n    \"\"\"\n    A form that creates a user, with no privileges, from the given username and\n    password.\n    \"\"\"\n    error_messages = {\n        'password_mismatch': _('The two password fields didn\u2019t match.'),\n    }\n    password1 = forms.CharField(\n        label=_(\"Password\"),\n        strip=False,\n        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password'}),\n        help_text=password_validation.password_validators_help_text_html(),\n    )\n    password2 = forms.CharField(\n        label=_(\"Password confirmation\"),\n        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password'}),\n        strip=False,\n        help_text=_(\"Enter the same password as before, for verification.\"),\n    )\n\n    class Meta:\n        model = User\n        fields = (\"username\",)\n        field_classes = {'username': UsernameField}\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if self._meta.model.USERNAME_FIELD in self.fields:\n            self.fields[self._meta.model.USERNAME_FIELD].widget.attrs['autofocus'] = True\n\n    def clean_password2(self):\n        password1 = self.cleaned_data.get(\"password1\")\n        password2 = self.cleaned_data.get(\"password2\")\n        if password1 and password2 and password1 != password2:\n            raise forms.ValidationError(\n                self.error_messages['password_mismatch'],\n                code='password_mismatch',\n            )\n        return password2\n\n    def _post_clean(self):\n        super()._post_clean()\n        # Validate the password after self.instance is updated with form data\n        # by super().\n        password = self.cleaned_data.get('password2')\n        if password:\n            try:\n                password_validation.validate_password(password, self.instance)\n            except forms.ValidationError as error:\n                self.add_error('password2', error)\n\n    def save(self, commit=True):\n        user = super().save(commit=False)\n        user.set_password(self.cleaned_data[\"password1\"])\n        if commit:\n            user.save()\n        return user\n\n\nclass UserChangeForm(forms.ModelForm):\n    password = ReadOnlyPasswordHashField(\n        label=_(\"Password\"),\n        help_text=_(\n            'Raw passwords are not stored, so there is no way to see this '\n            'user\u2019s password, but you can change the password using '\n            '<a href=\"{}\">this form</a>.'\n        ),\n    )\n\n    class Meta:\n        model = User\n        fields = '__all__'\n        field_classes = {'username': UsernameField}\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        password = self.fields.get('password')\n        if password:\n            password.help_text = password.help_text.format('../password/')\n        user_permissions = self.fields.get('user_permissions')\n        if user_permissions:\n            user_permissions.queryset = user_permissions.queryset.select_related('content_type')\n\n    def clean_password(self):\n        # Regardless of what the user provides, return the initial value.\n        # This is done here, rather than on the field, because the\n        # field does not have access to the initial value\n        return self.initial.get('password')\n\n\nclass AuthenticationForm(forms.Form):\n    \"\"\"\n    Base class for authenticating users. Extend this to get a form that accepts\n    username/password logins.\n    \"\"\"\n    username = UsernameField(widget=forms.TextInput(attrs={'autofocus': True}))\n    password = forms.CharField(\n        label=_(\"Password\"),\n        strip=False,\n        widget=forms.PasswordInput(attrs={'autocomplete': 'current-password'}),\n    )\n\n    error_messages = {\n        'invalid_login': _(\n            \"Please enter a correct %(username)s and password. Note that both \"\n            \"fields may be case-sensitive.\"\n        ),\n        'inactive': _(\"This account is inactive.\"),\n    }\n\n    def __init__(self, request=None, *args, **kwargs):\n        \"\"\"\n        The 'request' parameter is set for custom auth use by subclasses.\n        The form data comes in via the standard 'data' kwarg.\n        \"\"\"\n        self.request = request\n        self.user_cache = None\n        super().__init__(*args, **kwargs)\n\n        # Set the max length and label for the \"username\" field.\n        self.username_field = UserModel._meta.get_field(UserModel.USERNAME_FIELD)\n        self.fields['username'].max_length = self.username_field.max_length or 254\n        if self.fields['username'].label is None:\n            self.fields['username'].label = capfirst(self.username_field.verbose_name)\n\n    def clean(self):\n        username = self.cleaned_data.get('username')\n        password = self.cleaned_data.get('password')\n\n        if username is not None and password:\n            self.user_cache = authenticate(self.request, username=username, password=password)\n            if self.user_cache is None:\n                raise self.get_invalid_login_error()\n            else:\n                self.confirm_login_allowed(self.user_cache)\n\n        return self.cleaned_data\n\n    def confirm_login_allowed(self, user):\n        \"\"\"\n        Controls whether the given User may log in. This is a policy setting,\n        independent of end-user authentication. This default behavior is to\n        allow login by active users, and reject login by inactive users.\n\n        If the given user cannot log in, this method should raise a\n        ``forms.ValidationError``.\n\n        If the given user may log in, this method should return None.\n        \"\"\"\n        if not user.is_active:\n            raise forms.ValidationError(\n                self.error_messages['inactive'],\n                code='inactive',\n            )\n\n    def get_user(self):\n        return self.user_cache\n\n    def get_invalid_login_error(self):\n        return forms.ValidationError(\n            self.error_messages['invalid_login'],\n            code='invalid_login',\n            params={'username': self.username_field.verbose_name},\n        )\n\n\nclass PasswordResetForm(forms.Form):\n    email = forms.EmailField(\n        label=_(\"Email\"),\n        max_length=254,\n        widget=forms.EmailInput(attrs={'autocomplete': 'email'})\n    )\n\n    def send_mail(self, subject_template_name, email_template_name,\n                  context, from_email, to_email, html_email_template_name=None):\n        \"\"\"\n        Send a django.core.mail.EmailMultiAlternatives to `to_email`.\n        \"\"\"\n        subject = loader.render_to_string(subject_template_name, context)\n        # Email subject *must not* contain newlines\n        subject = ''.join(subject.splitlines())\n        body = loader.render_to_string(email_template_name, context)\n\n        email_message = EmailMultiAlternatives(subject, body, from_email, [to_email])\n        if html_email_template_name is not None:\n            html_email = loader.render_to_string(html_email_template_name, context)\n            email_message.attach_alternative(html_email, 'text/html')\n\n        email_message.send()\n\n    def get_users(self, email):\n        \"\"\"Given an email, return matching user(s) who should receive a reset.\n\n        This allows subclasses to more easily customize the default policies\n        that prevent inactive users and users with unusable passwords from\n        resetting their password.\n        \"\"\"\n        active_users = UserModel._default_manager.filter(**{\n            '%s__iexact' % UserModel.get_email_field_name(): email,\n            'is_active': True,\n        })\n        return (u for u in active_users if u.has_usable_password())\n\n    def save(self, domain_override=None,\n             subject_template_name='registration/password_reset_subject.txt',\n             email_template_name='registration/password_reset_email.html',\n             use_https=False, token_generator=default_token_generator,\n             from_email=None, request=None, html_email_template_name=None,\n             extra_email_context=None):\n        \"\"\"\n        Generate a one-use only link for resetting password and send it to the\n        user.\n        \"\"\"\n        email = self.cleaned_data[\"email\"]\n        for user in self.get_users(email):\n            if not domain_override:\n                current_site = get_current_site(request)\n                site_name = current_site.name\n                domain = current_site.domain\n            else:\n                site_name = domain = domain_override\n            context = {\n                'email': email,\n                'domain': domain,\n                'site_name': site_name,\n                'uid': urlsafe_base64_encode(force_bytes(user.pk)),\n                'user': user,\n                'token': token_generator.make_token(user),\n                'protocol': 'https' if use_https else 'http',\n                **(extra_email_context or {}),\n            }\n            self.send_mail(\n                subject_template_name, email_template_name, context, from_email,\n                email, html_email_template_name=html_email_template_name,\n            )\n\n\nclass SetPasswordForm(forms.Form):\n    \"\"\"\n    A form that lets a user change set their password without entering the old\n    password\n    \"\"\"\n    error_messages = {\n        'password_mismatch': _('The two password fields didn\u2019t match.'),\n    }\n    new_password1 = forms.CharField(\n        label=_(\"New password\"),\n        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password'}),\n        strip=False,\n        help_text=password_validation.password_validators_help_text_html(),\n    )\n    new_password2 = forms.CharField(\n        label=_(\"New password confirmation\"),\n        strip=False,\n        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password'}),\n    )\n\n    def __init__(self, user, *args, **kwargs):\n        self.user = user\n        super().__init__(*args, **kwargs)\n\n    def clean_new_password2(self):\n        password1 = self.cleaned_data.get('new_password1')\n        password2 = self.cleaned_data.get('new_password2')\n        if password1 and password2:\n            if password1 != password2:\n                raise forms.ValidationError(\n                    self.error_messages['password_mismatch'],\n                    code='password_mismatch',\n                )\n        password_validation.validate_password(password2, self.user)\n        return password2\n\n    def save(self, commit=True):\n        password = self.cleaned_data[\"new_password1\"]\n        self.user.set_password(password)\n        if commit:\n            self.user.save()\n        return self.user\n\n\nclass PasswordChangeForm(SetPasswordForm):\n    \"\"\"\n    A form that lets a user change their password by entering their old\n    password.\n    \"\"\"\n    error_messages = {\n        **SetPasswordForm.error_messages,\n        'password_incorrect': _(\"Your old password was entered incorrectly. Please enter it again.\"),\n    }\n    old_password = forms.CharField(\n        label=_(\"Old password\"),\n        strip=False,\n        widget=forms.PasswordInput(attrs={'autocomplete': 'current-password', 'autofocus': True}),\n    )\n\n    field_order = ['old_password', 'new_password1', 'new_password2']\n\n    def clean_old_password(self):\n        \"\"\"\n        Validate that the old_password field is correct.\n        \"\"\"\n        old_password = self.cleaned_data[\"old_password\"]\n        if not self.user.check_password(old_password):\n            raise forms.ValidationError(\n                self.error_messages['password_incorrect'],\n                code='password_incorrect',\n            )\n        return old_password\n\n\nclass AdminPasswordChangeForm(forms.Form):\n    \"\"\"\n    A form used to change the password of a user in the admin interface.\n    \"\"\"\n    error_messages = {\n        'password_mismatch': _('The two password fields didn\u2019t match.'),\n    }\n    required_css_class = 'required'\n    password1 = forms.CharField(\n        label=_(\"Password\"),\n        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password', 'autofocus': True}),\n        strip=False,\n        help_text=password_validation.password_validators_help_text_html(),\n    )\n    password2 = forms.CharField(\n        label=_(\"Password (again)\"),\n        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password'}),\n        strip=False,\n        help_text=_(\"Enter the same password as before, for verification.\"),\n    )\n\n    def __init__(self, user, *args, **kwargs):\n        self.user = user\n        super().__init__(*args, **kwargs)\n\n    def clean_password2(self):\n        password1 = self.cleaned_data.get('password1')\n        password2 = self.cleaned_data.get('password2')\n        if password1 and password2:\n            if password1 != password2:\n                raise forms.ValidationError(\n                    self.error_messages['password_mismatch'],\n                    code='password_mismatch',\n                )\n        password_validation.validate_password(password2, self.user)\n        return password2\n\n    def save(self, commit=True):\n        \"\"\"Save the new password.\"\"\"\n        password = self.cleaned_data[\"password1\"]\n        self.user.set_password(password)\n        if commit:\n            self.user.save()\n        return self.user\n\n    @property\n    def changed_data(self):\n        data = super().changed_data\n        for name in self.fields:\n            if name not in data:\n                return []\n        return ['password']"}
{"model_name_or_path": "agentless", "instance_id": "django__django-11848", "model_patch": "diff --git a/django/utils/http.py b/django/utils/http.py\nindex ab0ad07..7e5b565 100644\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -176,7 +176,9 @@ def parse_http_date(date):\n     try:\n         year = int(m.group('year'))\n         if year < 100:\n-            if year < 70:\n+            # Get the current year\n+            current_year = datetime.datetime.utcnow().year\n+            if year < (current_year % 100) - 50:\n                 year += 2000\n             else:\n                 year += 1900\n", "raw_model_patch": "diff --git a/django/utils/http.py b/django/utils/http.py\nindex ab0ad07..7e5b565 100644\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -176,7 +176,9 @@ def parse_http_date(date):\n     try:\n         year = int(m.group('year'))\n         if year < 100:\n-            if year < 70:\n+            # Get the current year\n+            current_year = datetime.datetime.utcnow().year\n+            if year < (current_year % 100) - 50:\n                 year += 2000\n             else:\n                 year += 1900\n", "original_file_content": "import base64\nimport calendar\nimport datetime\nimport re\nimport unicodedata\nimport warnings\nfrom binascii import Error as BinasciiError\nfrom email.utils import formatdate\nfrom urllib.parse import (\n    ParseResult, SplitResult, _coerce_args, _splitnetloc, _splitparams, quote,\n    quote_plus, scheme_chars, unquote, unquote_plus,\n    urlencode as original_urlencode, uses_params,\n)\n\nfrom django.core.exceptions import TooManyFieldsSent\nfrom django.utils.datastructures import MultiValueDict\nfrom django.utils.deprecation import RemovedInDjango40Warning\nfrom django.utils.functional import keep_lazy_text\n\n# based on RFC 7232, Appendix C\nETAG_MATCH = re.compile(r'''\n    \\A(      # start of string and capture group\n    (?:W/)?  # optional weak indicator\n    \"        # opening quote\n    [^\"]*    # any sequence of non-quote characters\n    \"        # end quote\n    )\\Z      # end of string and capture group\n''', re.X)\n\nMONTHS = 'jan feb mar apr may jun jul aug sep oct nov dec'.split()\n__D = r'(?P<day>\\d{2})'\n__D2 = r'(?P<day>[ \\d]\\d)'\n__M = r'(?P<mon>\\w{3})'\n__Y = r'(?P<year>\\d{4})'\n__Y2 = r'(?P<year>\\d{2})'\n__T = r'(?P<hour>\\d{2}):(?P<min>\\d{2}):(?P<sec>\\d{2})'\nRFC1123_DATE = re.compile(r'^\\w{3}, %s %s %s %s GMT$' % (__D, __M, __Y, __T))\nRFC850_DATE = re.compile(r'^\\w{6,9}, %s-%s-%s %s GMT$' % (__D, __M, __Y2, __T))\nASCTIME_DATE = re.compile(r'^\\w{3} %s %s %s %s$' % (__M, __D2, __T, __Y))\n\nRFC3986_GENDELIMS = \":/?#[]@\"\nRFC3986_SUBDELIMS = \"!$&'()*+,;=\"\n\nFIELDS_MATCH = re.compile('[&;]')\n\n\n@keep_lazy_text\ndef urlquote(url, safe='/'):\n    \"\"\"\n    A legacy compatibility wrapper to Python's urllib.parse.quote() function.\n    (was used for unicode handling on Python 2)\n    \"\"\"\n    warnings.warn(\n        'django.utils.http.urlquote() is deprecated in favor of '\n        'urllib.parse.quote().',\n        RemovedInDjango40Warning, stacklevel=2,\n    )\n    return quote(url, safe)\n\n\n@keep_lazy_text\ndef urlquote_plus(url, safe=''):\n    \"\"\"\n    A legacy compatibility wrapper to Python's urllib.parse.quote_plus()\n    function. (was used for unicode handling on Python 2)\n    \"\"\"\n    warnings.warn(\n        'django.utils.http.urlquote_plus() is deprecated in favor of '\n        'urllib.parse.quote_plus(),',\n        RemovedInDjango40Warning, stacklevel=2,\n    )\n    return quote_plus(url, safe)\n\n\n@keep_lazy_text\ndef urlunquote(quoted_url):\n    \"\"\"\n    A legacy compatibility wrapper to Python's urllib.parse.unquote() function.\n    (was used for unicode handling on Python 2)\n    \"\"\"\n    warnings.warn(\n        'django.utils.http.urlunquote() is deprecated in favor of '\n        'urllib.parse.unquote().',\n        RemovedInDjango40Warning, stacklevel=2,\n    )\n    return unquote(quoted_url)\n\n\n@keep_lazy_text\ndef urlunquote_plus(quoted_url):\n    \"\"\"\n    A legacy compatibility wrapper to Python's urllib.parse.unquote_plus()\n    function. (was used for unicode handling on Python 2)\n    \"\"\"\n    warnings.warn(\n        'django.utils.http.urlunquote_plus() is deprecated in favor of '\n        'urllib.parse.unquote_plus().',\n        RemovedInDjango40Warning, stacklevel=2,\n    )\n    return unquote_plus(quoted_url)\n\n\ndef urlencode(query, doseq=False):\n    \"\"\"\n    A version of Python's urllib.parse.urlencode() function that can operate on\n    MultiValueDict and non-string values.\n    \"\"\"\n    if isinstance(query, MultiValueDict):\n        query = query.lists()\n    elif hasattr(query, 'items'):\n        query = query.items()\n    query_params = []\n    for key, value in query:\n        if value is None:\n            raise TypeError(\n                \"Cannot encode None for key '%s' in a query string. Did you \"\n                \"mean to pass an empty string or omit the value?\" % key\n            )\n        elif not doseq or isinstance(value, (str, bytes)):\n            query_val = value\n        else:\n            try:\n                itr = iter(value)\n            except TypeError:\n                query_val = value\n            else:\n                # Consume generators and iterators, when doseq=True, to\n                # work around https://bugs.python.org/issue31706.\n                query_val = []\n                for item in itr:\n                    if item is None:\n                        raise TypeError(\n                            \"Cannot encode None for key '%s' in a query \"\n                            \"string. Did you mean to pass an empty string or \"\n                            \"omit the value?\" % key\n                        )\n                    elif not isinstance(item, bytes):\n                        item = str(item)\n                    query_val.append(item)\n        query_params.append((key, query_val))\n    return original_urlencode(query_params, doseq)\n\n\ndef http_date(epoch_seconds=None):\n    \"\"\"\n    Format the time to match the RFC1123 date format as specified by HTTP\n    RFC7231 section 7.1.1.1.\n\n    `epoch_seconds` is a floating point number expressed in seconds since the\n    epoch, in UTC - such as that outputted by time.time(). If set to None, it\n    defaults to the current time.\n\n    Output a string in the format 'Wdy, DD Mon YYYY HH:MM:SS GMT'.\n    \"\"\"\n    return formatdate(epoch_seconds, usegmt=True)\n\n\ndef parse_http_date(date):\n    \"\"\"\n    Parse a date format as specified by HTTP RFC7231 section 7.1.1.1.\n\n    The three formats allowed by the RFC are accepted, even if only the first\n    one is still in widespread use.\n\n    Return an integer expressed in seconds since the epoch, in UTC.\n    \"\"\"\n    # email.utils.parsedate() does the job for RFC1123 dates; unfortunately\n    # RFC7231 makes it mandatory to support RFC850 dates too. So we roll\n    # our own RFC-compliant parsing.\n    for regex in RFC1123_DATE, RFC850_DATE, ASCTIME_DATE:\n        m = regex.match(date)\n        if m is not None:\n            break\n    else:\n        raise ValueError(\"%r is not in a valid HTTP date format\" % date)\n    try:\n        year = int(m.group('year'))\n        if year < 100:\n            if year < 70:\n                year += 2000\n            else:\n                year += 1900\n        month = MONTHS.index(m.group('mon').lower()) + 1\n        day = int(m.group('day'))\n        hour = int(m.group('hour'))\n        min = int(m.group('min'))\n        sec = int(m.group('sec'))\n        result = datetime.datetime(year, month, day, hour, min, sec)\n        return calendar.timegm(result.utctimetuple())\n    except Exception as exc:\n        raise ValueError(\"%r is not a valid date\" % date) from exc\n\n\ndef parse_http_date_safe(date):\n    \"\"\"\n    Same as parse_http_date, but return None if the input is invalid.\n    \"\"\"\n    try:\n        return parse_http_date(date)\n    except Exception:\n        pass\n\n\n# Base 36 functions: useful for generating compact URLs\n\ndef base36_to_int(s):\n    \"\"\"\n    Convert a base 36 string to an int. Raise ValueError if the input won't fit\n    into an int.\n    \"\"\"\n    # To prevent overconsumption of server resources, reject any\n    # base36 string that is longer than 13 base36 digits (13 digits\n    # is sufficient to base36-encode any 64-bit integer)\n    if len(s) > 13:\n        raise ValueError(\"Base36 input too large\")\n    return int(s, 36)\n\n\ndef int_to_base36(i):\n    \"\"\"Convert an integer to a base36 string.\"\"\"\n    char_set = '0123456789abcdefghijklmnopqrstuvwxyz'\n    if i < 0:\n        raise ValueError(\"Negative base36 conversion input.\")\n    if i < 36:\n        return char_set[i]\n    b36 = ''\n    while i != 0:\n        i, n = divmod(i, 36)\n        b36 = char_set[n] + b36\n    return b36\n\n\ndef urlsafe_base64_encode(s):\n    \"\"\"\n    Encode a bytestring to a base64 string for use in URLs. Strip any trailing\n    equal signs.\n    \"\"\"\n    return base64.urlsafe_b64encode(s).rstrip(b'\\n=').decode('ascii')\n\n\ndef urlsafe_base64_decode(s):\n    \"\"\"\n    Decode a base64 encoded string. Add back any trailing equal signs that\n    might have been stripped.\n    \"\"\"\n    s = s.encode()\n    try:\n        return base64.urlsafe_b64decode(s.ljust(len(s) + len(s) % 4, b'='))\n    except (LookupError, BinasciiError) as e:\n        raise ValueError(e)\n\n\ndef parse_etags(etag_str):\n    \"\"\"\n    Parse a string of ETags given in an If-None-Match or If-Match header as\n    defined by RFC 7232. Return a list of quoted ETags, or ['*'] if all ETags\n    should be matched.\n    \"\"\"\n    if etag_str.strip() == '*':\n        return ['*']\n    else:\n        # Parse each ETag individually, and return any that are valid.\n        etag_matches = (ETAG_MATCH.match(etag.strip()) for etag in etag_str.split(','))\n        return [match.group(1) for match in etag_matches if match]\n\n\ndef quote_etag(etag_str):\n    \"\"\"\n    If the provided string is already a quoted ETag, return it. Otherwise, wrap\n    the string in quotes, making it a strong ETag.\n    \"\"\"\n    if ETAG_MATCH.match(etag_str):\n        return etag_str\n    else:\n        return '\"%s\"' % etag_str\n\n\ndef is_same_domain(host, pattern):\n    \"\"\"\n    Return ``True`` if the host is either an exact match or a match\n    to the wildcard pattern.\n\n    Any pattern beginning with a period matches a domain and all of its\n    subdomains. (e.g. ``.example.com`` matches ``example.com`` and\n    ``foo.example.com``). Anything else is an exact string match.\n    \"\"\"\n    if not pattern:\n        return False\n\n    pattern = pattern.lower()\n    return (\n        pattern[0] == '.' and (host.endswith(pattern) or host == pattern[1:]) or\n        pattern == host\n    )\n\n\ndef url_has_allowed_host_and_scheme(url, allowed_hosts, require_https=False):\n    \"\"\"\n    Return ``True`` if the url uses an allowed host and a safe scheme.\n\n    Always return ``False`` on an empty url.\n\n    If ``require_https`` is ``True``, only 'https' will be considered a valid\n    scheme, as opposed to 'http' and 'https' with the default, ``False``.\n\n    Note: \"True\" doesn't entail that a URL is \"safe\". It may still be e.g.\n    quoted incorrectly. Ensure to also use django.utils.encoding.iri_to_uri()\n    on the path component of untrusted URLs.\n    \"\"\"\n    if url is not None:\n        url = url.strip()\n    if not url:\n        return False\n    if allowed_hosts is None:\n        allowed_hosts = set()\n    elif isinstance(allowed_hosts, str):\n        allowed_hosts = {allowed_hosts}\n    # Chrome treats \\ completely as / in paths but it could be part of some\n    # basic auth credentials so we need to check both URLs.\n    return (\n        _url_has_allowed_host_and_scheme(url, allowed_hosts, require_https=require_https) and\n        _url_has_allowed_host_and_scheme(url.replace('\\\\', '/'), allowed_hosts, require_https=require_https)\n    )\n\n\ndef is_safe_url(url, allowed_hosts, require_https=False):\n    warnings.warn(\n        'django.utils.http.is_safe_url() is deprecated in favor of '\n        'url_has_allowed_host_and_scheme().',\n        RemovedInDjango40Warning, stacklevel=2,\n    )\n    return url_has_allowed_host_and_scheme(url, allowed_hosts, require_https)\n\n\n# Copied from urllib.parse.urlparse() but uses fixed urlsplit() function.\ndef _urlparse(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 6 components:\n    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n    Return a 6-tuple: (scheme, netloc, path, params, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    splitresult = _urlsplit(url, scheme, allow_fragments)\n    scheme, netloc, url, query, fragment = splitresult\n    if scheme in uses_params and ';' in url:\n        url, params = _splitparams(url)\n    else:\n        params = ''\n    result = ParseResult(scheme, netloc, url, params, query, fragment)\n    return _coerce_result(result)\n\n\n# Copied from urllib.parse.urlsplit() with\n# https://github.com/python/cpython/pull/661 applied.\ndef _urlsplit(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 5 components:\n    <scheme>://<netloc>/<path>?<query>#<fragment>\n    Return a 5-tuple: (scheme, netloc, path, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    netloc = query = fragment = ''\n    i = url.find(':')\n    if i > 0:\n        for c in url[:i]:\n            if c not in scheme_chars:\n                break\n        else:\n            scheme, url = url[:i].lower(), url[i + 1:]\n\n    if url[:2] == '//':\n        netloc, url = _splitnetloc(url, 2)\n        if (('[' in netloc and ']' not in netloc) or\n                (']' in netloc and '[' not in netloc)):\n            raise ValueError(\"Invalid IPv6 URL\")\n    if allow_fragments and '#' in url:\n        url, fragment = url.split('#', 1)\n    if '?' in url:\n        url, query = url.split('?', 1)\n    v = SplitResult(scheme, netloc, url, query, fragment)\n    return _coerce_result(v)\n\n\ndef _url_has_allowed_host_and_scheme(url, allowed_hosts, require_https=False):\n    # Chrome considers any URL with more than two slashes to be absolute, but\n    # urlparse is not so flexible. Treat any url with three slashes as unsafe.\n    if url.startswith('///'):\n        return False\n    try:\n        url_info = _urlparse(url)\n    except ValueError:  # e.g. invalid IPv6 addresses\n        return False\n    # Forbid URLs like http:///example.com - with a scheme, but without a hostname.\n    # In that URL, example.com is not the hostname but, a path component. However,\n    # Chrome will still consider example.com to be the hostname, so we must not\n    # allow this syntax.\n    if not url_info.netloc and url_info.scheme:\n        return False\n    # Forbid URLs that start with control characters. Some browsers (like\n    # Chrome) ignore quite a few control characters at the start of a\n    # URL and might consider the URL as scheme relative.\n    if unicodedata.category(url[0])[0] == 'C':\n        return False\n    scheme = url_info.scheme\n    # Consider URLs without a scheme (e.g. //example.com/p) to be http.\n    if not url_info.scheme and url_info.netloc:\n        scheme = 'http'\n    valid_schemes = ['https'] if require_https else ['http', 'https']\n    return ((not url_info.netloc or url_info.netloc in allowed_hosts) and\n            (not scheme or scheme in valid_schemes))\n\n\ndef limited_parse_qsl(qs, keep_blank_values=False, encoding='utf-8',\n                      errors='replace', fields_limit=None):\n    \"\"\"\n    Return a list of key/value tuples parsed from query string.\n\n    Copied from urlparse with an additional \"fields_limit\" argument.\n    Copyright (C) 2013 Python Software Foundation (see LICENSE.python).\n\n    Arguments:\n\n    qs: percent-encoded query string to be parsed\n\n    keep_blank_values: flag indicating whether blank values in\n        percent-encoded queries should be treated as blank strings. A\n        true value indicates that blanks should be retained as blank\n        strings. The default false value indicates that blank values\n        are to be ignored and treated as if they were  not included.\n\n    encoding and errors: specify how to decode percent-encoded sequences\n        into Unicode characters, as accepted by the bytes.decode() method.\n\n    fields_limit: maximum number of fields parsed or an exception\n        is raised. None means no limit and is the default.\n    \"\"\"\n    if fields_limit:\n        pairs = FIELDS_MATCH.split(qs, fields_limit)\n        if len(pairs) > fields_limit:\n            raise TooManyFieldsSent(\n                'The number of GET/POST parameters exceeded '\n                'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'\n            )\n    else:\n        pairs = FIELDS_MATCH.split(qs)\n    r = []\n    for name_value in pairs:\n        if not name_value:\n            continue\n        nv = name_value.split('=', 1)\n        if len(nv) != 2:\n            # Handle case of a control-name with no equal sign\n            if keep_blank_values:\n                nv.append('')\n            else:\n                continue\n        if nv[1] or keep_blank_values:\n            name = nv[0].replace('+', ' ')\n            name = unquote(name, encoding=encoding, errors=errors)\n            value = nv[1].replace('+', ' ')\n            value = unquote(value, encoding=encoding, errors=errors)\n            r.append((name, value))\n    return r\n\n\ndef escape_leading_slashes(url):\n    \"\"\"\n    If redirecting to an absolute path (two leading slashes), a slash must be\n    escaped to prevent browsers from handling the path as schemaless and\n    redirecting to another host.\n    \"\"\"\n    if url.startswith('//'):\n        url = '/%2F{}'.format(url[2:])\n    return url"}
{"model_name_or_path": "agentless", "instance_id": "django__django-12143", "model_patch": "diff --git a/django/contrib/admin/options.py b/django/contrib/admin/options.py\nindex 0fb4989..b3ffb52 100644\n--- a/django/contrib/admin/options.py\n+++ b/django/contrib/admin/options.py\n@@ -1631,7 +1631,7 @@ class ModelAdmin(BaseModelAdmin):\n \n     def _get_edited_object_pks(self, request, prefix):\n         \"\"\"Return POST data values of list_editable primary keys.\"\"\"\n-        pk_pattern = re.compile(r'{}-\\d+-{}$'.format(prefix, self.model._meta.pk.name))\n+        pk_pattern = re.compile(r'{}-\\d+-{}$'.format(re.escape(prefix), self.model._meta.pk.name))\n         return [value for key, value in request.POST.items() if pk_pattern.match(key)]\n \n     def _get_list_editable_queryset(self, request, prefix):\n", "raw_model_patch": "diff --git a/django/contrib/admin/options.py b/django/contrib/admin/options.py\nindex 0fb4989..b3ffb52 100644\n--- a/django/contrib/admin/options.py\n+++ b/django/contrib/admin/options.py\n@@ -1631,7 +1631,7 @@ class ModelAdmin(BaseModelAdmin):\n \n     def _get_edited_object_pks(self, request, prefix):\n         \"\"\"Return POST data values of list_editable primary keys.\"\"\"\n-        pk_pattern = re.compile(r'{}-\\d+-{}$'.format(prefix, self.model._meta.pk.name))\n+        pk_pattern = re.compile(r'{}-\\d+-{}$'.format(re.escape(prefix), self.model._meta.pk.name))\n         return [value for key, value in request.POST.items() if pk_pattern.match(key)]\n \n     def _get_list_editable_queryset(self, request, prefix):\n", "original_file_content": "import copy\nimport json\nimport operator\nimport re\nfrom functools import partial, reduce, update_wrapper\nfrom urllib.parse import quote as urlquote\n\nfrom django import forms\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.admin import helpers, widgets\nfrom django.contrib.admin.checks import (\n    BaseModelAdminChecks, InlineModelAdminChecks, ModelAdminChecks,\n)\nfrom django.contrib.admin.exceptions import DisallowedModelAdminToField\nfrom django.contrib.admin.templatetags.admin_urls import add_preserved_filters\nfrom django.contrib.admin.utils import (\n    NestedObjects, construct_change_message, flatten_fieldsets,\n    get_deleted_objects, lookup_needs_distinct, model_format_dict,\n    model_ngettext, quote, unquote,\n)\nfrom django.contrib.admin.views.autocomplete import AutocompleteJsonView\nfrom django.contrib.admin.widgets import (\n    AutocompleteSelect, AutocompleteSelectMultiple,\n)\nfrom django.contrib.auth import get_permission_codename\nfrom django.core.exceptions import (\n    FieldDoesNotExist, FieldError, PermissionDenied, ValidationError,\n)\nfrom django.core.paginator import Paginator\nfrom django.db import models, router, transaction\nfrom django.db.models.constants import LOOKUP_SEP\nfrom django.db.models.fields import BLANK_CHOICE_DASH\nfrom django.forms.formsets import DELETION_FIELD_NAME, all_valid\nfrom django.forms.models import (\n    BaseInlineFormSet, inlineformset_factory, modelform_defines_fields,\n    modelform_factory, modelformset_factory,\n)\nfrom django.forms.widgets import CheckboxSelectMultiple, SelectMultiple\nfrom django.http import HttpResponseRedirect\nfrom django.http.response import HttpResponseBase\nfrom django.template.response import SimpleTemplateResponse, TemplateResponse\nfrom django.urls import reverse\nfrom django.utils.decorators import method_decorator\nfrom django.utils.html import format_html\nfrom django.utils.http import urlencode\nfrom django.utils.safestring import mark_safe\nfrom django.utils.text import capfirst, format_lazy, get_text_list\nfrom django.utils.translation import gettext as _, ngettext\nfrom django.views.decorators.csrf import csrf_protect\nfrom django.views.generic import RedirectView\n\nIS_POPUP_VAR = '_popup'\nTO_FIELD_VAR = '_to_field'\n\n\nHORIZONTAL, VERTICAL = 1, 2\n\n\ndef get_content_type_for_model(obj):\n    # Since this module gets imported in the application's root package,\n    # it cannot import models from other applications at the module level.\n    from django.contrib.contenttypes.models import ContentType\n    return ContentType.objects.get_for_model(obj, for_concrete_model=False)\n\n\ndef get_ul_class(radio_style):\n    return 'radiolist' if radio_style == VERTICAL else 'radiolist inline'\n\n\nclass IncorrectLookupParameters(Exception):\n    pass\n\n\n# Defaults for formfield_overrides. ModelAdmin subclasses can change this\n# by adding to ModelAdmin.formfield_overrides.\n\nFORMFIELD_FOR_DBFIELD_DEFAULTS = {\n    models.DateTimeField: {\n        'form_class': forms.SplitDateTimeField,\n        'widget': widgets.AdminSplitDateTime\n    },\n    models.DateField: {'widget': widgets.AdminDateWidget},\n    models.TimeField: {'widget': widgets.AdminTimeWidget},\n    models.TextField: {'widget': widgets.AdminTextareaWidget},\n    models.URLField: {'widget': widgets.AdminURLFieldWidget},\n    models.IntegerField: {'widget': widgets.AdminIntegerFieldWidget},\n    models.BigIntegerField: {'widget': widgets.AdminBigIntegerFieldWidget},\n    models.CharField: {'widget': widgets.AdminTextInputWidget},\n    models.ImageField: {'widget': widgets.AdminFileWidget},\n    models.FileField: {'widget': widgets.AdminFileWidget},\n    models.EmailField: {'widget': widgets.AdminEmailInputWidget},\n    models.UUIDField: {'widget': widgets.AdminUUIDInputWidget},\n}\n\ncsrf_protect_m = method_decorator(csrf_protect)\n\n\nclass BaseModelAdmin(metaclass=forms.MediaDefiningClass):\n    \"\"\"Functionality common to both ModelAdmin and InlineAdmin.\"\"\"\n\n    autocomplete_fields = ()\n    raw_id_fields = ()\n    fields = None\n    exclude = None\n    fieldsets = None\n    form = forms.ModelForm\n    filter_vertical = ()\n    filter_horizontal = ()\n    radio_fields = {}\n    prepopulated_fields = {}\n    formfield_overrides = {}\n    readonly_fields = ()\n    ordering = None\n    sortable_by = None\n    view_on_site = True\n    show_full_result_count = True\n    checks_class = BaseModelAdminChecks\n\n    def check(self, **kwargs):\n        return self.checks_class().check(self, **kwargs)\n\n    def __init__(self):\n        # Merge FORMFIELD_FOR_DBFIELD_DEFAULTS with the formfield_overrides\n        # rather than simply overwriting.\n        overrides = copy.deepcopy(FORMFIELD_FOR_DBFIELD_DEFAULTS)\n        for k, v in self.formfield_overrides.items():\n            overrides.setdefault(k, {}).update(v)\n        self.formfield_overrides = overrides\n\n    def formfield_for_dbfield(self, db_field, request, **kwargs):\n        \"\"\"\n        Hook for specifying the form Field instance for a given database Field\n        instance.\n\n        If kwargs are given, they're passed to the form Field's constructor.\n        \"\"\"\n        # If the field specifies choices, we don't need to look for special\n        # admin widgets - we just need to use a select widget of some kind.\n        if db_field.choices:\n            return self.formfield_for_choice_field(db_field, request, **kwargs)\n\n        # ForeignKey or ManyToManyFields\n        if isinstance(db_field, (models.ForeignKey, models.ManyToManyField)):\n            # Combine the field kwargs with any options for formfield_overrides.\n            # Make sure the passed in **kwargs override anything in\n            # formfield_overrides because **kwargs is more specific, and should\n            # always win.\n            if db_field.__class__ in self.formfield_overrides:\n                kwargs = {**self.formfield_overrides[db_field.__class__], **kwargs}\n\n            # Get the correct formfield.\n            if isinstance(db_field, models.ForeignKey):\n                formfield = self.formfield_for_foreignkey(db_field, request, **kwargs)\n            elif isinstance(db_field, models.ManyToManyField):\n                formfield = self.formfield_for_manytomany(db_field, request, **kwargs)\n\n            # For non-raw_id fields, wrap the widget with a wrapper that adds\n            # extra HTML -- the \"add other\" interface -- to the end of the\n            # rendered output. formfield can be None if it came from a\n            # OneToOneField with parent_link=True or a M2M intermediary.\n            if formfield and db_field.name not in self.raw_id_fields:\n                related_modeladmin = self.admin_site._registry.get(db_field.remote_field.model)\n                wrapper_kwargs = {}\n                if related_modeladmin:\n                    wrapper_kwargs.update(\n                        can_add_related=related_modeladmin.has_add_permission(request),\n                        can_change_related=related_modeladmin.has_change_permission(request),\n                        can_delete_related=related_modeladmin.has_delete_permission(request),\n                        can_view_related=related_modeladmin.has_view_permission(request),\n                    )\n                formfield.widget = widgets.RelatedFieldWidgetWrapper(\n                    formfield.widget, db_field.remote_field, self.admin_site, **wrapper_kwargs\n                )\n\n            return formfield\n\n        # If we've got overrides for the formfield defined, use 'em. **kwargs\n        # passed to formfield_for_dbfield override the defaults.\n        for klass in db_field.__class__.mro():\n            if klass in self.formfield_overrides:\n                kwargs = {**copy.deepcopy(self.formfield_overrides[klass]), **kwargs}\n                return db_field.formfield(**kwargs)\n\n        # For any other type of field, just call its formfield() method.\n        return db_field.formfield(**kwargs)\n\n    def formfield_for_choice_field(self, db_field, request, **kwargs):\n        \"\"\"\n        Get a form Field for a database Field that has declared choices.\n        \"\"\"\n        # If the field is named as a radio_field, use a RadioSelect\n        if db_field.name in self.radio_fields:\n            # Avoid stomping on custom widget/choices arguments.\n            if 'widget' not in kwargs:\n                kwargs['widget'] = widgets.AdminRadioSelect(attrs={\n                    'class': get_ul_class(self.radio_fields[db_field.name]),\n                })\n            if 'choices' not in kwargs:\n                kwargs['choices'] = db_field.get_choices(\n                    include_blank=db_field.blank,\n                    blank_choice=[('', _('None'))]\n                )\n        return db_field.formfield(**kwargs)\n\n    def get_field_queryset(self, db, db_field, request):\n        \"\"\"\n        If the ModelAdmin specifies ordering, the queryset should respect that\n        ordering.  Otherwise don't specify the queryset, let the field decide\n        (return None in that case).\n        \"\"\"\n        related_admin = self.admin_site._registry.get(db_field.remote_field.model)\n        if related_admin is not None:\n            ordering = related_admin.get_ordering(request)\n            if ordering is not None and ordering != ():\n                return db_field.remote_field.model._default_manager.using(db).order_by(*ordering)\n        return None\n\n    def formfield_for_foreignkey(self, db_field, request, **kwargs):\n        \"\"\"\n        Get a form Field for a ForeignKey.\n        \"\"\"\n        db = kwargs.get('using')\n\n        if 'widget' not in kwargs:\n            if db_field.name in self.get_autocomplete_fields(request):\n                kwargs['widget'] = AutocompleteSelect(db_field.remote_field, self.admin_site, using=db)\n            elif db_field.name in self.raw_id_fields:\n                kwargs['widget'] = widgets.ForeignKeyRawIdWidget(db_field.remote_field, self.admin_site, using=db)\n            elif db_field.name in self.radio_fields:\n                kwargs['widget'] = widgets.AdminRadioSelect(attrs={\n                    'class': get_ul_class(self.radio_fields[db_field.name]),\n                })\n                kwargs['empty_label'] = _('None') if db_field.blank else None\n\n        if 'queryset' not in kwargs:\n            queryset = self.get_field_queryset(db, db_field, request)\n            if queryset is not None:\n                kwargs['queryset'] = queryset\n\n        return db_field.formfield(**kwargs)\n\n    def formfield_for_manytomany(self, db_field, request, **kwargs):\n        \"\"\"\n        Get a form Field for a ManyToManyField.\n        \"\"\"\n        # If it uses an intermediary model that isn't auto created, don't show\n        # a field in admin.\n        if not db_field.remote_field.through._meta.auto_created:\n            return None\n        db = kwargs.get('using')\n\n        autocomplete_fields = self.get_autocomplete_fields(request)\n        if db_field.name in autocomplete_fields:\n            kwargs['widget'] = AutocompleteSelectMultiple(db_field.remote_field, self.admin_site, using=db)\n        elif db_field.name in self.raw_id_fields:\n            kwargs['widget'] = widgets.ManyToManyRawIdWidget(db_field.remote_field, self.admin_site, using=db)\n        elif db_field.name in [*self.filter_vertical, *self.filter_horizontal]:\n            kwargs['widget'] = widgets.FilteredSelectMultiple(\n                db_field.verbose_name,\n                db_field.name in self.filter_vertical\n            )\n\n        if 'queryset' not in kwargs:\n            queryset = self.get_field_queryset(db, db_field, request)\n            if queryset is not None:\n                kwargs['queryset'] = queryset\n\n        form_field = db_field.formfield(**kwargs)\n        if (isinstance(form_field.widget, SelectMultiple) and\n                not isinstance(form_field.widget, (CheckboxSelectMultiple, AutocompleteSelectMultiple))):\n            msg = _('Hold down \u201cControl\u201d, or \u201cCommand\u201d on a Mac, to select more than one.')\n            help_text = form_field.help_text\n            form_field.help_text = format_lazy('{} {}', help_text, msg) if help_text else msg\n        return form_field\n\n    def get_autocomplete_fields(self, request):\n        \"\"\"\n        Return a list of ForeignKey and/or ManyToMany fields which should use\n        an autocomplete widget.\n        \"\"\"\n        return self.autocomplete_fields\n\n    def get_view_on_site_url(self, obj=None):\n        if obj is None or not self.view_on_site:\n            return None\n\n        if callable(self.view_on_site):\n            return self.view_on_site(obj)\n        elif self.view_on_site and hasattr(obj, 'get_absolute_url'):\n            # use the ContentType lookup if view_on_site is True\n            return reverse('admin:view_on_site', kwargs={\n                'content_type_id': get_content_type_for_model(obj).pk,\n                'object_id': obj.pk\n            })\n\n    def get_empty_value_display(self):\n        \"\"\"\n        Return the empty_value_display set on ModelAdmin or AdminSite.\n        \"\"\"\n        try:\n            return mark_safe(self.empty_value_display)\n        except AttributeError:\n            return mark_safe(self.admin_site.empty_value_display)\n\n    def get_exclude(self, request, obj=None):\n        \"\"\"\n        Hook for specifying exclude.\n        \"\"\"\n        return self.exclude\n\n    def get_fields(self, request, obj=None):\n        \"\"\"\n        Hook for specifying fields.\n        \"\"\"\n        if self.fields:\n            return self.fields\n        # _get_form_for_get_fields() is implemented in subclasses.\n        form = self._get_form_for_get_fields(request, obj)\n        return [*form.base_fields, *self.get_readonly_fields(request, obj)]\n\n    def get_fieldsets(self, request, obj=None):\n        \"\"\"\n        Hook for specifying fieldsets.\n        \"\"\"\n        if self.fieldsets:\n            return self.fieldsets\n        return [(None, {'fields': self.get_fields(request, obj)})]\n\n    def get_inlines(self, request, obj):\n        \"\"\"Hook for specifying custom inlines.\"\"\"\n        return self.inlines\n\n    def get_ordering(self, request):\n        \"\"\"\n        Hook for specifying field ordering.\n        \"\"\"\n        return self.ordering or ()  # otherwise we might try to *None, which is bad ;)\n\n    def get_readonly_fields(self, request, obj=None):\n        \"\"\"\n        Hook for specifying custom readonly fields.\n        \"\"\"\n        return self.readonly_fields\n\n    def get_prepopulated_fields(self, request, obj=None):\n        \"\"\"\n        Hook for specifying custom prepopulated fields.\n        \"\"\"\n        return self.prepopulated_fields\n\n    def get_queryset(self, request):\n        \"\"\"\n        Return a QuerySet of all model instances that can be edited by the\n        admin site. This is used by changelist_view.\n        \"\"\"\n        qs = self.model._default_manager.get_queryset()\n        # TODO: this should be handled by some parameter to the ChangeList.\n        ordering = self.get_ordering(request)\n        if ordering:\n            qs = qs.order_by(*ordering)\n        return qs\n\n    def get_sortable_by(self, request):\n        \"\"\"Hook for specifying which fields can be sorted in the changelist.\"\"\"\n        return self.sortable_by if self.sortable_by is not None else self.get_list_display(request)\n\n    def lookup_allowed(self, lookup, value):\n        from django.contrib.admin.filters import SimpleListFilter\n\n        model = self.model\n        # Check FKey lookups that are allowed, so that popups produced by\n        # ForeignKeyRawIdWidget, on the basis of ForeignKey.limit_choices_to,\n        # are allowed to work.\n        for fk_lookup in model._meta.related_fkey_lookups:\n            # As ``limit_choices_to`` can be a callable, invoke it here.\n            if callable(fk_lookup):\n                fk_lookup = fk_lookup()\n            if (lookup, value) in widgets.url_params_from_lookup_dict(fk_lookup).items():\n                return True\n\n        relation_parts = []\n        prev_field = None\n        for part in lookup.split(LOOKUP_SEP):\n            try:\n                field = model._meta.get_field(part)\n            except FieldDoesNotExist:\n                # Lookups on nonexistent fields are ok, since they're ignored\n                # later.\n                break\n            # It is allowed to filter on values that would be found from local\n            # model anyways. For example, if you filter on employee__department__id,\n            # then the id value would be found already from employee__department_id.\n            if not prev_field or (prev_field.is_relation and\n                                  field not in prev_field.get_path_info()[-1].target_fields):\n                relation_parts.append(part)\n            if not getattr(field, 'get_path_info', None):\n                # This is not a relational field, so further parts\n                # must be transforms.\n                break\n            prev_field = field\n            model = field.get_path_info()[-1].to_opts.model\n\n        if len(relation_parts) <= 1:\n            # Either a local field filter, or no fields at all.\n            return True\n        valid_lookups = {self.date_hierarchy}\n        for filter_item in self.list_filter:\n            if isinstance(filter_item, type) and issubclass(filter_item, SimpleListFilter):\n                valid_lookups.add(filter_item.parameter_name)\n            elif isinstance(filter_item, (list, tuple)):\n                valid_lookups.add(filter_item[0])\n            else:\n                valid_lookups.add(filter_item)\n\n        # Is it a valid relational lookup?\n        return not {\n            LOOKUP_SEP.join(relation_parts),\n            LOOKUP_SEP.join(relation_parts + [part])\n        }.isdisjoint(valid_lookups)\n\n    def to_field_allowed(self, request, to_field):\n        \"\"\"\n        Return True if the model associated with this admin should be\n        allowed to be referenced by the specified field.\n        \"\"\"\n        opts = self.model._meta\n\n        try:\n            field = opts.get_field(to_field)\n        except FieldDoesNotExist:\n            return False\n\n        # Always allow referencing the primary key since it's already possible\n        # to get this information from the change view URL.\n        if field.primary_key:\n            return True\n\n        # Allow reverse relationships to models defining m2m fields if they\n        # target the specified field.\n        for many_to_many in opts.many_to_many:\n            if many_to_many.m2m_target_field_name() == to_field:\n                return True\n\n        # Make sure at least one of the models registered for this site\n        # references this field through a FK or a M2M relationship.\n        registered_models = set()\n        for model, admin in self.admin_site._registry.items():\n            registered_models.add(model)\n            for inline in admin.inlines:\n                registered_models.add(inline.model)\n\n        related_objects = (\n            f for f in opts.get_fields(include_hidden=True)\n            if (f.auto_created and not f.concrete)\n        )\n        for related_object in related_objects:\n            related_model = related_object.related_model\n            remote_field = related_object.field.remote_field\n            if (any(issubclass(model, related_model) for model in registered_models) and\n                    hasattr(remote_field, 'get_related_field') and\n                    remote_field.get_related_field() == field):\n                return True\n\n        return False\n\n    def has_add_permission(self, request):\n        \"\"\"\n        Return True if the given request has permission to add an object.\n        Can be overridden by the user in subclasses.\n        \"\"\"\n        opts = self.opts\n        codename = get_permission_codename('add', opts)\n        return request.user.has_perm(\"%s.%s\" % (opts.app_label, codename))\n\n    def has_change_permission(self, request, obj=None):\n        \"\"\"\n        Return True if the given request has permission to change the given\n        Django model instance, the default implementation doesn't examine the\n        `obj` parameter.\n\n        Can be overridden by the user in subclasses. In such case it should\n        return True if the given request has permission to change the `obj`\n        model instance. If `obj` is None, this should return True if the given\n        request has permission to change *any* object of the given type.\n        \"\"\"\n        opts = self.opts\n        codename = get_permission_codename('change', opts)\n        return request.user.has_perm(\"%s.%s\" % (opts.app_label, codename))\n\n    def has_delete_permission(self, request, obj=None):\n        \"\"\"\n        Return True if the given request has permission to change the given\n        Django model instance, the default implementation doesn't examine the\n        `obj` parameter.\n\n        Can be overridden by the user in subclasses. In such case it should\n        return True if the given request has permission to delete the `obj`\n        model instance. If `obj` is None, this should return True if the given\n        request has permission to delete *any* object of the given type.\n        \"\"\"\n        opts = self.opts\n        codename = get_permission_codename('delete', opts)\n        return request.user.has_perm(\"%s.%s\" % (opts.app_label, codename))\n\n    def has_view_permission(self, request, obj=None):\n        \"\"\"\n        Return True if the given request has permission to view the given\n        Django model instance. The default implementation doesn't examine the\n        `obj` parameter.\n\n        If overridden by the user in subclasses, it should return True if the\n        given request has permission to view the `obj` model instance. If `obj`\n        is None, it should return True if the request has permission to view\n        any object of the given type.\n        \"\"\"\n        opts = self.opts\n        codename_view = get_permission_codename('view', opts)\n        codename_change = get_permission_codename('change', opts)\n        return (\n            request.user.has_perm('%s.%s' % (opts.app_label, codename_view)) or\n            request.user.has_perm('%s.%s' % (opts.app_label, codename_change))\n        )\n\n    def has_view_or_change_permission(self, request, obj=None):\n        return self.has_view_permission(request, obj) or self.has_change_permission(request, obj)\n\n    def has_module_permission(self, request):\n        \"\"\"\n        Return True if the given request has any permission in the given\n        app label.\n\n        Can be overridden by the user in subclasses. In such case it should\n        return True if the given request has permission to view the module on\n        the admin index page and access the module's index page. Overriding it\n        does not restrict access to the add, change or delete views. Use\n        `ModelAdmin.has_(add|change|delete)_permission` for that.\n        \"\"\"\n        return request.user.has_module_perms(self.opts.app_label)\n\n\nclass ModelAdmin(BaseModelAdmin):\n    \"\"\"Encapsulate all admin options and functionality for a given model.\"\"\"\n\n    list_display = ('__str__',)\n    list_display_links = ()\n    list_filter = ()\n    list_select_related = False\n    list_per_page = 100\n    list_max_show_all = 200\n    list_editable = ()\n    search_fields = ()\n    date_hierarchy = None\n    save_as = False\n    save_as_continue = True\n    save_on_top = False\n    paginator = Paginator\n    preserve_filters = True\n    inlines = []\n\n    # Custom templates (designed to be over-ridden in subclasses)\n    add_form_template = None\n    change_form_template = None\n    change_list_template = None\n    delete_confirmation_template = None\n    delete_selected_confirmation_template = None\n    object_history_template = None\n    popup_response_template = None\n\n    # Actions\n    actions = []\n    action_form = helpers.ActionForm\n    actions_on_top = True\n    actions_on_bottom = False\n    actions_selection_counter = True\n    checks_class = ModelAdminChecks\n\n    def __init__(self, model, admin_site):\n        self.model = model\n        self.opts = model._meta\n        self.admin_site = admin_site\n        super().__init__()\n\n    def __str__(self):\n        return \"%s.%s\" % (self.model._meta.app_label, self.__class__.__name__)\n\n    def get_inline_instances(self, request, obj=None):\n        inline_instances = []\n        for inline_class in self.get_inlines(request, obj):\n            inline = inline_class(self.model, self.admin_site)\n            if request:\n                if not (inline.has_view_or_change_permission(request, obj) or\n                        inline.has_add_permission(request, obj) or\n                        inline.has_delete_permission(request, obj)):\n                    continue\n                if not inline.has_add_permission(request, obj):\n                    inline.max_num = 0\n            inline_instances.append(inline)\n\n        return inline_instances\n\n    def get_urls(self):\n        from django.urls import path\n\n        def wrap(view):\n            def wrapper(*args, **kwargs):\n                return self.admin_site.admin_view(view)(*args, **kwargs)\n            wrapper.model_admin = self\n            return update_wrapper(wrapper, view)\n\n        info = self.model._meta.app_label, self.model._meta.model_name\n\n        return [\n            path('', wrap(self.changelist_view), name='%s_%s_changelist' % info),\n            path('add/', wrap(self.add_view), name='%s_%s_add' % info),\n            path('autocomplete/', wrap(self.autocomplete_view), name='%s_%s_autocomplete' % info),\n            path('<path:object_id>/history/', wrap(self.history_view), name='%s_%s_history' % info),\n            path('<path:object_id>/delete/', wrap(self.delete_view), name='%s_%s_delete' % info),\n            path('<path:object_id>/change/', wrap(self.change_view), name='%s_%s_change' % info),\n            # For backwards compatibility (was the change url before 1.9)\n            path('<path:object_id>/', wrap(RedirectView.as_view(\n                pattern_name='%s:%s_%s_change' % ((self.admin_site.name,) + info)\n            ))),\n        ]\n\n    @property\n    def urls(self):\n        return self.get_urls()\n\n    @property\n    def media(self):\n        extra = '' if settings.DEBUG else '.min'\n        js = [\n            'vendor/jquery/jquery%s.js' % extra,\n            'jquery.init.js',\n            'core.js',\n            'admin/RelatedObjectLookups.js',\n            'actions%s.js' % extra,\n            'urlify.js',\n            'prepopulate%s.js' % extra,\n            'vendor/xregexp/xregexp%s.js' % extra,\n        ]\n        return forms.Media(js=['admin/js/%s' % url for url in js])\n\n    def get_model_perms(self, request):\n        \"\"\"\n        Return a dict of all perms for this model. This dict has the keys\n        ``add``, ``change``, ``delete``, and ``view`` mapping to the True/False\n        for each of those actions.\n        \"\"\"\n        return {\n            'add': self.has_add_permission(request),\n            'change': self.has_change_permission(request),\n            'delete': self.has_delete_permission(request),\n            'view': self.has_view_permission(request),\n        }\n\n    def _get_form_for_get_fields(self, request, obj):\n        return self.get_form(request, obj, fields=None)\n\n    def get_form(self, request, obj=None, change=False, **kwargs):\n        \"\"\"\n        Return a Form class for use in the admin add view. This is used by\n        add_view and change_view.\n        \"\"\"\n        if 'fields' in kwargs:\n            fields = kwargs.pop('fields')\n        else:\n            fields = flatten_fieldsets(self.get_fieldsets(request, obj))\n        excluded = self.get_exclude(request, obj)\n        exclude = [] if excluded is None else list(excluded)\n        readonly_fields = self.get_readonly_fields(request, obj)\n        exclude.extend(readonly_fields)\n        # Exclude all fields if it's a change form and the user doesn't have\n        # the change permission.\n        if change and hasattr(request, 'user') and not self.has_change_permission(request, obj):\n            exclude.extend(fields)\n        if excluded is None and hasattr(self.form, '_meta') and self.form._meta.exclude:\n            # Take the custom ModelForm's Meta.exclude into account only if the\n            # ModelAdmin doesn't define its own.\n            exclude.extend(self.form._meta.exclude)\n        # if exclude is an empty list we pass None to be consistent with the\n        # default on modelform_factory\n        exclude = exclude or None\n\n        # Remove declared form fields which are in readonly_fields.\n        new_attrs = dict.fromkeys(f for f in readonly_fields if f in self.form.declared_fields)\n        form = type(self.form.__name__, (self.form,), new_attrs)\n\n        defaults = {\n            'form': form,\n            'fields': fields,\n            'exclude': exclude,\n            'formfield_callback': partial(self.formfield_for_dbfield, request=request),\n            **kwargs,\n        }\n\n        if defaults['fields'] is None and not modelform_defines_fields(defaults['form']):\n            defaults['fields'] = forms.ALL_FIELDS\n\n        try:\n            return modelform_factory(self.model, **defaults)\n        except FieldError as e:\n            raise FieldError(\n                '%s. Check fields/fieldsets/exclude attributes of class %s.'\n                % (e, self.__class__.__name__)\n            )\n\n    def get_changelist(self, request, **kwargs):\n        \"\"\"\n        Return the ChangeList class for use on the changelist page.\n        \"\"\"\n        from django.contrib.admin.views.main import ChangeList\n        return ChangeList\n\n    def get_changelist_instance(self, request):\n        \"\"\"\n        Return a `ChangeList` instance based on `request`. May raise\n        `IncorrectLookupParameters`.\n        \"\"\"\n        list_display = self.get_list_display(request)\n        list_display_links = self.get_list_display_links(request, list_display)\n        # Add the action checkboxes if any actions are available.\n        if self.get_actions(request):\n            list_display = ['action_checkbox', *list_display]\n        sortable_by = self.get_sortable_by(request)\n        ChangeList = self.get_changelist(request)\n        return ChangeList(\n            request,\n            self.model,\n            list_display,\n            list_display_links,\n            self.get_list_filter(request),\n            self.date_hierarchy,\n            self.get_search_fields(request),\n            self.get_list_select_related(request),\n            self.list_per_page,\n            self.list_max_show_all,\n            self.list_editable,\n            self,\n            sortable_by,\n        )\n\n    def get_object(self, request, object_id, from_field=None):\n        \"\"\"\n        Return an instance matching the field and value provided, the primary\n        key is used if no field is provided. Return ``None`` if no match is\n        found or the object_id fails validation.\n        \"\"\"\n        queryset = self.get_queryset(request)\n        model = queryset.model\n        field = model._meta.pk if from_field is None else model._meta.get_field(from_field)\n        try:\n            object_id = field.to_python(object_id)\n            return queryset.get(**{field.name: object_id})\n        except (model.DoesNotExist, ValidationError, ValueError):\n            return None\n\n    def get_changelist_form(self, request, **kwargs):\n        \"\"\"\n        Return a Form class for use in the Formset on the changelist page.\n        \"\"\"\n        defaults = {\n            'formfield_callback': partial(self.formfield_for_dbfield, request=request),\n            **kwargs,\n        }\n        if defaults.get('fields') is None and not modelform_defines_fields(defaults.get('form')):\n            defaults['fields'] = forms.ALL_FIELDS\n\n        return modelform_factory(self.model, **defaults)\n\n    def get_changelist_formset(self, request, **kwargs):\n        \"\"\"\n        Return a FormSet class for use on the changelist page if list_editable\n        is used.\n        \"\"\"\n        defaults = {\n            'formfield_callback': partial(self.formfield_for_dbfield, request=request),\n            **kwargs,\n        }\n        return modelformset_factory(\n            self.model, self.get_changelist_form(request), extra=0,\n            fields=self.list_editable, **defaults\n        )\n\n    def get_formsets_with_inlines(self, request, obj=None):\n        \"\"\"\n        Yield formsets and the corresponding inlines.\n        \"\"\"\n        for inline in self.get_inline_instances(request, obj):\n            yield inline.get_formset(request, obj), inline\n\n    def get_paginator(self, request, queryset, per_page, orphans=0, allow_empty_first_page=True):\n        return self.paginator(queryset, per_page, orphans, allow_empty_first_page)\n\n    def log_addition(self, request, object, message):\n        \"\"\"\n        Log that an object has been successfully added.\n\n        The default implementation creates an admin LogEntry object.\n        \"\"\"\n        from django.contrib.admin.models import LogEntry, ADDITION\n        return LogEntry.objects.log_action(\n            user_id=request.user.pk,\n            content_type_id=get_content_type_for_model(object).pk,\n            object_id=object.pk,\n            object_repr=str(object),\n            action_flag=ADDITION,\n            change_message=message,\n        )\n\n    def log_change(self, request, object, message):\n        \"\"\"\n        Log that an object has been successfully changed.\n\n        The default implementation creates an admin LogEntry object.\n        \"\"\"\n        from django.contrib.admin.models import LogEntry, CHANGE\n        return LogEntry.objects.log_action(\n            user_id=request.user.pk,\n            content_type_id=get_content_type_for_model(object).pk,\n            object_id=object.pk,\n            object_repr=str(object),\n            action_flag=CHANGE,\n            change_message=message,\n        )\n\n    def log_deletion(self, request, object, object_repr):\n        \"\"\"\n        Log that an object will be deleted. Note that this method must be\n        called before the deletion.\n\n        The default implementation creates an admin LogEntry object.\n        \"\"\"\n        from django.contrib.admin.models import LogEntry, DELETION\n        return LogEntry.objects.log_action(\n            user_id=request.user.pk,\n            content_type_id=get_content_type_for_model(object).pk,\n            object_id=object.pk,\n            object_repr=object_repr,\n            action_flag=DELETION,\n        )\n\n    def action_checkbox(self, obj):\n        \"\"\"\n        A list_display column containing a checkbox widget.\n        \"\"\"\n        return helpers.checkbox.render(helpers.ACTION_CHECKBOX_NAME, str(obj.pk))\n    action_checkbox.short_description = mark_safe('<input type=\"checkbox\" id=\"action-toggle\">')\n\n    def _get_base_actions(self):\n        \"\"\"Return the list of actions, prior to any request-based filtering.\"\"\"\n        actions = []\n\n        # Gather actions from the admin site first\n        for (name, func) in self.admin_site.actions:\n            description = getattr(func, 'short_description', name.replace('_', ' '))\n            actions.append((func, name, description))\n        # Add actions from this ModelAdmin.\n        actions.extend(self.get_action(action) for action in self.actions or [])\n        # get_action might have returned None, so filter any of those out.\n        return filter(None, actions)\n\n    def _filter_actions_by_permissions(self, request, actions):\n        \"\"\"Filter out any actions that the user doesn't have access to.\"\"\"\n        filtered_actions = []\n        for action in actions:\n            callable = action[0]\n            if not hasattr(callable, 'allowed_permissions'):\n                filtered_actions.append(action)\n                continue\n            permission_checks = (\n                getattr(self, 'has_%s_permission' % permission)\n                for permission in callable.allowed_permissions\n            )\n            if any(has_permission(request) for has_permission in permission_checks):\n                filtered_actions.append(action)\n        return filtered_actions\n\n    def get_actions(self, request):\n        \"\"\"\n        Return a dictionary mapping the names of all actions for this\n        ModelAdmin to a tuple of (callable, name, description) for each action.\n        \"\"\"\n        # If self.actions is set to None that means actions are disabled on\n        # this page.\n        if self.actions is None or IS_POPUP_VAR in request.GET:\n            return {}\n        actions = self._filter_actions_by_permissions(request, self._get_base_actions())\n        return {name: (func, name, desc) for func, name, desc in actions}\n\n    def get_action_choices(self, request, default_choices=BLANK_CHOICE_DASH):\n        \"\"\"\n        Return a list of choices for use in a form object.  Each choice is a\n        tuple (name, description).\n        \"\"\"\n        choices = [] + default_choices\n        for func, name, description in self.get_actions(request).values():\n            choice = (name, description % model_format_dict(self.opts))\n            choices.append(choice)\n        return choices\n\n    def get_action(self, action):\n        \"\"\"\n        Return a given action from a parameter, which can either be a callable,\n        or the name of a method on the ModelAdmin.  Return is a tuple of\n        (callable, name, description).\n        \"\"\"\n        # If the action is a callable, just use it.\n        if callable(action):\n            func = action\n            action = action.__name__\n\n        # Next, look for a method. Grab it off self.__class__ to get an unbound\n        # method instead of a bound one; this ensures that the calling\n        # conventions are the same for functions and methods.\n        elif hasattr(self.__class__, action):\n            func = getattr(self.__class__, action)\n\n        # Finally, look for a named method on the admin site\n        else:\n            try:\n                func = self.admin_site.get_action(action)\n            except KeyError:\n                return None\n\n        if hasattr(func, 'short_description'):\n            description = func.short_description\n        else:\n            description = capfirst(action.replace('_', ' '))\n        return func, action, description\n\n    def get_list_display(self, request):\n        \"\"\"\n        Return a sequence containing the fields to be displayed on the\n        changelist.\n        \"\"\"\n        return self.list_display\n\n    def get_list_display_links(self, request, list_display):\n        \"\"\"\n        Return a sequence containing the fields to be displayed as links\n        on the changelist. The list_display parameter is the list of fields\n        returned by get_list_display().\n        \"\"\"\n        if self.list_display_links or self.list_display_links is None or not list_display:\n            return self.list_display_links\n        else:\n            # Use only the first item in list_display as link\n            return list(list_display)[:1]\n\n    def get_list_filter(self, request):\n        \"\"\"\n        Return a sequence containing the fields to be displayed as filters in\n        the right sidebar of the changelist page.\n        \"\"\"\n        return self.list_filter\n\n    def get_list_select_related(self, request):\n        \"\"\"\n        Return a list of fields to add to the select_related() part of the\n        changelist items query.\n        \"\"\"\n        return self.list_select_related\n\n    def get_search_fields(self, request):\n        \"\"\"\n        Return a sequence containing the fields to be searched whenever\n        somebody submits a search query.\n        \"\"\"\n        return self.search_fields\n\n    def get_search_results(self, request, queryset, search_term):\n        \"\"\"\n        Return a tuple containing a queryset to implement the search\n        and a boolean indicating if the results may contain duplicates.\n        \"\"\"\n        # Apply keyword searches.\n        def construct_search(field_name):\n            if field_name.startswith('^'):\n                return \"%s__istartswith\" % field_name[1:]\n            elif field_name.startswith('='):\n                return \"%s__iexact\" % field_name[1:]\n            elif field_name.startswith('@'):\n                return \"%s__search\" % field_name[1:]\n            # Use field_name if it includes a lookup.\n            opts = queryset.model._meta\n            lookup_fields = field_name.split(LOOKUP_SEP)\n            # Go through the fields, following all relations.\n            prev_field = None\n            for path_part in lookup_fields:\n                if path_part == 'pk':\n                    path_part = opts.pk.name\n                try:\n                    field = opts.get_field(path_part)\n                except FieldDoesNotExist:\n                    # Use valid query lookups.\n                    if prev_field and prev_field.get_lookup(path_part):\n                        return field_name\n                else:\n                    prev_field = field\n                    if hasattr(field, 'get_path_info'):\n                        # Update opts to follow the relation.\n                        opts = field.get_path_info()[-1].to_opts\n            # Otherwise, use the field with icontains.\n            return \"%s__icontains\" % field_name\n\n        use_distinct = False\n        search_fields = self.get_search_fields(request)\n        if search_fields and search_term:\n            orm_lookups = [construct_search(str(search_field))\n                           for search_field in search_fields]\n            for bit in search_term.split():\n                or_queries = [models.Q(**{orm_lookup: bit})\n                              for orm_lookup in orm_lookups]\n                queryset = queryset.filter(reduce(operator.or_, or_queries))\n            use_distinct |= any(lookup_needs_distinct(self.opts, search_spec) for search_spec in orm_lookups)\n\n        return queryset, use_distinct\n\n    def get_preserved_filters(self, request):\n        \"\"\"\n        Return the preserved filters querystring.\n        \"\"\"\n        match = request.resolver_match\n        if self.preserve_filters and match:\n            opts = self.model._meta\n            current_url = '%s:%s' % (match.app_name, match.url_name)\n            changelist_url = 'admin:%s_%s_changelist' % (opts.app_label, opts.model_name)\n            if current_url == changelist_url:\n                preserved_filters = request.GET.urlencode()\n            else:\n                preserved_filters = request.GET.get('_changelist_filters')\n\n            if preserved_filters:\n                return urlencode({'_changelist_filters': preserved_filters})\n        return ''\n\n    def construct_change_message(self, request, form, formsets, add=False):\n        \"\"\"\n        Construct a JSON structure describing changes from a changed object.\n        \"\"\"\n        return construct_change_message(form, formsets, add)\n\n    def message_user(self, request, message, level=messages.INFO, extra_tags='',\n                     fail_silently=False):\n        \"\"\"\n        Send a message to the user. The default implementation\n        posts a message using the django.contrib.messages backend.\n\n        Exposes almost the same API as messages.add_message(), but accepts the\n        positional arguments in a different order to maintain backwards\n        compatibility. For convenience, it accepts the `level` argument as\n        a string rather than the usual level number.\n        \"\"\"\n        if not isinstance(level, int):\n            # attempt to get the level if passed a string\n            try:\n                level = getattr(messages.constants, level.upper())\n            except AttributeError:\n                levels = messages.constants.DEFAULT_TAGS.values()\n                levels_repr = ', '.join('`%s`' % l for l in levels)\n                raise ValueError(\n                    'Bad message level string: `%s`. Possible values are: %s'\n                    % (level, levels_repr)\n                )\n\n        messages.add_message(request, level, message, extra_tags=extra_tags, fail_silently=fail_silently)\n\n    def save_form(self, request, form, change):\n        \"\"\"\n        Given a ModelForm return an unsaved instance. ``change`` is True if\n        the object is being changed, and False if it's being added.\n        \"\"\"\n        return form.save(commit=False)\n\n    def save_model(self, request, obj, form, change):\n        \"\"\"\n        Given a model instance save it to the database.\n        \"\"\"\n        obj.save()\n\n    def delete_model(self, request, obj):\n        \"\"\"\n        Given a model instance delete it from the database.\n        \"\"\"\n        obj.delete()\n\n    def delete_queryset(self, request, queryset):\n        \"\"\"Given a queryset, delete it from the database.\"\"\"\n        queryset.delete()\n\n    def save_formset(self, request, form, formset, change):\n        \"\"\"\n        Given an inline formset save it to the database.\n        \"\"\"\n        formset.save()\n\n    def save_related(self, request, form, formsets, change):\n        \"\"\"\n        Given the ``HttpRequest``, the parent ``ModelForm`` instance, the\n        list of inline formsets and a boolean value based on whether the\n        parent is being added or changed, save the related objects to the\n        database. Note that at this point save_form() and save_model() have\n        already been called.\n        \"\"\"\n        form.save_m2m()\n        for formset in formsets:\n            self.save_formset(request, form, formset, change=change)\n\n    def render_change_form(self, request, context, add=False, change=False, form_url='', obj=None):\n        opts = self.model._meta\n        app_label = opts.app_label\n        preserved_filters = self.get_preserved_filters(request)\n        form_url = add_preserved_filters({'preserved_filters': preserved_filters, 'opts': opts}, form_url)\n        view_on_site_url = self.get_view_on_site_url(obj)\n        has_editable_inline_admin_formsets = False\n        for inline in context['inline_admin_formsets']:\n            if inline.has_add_permission or inline.has_change_permission or inline.has_delete_permission:\n                has_editable_inline_admin_formsets = True\n                break\n        context.update({\n            'add': add,\n            'change': change,\n            'has_view_permission': self.has_view_permission(request, obj),\n            'has_add_permission': self.has_add_permission(request),\n            'has_change_permission': self.has_change_permission(request, obj),\n            'has_delete_permission': self.has_delete_permission(request, obj),\n            'has_editable_inline_admin_formsets': has_editable_inline_admin_formsets,\n            'has_file_field': context['adminform'].form.is_multipart() or any(\n                admin_formset.formset.is_multipart()\n                for admin_formset in context['inline_admin_formsets']\n            ),\n            'has_absolute_url': view_on_site_url is not None,\n            'absolute_url': view_on_site_url,\n            'form_url': form_url,\n            'opts': opts,\n            'content_type_id': get_content_type_for_model(self.model).pk,\n            'save_as': self.save_as,\n            'save_on_top': self.save_on_top,\n            'to_field_var': TO_FIELD_VAR,\n            'is_popup_var': IS_POPUP_VAR,\n            'app_label': app_label,\n        })\n        if add and self.add_form_template is not None:\n            form_template = self.add_form_template\n        else:\n            form_template = self.change_form_template\n\n        request.current_app = self.admin_site.name\n\n        return TemplateResponse(request, form_template or [\n            \"admin/%s/%s/change_form.html\" % (app_label, opts.model_name),\n            \"admin/%s/change_form.html\" % app_label,\n            \"admin/change_form.html\"\n        ], context)\n\n    def response_add(self, request, obj, post_url_continue=None):\n        \"\"\"\n        Determine the HttpResponse for the add_view stage.\n        \"\"\"\n        opts = obj._meta\n        preserved_filters = self.get_preserved_filters(request)\n        obj_url = reverse(\n            'admin:%s_%s_change' % (opts.app_label, opts.model_name),\n            args=(quote(obj.pk),),\n            current_app=self.admin_site.name,\n        )\n        # Add a link to the object's change form if the user can edit the obj.\n        if self.has_change_permission(request, obj):\n            obj_repr = format_html('<a href=\"{}\">{}</a>', urlquote(obj_url), obj)\n        else:\n            obj_repr = str(obj)\n        msg_dict = {\n            'name': opts.verbose_name,\n            'obj': obj_repr,\n        }\n        # Here, we distinguish between different save types by checking for\n        # the presence of keys in request.POST.\n\n        if IS_POPUP_VAR in request.POST:\n            to_field = request.POST.get(TO_FIELD_VAR)\n            if to_field:\n                attr = str(to_field)\n            else:\n                attr = obj._meta.pk.attname\n            value = obj.serializable_value(attr)\n            popup_response_data = json.dumps({\n                'value': str(value),\n                'obj': str(obj),\n            })\n            return TemplateResponse(request, self.popup_response_template or [\n                'admin/%s/%s/popup_response.html' % (opts.app_label, opts.model_name),\n                'admin/%s/popup_response.html' % opts.app_label,\n                'admin/popup_response.html',\n            ], {\n                'popup_response_data': popup_response_data,\n            })\n\n        elif \"_continue\" in request.POST or (\n                # Redirecting after \"Save as new\".\n                \"_saveasnew\" in request.POST and self.save_as_continue and\n                self.has_change_permission(request, obj)\n        ):\n            msg = _('The {name} \u201c{obj}\u201d was added successfully.')\n            if self.has_change_permission(request, obj):\n                msg += ' ' + _('You may edit it again below.')\n            self.message_user(request, format_html(msg, **msg_dict), messages.SUCCESS)\n            if post_url_continue is None:\n                post_url_continue = obj_url\n            post_url_continue = add_preserved_filters(\n                {'preserved_filters': preserved_filters, 'opts': opts},\n                post_url_continue\n            )\n            return HttpResponseRedirect(post_url_continue)\n\n        elif \"_addanother\" in request.POST:\n            msg = format_html(\n                _('The {name} \u201c{obj}\u201d was added successfully. You may add another {name} below.'),\n                **msg_dict\n            )\n            self.message_user(request, msg, messages.SUCCESS)\n            redirect_url = request.path\n            redirect_url = add_preserved_filters({'preserved_filters': preserved_filters, 'opts': opts}, redirect_url)\n            return HttpResponseRedirect(redirect_url)\n\n        else:\n            msg = format_html(\n                _('The {name} \u201c{obj}\u201d was added successfully.'),\n                **msg_dict\n            )\n            self.message_user(request, msg, messages.SUCCESS)\n            return self.response_post_save_add(request, obj)\n\n    def response_change(self, request, obj):\n        \"\"\"\n        Determine the HttpResponse for the change_view stage.\n        \"\"\"\n\n        if IS_POPUP_VAR in request.POST:\n            opts = obj._meta\n            to_field = request.POST.get(TO_FIELD_VAR)\n            attr = str(to_field) if to_field else opts.pk.attname\n            value = request.resolver_match.kwargs['object_id']\n            new_value = obj.serializable_value(attr)\n            popup_response_data = json.dumps({\n                'action': 'change',\n                'value': str(value),\n                'obj': str(obj),\n                'new_value': str(new_value),\n            })\n            return TemplateResponse(request, self.popup_response_template or [\n                'admin/%s/%s/popup_response.html' % (opts.app_label, opts.model_name),\n                'admin/%s/popup_response.html' % opts.app_label,\n                'admin/popup_response.html',\n            ], {\n                'popup_response_data': popup_response_data,\n            })\n\n        opts = self.model._meta\n        preserved_filters = self.get_preserved_filters(request)\n\n        msg_dict = {\n            'name': opts.verbose_name,\n            'obj': format_html('<a href=\"{}\">{}</a>', urlquote(request.path), obj),\n        }\n        if \"_continue\" in request.POST:\n            msg = format_html(\n                _('The {name} \u201c{obj}\u201d was changed successfully. You may edit it again below.'),\n                **msg_dict\n            )\n            self.message_user(request, msg, messages.SUCCESS)\n            redirect_url = request.path\n            redirect_url = add_preserved_filters({'preserved_filters': preserved_filters, 'opts': opts}, redirect_url)\n            return HttpResponseRedirect(redirect_url)\n\n        elif \"_saveasnew\" in request.POST:\n            msg = format_html(\n                _('The {name} \u201c{obj}\u201d was added successfully. You may edit it again below.'),\n                **msg_dict\n            )\n            self.message_user(request, msg, messages.SUCCESS)\n            redirect_url = reverse('admin:%s_%s_change' %\n                                   (opts.app_label, opts.model_name),\n                                   args=(obj.pk,),\n                                   current_app=self.admin_site.name)\n            redirect_url = add_preserved_filters({'preserved_filters': preserved_filters, 'opts': opts}, redirect_url)\n            return HttpResponseRedirect(redirect_url)\n\n        elif \"_addanother\" in request.POST:\n            msg = format_html(\n                _('The {name} \u201c{obj}\u201d was changed successfully. You may add another {name} below.'),\n                **msg_dict\n            )\n            self.message_user(request, msg, messages.SUCCESS)\n            redirect_url = reverse('admin:%s_%s_add' %\n                                   (opts.app_label, opts.model_name),\n                                   current_app=self.admin_site.name)\n            redirect_url = add_preserved_filters({'preserved_filters': preserved_filters, 'opts': opts}, redirect_url)\n            return HttpResponseRedirect(redirect_url)\n\n        else:\n            msg = format_html(\n                _('The {name} \u201c{obj}\u201d was changed successfully.'),\n                **msg_dict\n            )\n            self.message_user(request, msg, messages.SUCCESS)\n            return self.response_post_save_change(request, obj)\n\n    def _response_post_save(self, request, obj):\n        opts = self.model._meta\n        if self.has_view_or_change_permission(request):\n            post_url = reverse('admin:%s_%s_changelist' %\n                               (opts.app_label, opts.model_name),\n                               current_app=self.admin_site.name)\n            preserved_filters = self.get_preserved_filters(request)\n            post_url = add_preserved_filters({'preserved_filters': preserved_filters, 'opts': opts}, post_url)\n        else:\n            post_url = reverse('admin:index',\n                               current_app=self.admin_site.name)\n        return HttpResponseRedirect(post_url)\n\n    def response_post_save_add(self, request, obj):\n        \"\"\"\n        Figure out where to redirect after the 'Save' button has been pressed\n        when adding a new object.\n        \"\"\"\n        return self._response_post_save(request, obj)\n\n    def response_post_save_change(self, request, obj):\n        \"\"\"\n        Figure out where to redirect after the 'Save' button has been pressed\n        when editing an existing object.\n        \"\"\"\n        return self._response_post_save(request, obj)\n\n    def response_action(self, request, queryset):\n        \"\"\"\n        Handle an admin action. This is called if a request is POSTed to the\n        changelist; it returns an HttpResponse if the action was handled, and\n        None otherwise.\n        \"\"\"\n\n        # There can be multiple action forms on the page (at the top\n        # and bottom of the change list, for example). Get the action\n        # whose button was pushed.\n        try:\n            action_index = int(request.POST.get('index', 0))\n        except ValueError:\n            action_index = 0\n\n        # Construct the action form.\n        data = request.POST.copy()\n        data.pop(helpers.ACTION_CHECKBOX_NAME, None)\n        data.pop(\"index\", None)\n\n        # Use the action whose button was pushed\n        try:\n            data.update({'action': data.getlist('action')[action_index]})\n        except IndexError:\n            # If we didn't get an action from the chosen form that's invalid\n            # POST data, so by deleting action it'll fail the validation check\n            # below. So no need to do anything here\n            pass\n\n        action_form = self.action_form(data, auto_id=None)\n        action_form.fields['action'].choices = self.get_action_choices(request)\n\n        # If the form's valid we can handle the action.\n        if action_form.is_valid():\n            action = action_form.cleaned_data['action']\n            select_across = action_form.cleaned_data['select_across']\n            func = self.get_actions(request)[action][0]\n\n            # Get the list of selected PKs. If nothing's selected, we can't\n            # perform an action on it, so bail. Except we want to perform\n            # the action explicitly on all objects.\n            selected = request.POST.getlist(helpers.ACTION_CHECKBOX_NAME)\n            if not selected and not select_across:\n                # Reminder that something needs to be selected or nothing will happen\n                msg = _(\"Items must be selected in order to perform \"\n                        \"actions on them. No items have been changed.\")\n                self.message_user(request, msg, messages.WARNING)\n                return None\n\n            if not select_across:\n                # Perform the action only on the selected objects\n                queryset = queryset.filter(pk__in=selected)\n\n            response = func(self, request, queryset)\n\n            # Actions may return an HttpResponse-like object, which will be\n            # used as the response from the POST. If not, we'll be a good\n            # little HTTP citizen and redirect back to the changelist page.\n            if isinstance(response, HttpResponseBase):\n                return response\n            else:\n                return HttpResponseRedirect(request.get_full_path())\n        else:\n            msg = _(\"No action selected.\")\n            self.message_user(request, msg, messages.WARNING)\n            return None\n\n    def response_delete(self, request, obj_display, obj_id):\n        \"\"\"\n        Determine the HttpResponse for the delete_view stage.\n        \"\"\"\n        opts = self.model._meta\n\n        if IS_POPUP_VAR in request.POST:\n            popup_response_data = json.dumps({\n                'action': 'delete',\n                'value': str(obj_id),\n            })\n            return TemplateResponse(request, self.popup_response_template or [\n                'admin/%s/%s/popup_response.html' % (opts.app_label, opts.model_name),\n                'admin/%s/popup_response.html' % opts.app_label,\n                'admin/popup_response.html',\n            ], {\n                'popup_response_data': popup_response_data,\n            })\n\n        self.message_user(\n            request,\n            _('The %(name)s \u201c%(obj)s\u201d was deleted successfully.') % {\n                'name': opts.verbose_name,\n                'obj': obj_display,\n            },\n            messages.SUCCESS,\n        )\n\n        if self.has_change_permission(request, None):\n            post_url = reverse(\n                'admin:%s_%s_changelist' % (opts.app_label, opts.model_name),\n                current_app=self.admin_site.name,\n            )\n            preserved_filters = self.get_preserved_filters(request)\n            post_url = add_preserved_filters(\n                {'preserved_filters': preserved_filters, 'opts': opts}, post_url\n            )\n        else:\n            post_url = reverse('admin:index', current_app=self.admin_site.name)\n        return HttpResponseRedirect(post_url)\n\n    def render_delete_form(self, request, context):\n        opts = self.model._meta\n        app_label = opts.app_label\n\n        request.current_app = self.admin_site.name\n        context.update(\n            to_field_var=TO_FIELD_VAR,\n            is_popup_var=IS_POPUP_VAR,\n            media=self.media,\n        )\n\n        return TemplateResponse(\n            request,\n            self.delete_confirmation_template or [\n                \"admin/{}/{}/delete_confirmation.html\".format(app_label, opts.model_name),\n                \"admin/{}/delete_confirmation.html\".format(app_label),\n                \"admin/delete_confirmation.html\",\n            ],\n            context,\n        )\n\n    def get_inline_formsets(self, request, formsets, inline_instances, obj=None):\n        inline_admin_formsets = []\n        for inline, formset in zip(inline_instances, formsets):\n            fieldsets = list(inline.get_fieldsets(request, obj))\n            readonly = list(inline.get_readonly_fields(request, obj))\n            has_add_permission = inline.has_add_permission(request, obj)\n            has_change_permission = inline.has_change_permission(request, obj)\n            has_delete_permission = inline.has_delete_permission(request, obj)\n            has_view_permission = inline.has_view_permission(request, obj)\n            prepopulated = dict(inline.get_prepopulated_fields(request, obj))\n            inline_admin_formset = helpers.InlineAdminFormSet(\n                inline, formset, fieldsets, prepopulated, readonly, model_admin=self,\n                has_add_permission=has_add_permission, has_change_permission=has_change_permission,\n                has_delete_permission=has_delete_permission, has_view_permission=has_view_permission,\n            )\n            inline_admin_formsets.append(inline_admin_formset)\n        return inline_admin_formsets\n\n    def get_changeform_initial_data(self, request):\n        \"\"\"\n        Get the initial form data from the request's GET params.\n        \"\"\"\n        initial = dict(request.GET.items())\n        for k in initial:\n            try:\n                f = self.model._meta.get_field(k)\n            except FieldDoesNotExist:\n                continue\n            # We have to special-case M2Ms as a list of comma-separated PKs.\n            if isinstance(f, models.ManyToManyField):\n                initial[k] = initial[k].split(\",\")\n        return initial\n\n    def _get_obj_does_not_exist_redirect(self, request, opts, object_id):\n        \"\"\"\n        Create a message informing the user that the object doesn't exist\n        and return a redirect to the admin index page.\n        \"\"\"\n        msg = _('%(name)s with ID \u201c%(key)s\u201d doesn\u2019t exist. Perhaps it was deleted?') % {\n            'name': opts.verbose_name,\n            'key': unquote(object_id),\n        }\n        self.message_user(request, msg, messages.WARNING)\n        url = reverse('admin:index', current_app=self.admin_site.name)\n        return HttpResponseRedirect(url)\n\n    @csrf_protect_m\n    def changeform_view(self, request, object_id=None, form_url='', extra_context=None):\n        with transaction.atomic(using=router.db_for_write(self.model)):\n            return self._changeform_view(request, object_id, form_url, extra_context)\n\n    def _changeform_view(self, request, object_id, form_url, extra_context):\n        to_field = request.POST.get(TO_FIELD_VAR, request.GET.get(TO_FIELD_VAR))\n        if to_field and not self.to_field_allowed(request, to_field):\n            raise DisallowedModelAdminToField(\"The field %s cannot be referenced.\" % to_field)\n\n        model = self.model\n        opts = model._meta\n\n        if request.method == 'POST' and '_saveasnew' in request.POST:\n            object_id = None\n\n        add = object_id is None\n\n        if add:\n            if not self.has_add_permission(request):\n                raise PermissionDenied\n            obj = None\n\n        else:\n            obj = self.get_object(request, unquote(object_id), to_field)\n\n            if not self.has_view_or_change_permission(request, obj):\n                raise PermissionDenied\n\n            if obj is None:\n                return self._get_obj_does_not_exist_redirect(request, opts, object_id)\n\n        ModelForm = self.get_form(request, obj, change=not add)\n        if request.method == 'POST':\n            form = ModelForm(request.POST, request.FILES, instance=obj)\n            form_validated = form.is_valid()\n            if form_validated:\n                new_object = self.save_form(request, form, change=not add)\n            else:\n                new_object = form.instance\n            formsets, inline_instances = self._create_formsets(request, new_object, change=not add)\n            if all_valid(formsets) and form_validated:\n                self.save_model(request, new_object, form, not add)\n                self.save_related(request, form, formsets, not add)\n                change_message = self.construct_change_message(request, form, formsets, add)\n                if add:\n                    self.log_addition(request, new_object, change_message)\n                    return self.response_add(request, new_object)\n                else:\n                    self.log_change(request, new_object, change_message)\n                    return self.response_change(request, new_object)\n            else:\n                form_validated = False\n        else:\n            if add:\n                initial = self.get_changeform_initial_data(request)\n                form = ModelForm(initial=initial)\n                formsets, inline_instances = self._create_formsets(request, form.instance, change=False)\n            else:\n                form = ModelForm(instance=obj)\n                formsets, inline_instances = self._create_formsets(request, obj, change=True)\n\n        if not add and not self.has_change_permission(request, obj):\n            readonly_fields = flatten_fieldsets(self.get_fieldsets(request, obj))\n        else:\n            readonly_fields = self.get_readonly_fields(request, obj)\n        adminForm = helpers.AdminForm(\n            form,\n            list(self.get_fieldsets(request, obj)),\n            # Clear prepopulated fields on a view-only form to avoid a crash.\n            self.get_prepopulated_fields(request, obj) if add or self.has_change_permission(request, obj) else {},\n            readonly_fields,\n            model_admin=self)\n        media = self.media + adminForm.media\n\n        inline_formsets = self.get_inline_formsets(request, formsets, inline_instances, obj)\n        for inline_formset in inline_formsets:\n            media = media + inline_formset.media\n\n        if add:\n            title = _('Add %s')\n        elif self.has_change_permission(request, obj):\n            title = _('Change %s')\n        else:\n            title = _('View %s')\n        context = {\n            **self.admin_site.each_context(request),\n            'title': title % opts.verbose_name,\n            'adminform': adminForm,\n            'object_id': object_id,\n            'original': obj,\n            'is_popup': IS_POPUP_VAR in request.POST or IS_POPUP_VAR in request.GET,\n            'to_field': to_field,\n            'media': media,\n            'inline_admin_formsets': inline_formsets,\n            'errors': helpers.AdminErrorList(form, formsets),\n            'preserved_filters': self.get_preserved_filters(request),\n        }\n\n        # Hide the \"Save\" and \"Save and continue\" buttons if \"Save as New\" was\n        # previously chosen to prevent the interface from getting confusing.\n        if request.method == 'POST' and not form_validated and \"_saveasnew\" in request.POST:\n            context['show_save'] = False\n            context['show_save_and_continue'] = False\n            # Use the change template instead of the add template.\n            add = False\n\n        context.update(extra_context or {})\n\n        return self.render_change_form(request, context, add=add, change=not add, obj=obj, form_url=form_url)\n\n    def autocomplete_view(self, request):\n        return AutocompleteJsonView.as_view(model_admin=self)(request)\n\n    def add_view(self, request, form_url='', extra_context=None):\n        return self.changeform_view(request, None, form_url, extra_context)\n\n    def change_view(self, request, object_id, form_url='', extra_context=None):\n        return self.changeform_view(request, object_id, form_url, extra_context)\n\n    def _get_edited_object_pks(self, request, prefix):\n        \"\"\"Return POST data values of list_editable primary keys.\"\"\"\n        pk_pattern = re.compile(r'{}-\\d+-{}$'.format(prefix, self.model._meta.pk.name))\n        return [value for key, value in request.POST.items() if pk_pattern.match(key)]\n\n    def _get_list_editable_queryset(self, request, prefix):\n        \"\"\"\n        Based on POST data, return a queryset of the objects that were edited\n        via list_editable.\n        \"\"\"\n        object_pks = self._get_edited_object_pks(request, prefix)\n        queryset = self.get_queryset(request)\n        validate = queryset.model._meta.pk.to_python\n        try:\n            for pk in object_pks:\n                validate(pk)\n        except ValidationError:\n            # Disable the optimization if the POST data was tampered with.\n            return queryset\n        return queryset.filter(pk__in=object_pks)\n\n    @csrf_protect_m\n    def changelist_view(self, request, extra_context=None):\n        \"\"\"\n        The 'change list' admin view for this model.\n        \"\"\"\n        from django.contrib.admin.views.main import ERROR_FLAG\n        opts = self.model._meta\n        app_label = opts.app_label\n        if not self.has_view_or_change_permission(request):\n            raise PermissionDenied\n\n        try:\n            cl = self.get_changelist_instance(request)\n        except IncorrectLookupParameters:\n            # Wacky lookup parameters were given, so redirect to the main\n            # changelist page, without parameters, and pass an 'invalid=1'\n            # parameter via the query string. If wacky parameters were given\n            # and the 'invalid=1' parameter was already in the query string,\n            # something is screwed up with the database, so display an error\n            # page.\n            if ERROR_FLAG in request.GET:\n                return SimpleTemplateResponse('admin/invalid_setup.html', {\n                    'title': _('Database error'),\n                })\n            return HttpResponseRedirect(request.path + '?' + ERROR_FLAG + '=1')\n\n        # If the request was POSTed, this might be a bulk action or a bulk\n        # edit. Try to look up an action or confirmation first, but if this\n        # isn't an action the POST will fall through to the bulk edit check,\n        # below.\n        action_failed = False\n        selected = request.POST.getlist(helpers.ACTION_CHECKBOX_NAME)\n\n        actions = self.get_actions(request)\n        # Actions with no confirmation\n        if (actions and request.method == 'POST' and\n                'index' in request.POST and '_save' not in request.POST):\n            if selected:\n                response = self.response_action(request, queryset=cl.get_queryset(request))\n                if response:\n                    return response\n                else:\n                    action_failed = True\n            else:\n                msg = _(\"Items must be selected in order to perform \"\n                        \"actions on them. No items have been changed.\")\n                self.message_user(request, msg, messages.WARNING)\n                action_failed = True\n\n        # Actions with confirmation\n        if (actions and request.method == 'POST' and\n                helpers.ACTION_CHECKBOX_NAME in request.POST and\n                'index' not in request.POST and '_save' not in request.POST):\n            if selected:\n                response = self.response_action(request, queryset=cl.get_queryset(request))\n                if response:\n                    return response\n                else:\n                    action_failed = True\n\n        if action_failed:\n            # Redirect back to the changelist page to avoid resubmitting the\n            # form if the user refreshes the browser or uses the \"No, take\n            # me back\" button on the action confirmation page.\n            return HttpResponseRedirect(request.get_full_path())\n\n        # If we're allowing changelist editing, we need to construct a formset\n        # for the changelist given all the fields to be edited. Then we'll\n        # use the formset to validate/process POSTed data.\n        formset = cl.formset = None\n\n        # Handle POSTed bulk-edit data.\n        if request.method == 'POST' and cl.list_editable and '_save' in request.POST:\n            if not self.has_change_permission(request):\n                raise PermissionDenied\n            FormSet = self.get_changelist_formset(request)\n            modified_objects = self._get_list_editable_queryset(request, FormSet.get_default_prefix())\n            formset = cl.formset = FormSet(request.POST, request.FILES, queryset=modified_objects)\n            if formset.is_valid():\n                changecount = 0\n                for form in formset.forms:\n                    if form.has_changed():\n                        obj = self.save_form(request, form, change=True)\n                        self.save_model(request, obj, form, change=True)\n                        self.save_related(request, form, formsets=[], change=True)\n                        change_msg = self.construct_change_message(request, form, None)\n                        self.log_change(request, obj, change_msg)\n                        changecount += 1\n\n                if changecount:\n                    msg = ngettext(\n                        \"%(count)s %(name)s was changed successfully.\",\n                        \"%(count)s %(name)s were changed successfully.\",\n                        changecount\n                    ) % {\n                        'count': changecount,\n                        'name': model_ngettext(opts, changecount),\n                    }\n                    self.message_user(request, msg, messages.SUCCESS)\n\n                return HttpResponseRedirect(request.get_full_path())\n\n        # Handle GET -- construct a formset for display.\n        elif cl.list_editable and self.has_change_permission(request):\n            FormSet = self.get_changelist_formset(request)\n            formset = cl.formset = FormSet(queryset=cl.result_list)\n\n        # Build the list of media to be used by the formset.\n        if formset:\n            media = self.media + formset.media\n        else:\n            media = self.media\n\n        # Build the action form and populate it with available actions.\n        if actions:\n            action_form = self.action_form(auto_id=None)\n            action_form.fields['action'].choices = self.get_action_choices(request)\n            media += action_form.media\n        else:\n            action_form = None\n\n        selection_note_all = ngettext(\n            '%(total_count)s selected',\n            'All %(total_count)s selected',\n            cl.result_count\n        )\n\n        context = {\n            **self.admin_site.each_context(request),\n            'module_name': str(opts.verbose_name_plural),\n            'selection_note': _('0 of %(cnt)s selected') % {'cnt': len(cl.result_list)},\n            'selection_note_all': selection_note_all % {'total_count': cl.result_count},\n            'title': cl.title,\n            'is_popup': cl.is_popup,\n            'to_field': cl.to_field,\n            'cl': cl,\n            'media': media,\n            'has_add_permission': self.has_add_permission(request),\n            'opts': cl.opts,\n            'action_form': action_form,\n            'actions_on_top': self.actions_on_top,\n            'actions_on_bottom': self.actions_on_bottom,\n            'actions_selection_counter': self.actions_selection_counter,\n            'preserved_filters': self.get_preserved_filters(request),\n            **(extra_context or {}),\n        }\n\n        request.current_app = self.admin_site.name\n\n        return TemplateResponse(request, self.change_list_template or [\n            'admin/%s/%s/change_list.html' % (app_label, opts.model_name),\n            'admin/%s/change_list.html' % app_label,\n            'admin/change_list.html'\n        ], context)\n\n    def get_deleted_objects(self, objs, request):\n        \"\"\"\n        Hook for customizing the delete process for the delete view and the\n        \"delete selected\" action.\n        \"\"\"\n        return get_deleted_objects(objs, request, self.admin_site)\n\n    @csrf_protect_m\n    def delete_view(self, request, object_id, extra_context=None):\n        with transaction.atomic(using=router.db_for_write(self.model)):\n            return self._delete_view(request, object_id, extra_context)\n\n    def _delete_view(self, request, object_id, extra_context):\n        \"The 'delete' admin view for this model.\"\n        opts = self.model._meta\n        app_label = opts.app_label\n\n        to_field = request.POST.get(TO_FIELD_VAR, request.GET.get(TO_FIELD_VAR))\n        if to_field and not self.to_field_allowed(request, to_field):\n            raise DisallowedModelAdminToField(\"The field %s cannot be referenced.\" % to_field)\n\n        obj = self.get_object(request, unquote(object_id), to_field)\n\n        if not self.has_delete_permission(request, obj):\n            raise PermissionDenied\n\n        if obj is None:\n            return self._get_obj_does_not_exist_redirect(request, opts, object_id)\n\n        # Populate deleted_objects, a data structure of all related objects that\n        # will also be deleted.\n        deleted_objects, model_count, perms_needed, protected = self.get_deleted_objects([obj], request)\n\n        if request.POST and not protected:  # The user has confirmed the deletion.\n            if perms_needed:\n                raise PermissionDenied\n            obj_display = str(obj)\n            attr = str(to_field) if to_field else opts.pk.attname\n            obj_id = obj.serializable_value(attr)\n            self.log_deletion(request, obj, obj_display)\n            self.delete_model(request, obj)\n\n            return self.response_delete(request, obj_display, obj_id)\n\n        object_name = str(opts.verbose_name)\n\n        if perms_needed or protected:\n            title = _(\"Cannot delete %(name)s\") % {\"name\": object_name}\n        else:\n            title = _(\"Are you sure?\")\n\n        context = {\n            **self.admin_site.each_context(request),\n            'title': title,\n            'object_name': object_name,\n            'object': obj,\n            'deleted_objects': deleted_objects,\n            'model_count': dict(model_count).items(),\n            'perms_lacking': perms_needed,\n            'protected': protected,\n            'opts': opts,\n            'app_label': app_label,\n            'preserved_filters': self.get_preserved_filters(request),\n            'is_popup': IS_POPUP_VAR in request.POST or IS_POPUP_VAR in request.GET,\n            'to_field': to_field,\n            **(extra_context or {}),\n        }\n\n        return self.render_delete_form(request, context)\n\n    def history_view(self, request, object_id, extra_context=None):\n        \"The 'history' admin view for this model.\"\n        from django.contrib.admin.models import LogEntry\n        # First check if the user can see this history.\n        model = self.model\n        obj = self.get_object(request, unquote(object_id))\n        if obj is None:\n            return self._get_obj_does_not_exist_redirect(request, model._meta, object_id)\n\n        if not self.has_view_or_change_permission(request, obj):\n            raise PermissionDenied\n\n        # Then get the history for this object.\n        opts = model._meta\n        app_label = opts.app_label\n        action_list = LogEntry.objects.filter(\n            object_id=unquote(object_id),\n            content_type=get_content_type_for_model(model)\n        ).select_related().order_by('action_time')\n\n        context = {\n            **self.admin_site.each_context(request),\n            'title': _('Change history: %s') % obj,\n            'action_list': action_list,\n            'module_name': str(capfirst(opts.verbose_name_plural)),\n            'object': obj,\n            'opts': opts,\n            'preserved_filters': self.get_preserved_filters(request),\n            **(extra_context or {}),\n        }\n\n        request.current_app = self.admin_site.name\n\n        return TemplateResponse(request, self.object_history_template or [\n            \"admin/%s/%s/object_history.html\" % (app_label, opts.model_name),\n            \"admin/%s/object_history.html\" % app_label,\n            \"admin/object_history.html\"\n        ], context)\n\n    def _create_formsets(self, request, obj, change):\n        \"Helper function to generate formsets for add/change_view.\"\n        formsets = []\n        inline_instances = []\n        prefixes = {}\n        get_formsets_args = [request]\n        if change:\n            get_formsets_args.append(obj)\n        for FormSet, inline in self.get_formsets_with_inlines(*get_formsets_args):\n            prefix = FormSet.get_default_prefix()\n            prefixes[prefix] = prefixes.get(prefix, 0) + 1\n            if prefixes[prefix] != 1 or not prefix:\n                prefix = \"%s-%s\" % (prefix, prefixes[prefix])\n            formset_params = {\n                'instance': obj,\n                'prefix': prefix,\n                'queryset': inline.get_queryset(request),\n            }\n            if request.method == 'POST':\n                formset_params.update({\n                    'data': request.POST.copy(),\n                    'files': request.FILES,\n                    'save_as_new': '_saveasnew' in request.POST\n                })\n            formset = FormSet(**formset_params)\n\n            def user_deleted_form(request, obj, formset, index):\n                \"\"\"Return whether or not the user deleted the form.\"\"\"\n                return (\n                    inline.has_delete_permission(request, obj) and\n                    '{}-{}-DELETE'.format(formset.prefix, index) in request.POST\n                )\n\n            # Bypass validation of each view-only inline form (since the form's\n            # data won't be in request.POST), unless the form was deleted.\n            if not inline.has_change_permission(request, obj if change else None):\n                for index, form in enumerate(formset.initial_forms):\n                    if user_deleted_form(request, obj, formset, index):\n                        continue\n                    form._errors = {}\n                    form.cleaned_data = form.initial\n            formsets.append(formset)\n            inline_instances.append(inline)\n        return formsets, inline_instances\n\n\nclass InlineModelAdmin(BaseModelAdmin):\n    \"\"\"\n    Options for inline editing of ``model`` instances.\n\n    Provide ``fk_name`` to specify the attribute name of the ``ForeignKey``\n    from ``model`` to its parent. This is required if ``model`` has more than\n    one ``ForeignKey`` to its parent.\n    \"\"\"\n    model = None\n    fk_name = None\n    formset = BaseInlineFormSet\n    extra = 3\n    min_num = None\n    max_num = None\n    template = None\n    verbose_name = None\n    verbose_name_plural = None\n    can_delete = True\n    show_change_link = False\n    checks_class = InlineModelAdminChecks\n    classes = None\n\n    def __init__(self, parent_model, admin_site):\n        self.admin_site = admin_site\n        self.parent_model = parent_model\n        self.opts = self.model._meta\n        self.has_registered_model = admin_site.is_registered(self.model)\n        super().__init__()\n        if self.verbose_name is None:\n            self.verbose_name = self.model._meta.verbose_name\n        if self.verbose_name_plural is None:\n            self.verbose_name_plural = self.model._meta.verbose_name_plural\n\n    @property\n    def media(self):\n        extra = '' if settings.DEBUG else '.min'\n        js = ['vendor/jquery/jquery%s.js' % extra, 'jquery.init.js',\n              'inlines%s.js' % extra]\n        if self.filter_vertical or self.filter_horizontal:\n            js.extend(['SelectBox.js', 'SelectFilter2.js'])\n        if self.classes and 'collapse' in self.classes:\n            js.append('collapse%s.js' % extra)\n        return forms.Media(js=['admin/js/%s' % url for url in js])\n\n    def get_extra(self, request, obj=None, **kwargs):\n        \"\"\"Hook for customizing the number of extra inline forms.\"\"\"\n        return self.extra\n\n    def get_min_num(self, request, obj=None, **kwargs):\n        \"\"\"Hook for customizing the min number of inline forms.\"\"\"\n        return self.min_num\n\n    def get_max_num(self, request, obj=None, **kwargs):\n        \"\"\"Hook for customizing the max number of extra inline forms.\"\"\"\n        return self.max_num\n\n    def get_formset(self, request, obj=None, **kwargs):\n        \"\"\"Return a BaseInlineFormSet class for use in admin add/change views.\"\"\"\n        if 'fields' in kwargs:\n            fields = kwargs.pop('fields')\n        else:\n            fields = flatten_fieldsets(self.get_fieldsets(request, obj))\n        excluded = self.get_exclude(request, obj)\n        exclude = [] if excluded is None else list(excluded)\n        exclude.extend(self.get_readonly_fields(request, obj))\n        if excluded is None and hasattr(self.form, '_meta') and self.form._meta.exclude:\n            # Take the custom ModelForm's Meta.exclude into account only if the\n            # InlineModelAdmin doesn't define its own.\n            exclude.extend(self.form._meta.exclude)\n        # If exclude is an empty list we use None, since that's the actual\n        # default.\n        exclude = exclude or None\n        can_delete = self.can_delete and self.has_delete_permission(request, obj)\n        defaults = {\n            'form': self.form,\n            'formset': self.formset,\n            'fk_name': self.fk_name,\n            'fields': fields,\n            'exclude': exclude,\n            'formfield_callback': partial(self.formfield_for_dbfield, request=request),\n            'extra': self.get_extra(request, obj, **kwargs),\n            'min_num': self.get_min_num(request, obj, **kwargs),\n            'max_num': self.get_max_num(request, obj, **kwargs),\n            'can_delete': can_delete,\n            **kwargs,\n        }\n\n        base_model_form = defaults['form']\n        can_change = self.has_change_permission(request, obj) if request else True\n        can_add = self.has_add_permission(request, obj) if request else True\n\n        class DeleteProtectedModelForm(base_model_form):\n\n            def hand_clean_DELETE(self):\n                \"\"\"\n                We don't validate the 'DELETE' field itself because on\n                templates it's not rendered using the field information, but\n                just using a generic \"deletion_field\" of the InlineModelAdmin.\n                \"\"\"\n                if self.cleaned_data.get(DELETION_FIELD_NAME, False):\n                    using = router.db_for_write(self._meta.model)\n                    collector = NestedObjects(using=using)\n                    if self.instance._state.adding:\n                        return\n                    collector.collect([self.instance])\n                    if collector.protected:\n                        objs = []\n                        for p in collector.protected:\n                            objs.append(\n                                # Translators: Model verbose name and instance representation,\n                                # suitable to be an item in a list.\n                                _('%(class_name)s %(instance)s') % {\n                                    'class_name': p._meta.verbose_name,\n                                    'instance': p}\n                            )\n                        params = {\n                            'class_name': self._meta.model._meta.verbose_name,\n                            'instance': self.instance,\n                            'related_objects': get_text_list(objs, _('and')),\n                        }\n                        msg = _(\"Deleting %(class_name)s %(instance)s would require \"\n                                \"deleting the following protected related objects: \"\n                                \"%(related_objects)s\")\n                        raise ValidationError(msg, code='deleting_protected', params=params)\n\n            def is_valid(self):\n                result = super().is_valid()\n                self.hand_clean_DELETE()\n                return result\n\n            def has_changed(self):\n                # Protect against unauthorized edits.\n                if not can_change and not self.instance._state.adding:\n                    return False\n                if not can_add and self.instance._state.adding:\n                    return False\n                return super().has_changed()\n\n        defaults['form'] = DeleteProtectedModelForm\n\n        if defaults['fields'] is None and not modelform_defines_fields(defaults['form']):\n            defaults['fields'] = forms.ALL_FIELDS\n\n        return inlineformset_factory(self.parent_model, self.model, **defaults)\n\n    def _get_form_for_get_fields(self, request, obj=None):\n        return self.get_formset(request, obj, fields=None).form\n\n    def get_queryset(self, request):\n        queryset = super().get_queryset(request)\n        if not self.has_view_or_change_permission(request):\n            queryset = queryset.none()\n        return queryset\n\n    def _has_any_perms_for_target_model(self, request, perms):\n        \"\"\"\n        This method is called only when the ModelAdmin's model is for an\n        ManyToManyField's implicit through model (if self.opts.auto_created).\n        Return True if the user has any of the given permissions ('add',\n        'change', etc.) for the model that points to the through model.\n        \"\"\"\n        opts = self.opts\n        # Find the target model of an auto-created many-to-many relationship.\n        for field in opts.fields:\n            if field.remote_field and field.remote_field.model != self.parent_model:\n                opts = field.remote_field.model._meta\n                break\n        return any(\n            request.user.has_perm('%s.%s' % (opts.app_label, get_permission_codename(perm, opts)))\n            for perm in perms\n        )\n\n    def has_add_permission(self, request, obj):\n        if self.opts.auto_created:\n            # Auto-created intermediate models don't have their own\n            # permissions. The user needs to have the change permission for the\n            # related model in order to be able to do anything with the\n            # intermediate model.\n            return self._has_any_perms_for_target_model(request, ['change'])\n        return super().has_add_permission(request)\n\n    def has_change_permission(self, request, obj=None):\n        if self.opts.auto_created:\n            # Same comment as has_add_permission().\n            return self._has_any_perms_for_target_model(request, ['change'])\n        return super().has_change_permission(request)\n\n    def has_delete_permission(self, request, obj=None):\n        if self.opts.auto_created:\n            # Same comment as has_add_permission().\n            return self._has_any_perms_for_target_model(request, ['change'])\n        return super().has_delete_permission(request, obj)\n\n    def has_view_permission(self, request, obj=None):\n        if self.opts.auto_created:\n            # Same comment as has_add_permission(). The 'change' permission\n            # also implies the 'view' permission.\n            return self._has_any_perms_for_target_model(request, ['view', 'change'])\n        return super().has_view_permission(request)\n\n\nclass StackedInline(InlineModelAdmin):\n    template = 'admin/edit_inline/stacked.html'\n\n\nclass TabularInline(InlineModelAdmin):\n    template = 'admin/edit_inline/tabular.html'"}
{"model_name_or_path": "agentless", "instance_id": "django__django-13195", "model_patch": "diff --git a/django/http/response.py b/django/http/response.py\nindex 244654c..ed6ef76 100644\n--- a/django/http/response.py\n+++ b/django/http/response.py\n@@ -210,13 +210,14 @@ class HttpResponseBase:\n         value = signing.get_cookie_signer(salt=key + salt).sign(value)\n         return self.set_cookie(key, value, **kwargs)\n \n-    def delete_cookie(self, key, path='/', domain=None):\n+    def delete_cookie(self, key, path='/', domain=None, samesite=None):\n         # Most browsers ignore the Set-Cookie header if the cookie name starts\n         # with __Host- or __Secure- and the cookie doesn't use the secure flag.\n         secure = key.startswith(('__Secure-', '__Host-'))\n         self.set_cookie(\n             key, max_age=0, path=path, domain=domain, secure=secure,\n             expires='Thu, 01 Jan 1970 00:00:00 GMT',\n+            samesite=samesite,\n         )\n \n     # Common methods used by subclasses\n", "raw_model_patch": "diff --git a/django/http/response.py b/django/http/response.py\nindex 244654c..ed6ef76 100644\n--- a/django/http/response.py\n+++ b/django/http/response.py\n@@ -210,13 +210,14 @@ class HttpResponseBase:\n         value = signing.get_cookie_signer(salt=key + salt).sign(value)\n         return self.set_cookie(key, value, **kwargs)\n \n-    def delete_cookie(self, key, path='/', domain=None):\n+    def delete_cookie(self, key, path='/', domain=None, samesite=None):\n         # Most browsers ignore the Set-Cookie header if the cookie name starts\n         # with __Host- or __Secure- and the cookie doesn't use the secure flag.\n         secure = key.startswith(('__Secure-', '__Host-'))\n         self.set_cookie(\n             key, max_age=0, path=path, domain=domain, secure=secure,\n             expires='Thu, 01 Jan 1970 00:00:00 GMT',\n+            samesite=samesite,\n         )\n \n     # Common methods used by subclasses\n", "original_file_content": "import datetime\nimport json\nimport mimetypes\nimport os\nimport re\nimport sys\nimport time\nfrom email.header import Header\nfrom http.client import responses\nfrom urllib.parse import quote, urlparse\n\nfrom django.conf import settings\nfrom django.core import signals, signing\nfrom django.core.exceptions import DisallowedRedirect\nfrom django.core.serializers.json import DjangoJSONEncoder\nfrom django.http.cookie import SimpleCookie\nfrom django.utils import timezone\nfrom django.utils.encoding import iri_to_uri\nfrom django.utils.http import http_date\nfrom django.utils.regex_helper import _lazy_re_compile\n\n_charset_from_content_type_re = _lazy_re_compile(r';\\s*charset=(?P<charset>[^\\s;]+)', re.I)\n\n\nclass BadHeaderError(ValueError):\n    pass\n\n\nclass HttpResponseBase:\n    \"\"\"\n    An HTTP response base class with dictionary-accessed headers.\n\n    This class doesn't handle content. It should not be used directly.\n    Use the HttpResponse and StreamingHttpResponse subclasses instead.\n    \"\"\"\n\n    status_code = 200\n\n    def __init__(self, content_type=None, status=None, reason=None, charset=None):\n        # _headers is a mapping of the lowercase name to the original case of\n        # the header (required for working with legacy systems) and the header\n        # value. Both the name of the header and its value are ASCII strings.\n        self._headers = {}\n        self._resource_closers = []\n        # This parameter is set by the handler. It's necessary to preserve the\n        # historical behavior of request_finished.\n        self._handler_class = None\n        self.cookies = SimpleCookie()\n        self.closed = False\n        if status is not None:\n            try:\n                self.status_code = int(status)\n            except (ValueError, TypeError):\n                raise TypeError('HTTP status code must be an integer.')\n\n            if not 100 <= self.status_code <= 599:\n                raise ValueError('HTTP status code must be an integer from 100 to 599.')\n        self._reason_phrase = reason\n        self._charset = charset\n        if content_type is None:\n            content_type = 'text/html; charset=%s' % self.charset\n        self['Content-Type'] = content_type\n\n    @property\n    def reason_phrase(self):\n        if self._reason_phrase is not None:\n            return self._reason_phrase\n        # Leave self._reason_phrase unset in order to use the default\n        # reason phrase for status code.\n        return responses.get(self.status_code, 'Unknown Status Code')\n\n    @reason_phrase.setter\n    def reason_phrase(self, value):\n        self._reason_phrase = value\n\n    @property\n    def charset(self):\n        if self._charset is not None:\n            return self._charset\n        content_type = self.get('Content-Type', '')\n        matched = _charset_from_content_type_re.search(content_type)\n        if matched:\n            # Extract the charset and strip its double quotes\n            return matched['charset'].replace('\"', '')\n        return settings.DEFAULT_CHARSET\n\n    @charset.setter\n    def charset(self, value):\n        self._charset = value\n\n    def serialize_headers(self):\n        \"\"\"HTTP headers as a bytestring.\"\"\"\n        def to_bytes(val, encoding):\n            return val if isinstance(val, bytes) else val.encode(encoding)\n\n        headers = [\n            (to_bytes(key, 'ascii') + b': ' + to_bytes(value, 'latin-1'))\n            for key, value in self._headers.values()\n        ]\n        return b'\\r\\n'.join(headers)\n\n    __bytes__ = serialize_headers\n\n    @property\n    def _content_type_for_repr(self):\n        return ', \"%s\"' % self['Content-Type'] if 'Content-Type' in self else ''\n\n    def _convert_to_charset(self, value, charset, mime_encode=False):\n        \"\"\"\n        Convert headers key/value to ascii/latin-1 native strings.\n\n        `charset` must be 'ascii' or 'latin-1'. If `mime_encode` is True and\n        `value` can't be represented in the given charset, apply MIME-encoding.\n        \"\"\"\n        if not isinstance(value, (bytes, str)):\n            value = str(value)\n        if ((isinstance(value, bytes) and (b'\\n' in value or b'\\r' in value)) or\n                isinstance(value, str) and ('\\n' in value or '\\r' in value)):\n            raise BadHeaderError(\"Header values can't contain newlines (got %r)\" % value)\n        try:\n            if isinstance(value, str):\n                # Ensure string is valid in given charset\n                value.encode(charset)\n            else:\n                # Convert bytestring using given charset\n                value = value.decode(charset)\n        except UnicodeError as e:\n            if mime_encode:\n                value = Header(value, 'utf-8', maxlinelen=sys.maxsize).encode()\n            else:\n                e.reason += ', HTTP response headers must be in %s format' % charset\n                raise\n        return value\n\n    def __setitem__(self, header, value):\n        header = self._convert_to_charset(header, 'ascii')\n        value = self._convert_to_charset(value, 'latin-1', mime_encode=True)\n        self._headers[header.lower()] = (header, value)\n\n    def __delitem__(self, header):\n        self._headers.pop(header.lower(), False)\n\n    def __getitem__(self, header):\n        return self._headers[header.lower()][1]\n\n    def has_header(self, header):\n        \"\"\"Case-insensitive check for a header.\"\"\"\n        return header.lower() in self._headers\n\n    __contains__ = has_header\n\n    def items(self):\n        return self._headers.values()\n\n    def get(self, header, alternate=None):\n        return self._headers.get(header.lower(), (None, alternate))[1]\n\n    def set_cookie(self, key, value='', max_age=None, expires=None, path='/',\n                   domain=None, secure=False, httponly=False, samesite=None):\n        \"\"\"\n        Set a cookie.\n\n        ``expires`` can be:\n        - a string in the correct format,\n        - a naive ``datetime.datetime`` object in UTC,\n        - an aware ``datetime.datetime`` object in any time zone.\n        If it is a ``datetime.datetime`` object then calculate ``max_age``.\n        \"\"\"\n        self.cookies[key] = value\n        if expires is not None:\n            if isinstance(expires, datetime.datetime):\n                if timezone.is_aware(expires):\n                    expires = timezone.make_naive(expires, timezone.utc)\n                delta = expires - expires.utcnow()\n                # Add one second so the date matches exactly (a fraction of\n                # time gets lost between converting to a timedelta and\n                # then the date string).\n                delta = delta + datetime.timedelta(seconds=1)\n                # Just set max_age - the max_age logic will set expires.\n                expires = None\n                max_age = max(0, delta.days * 86400 + delta.seconds)\n            else:\n                self.cookies[key]['expires'] = expires\n        else:\n            self.cookies[key]['expires'] = ''\n        if max_age is not None:\n            self.cookies[key]['max-age'] = max_age\n            # IE requires expires, so set it if hasn't been already.\n            if not expires:\n                self.cookies[key]['expires'] = http_date(time.time() + max_age)\n        if path is not None:\n            self.cookies[key]['path'] = path\n        if domain is not None:\n            self.cookies[key]['domain'] = domain\n        if secure:\n            self.cookies[key]['secure'] = True\n        if httponly:\n            self.cookies[key]['httponly'] = True\n        if samesite:\n            if samesite.lower() not in ('lax', 'none', 'strict'):\n                raise ValueError('samesite must be \"lax\", \"none\", or \"strict\".')\n            self.cookies[key]['samesite'] = samesite\n\n    def setdefault(self, key, value):\n        \"\"\"Set a header unless it has already been set.\"\"\"\n        if key not in self:\n            self[key] = value\n\n    def set_signed_cookie(self, key, value, salt='', **kwargs):\n        value = signing.get_cookie_signer(salt=key + salt).sign(value)\n        return self.set_cookie(key, value, **kwargs)\n\n    def delete_cookie(self, key, path='/', domain=None):\n        # Most browsers ignore the Set-Cookie header if the cookie name starts\n        # with __Host- or __Secure- and the cookie doesn't use the secure flag.\n        secure = key.startswith(('__Secure-', '__Host-'))\n        self.set_cookie(\n            key, max_age=0, path=path, domain=domain, secure=secure,\n            expires='Thu, 01 Jan 1970 00:00:00 GMT',\n        )\n\n    # Common methods used by subclasses\n\n    def make_bytes(self, value):\n        \"\"\"Turn a value into a bytestring encoded in the output charset.\"\"\"\n        # Per PEP 3333, this response body must be bytes. To avoid returning\n        # an instance of a subclass, this function returns `bytes(value)`.\n        # This doesn't make a copy when `value` already contains bytes.\n\n        # Handle string types -- we can't rely on force_bytes here because:\n        # - Python attempts str conversion first\n        # - when self._charset != 'utf-8' it re-encodes the content\n        if isinstance(value, (bytes, memoryview)):\n            return bytes(value)\n        if isinstance(value, str):\n            return bytes(value.encode(self.charset))\n        # Handle non-string types.\n        return str(value).encode(self.charset)\n\n    # These methods partially implement the file-like object interface.\n    # See https://docs.python.org/library/io.html#io.IOBase\n\n    # The WSGI server must call this method upon completion of the request.\n    # See http://blog.dscpl.com.au/2012/10/obligations-for-calling-close-on.html\n    def close(self):\n        for closer in self._resource_closers:\n            try:\n                closer()\n            except Exception:\n                pass\n        # Free resources that were still referenced.\n        self._resource_closers.clear()\n        self.closed = True\n        signals.request_finished.send(sender=self._handler_class)\n\n    def write(self, content):\n        raise OSError('This %s instance is not writable' % self.__class__.__name__)\n\n    def flush(self):\n        pass\n\n    def tell(self):\n        raise OSError('This %s instance cannot tell its position' % self.__class__.__name__)\n\n    # These methods partially implement a stream-like object interface.\n    # See https://docs.python.org/library/io.html#io.IOBase\n\n    def readable(self):\n        return False\n\n    def seekable(self):\n        return False\n\n    def writable(self):\n        return False\n\n    def writelines(self, lines):\n        raise OSError('This %s instance is not writable' % self.__class__.__name__)\n\n\nclass HttpResponse(HttpResponseBase):\n    \"\"\"\n    An HTTP response class with a string as content.\n\n    This content that can be read, appended to, or replaced.\n    \"\"\"\n\n    streaming = False\n\n    def __init__(self, content=b'', *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Content is a bytestring. See the `content` property methods.\n        self.content = content\n\n    def __repr__(self):\n        return '<%(cls)s status_code=%(status_code)d%(content_type)s>' % {\n            'cls': self.__class__.__name__,\n            'status_code': self.status_code,\n            'content_type': self._content_type_for_repr,\n        }\n\n    def serialize(self):\n        \"\"\"Full HTTP message, including headers, as a bytestring.\"\"\"\n        return self.serialize_headers() + b'\\r\\n\\r\\n' + self.content\n\n    __bytes__ = serialize\n\n    @property\n    def content(self):\n        return b''.join(self._container)\n\n    @content.setter\n    def content(self, value):\n        # Consume iterators upon assignment to allow repeated iteration.\n        if hasattr(value, '__iter__') and not isinstance(value, (bytes, str)):\n            content = b''.join(self.make_bytes(chunk) for chunk in value)\n            if hasattr(value, 'close'):\n                try:\n                    value.close()\n                except Exception:\n                    pass\n        else:\n            content = self.make_bytes(value)\n        # Create a list of properly encoded bytestrings to support write().\n        self._container = [content]\n\n    def __iter__(self):\n        return iter(self._container)\n\n    def write(self, content):\n        self._container.append(self.make_bytes(content))\n\n    def tell(self):\n        return len(self.content)\n\n    def getvalue(self):\n        return self.content\n\n    def writable(self):\n        return True\n\n    def writelines(self, lines):\n        for line in lines:\n            self.write(line)\n\n\nclass StreamingHttpResponse(HttpResponseBase):\n    \"\"\"\n    A streaming HTTP response class with an iterator as content.\n\n    This should only be iterated once, when the response is streamed to the\n    client. However, it can be appended to or replaced with a new iterator\n    that wraps the original content (or yields entirely new content).\n    \"\"\"\n\n    streaming = True\n\n    def __init__(self, streaming_content=(), *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # `streaming_content` should be an iterable of bytestrings.\n        # See the `streaming_content` property methods.\n        self.streaming_content = streaming_content\n\n    @property\n    def content(self):\n        raise AttributeError(\n            \"This %s instance has no `content` attribute. Use \"\n            \"`streaming_content` instead.\" % self.__class__.__name__\n        )\n\n    @property\n    def streaming_content(self):\n        return map(self.make_bytes, self._iterator)\n\n    @streaming_content.setter\n    def streaming_content(self, value):\n        self._set_streaming_content(value)\n\n    def _set_streaming_content(self, value):\n        # Ensure we can never iterate on \"value\" more than once.\n        self._iterator = iter(value)\n        if hasattr(value, 'close'):\n            self._resource_closers.append(value.close)\n\n    def __iter__(self):\n        return self.streaming_content\n\n    def getvalue(self):\n        return b''.join(self.streaming_content)\n\n\nclass FileResponse(StreamingHttpResponse):\n    \"\"\"\n    A streaming HTTP response class optimized for files.\n    \"\"\"\n    block_size = 4096\n\n    def __init__(self, *args, as_attachment=False, filename='', **kwargs):\n        self.as_attachment = as_attachment\n        self.filename = filename\n        super().__init__(*args, **kwargs)\n\n    def _set_streaming_content(self, value):\n        if not hasattr(value, 'read'):\n            self.file_to_stream = None\n            return super()._set_streaming_content(value)\n\n        self.file_to_stream = filelike = value\n        if hasattr(filelike, 'close'):\n            self._resource_closers.append(filelike.close)\n        value = iter(lambda: filelike.read(self.block_size), b'')\n        self.set_headers(filelike)\n        super()._set_streaming_content(value)\n\n    def set_headers(self, filelike):\n        \"\"\"\n        Set some common response headers (Content-Length, Content-Type, and\n        Content-Disposition) based on the `filelike` response content.\n        \"\"\"\n        encoding_map = {\n            'bzip2': 'application/x-bzip',\n            'gzip': 'application/gzip',\n            'xz': 'application/x-xz',\n        }\n        filename = getattr(filelike, 'name', None)\n        filename = filename if (isinstance(filename, str) and filename) else self.filename\n        if os.path.isabs(filename):\n            self['Content-Length'] = os.path.getsize(filelike.name)\n        elif hasattr(filelike, 'getbuffer'):\n            self['Content-Length'] = filelike.getbuffer().nbytes\n\n        if self.get('Content-Type', '').startswith('text/html'):\n            if filename:\n                content_type, encoding = mimetypes.guess_type(filename)\n                # Encoding isn't set to prevent browsers from automatically\n                # uncompressing files.\n                content_type = encoding_map.get(encoding, content_type)\n                self['Content-Type'] = content_type or 'application/octet-stream'\n            else:\n                self['Content-Type'] = 'application/octet-stream'\n\n        filename = self.filename or os.path.basename(filename)\n        if filename:\n            disposition = 'attachment' if self.as_attachment else 'inline'\n            try:\n                filename.encode('ascii')\n                file_expr = 'filename=\"{}\"'.format(filename)\n            except UnicodeEncodeError:\n                file_expr = \"filename*=utf-8''{}\".format(quote(filename))\n            self['Content-Disposition'] = '{}; {}'.format(disposition, file_expr)\n        elif self.as_attachment:\n            self['Content-Disposition'] = 'attachment'\n\n\nclass HttpResponseRedirectBase(HttpResponse):\n    allowed_schemes = ['http', 'https', 'ftp']\n\n    def __init__(self, redirect_to, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self['Location'] = iri_to_uri(redirect_to)\n        parsed = urlparse(str(redirect_to))\n        if parsed.scheme and parsed.scheme not in self.allowed_schemes:\n            raise DisallowedRedirect(\"Unsafe redirect to URL with protocol '%s'\" % parsed.scheme)\n\n    url = property(lambda self: self['Location'])\n\n    def __repr__(self):\n        return '<%(cls)s status_code=%(status_code)d%(content_type)s, url=\"%(url)s\">' % {\n            'cls': self.__class__.__name__,\n            'status_code': self.status_code,\n            'content_type': self._content_type_for_repr,\n            'url': self.url,\n        }\n\n\nclass HttpResponseRedirect(HttpResponseRedirectBase):\n    status_code = 302\n\n\nclass HttpResponsePermanentRedirect(HttpResponseRedirectBase):\n    status_code = 301\n\n\nclass HttpResponseNotModified(HttpResponse):\n    status_code = 304\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        del self['content-type']\n\n    @HttpResponse.content.setter\n    def content(self, value):\n        if value:\n            raise AttributeError(\"You cannot set content to a 304 (Not Modified) response\")\n        self._container = []\n\n\nclass HttpResponseBadRequest(HttpResponse):\n    status_code = 400\n\n\nclass HttpResponseNotFound(HttpResponse):\n    status_code = 404\n\n\nclass HttpResponseForbidden(HttpResponse):\n    status_code = 403\n\n\nclass HttpResponseNotAllowed(HttpResponse):\n    status_code = 405\n\n    def __init__(self, permitted_methods, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self['Allow'] = ', '.join(permitted_methods)\n\n    def __repr__(self):\n        return '<%(cls)s [%(methods)s] status_code=%(status_code)d%(content_type)s>' % {\n            'cls': self.__class__.__name__,\n            'status_code': self.status_code,\n            'content_type': self._content_type_for_repr,\n            'methods': self['Allow'],\n        }\n\n\nclass HttpResponseGone(HttpResponse):\n    status_code = 410\n\n\nclass HttpResponseServerError(HttpResponse):\n    status_code = 500\n\n\nclass Http404(Exception):\n    pass\n\n\nclass JsonResponse(HttpResponse):\n    \"\"\"\n    An HTTP response class that consumes data to be serialized to JSON.\n\n    :param data: Data to be dumped into json. By default only ``dict`` objects\n      are allowed to be passed due to a security flaw before EcmaScript 5. See\n      the ``safe`` parameter for more information.\n    :param encoder: Should be a json encoder class. Defaults to\n      ``django.core.serializers.json.DjangoJSONEncoder``.\n    :param safe: Controls if only ``dict`` objects may be serialized. Defaults\n      to ``True``.\n    :param json_dumps_params: A dictionary of kwargs passed to json.dumps().\n    \"\"\"\n\n    def __init__(self, data, encoder=DjangoJSONEncoder, safe=True,\n                 json_dumps_params=None, **kwargs):\n        if safe and not isinstance(data, dict):\n            raise TypeError(\n                'In order to allow non-dict objects to be serialized set the '\n                'safe parameter to False.'\n            )\n        if json_dumps_params is None:\n            json_dumps_params = {}\n        kwargs.setdefault('content_type', 'application/json')\n        data = json.dumps(data, cls=encoder, **json_dumps_params)\n        super().__init__(content=data, **kwargs)"}
{"model_name_or_path": "agentless", "instance_id": "django__django-14315", "model_patch": "diff --git a/django/db/backends/postgresql/client.py b/django/db/backends/postgresql/client.py\nindex 8b69ea1..e355a66 100644\n--- a/django/db/backends/postgresql/client.py\n+++ b/django/db/backends/postgresql/client.py\n@@ -14,8 +14,8 @@ class DatabaseClient(BaseDatabaseClient):\n         host = settings_dict.get('HOST')\n         port = settings_dict.get('PORT')\n         dbname = settings_dict.get('NAME')\n-        user = settings_dict.get('USER')\n-        passwd = settings_dict.get('PASSWORD')\n+        user = settings_dict.get('USER') or os.environ.get('USER')\n+        passwd = settings_dict.get('PASSWORD') or os.environ.get('PASSWORD')\n         passfile = options.get('passfile')\n         service = options.get('service')\n         sslmode = options.get('sslmode')\n", "raw_model_patch": "diff --git a/django/db/backends/postgresql/client.py b/django/db/backends/postgresql/client.py\nindex 8b69ea1..e355a66 100644\n--- a/django/db/backends/postgresql/client.py\n+++ b/django/db/backends/postgresql/client.py\n@@ -14,8 +14,8 @@ class DatabaseClient(BaseDatabaseClient):\n         host = settings_dict.get('HOST')\n         port = settings_dict.get('PORT')\n         dbname = settings_dict.get('NAME')\n-        user = settings_dict.get('USER')\n-        passwd = settings_dict.get('PASSWORD')\n+        user = settings_dict.get('USER') or os.environ.get('USER')\n+        passwd = settings_dict.get('PASSWORD') or os.environ.get('PASSWORD')\n         passfile = options.get('passfile')\n         service = options.get('service')\n         sslmode = options.get('sslmode')\n", "original_file_content": "import signal\n\nfrom django.db.backends.base.client import BaseDatabaseClient\n\n\nclass DatabaseClient(BaseDatabaseClient):\n    executable_name = 'psql'\n\n    @classmethod\n    def settings_to_cmd_args_env(cls, settings_dict, parameters):\n        args = [cls.executable_name]\n        options = settings_dict.get('OPTIONS', {})\n\n        host = settings_dict.get('HOST')\n        port = settings_dict.get('PORT')\n        dbname = settings_dict.get('NAME')\n        user = settings_dict.get('USER')\n        passwd = settings_dict.get('PASSWORD')\n        passfile = options.get('passfile')\n        service = options.get('service')\n        sslmode = options.get('sslmode')\n        sslrootcert = options.get('sslrootcert')\n        sslcert = options.get('sslcert')\n        sslkey = options.get('sslkey')\n\n        if not dbname and not service:\n            # Connect to the default 'postgres' db.\n            dbname = 'postgres'\n        if user:\n            args += ['-U', user]\n        if host:\n            args += ['-h', host]\n        if port:\n            args += ['-p', str(port)]\n        if dbname:\n            args += [dbname]\n        args.extend(parameters)\n\n        env = {}\n        if passwd:\n            env['PGPASSWORD'] = str(passwd)\n        if service:\n            env['PGSERVICE'] = str(service)\n        if sslmode:\n            env['PGSSLMODE'] = str(sslmode)\n        if sslrootcert:\n            env['PGSSLROOTCERT'] = str(sslrootcert)\n        if sslcert:\n            env['PGSSLCERT'] = str(sslcert)\n        if sslkey:\n            env['PGSSLKEY'] = str(sslkey)\n        if passfile:\n            env['PGPASSFILE'] = str(passfile)\n        return args, env\n\n    def runshell(self, parameters):\n        sigint_handler = signal.getsignal(signal.SIGINT)\n        try:\n            # Allow SIGINT to pass to psql to abort queries.\n            signal.signal(signal.SIGINT, signal.SIG_IGN)\n            super().runshell(parameters)\n        finally:\n            # Restore the original SIGINT handler.\n            signal.signal(signal.SIGINT, sigint_handler)"}
{"model_name_or_path": "agentless", "instance_id": "django__django-15098", "model_patch": "diff --git a/django/utils/translation/trans_real.py b/django/utils/translation/trans_real.py\nindex 488a2d8..c87483f 100644\n--- a/django/utils/translation/trans_real.py\n+++ b/django/utils/translation/trans_real.py\n@@ -486,6 +486,8 @@ def get_supported_language_variant(lang_code, strict=False):\n             possible_lang_codes.append(lang_code[:i])\n         generic_lang_code = possible_lang_codes[-1]\n         supported_lang_codes = get_languages()\n+        # Normalize script and region in language codes\n+        lang_code_normalized = lang_code.lower()  # Normalize to lowercase\n \n         for code in possible_lang_codes:\n             if code in supported_lang_codes and check_for_language(code):\n", "raw_model_patch": "diff --git a/django/utils/translation/trans_real.py b/django/utils/translation/trans_real.py\nindex 488a2d8..c87483f 100644\n--- a/django/utils/translation/trans_real.py\n+++ b/django/utils/translation/trans_real.py\n@@ -486,6 +486,8 @@ def get_supported_language_variant(lang_code, strict=False):\n             possible_lang_codes.append(lang_code[:i])\n         generic_lang_code = possible_lang_codes[-1]\n         supported_lang_codes = get_languages()\n+        # Normalize script and region in language codes\n+        lang_code_normalized = lang_code.lower()  # Normalize to lowercase\n \n         for code in possible_lang_codes:\n             if code in supported_lang_codes and check_for_language(code):\n", "original_file_content": "\"\"\"Translation helper functions.\"\"\"\nimport functools\nimport gettext as gettext_module\nimport os\nimport re\nimport sys\nimport warnings\n\nfrom asgiref.local import Local\n\nfrom django.apps import apps\nfrom django.conf import settings\nfrom django.conf.locale import LANG_INFO\nfrom django.core.exceptions import AppRegistryNotReady\nfrom django.core.signals import setting_changed\nfrom django.dispatch import receiver\nfrom django.utils.regex_helper import _lazy_re_compile\nfrom django.utils.safestring import SafeData, mark_safe\n\nfrom . import to_language, to_locale\n\n# Translations are cached in a dictionary for every language.\n# The active translations are stored by threadid to make them thread local.\n_translations = {}\n_active = Local()\n\n# The default translation is based on the settings file.\n_default = None\n\n# magic gettext number to separate context from message\nCONTEXT_SEPARATOR = \"\\x04\"\n\n# Format of Accept-Language header values. From RFC 2616, section 14.4 and 3.9\n# and RFC 3066, section 2.1\naccept_language_re = _lazy_re_compile(r'''\n        ([A-Za-z]{1,8}(?:-[A-Za-z0-9]{1,8})*|\\*)      # \"en\", \"en-au\", \"x-y-z\", \"es-419\", \"*\"\n        (?:\\s*;\\s*q=(0(?:\\.\\d{,3})?|1(?:\\.0{,3})?))?  # Optional \"q=1.00\", \"q=0.8\"\n        (?:\\s*,\\s*|$)                                 # Multiple accepts per header.\n        ''', re.VERBOSE)\n\nlanguage_code_re = _lazy_re_compile(\n    r'^[a-z]{1,8}(?:-[a-z0-9]{1,8})*(?:@[a-z0-9]{1,20})?$',\n    re.IGNORECASE\n)\n\nlanguage_code_prefix_re = _lazy_re_compile(r'^/(\\w+([@-]\\w+)?)(/|$)')\n\n\n@receiver(setting_changed)\ndef reset_cache(**kwargs):\n    \"\"\"\n    Reset global state when LANGUAGES setting has been changed, as some\n    languages should no longer be accepted.\n    \"\"\"\n    if kwargs['setting'] in ('LANGUAGES', 'LANGUAGE_CODE'):\n        check_for_language.cache_clear()\n        get_languages.cache_clear()\n        get_supported_language_variant.cache_clear()\n\n\nclass TranslationCatalog:\n    \"\"\"\n    Simulate a dict for DjangoTranslation._catalog so as multiple catalogs\n    with different plural equations are kept separate.\n    \"\"\"\n    def __init__(self, trans=None):\n        self._catalogs = [trans._catalog.copy()] if trans else [{}]\n        self._plurals = [trans.plural] if trans else [lambda n: int(n != 1)]\n\n    def __getitem__(self, key):\n        for cat in self._catalogs:\n            try:\n                return cat[key]\n            except KeyError:\n                pass\n        raise KeyError(key)\n\n    def __setitem__(self, key, value):\n        self._catalogs[0][key] = value\n\n    def __contains__(self, key):\n        return any(key in cat for cat in self._catalogs)\n\n    def items(self):\n        for cat in self._catalogs:\n            yield from cat.items()\n\n    def keys(self):\n        for cat in self._catalogs:\n            yield from cat.keys()\n\n    def update(self, trans):\n        # Merge if plural function is the same, else prepend.\n        for cat, plural in zip(self._catalogs, self._plurals):\n            if trans.plural.__code__ == plural.__code__:\n                cat.update(trans._catalog)\n                break\n        else:\n            self._catalogs.insert(0, trans._catalog.copy())\n            self._plurals.insert(0, trans.plural)\n\n    def get(self, key, default=None):\n        missing = object()\n        for cat in self._catalogs:\n            result = cat.get(key, missing)\n            if result is not missing:\n                return result\n        return default\n\n    def plural(self, msgid, num):\n        for cat, plural in zip(self._catalogs, self._plurals):\n            tmsg = cat.get((msgid, plural(num)))\n            if tmsg is not None:\n                return tmsg\n        raise KeyError\n\n\nclass DjangoTranslation(gettext_module.GNUTranslations):\n    \"\"\"\n    Set up the GNUTranslations context with regard to output charset.\n\n    This translation object will be constructed out of multiple GNUTranslations\n    objects by merging their catalogs. It will construct an object for the\n    requested language and add a fallback to the default language, if it's\n    different from the requested language.\n    \"\"\"\n    domain = 'django'\n\n    def __init__(self, language, domain=None, localedirs=None):\n        \"\"\"Create a GNUTranslations() using many locale directories\"\"\"\n        gettext_module.GNUTranslations.__init__(self)\n        if domain is not None:\n            self.domain = domain\n\n        self.__language = language\n        self.__to_language = to_language(language)\n        self.__locale = to_locale(language)\n        self._catalog = None\n        # If a language doesn't have a catalog, use the Germanic default for\n        # pluralization: anything except one is pluralized.\n        self.plural = lambda n: int(n != 1)\n\n        if self.domain == 'django':\n            if localedirs is not None:\n                # A module-level cache is used for caching 'django' translations\n                warnings.warn(\"localedirs is ignored when domain is 'django'.\", RuntimeWarning)\n                localedirs = None\n            self._init_translation_catalog()\n\n        if localedirs:\n            for localedir in localedirs:\n                translation = self._new_gnu_trans(localedir)\n                self.merge(translation)\n        else:\n            self._add_installed_apps_translations()\n\n        self._add_local_translations()\n        if self.__language == settings.LANGUAGE_CODE and self.domain == 'django' and self._catalog is None:\n            # default lang should have at least one translation file available.\n            raise OSError('No translation files found for default language %s.' % settings.LANGUAGE_CODE)\n        self._add_fallback(localedirs)\n        if self._catalog is None:\n            # No catalogs found for this language, set an empty catalog.\n            self._catalog = TranslationCatalog()\n\n    def __repr__(self):\n        return \"<DjangoTranslation lang:%s>\" % self.__language\n\n    def _new_gnu_trans(self, localedir, use_null_fallback=True):\n        \"\"\"\n        Return a mergeable gettext.GNUTranslations instance.\n\n        A convenience wrapper. By default gettext uses 'fallback=False'.\n        Using param `use_null_fallback` to avoid confusion with any other\n        references to 'fallback'.\n        \"\"\"\n        return gettext_module.translation(\n            domain=self.domain,\n            localedir=localedir,\n            languages=[self.__locale],\n            fallback=use_null_fallback,\n        )\n\n    def _init_translation_catalog(self):\n        \"\"\"Create a base catalog using global django translations.\"\"\"\n        settingsfile = sys.modules[settings.__module__].__file__\n        localedir = os.path.join(os.path.dirname(settingsfile), 'locale')\n        translation = self._new_gnu_trans(localedir)\n        self.merge(translation)\n\n    def _add_installed_apps_translations(self):\n        \"\"\"Merge translations from each installed app.\"\"\"\n        try:\n            app_configs = reversed(list(apps.get_app_configs()))\n        except AppRegistryNotReady:\n            raise AppRegistryNotReady(\n                \"The translation infrastructure cannot be initialized before the \"\n                \"apps registry is ready. Check that you don't make non-lazy \"\n                \"gettext calls at import time.\")\n        for app_config in app_configs:\n            localedir = os.path.join(app_config.path, 'locale')\n            if os.path.exists(localedir):\n                translation = self._new_gnu_trans(localedir)\n                self.merge(translation)\n\n    def _add_local_translations(self):\n        \"\"\"Merge translations defined in LOCALE_PATHS.\"\"\"\n        for localedir in reversed(settings.LOCALE_PATHS):\n            translation = self._new_gnu_trans(localedir)\n            self.merge(translation)\n\n    def _add_fallback(self, localedirs=None):\n        \"\"\"Set the GNUTranslations() fallback with the default language.\"\"\"\n        # Don't set a fallback for the default language or any English variant\n        # (as it's empty, so it'll ALWAYS fall back to the default language)\n        if self.__language == settings.LANGUAGE_CODE or self.__language.startswith('en'):\n            return\n        if self.domain == 'django':\n            # Get from cache\n            default_translation = translation(settings.LANGUAGE_CODE)\n        else:\n            default_translation = DjangoTranslation(\n                settings.LANGUAGE_CODE, domain=self.domain, localedirs=localedirs\n            )\n        self.add_fallback(default_translation)\n\n    def merge(self, other):\n        \"\"\"Merge another translation into this catalog.\"\"\"\n        if not getattr(other, '_catalog', None):\n            return  # NullTranslations() has no _catalog\n        if self._catalog is None:\n            # Take plural and _info from first catalog found (generally Django's).\n            self.plural = other.plural\n            self._info = other._info.copy()\n            self._catalog = TranslationCatalog(other)\n        else:\n            self._catalog.update(other)\n        if other._fallback:\n            self.add_fallback(other._fallback)\n\n    def language(self):\n        \"\"\"Return the translation language.\"\"\"\n        return self.__language\n\n    def to_language(self):\n        \"\"\"Return the translation language name.\"\"\"\n        return self.__to_language\n\n    def ngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog.plural(msgid1, n)\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ngettext(msgid1, msgid2, n)\n            if n == 1:\n                tmsg = msgid1\n            else:\n                tmsg = msgid2\n        return tmsg\n\n\ndef translation(language):\n    \"\"\"\n    Return a translation object in the default 'django' domain.\n    \"\"\"\n    global _translations\n    if language not in _translations:\n        _translations[language] = DjangoTranslation(language)\n    return _translations[language]\n\n\ndef activate(language):\n    \"\"\"\n    Fetch the translation object for a given language and install it as the\n    current translation object for the current thread.\n    \"\"\"\n    if not language:\n        return\n    _active.value = translation(language)\n\n\ndef deactivate():\n    \"\"\"\n    Uninstall the active translation object so that further _() calls resolve\n    to the default translation object.\n    \"\"\"\n    if hasattr(_active, \"value\"):\n        del _active.value\n\n\ndef deactivate_all():\n    \"\"\"\n    Make the active translation object a NullTranslations() instance. This is\n    useful when we want delayed translations to appear as the original string\n    for some reason.\n    \"\"\"\n    _active.value = gettext_module.NullTranslations()\n    _active.value.to_language = lambda *args: None\n\n\ndef get_language():\n    \"\"\"Return the currently selected language.\"\"\"\n    t = getattr(_active, \"value\", None)\n    if t is not None:\n        try:\n            return t.to_language()\n        except AttributeError:\n            pass\n    # If we don't have a real translation object, assume it's the default language.\n    return settings.LANGUAGE_CODE\n\n\ndef get_language_bidi():\n    \"\"\"\n    Return selected language's BiDi layout.\n\n    * False = left-to-right layout\n    * True = right-to-left layout\n    \"\"\"\n    lang = get_language()\n    if lang is None:\n        return False\n    else:\n        base_lang = get_language().split('-')[0]\n        return base_lang in settings.LANGUAGES_BIDI\n\n\ndef catalog():\n    \"\"\"\n    Return the current active catalog for further processing.\n    This can be used if you need to modify the catalog or want to access the\n    whole message catalog instead of just translating one string.\n    \"\"\"\n    global _default\n\n    t = getattr(_active, \"value\", None)\n    if t is not None:\n        return t\n    if _default is None:\n        _default = translation(settings.LANGUAGE_CODE)\n    return _default\n\n\ndef gettext(message):\n    \"\"\"\n    Translate the 'message' string. It uses the current thread to find the\n    translation object to use. If no current translation is activated, the\n    message will be run through the default translation object.\n    \"\"\"\n    global _default\n\n    eol_message = message.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n\n    if eol_message:\n        _default = _default or translation(settings.LANGUAGE_CODE)\n        translation_object = getattr(_active, \"value\", _default)\n\n        result = translation_object.gettext(eol_message)\n    else:\n        # Return an empty value of the corresponding type if an empty message\n        # is given, instead of metadata, which is the default gettext behavior.\n        result = type(message)('')\n\n    if isinstance(message, SafeData):\n        return mark_safe(result)\n\n    return result\n\n\ndef pgettext(context, message):\n    msg_with_ctxt = \"%s%s%s\" % (context, CONTEXT_SEPARATOR, message)\n    result = gettext(msg_with_ctxt)\n    if CONTEXT_SEPARATOR in result:\n        # Translation not found\n        result = message\n    elif isinstance(message, SafeData):\n        result = mark_safe(result)\n    return result\n\n\ndef gettext_noop(message):\n    \"\"\"\n    Mark strings for translation but don't translate them now. This can be\n    used to store strings in global variables that should stay in the base\n    language (because they might be used externally) and will be translated\n    later.\n    \"\"\"\n    return message\n\n\ndef do_ntranslate(singular, plural, number, translation_function):\n    global _default\n\n    t = getattr(_active, \"value\", None)\n    if t is not None:\n        return getattr(t, translation_function)(singular, plural, number)\n    if _default is None:\n        _default = translation(settings.LANGUAGE_CODE)\n    return getattr(_default, translation_function)(singular, plural, number)\n\n\ndef ngettext(singular, plural, number):\n    \"\"\"\n    Return a string of the translation of either the singular or plural,\n    based on the number.\n    \"\"\"\n    return do_ntranslate(singular, plural, number, 'ngettext')\n\n\ndef npgettext(context, singular, plural, number):\n    msgs_with_ctxt = (\"%s%s%s\" % (context, CONTEXT_SEPARATOR, singular),\n                      \"%s%s%s\" % (context, CONTEXT_SEPARATOR, plural),\n                      number)\n    result = ngettext(*msgs_with_ctxt)\n    if CONTEXT_SEPARATOR in result:\n        # Translation not found\n        result = ngettext(singular, plural, number)\n    return result\n\n\ndef all_locale_paths():\n    \"\"\"\n    Return a list of paths to user-provides languages files.\n    \"\"\"\n    globalpath = os.path.join(\n        os.path.dirname(sys.modules[settings.__module__].__file__), 'locale')\n    app_paths = []\n    for app_config in apps.get_app_configs():\n        locale_path = os.path.join(app_config.path, 'locale')\n        if os.path.exists(locale_path):\n            app_paths.append(locale_path)\n    return [globalpath, *settings.LOCALE_PATHS, *app_paths]\n\n\n@functools.lru_cache(maxsize=1000)\ndef check_for_language(lang_code):\n    \"\"\"\n    Check whether there is a global language file for the given language\n    code. This is used to decide whether a user-provided language is\n    available.\n\n    lru_cache should have a maxsize to prevent from memory exhaustion attacks,\n    as the provided language codes are taken from the HTTP request. See also\n    <https://www.djangoproject.com/weblog/2007/oct/26/security-fix/>.\n    \"\"\"\n    # First, a quick check to make sure lang_code is well-formed (#21458)\n    if lang_code is None or not language_code_re.search(lang_code):\n        return False\n    return any(\n        gettext_module.find('django', path, [to_locale(lang_code)]) is not None\n        for path in all_locale_paths()\n    )\n\n\n@functools.lru_cache\ndef get_languages():\n    \"\"\"\n    Cache of settings.LANGUAGES in a dictionary for easy lookups by key.\n    \"\"\"\n    return dict(settings.LANGUAGES)\n\n\n@functools.lru_cache(maxsize=1000)\ndef get_supported_language_variant(lang_code, strict=False):\n    \"\"\"\n    Return the language code that's listed in supported languages, possibly\n    selecting a more generic variant. Raise LookupError if nothing is found.\n\n    If `strict` is False (the default), look for a country-specific variant\n    when neither the language code nor its generic variant is found.\n\n    lru_cache should have a maxsize to prevent from memory exhaustion attacks,\n    as the provided language codes are taken from the HTTP request. See also\n    <https://www.djangoproject.com/weblog/2007/oct/26/security-fix/>.\n    \"\"\"\n    if lang_code:\n        # If 'zh-hant-tw' is not supported, try special fallback or subsequent\n        # language codes i.e. 'zh-hant' and 'zh'.\n        possible_lang_codes = [lang_code]\n        try:\n            possible_lang_codes.extend(LANG_INFO[lang_code]['fallback'])\n        except KeyError:\n            pass\n        i = None\n        while (i := lang_code.rfind('-', 0, i)) > -1:\n            possible_lang_codes.append(lang_code[:i])\n        generic_lang_code = possible_lang_codes[-1]\n        supported_lang_codes = get_languages()\n\n        for code in possible_lang_codes:\n            if code in supported_lang_codes and check_for_language(code):\n                return code\n        if not strict:\n            # if fr-fr is not supported, try fr-ca.\n            for supported_code in supported_lang_codes:\n                if supported_code.startswith(generic_lang_code + '-'):\n                    return supported_code\n    raise LookupError(lang_code)\n\n\ndef get_language_from_path(path, strict=False):\n    \"\"\"\n    Return the language code if there's a valid language code found in `path`.\n\n    If `strict` is False (the default), look for a country-specific variant\n    when neither the language code nor its generic variant is found.\n    \"\"\"\n    regex_match = language_code_prefix_re.match(path)\n    if not regex_match:\n        return None\n    lang_code = regex_match[1]\n    try:\n        return get_supported_language_variant(lang_code, strict=strict)\n    except LookupError:\n        return None\n\n\ndef get_language_from_request(request, check_path=False):\n    \"\"\"\n    Analyze the request to find what language the user wants the system to\n    show. Only languages listed in settings.LANGUAGES are taken into account.\n    If the user requests a sublanguage where we have a main language, we send\n    out the main language.\n\n    If check_path is True, the URL path prefix will be checked for a language\n    code, otherwise this is skipped for backwards compatibility.\n    \"\"\"\n    if check_path:\n        lang_code = get_language_from_path(request.path_info)\n        if lang_code is not None:\n            return lang_code\n\n    lang_code = request.COOKIES.get(settings.LANGUAGE_COOKIE_NAME)\n    if lang_code is not None and lang_code in get_languages() and check_for_language(lang_code):\n        return lang_code\n\n    try:\n        return get_supported_language_variant(lang_code)\n    except LookupError:\n        pass\n\n    accept = request.META.get('HTTP_ACCEPT_LANGUAGE', '')\n    for accept_lang, unused in parse_accept_lang_header(accept):\n        if accept_lang == '*':\n            break\n\n        if not language_code_re.search(accept_lang):\n            continue\n\n        try:\n            return get_supported_language_variant(accept_lang)\n        except LookupError:\n            continue\n\n    try:\n        return get_supported_language_variant(settings.LANGUAGE_CODE)\n    except LookupError:\n        return settings.LANGUAGE_CODE\n\n\n@functools.lru_cache(maxsize=1000)\ndef parse_accept_lang_header(lang_string):\n    \"\"\"\n    Parse the lang_string, which is the body of an HTTP Accept-Language\n    header, and return a tuple of (lang, q-value), ordered by 'q' values.\n\n    Return an empty tuple if there are any format errors in lang_string.\n    \"\"\"\n    result = []\n    pieces = accept_language_re.split(lang_string.lower())\n    if pieces[-1]:\n        return ()\n    for i in range(0, len(pieces) - 1, 3):\n        first, lang, priority = pieces[i:i + 3]\n        if first:\n            return ()\n        if priority:\n            priority = float(priority)\n        else:\n            priority = 1.0\n        result.append((lang, priority))\n    result.sort(key=lambda k: k[1], reverse=True)\n    return tuple(result)"}
{"model_name_or_path": "agentless", "instance_id": "django__django-15814", "model_patch": "diff --git a/django/db/models/query.py b/django/db/models/query.py\nindex 5314293..389ad3d 100644\n--- a/django/db/models/query.py\n+++ b/django/db/models/query.py\n@@ -2596,7 +2596,7 @@ class RelatedPopulator:\n             )\n \n         self.model_cls = klass_info[\"model\"]\n-        self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)\n+        self.pk_idx = self.init_list.index(self.model_cls._meta.concrete_model._meta.pk.attname)\n         self.related_populators = get_related_populators(klass_info, select, self.db)\n         self.local_setter = klass_info[\"local_setter\"]\n         self.remote_setter = klass_info[\"remote_setter\"]\n", "raw_model_patch": "diff --git a/django/db/models/query.py b/django/db/models/query.py\nindex 5314293..389ad3d 100644\n--- a/django/db/models/query.py\n+++ b/django/db/models/query.py\n@@ -2596,7 +2596,7 @@ class RelatedPopulator:\n             )\n \n         self.model_cls = klass_info[\"model\"]\n-        self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)\n+        self.pk_idx = self.init_list.index(self.model_cls._meta.concrete_model._meta.pk.attname)\n         self.related_populators = get_related_populators(klass_info, select, self.db)\n         self.local_setter = klass_info[\"local_setter\"]\n         self.remote_setter = klass_info[\"remote_setter\"]\n", "original_file_content": "\"\"\"\nThe main QuerySet implementation. This provides the public API for the ORM.\n\"\"\"\n\nimport copy\nimport operator\nimport warnings\nfrom itertools import chain, islice\n\nfrom asgiref.sync import sync_to_async\n\nimport django\nfrom django.conf import settings\nfrom django.core import exceptions\nfrom django.db import (\n    DJANGO_VERSION_PICKLE_KEY,\n    IntegrityError,\n    NotSupportedError,\n    connections,\n    router,\n    transaction,\n)\nfrom django.db.models import AutoField, DateField, DateTimeField, sql\nfrom django.db.models.constants import LOOKUP_SEP, OnConflict\nfrom django.db.models.deletion import Collector\nfrom django.db.models.expressions import Case, F, Ref, Value, When\nfrom django.db.models.functions import Cast, Trunc\nfrom django.db.models.query_utils import FilteredRelation, Q\nfrom django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE\nfrom django.db.models.utils import create_namedtuple_class, resolve_callables\nfrom django.utils import timezone\nfrom django.utils.deprecation import RemovedInDjango50Warning\nfrom django.utils.functional import cached_property, partition\n\n# The maximum number of results to fetch in a get() query.\nMAX_GET_RESULTS = 21\n\n# The maximum number of items to display in a QuerySet.__repr__\nREPR_OUTPUT_SIZE = 20\n\n\nclass BaseIterable:\n    def __init__(\n        self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE\n    ):\n        self.queryset = queryset\n        self.chunked_fetch = chunked_fetch\n        self.chunk_size = chunk_size\n\n    async def _async_generator(self):\n        # Generators don't actually start running until the first time you call\n        # next() on them, so make the generator object in the async thread and\n        # then repeatedly dispatch to it in a sync thread.\n        sync_generator = self.__iter__()\n\n        def next_slice(gen):\n            return list(islice(gen, self.chunk_size))\n\n        while True:\n            chunk = await sync_to_async(next_slice)(sync_generator)\n            for item in chunk:\n                yield item\n            if len(chunk) < self.chunk_size:\n                break\n\n    # __aiter__() is a *synchronous* method that has to then return an\n    # *asynchronous* iterator/generator. Thus, nest an async generator inside\n    # it.\n    # This is a generic iterable converter for now, and is going to suffer a\n    # performance penalty on large sets of items due to the cost of crossing\n    # over the sync barrier for each chunk. Custom __aiter__() methods should\n    # be added to each Iterable subclass, but that needs some work in the\n    # Compiler first.\n    def __aiter__(self):\n        return self._async_generator()\n\n\nclass ModelIterable(BaseIterable):\n    \"\"\"Iterable that yields a model instance for each row.\"\"\"\n\n    def __iter__(self):\n        queryset = self.queryset\n        db = queryset.db\n        compiler = queryset.query.get_compiler(using=db)\n        # Execute the query. This will also fill compiler.select, klass_info,\n        # and annotations.\n        results = compiler.execute_sql(\n            chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size\n        )\n        select, klass_info, annotation_col_map = (\n            compiler.select,\n            compiler.klass_info,\n            compiler.annotation_col_map,\n        )\n        model_cls = klass_info[\"model\"]\n        select_fields = klass_info[\"select_fields\"]\n        model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1\n        init_list = [\n            f[0].target.attname for f in select[model_fields_start:model_fields_end]\n        ]\n        related_populators = get_related_populators(klass_info, select, db)\n        known_related_objects = [\n            (\n                field,\n                related_objs,\n                operator.attrgetter(\n                    *[\n                        field.attname\n                        if from_field == \"self\"\n                        else queryset.model._meta.get_field(from_field).attname\n                        for from_field in field.from_fields\n                    ]\n                ),\n            )\n            for field, related_objs in queryset._known_related_objects.items()\n        ]\n        for row in compiler.results_iter(results):\n            obj = model_cls.from_db(\n                db, init_list, row[model_fields_start:model_fields_end]\n            )\n            for rel_populator in related_populators:\n                rel_populator.populate(row, obj)\n            if annotation_col_map:\n                for attr_name, col_pos in annotation_col_map.items():\n                    setattr(obj, attr_name, row[col_pos])\n\n            # Add the known related objects to the model.\n            for field, rel_objs, rel_getter in known_related_objects:\n                # Avoid overwriting objects loaded by, e.g., select_related().\n                if field.is_cached(obj):\n                    continue\n                rel_obj_id = rel_getter(obj)\n                try:\n                    rel_obj = rel_objs[rel_obj_id]\n                except KeyError:\n                    pass  # May happen in qs1 | qs2 scenarios.\n                else:\n                    setattr(obj, field.name, rel_obj)\n\n            yield obj\n\n\nclass RawModelIterable(BaseIterable):\n    \"\"\"\n    Iterable that yields a model instance for each row from a raw queryset.\n    \"\"\"\n\n    def __iter__(self):\n        # Cache some things for performance reasons outside the loop.\n        db = self.queryset.db\n        query = self.queryset.query\n        connection = connections[db]\n        compiler = connection.ops.compiler(\"SQLCompiler\")(query, connection, db)\n        query_iterator = iter(query)\n\n        try:\n            (\n                model_init_names,\n                model_init_pos,\n                annotation_fields,\n            ) = self.queryset.resolve_model_init_order()\n            model_cls = self.queryset.model\n            if model_cls._meta.pk.attname not in model_init_names:\n                raise exceptions.FieldDoesNotExist(\n                    \"Raw query must include the primary key\"\n                )\n            fields = [self.queryset.model_fields.get(c) for c in self.queryset.columns]\n            converters = compiler.get_converters(\n                [f.get_col(f.model._meta.db_table) if f else None for f in fields]\n            )\n            if converters:\n                query_iterator = compiler.apply_converters(query_iterator, converters)\n            for values in query_iterator:\n                # Associate fields to values\n                model_init_values = [values[pos] for pos in model_init_pos]\n                instance = model_cls.from_db(db, model_init_names, model_init_values)\n                if annotation_fields:\n                    for column, pos in annotation_fields:\n                        setattr(instance, column, values[pos])\n                yield instance\n        finally:\n            # Done iterating the Query. If it has its own cursor, close it.\n            if hasattr(query, \"cursor\") and query.cursor:\n                query.cursor.close()\n\n\nclass ValuesIterable(BaseIterable):\n    \"\"\"\n    Iterable returned by QuerySet.values() that yields a dict for each row.\n    \"\"\"\n\n    def __iter__(self):\n        queryset = self.queryset\n        query = queryset.query\n        compiler = query.get_compiler(queryset.db)\n\n        # extra(select=...) cols are always at the start of the row.\n        names = [\n            *query.extra_select,\n            *query.values_select,\n            *query.annotation_select,\n        ]\n        indexes = range(len(names))\n        for row in compiler.results_iter(\n            chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size\n        ):\n            yield {names[i]: row[i] for i in indexes}\n\n\nclass ValuesListIterable(BaseIterable):\n    \"\"\"\n    Iterable returned by QuerySet.values_list(flat=False) that yields a tuple\n    for each row.\n    \"\"\"\n\n    def __iter__(self):\n        queryset = self.queryset\n        query = queryset.query\n        compiler = query.get_compiler(queryset.db)\n\n        if queryset._fields:\n            # extra(select=...) cols are always at the start of the row.\n            names = [\n                *query.extra_select,\n                *query.values_select,\n                *query.annotation_select,\n            ]\n            fields = [\n                *queryset._fields,\n                *(f for f in query.annotation_select if f not in queryset._fields),\n            ]\n            if fields != names:\n                # Reorder according to fields.\n                index_map = {name: idx for idx, name in enumerate(names)}\n                rowfactory = operator.itemgetter(*[index_map[f] for f in fields])\n                return map(\n                    rowfactory,\n                    compiler.results_iter(\n                        chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size\n                    ),\n                )\n        return compiler.results_iter(\n            tuple_expected=True,\n            chunked_fetch=self.chunked_fetch,\n            chunk_size=self.chunk_size,\n        )\n\n\nclass NamedValuesListIterable(ValuesListIterable):\n    \"\"\"\n    Iterable returned by QuerySet.values_list(named=True) that yields a\n    namedtuple for each row.\n    \"\"\"\n\n    def __iter__(self):\n        queryset = self.queryset\n        if queryset._fields:\n            names = queryset._fields\n        else:\n            query = queryset.query\n            names = [\n                *query.extra_select,\n                *query.values_select,\n                *query.annotation_select,\n            ]\n        tuple_class = create_namedtuple_class(*names)\n        new = tuple.__new__\n        for row in super().__iter__():\n            yield new(tuple_class, row)\n\n\nclass FlatValuesListIterable(BaseIterable):\n    \"\"\"\n    Iterable returned by QuerySet.values_list(flat=True) that yields single\n    values.\n    \"\"\"\n\n    def __iter__(self):\n        queryset = self.queryset\n        compiler = queryset.query.get_compiler(queryset.db)\n        for row in compiler.results_iter(\n            chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size\n        ):\n            yield row[0]\n\n\nclass QuerySet:\n    \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n\n    def __init__(self, model=None, query=None, using=None, hints=None):\n        self.model = model\n        self._db = using\n        self._hints = hints or {}\n        self._query = query or sql.Query(self.model)\n        self._result_cache = None\n        self._sticky_filter = False\n        self._for_write = False\n        self._prefetch_related_lookups = ()\n        self._prefetch_done = False\n        self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n        self._iterable_class = ModelIterable\n        self._fields = None\n        self._defer_next_filter = False\n        self._deferred_filter = None\n\n    @property\n    def query(self):\n        if self._deferred_filter:\n            negate, args, kwargs = self._deferred_filter\n            self._filter_or_exclude_inplace(negate, args, kwargs)\n            self._deferred_filter = None\n        return self._query\n\n    @query.setter\n    def query(self, value):\n        if value.values_select:\n            self._iterable_class = ValuesIterable\n        self._query = value\n\n    def as_manager(cls):\n        # Address the circular dependency between `Queryset` and `Manager`.\n        from django.db.models.manager import Manager\n\n        manager = Manager.from_queryset(cls)()\n        manager._built_with_as_manager = True\n        return manager\n\n    as_manager.queryset_only = True\n    as_manager = classmethod(as_manager)\n\n    ########################\n    # PYTHON MAGIC METHODS #\n    ########################\n\n    def __deepcopy__(self, memo):\n        \"\"\"Don't populate the QuerySet's cache.\"\"\"\n        obj = self.__class__()\n        for k, v in self.__dict__.items():\n            if k == \"_result_cache\":\n                obj.__dict__[k] = None\n            else:\n                obj.__dict__[k] = copy.deepcopy(v, memo)\n        return obj\n\n    def __getstate__(self):\n        # Force the cache to be fully populated.\n        self._fetch_all()\n        return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}\n\n    def __setstate__(self, state):\n        pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n        if pickled_version:\n            if pickled_version != django.__version__:\n                warnings.warn(\n                    \"Pickled queryset instance's Django version %s does not \"\n                    \"match the current version %s.\"\n                    % (pickled_version, django.__version__),\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n        else:\n            warnings.warn(\n                \"Pickled queryset instance's Django version is not specified.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        self.__dict__.update(state)\n\n    def __repr__(self):\n        data = list(self[: REPR_OUTPUT_SIZE + 1])\n        if len(data) > REPR_OUTPUT_SIZE:\n            data[-1] = \"...(remaining elements truncated)...\"\n        return \"<%s %r>\" % (self.__class__.__name__, data)\n\n    def __len__(self):\n        self._fetch_all()\n        return len(self._result_cache)\n\n    def __iter__(self):\n        \"\"\"\n        The queryset iterator protocol uses three nested iterators in the\n        default case:\n            1. sql.compiler.execute_sql()\n               - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n                 using cursor.fetchmany(). This part is responsible for\n                 doing some column masking, and returning the rows in chunks.\n            2. sql.compiler.results_iter()\n               - Returns one row at time. At this point the rows are still just\n                 tuples. In some cases the return values are converted to\n                 Python values at this location.\n            3. self.iterator()\n               - Responsible for turning the rows into model objects.\n        \"\"\"\n        self._fetch_all()\n        return iter(self._result_cache)\n\n    def __aiter__(self):\n        # Remember, __aiter__ itself is synchronous, it's the thing it returns\n        # that is async!\n        async def generator():\n            await sync_to_async(self._fetch_all)()\n            for item in self._result_cache:\n                yield item\n\n        return generator()\n\n    def __bool__(self):\n        self._fetch_all()\n        return bool(self._result_cache)\n\n    def __getitem__(self, k):\n        \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n        if not isinstance(k, (int, slice)):\n            raise TypeError(\n                \"QuerySet indices must be integers or slices, not %s.\"\n                % type(k).__name__\n            )\n        if (isinstance(k, int) and k < 0) or (\n            isinstance(k, slice)\n            and (\n                (k.start is not None and k.start < 0)\n                or (k.stop is not None and k.stop < 0)\n            )\n        ):\n            raise ValueError(\"Negative indexing is not supported.\")\n\n        if self._result_cache is not None:\n            return self._result_cache[k]\n\n        if isinstance(k, slice):\n            qs = self._chain()\n            if k.start is not None:\n                start = int(k.start)\n            else:\n                start = None\n            if k.stop is not None:\n                stop = int(k.stop)\n            else:\n                stop = None\n            qs.query.set_limits(start, stop)\n            return list(qs)[:: k.step] if k.step else qs\n\n        qs = self._chain()\n        qs.query.set_limits(k, k + 1)\n        qs._fetch_all()\n        return qs._result_cache[0]\n\n    def __class_getitem__(cls, *args, **kwargs):\n        return cls\n\n    def __and__(self, other):\n        self._check_operator_queryset(other, \"&\")\n        self._merge_sanity_check(other)\n        if isinstance(other, EmptyQuerySet):\n            return other\n        if isinstance(self, EmptyQuerySet):\n            return self\n        combined = self._chain()\n        combined._merge_known_related_objects(other)\n        combined.query.combine(other.query, sql.AND)\n        return combined\n\n    def __or__(self, other):\n        self._check_operator_queryset(other, \"|\")\n        self._merge_sanity_check(other)\n        if isinstance(self, EmptyQuerySet):\n            return other\n        if isinstance(other, EmptyQuerySet):\n            return self\n        query = (\n            self\n            if self.query.can_filter()\n            else self.model._base_manager.filter(pk__in=self.values(\"pk\"))\n        )\n        combined = query._chain()\n        combined._merge_known_related_objects(other)\n        if not other.query.can_filter():\n            other = other.model._base_manager.filter(pk__in=other.values(\"pk\"))\n        combined.query.combine(other.query, sql.OR)\n        return combined\n\n    def __xor__(self, other):\n        self._check_operator_queryset(other, \"^\")\n        self._merge_sanity_check(other)\n        if isinstance(self, EmptyQuerySet):\n            return other\n        if isinstance(other, EmptyQuerySet):\n            return self\n        query = (\n            self\n            if self.query.can_filter()\n            else self.model._base_manager.filter(pk__in=self.values(\"pk\"))\n        )\n        combined = query._chain()\n        combined._merge_known_related_objects(other)\n        if not other.query.can_filter():\n            other = other.model._base_manager.filter(pk__in=other.values(\"pk\"))\n        combined.query.combine(other.query, sql.XOR)\n        return combined\n\n    ####################################\n    # METHODS THAT DO DATABASE QUERIES #\n    ####################################\n\n    def _iterator(self, use_chunked_fetch, chunk_size):\n        iterable = self._iterable_class(\n            self,\n            chunked_fetch=use_chunked_fetch,\n            chunk_size=chunk_size or 2000,\n        )\n        if not self._prefetch_related_lookups or chunk_size is None:\n            yield from iterable\n            return\n\n        iterator = iter(iterable)\n        while results := list(islice(iterator, chunk_size)):\n            prefetch_related_objects(results, *self._prefetch_related_lookups)\n            yield from results\n\n    def iterator(self, chunk_size=None):\n        \"\"\"\n        An iterator over the results from applying this QuerySet to the\n        database. chunk_size must be provided for QuerySets that prefetch\n        related objects. Otherwise, a default chunk_size of 2000 is supplied.\n        \"\"\"\n        if chunk_size is None:\n            if self._prefetch_related_lookups:\n                # When the deprecation ends, replace with:\n                # raise ValueError(\n                #     'chunk_size must be provided when using '\n                #     'QuerySet.iterator() after prefetch_related().'\n                # )\n                warnings.warn(\n                    \"Using QuerySet.iterator() after prefetch_related() \"\n                    \"without specifying chunk_size is deprecated.\",\n                    category=RemovedInDjango50Warning,\n                    stacklevel=2,\n                )\n        elif chunk_size <= 0:\n            raise ValueError(\"Chunk size must be strictly positive.\")\n        use_chunked_fetch = not connections[self.db].settings_dict.get(\n            \"DISABLE_SERVER_SIDE_CURSORS\"\n        )\n        return self._iterator(use_chunked_fetch, chunk_size)\n\n    async def aiterator(self, chunk_size=2000):\n        \"\"\"\n        An asynchronous iterator over the results from applying this QuerySet\n        to the database.\n        \"\"\"\n        if self._prefetch_related_lookups:\n            raise NotSupportedError(\n                \"Using QuerySet.aiterator() after prefetch_related() is not supported.\"\n            )\n        if chunk_size <= 0:\n            raise ValueError(\"Chunk size must be strictly positive.\")\n        use_chunked_fetch = not connections[self.db].settings_dict.get(\n            \"DISABLE_SERVER_SIDE_CURSORS\"\n        )\n        async for item in self._iterable_class(\n            self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size\n        ):\n            yield item\n\n    def aggregate(self, *args, **kwargs):\n        \"\"\"\n        Return a dictionary containing the calculations (aggregation)\n        over the current queryset.\n\n        If args is present the expression is passed as a kwarg using\n        the Aggregate object's default alias.\n        \"\"\"\n        if self.query.distinct_fields:\n            raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n        self._validate_values_are_expressions(\n            (*args, *kwargs.values()), method_name=\"aggregate\"\n        )\n        for arg in args:\n            # The default_alias property raises TypeError if default_alias\n            # can't be set automatically or AttributeError if it isn't an\n            # attribute.\n            try:\n                arg.default_alias\n            except (AttributeError, TypeError):\n                raise TypeError(\"Complex aggregates require an alias\")\n            kwargs[arg.default_alias] = arg\n\n        query = self.query.chain()\n        for (alias, aggregate_expr) in kwargs.items():\n            query.add_annotation(aggregate_expr, alias, is_summary=True)\n            annotation = query.annotations[alias]\n            if not annotation.contains_aggregate:\n                raise TypeError(\"%s is not an aggregate expression\" % alias)\n            for expr in annotation.get_source_expressions():\n                if (\n                    expr.contains_aggregate\n                    and isinstance(expr, Ref)\n                    and expr.refs in kwargs\n                ):\n                    name = expr.refs\n                    raise exceptions.FieldError(\n                        \"Cannot compute %s('%s'): '%s' is an aggregate\"\n                        % (annotation.name, name, name)\n                    )\n        return query.get_aggregation(self.db, kwargs)\n\n    async def aaggregate(self, *args, **kwargs):\n        return await sync_to_async(self.aggregate)(*args, **kwargs)\n\n    def count(self):\n        \"\"\"\n        Perform a SELECT COUNT() and return the number of records as an\n        integer.\n\n        If the QuerySet is already fully cached, return the length of the\n        cached results set to avoid multiple SELECT COUNT(*) calls.\n        \"\"\"\n        if self._result_cache is not None:\n            return len(self._result_cache)\n\n        return self.query.get_count(using=self.db)\n\n    async def acount(self):\n        return await sync_to_async(self.count)()\n\n    def get(self, *args, **kwargs):\n        \"\"\"\n        Perform the query and return a single object matching the given\n        keyword arguments.\n        \"\"\"\n        if self.query.combinator and (args or kwargs):\n            raise NotSupportedError(\n                \"Calling QuerySet.get(...) with filters after %s() is not \"\n                \"supported.\" % self.query.combinator\n            )\n        clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n        if self.query.can_filter() and not self.query.distinct_fields:\n            clone = clone.order_by()\n        limit = None\n        if (\n            not clone.query.select_for_update\n            or connections[clone.db].features.supports_select_for_update_with_limit\n        ):\n            limit = MAX_GET_RESULTS\n            clone.query.set_limits(high=limit)\n        num = len(clone)\n        if num == 1:\n            return clone._result_cache[0]\n        if not num:\n            raise self.model.DoesNotExist(\n                \"%s matching query does not exist.\" % self.model._meta.object_name\n            )\n        raise self.model.MultipleObjectsReturned(\n            \"get() returned more than one %s -- it returned %s!\"\n            % (\n                self.model._meta.object_name,\n                num if not limit or num < limit else \"more than %s\" % (limit - 1),\n            )\n        )\n\n    async def aget(self, *args, **kwargs):\n        return await sync_to_async(self.get)(*args, **kwargs)\n\n    def create(self, **kwargs):\n        \"\"\"\n        Create a new object with the given kwargs, saving it to the database\n        and returning the created object.\n        \"\"\"\n        obj = self.model(**kwargs)\n        self._for_write = True\n        obj.save(force_insert=True, using=self.db)\n        return obj\n\n    async def acreate(self, **kwargs):\n        return await sync_to_async(self.create)(**kwargs)\n\n    def _prepare_for_bulk_create(self, objs):\n        for obj in objs:\n            if obj.pk is None:\n                # Populate new PK values.\n                obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n            obj._prepare_related_fields_for_save(operation_name=\"bulk_create\")\n\n    def _check_bulk_create_options(\n        self, ignore_conflicts, update_conflicts, update_fields, unique_fields\n    ):\n        if ignore_conflicts and update_conflicts:\n            raise ValueError(\n                \"ignore_conflicts and update_conflicts are mutually exclusive.\"\n            )\n        db_features = connections[self.db].features\n        if ignore_conflicts:\n            if not db_features.supports_ignore_conflicts:\n                raise NotSupportedError(\n                    \"This database backend does not support ignoring conflicts.\"\n                )\n            return OnConflict.IGNORE\n        elif update_conflicts:\n            if not db_features.supports_update_conflicts:\n                raise NotSupportedError(\n                    \"This database backend does not support updating conflicts.\"\n                )\n            if not update_fields:\n                raise ValueError(\n                    \"Fields that will be updated when a row insertion fails \"\n                    \"on conflicts must be provided.\"\n                )\n            if unique_fields and not db_features.supports_update_conflicts_with_target:\n                raise NotSupportedError(\n                    \"This database backend does not support updating \"\n                    \"conflicts with specifying unique fields that can trigger \"\n                    \"the upsert.\"\n                )\n            if not unique_fields and db_features.supports_update_conflicts_with_target:\n                raise ValueError(\n                    \"Unique fields that can trigger the upsert must be provided.\"\n                )\n            # Updating primary keys and non-concrete fields is forbidden.\n            update_fields = [self.model._meta.get_field(name) for name in update_fields]\n            if any(not f.concrete or f.many_to_many for f in update_fields):\n                raise ValueError(\n                    \"bulk_create() can only be used with concrete fields in \"\n                    \"update_fields.\"\n                )\n            if any(f.primary_key for f in update_fields):\n                raise ValueError(\n                    \"bulk_create() cannot be used with primary keys in \"\n                    \"update_fields.\"\n                )\n            if unique_fields:\n                # Primary key is allowed in unique_fields.\n                unique_fields = [\n                    self.model._meta.get_field(name)\n                    for name in unique_fields\n                    if name != \"pk\"\n                ]\n                if any(not f.concrete or f.many_to_many for f in unique_fields):\n                    raise ValueError(\n                        \"bulk_create() can only be used with concrete fields \"\n                        \"in unique_fields.\"\n                    )\n            return OnConflict.UPDATE\n        return None\n\n    def bulk_create(\n        self,\n        objs,\n        batch_size=None,\n        ignore_conflicts=False,\n        update_conflicts=False,\n        update_fields=None,\n        unique_fields=None,\n    ):\n        \"\"\"\n        Insert each of the instances into the database. Do *not* call\n        save() on each of the instances, do not send any pre/post_save\n        signals, and do not set the primary key attribute if it is an\n        autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n        Multi-table models are not supported.\n        \"\"\"\n        # When you bulk insert you don't get the primary keys back (if it's an\n        # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n        # you can't insert into the child tables which references this. There\n        # are two workarounds:\n        # 1) This could be implemented if you didn't have an autoincrement pk\n        # 2) You could do it by doing O(n) normal inserts into the parent\n        #    tables to get the primary keys back and then doing a single bulk\n        #    insert into the childmost table.\n        # We currently set the primary keys on the objects when using\n        # PostgreSQL via the RETURNING ID clause. It should be possible for\n        # Oracle as well, but the semantics for extracting the primary keys is\n        # trickier so it's not done yet.\n        if batch_size is not None and batch_size <= 0:\n            raise ValueError(\"Batch size must be a positive integer.\")\n        # Check that the parents share the same concrete model with the our\n        # model to detect the inheritance pattern ConcreteGrandParent ->\n        # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n        # would not identify that case as involving multiple tables.\n        for parent in self.model._meta.get_parent_list():\n            if parent._meta.concrete_model is not self.model._meta.concrete_model:\n                raise ValueError(\"Can't bulk create a multi-table inherited model\")\n        if not objs:\n            return objs\n        on_conflict = self._check_bulk_create_options(\n            ignore_conflicts,\n            update_conflicts,\n            update_fields,\n            unique_fields,\n        )\n        self._for_write = True\n        opts = self.model._meta\n        fields = opts.concrete_fields\n        objs = list(objs)\n        self._prepare_for_bulk_create(objs)\n        with transaction.atomic(using=self.db, savepoint=False):\n            objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n            if objs_with_pk:\n                returned_columns = self._batched_insert(\n                    objs_with_pk,\n                    fields,\n                    batch_size,\n                    on_conflict=on_conflict,\n                    update_fields=update_fields,\n                    unique_fields=unique_fields,\n                )\n                for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n                    for result, field in zip(results, opts.db_returning_fields):\n                        if field != opts.pk:\n                            setattr(obj_with_pk, field.attname, result)\n                for obj_with_pk in objs_with_pk:\n                    obj_with_pk._state.adding = False\n                    obj_with_pk._state.db = self.db\n            if objs_without_pk:\n                fields = [f for f in fields if not isinstance(f, AutoField)]\n                returned_columns = self._batched_insert(\n                    objs_without_pk,\n                    fields,\n                    batch_size,\n                    on_conflict=on_conflict,\n                    update_fields=update_fields,\n                    unique_fields=unique_fields,\n                )\n                connection = connections[self.db]\n                if (\n                    connection.features.can_return_rows_from_bulk_insert\n                    and on_conflict is None\n                ):\n                    assert len(returned_columns) == len(objs_without_pk)\n                for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n                    for result, field in zip(results, opts.db_returning_fields):\n                        setattr(obj_without_pk, field.attname, result)\n                    obj_without_pk._state.adding = False\n                    obj_without_pk._state.db = self.db\n\n        return objs\n\n    async def abulk_create(\n        self,\n        objs,\n        batch_size=None,\n        ignore_conflicts=False,\n        update_conflicts=False,\n        update_fields=None,\n        unique_fields=None,\n    ):\n        return await sync_to_async(self.bulk_create)(\n            objs=objs,\n            batch_size=batch_size,\n            ignore_conflicts=ignore_conflicts,\n            update_conflicts=update_conflicts,\n            update_fields=update_fields,\n            unique_fields=unique_fields,\n        )\n\n    def bulk_update(self, objs, fields, batch_size=None):\n        \"\"\"\n        Update the given fields in each of the given objects in the database.\n        \"\"\"\n        if batch_size is not None and batch_size <= 0:\n            raise ValueError(\"Batch size must be a positive integer.\")\n        if not fields:\n            raise ValueError(\"Field names must be given to bulk_update().\")\n        objs = tuple(objs)\n        if any(obj.pk is None for obj in objs):\n            raise ValueError(\"All bulk_update() objects must have a primary key set.\")\n        fields = [self.model._meta.get_field(name) for name in fields]\n        if any(not f.concrete or f.many_to_many for f in fields):\n            raise ValueError(\"bulk_update() can only be used with concrete fields.\")\n        if any(f.primary_key for f in fields):\n            raise ValueError(\"bulk_update() cannot be used with primary key fields.\")\n        if not objs:\n            return 0\n        for obj in objs:\n            obj._prepare_related_fields_for_save(\n                operation_name=\"bulk_update\", fields=fields\n            )\n        # PK is used twice in the resulting update query, once in the filter\n        # and once in the WHEN. Each field will also have one CAST.\n        self._for_write = True\n        connection = connections[self.db]\n        max_batch_size = connection.ops.bulk_batch_size([\"pk\", \"pk\"] + fields, objs)\n        batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n        requires_casting = connection.features.requires_casted_case_in_updates\n        batches = (objs[i : i + batch_size] for i in range(0, len(objs), batch_size))\n        updates = []\n        for batch_objs in batches:\n            update_kwargs = {}\n            for field in fields:\n                when_statements = []\n                for obj in batch_objs:\n                    attr = getattr(obj, field.attname)\n                    if not hasattr(attr, \"resolve_expression\"):\n                        attr = Value(attr, output_field=field)\n                    when_statements.append(When(pk=obj.pk, then=attr))\n                case_statement = Case(*when_statements, output_field=field)\n                if requires_casting:\n                    case_statement = Cast(case_statement, output_field=field)\n                update_kwargs[field.attname] = case_statement\n            updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n        rows_updated = 0\n        queryset = self.using(self.db)\n        with transaction.atomic(using=self.db, savepoint=False):\n            for pks, update_kwargs in updates:\n                rows_updated += queryset.filter(pk__in=pks).update(**update_kwargs)\n        return rows_updated\n\n    bulk_update.alters_data = True\n\n    async def abulk_update(self, objs, fields, batch_size=None):\n        return await sync_to_async(self.bulk_update)(\n            objs=objs,\n            fields=fields,\n            batch_size=batch_size,\n        )\n\n    abulk_update.alters_data = True\n\n    def get_or_create(self, defaults=None, **kwargs):\n        \"\"\"\n        Look up an object with the given kwargs, creating one if necessary.\n        Return a tuple of (object, created), where created is a boolean\n        specifying whether an object was created.\n        \"\"\"\n        # The get() needs to be targeted at the write database in order\n        # to avoid potential transaction consistency problems.\n        self._for_write = True\n        try:\n            return self.get(**kwargs), False\n        except self.model.DoesNotExist:\n            params = self._extract_model_params(defaults, **kwargs)\n            # Try to create an object using passed params.\n            try:\n                with transaction.atomic(using=self.db):\n                    params = dict(resolve_callables(params))\n                    return self.create(**params), True\n            except IntegrityError:\n                try:\n                    return self.get(**kwargs), False\n                except self.model.DoesNotExist:\n                    pass\n                raise\n\n    async def aget_or_create(self, defaults=None, **kwargs):\n        return await sync_to_async(self.get_or_create)(\n            defaults=defaults,\n            **kwargs,\n        )\n\n    def update_or_create(self, defaults=None, **kwargs):\n        \"\"\"\n        Look up an object with the given kwargs, updating one with defaults\n        if it exists, otherwise create a new one.\n        Return a tuple (object, created), where created is a boolean\n        specifying whether an object was created.\n        \"\"\"\n        defaults = defaults or {}\n        self._for_write = True\n        with transaction.atomic(using=self.db):\n            # Lock the row so that a concurrent update is blocked until\n            # update_or_create() has performed its save.\n            obj, created = self.select_for_update().get_or_create(defaults, **kwargs)\n            if created:\n                return obj, created\n            for k, v in resolve_callables(defaults):\n                setattr(obj, k, v)\n            obj.save(using=self.db)\n        return obj, False\n\n    async def aupdate_or_create(self, defaults=None, **kwargs):\n        return await sync_to_async(self.update_or_create)(\n            defaults=defaults,\n            **kwargs,\n        )\n\n    def _extract_model_params(self, defaults, **kwargs):\n        \"\"\"\n        Prepare `params` for creating a model instance based on the given\n        kwargs; for use by get_or_create().\n        \"\"\"\n        defaults = defaults or {}\n        params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}\n        params.update(defaults)\n        property_names = self.model._meta._property_names\n        invalid_params = []\n        for param in params:\n            try:\n                self.model._meta.get_field(param)\n            except exceptions.FieldDoesNotExist:\n                # It's okay to use a model's property if it has a setter.\n                if not (param in property_names and getattr(self.model, param).fset):\n                    invalid_params.append(param)\n        if invalid_params:\n            raise exceptions.FieldError(\n                \"Invalid field name(s) for model %s: '%s'.\"\n                % (\n                    self.model._meta.object_name,\n                    \"', '\".join(sorted(invalid_params)),\n                )\n            )\n        return params\n\n    def _earliest(self, *fields):\n        \"\"\"\n        Return the earliest object according to fields (if given) or by the\n        model's Meta.get_latest_by.\n        \"\"\"\n        if fields:\n            order_by = fields\n        else:\n            order_by = getattr(self.model._meta, \"get_latest_by\")\n            if order_by and not isinstance(order_by, (tuple, list)):\n                order_by = (order_by,)\n        if order_by is None:\n            raise ValueError(\n                \"earliest() and latest() require either fields as positional \"\n                \"arguments or 'get_latest_by' in the model's Meta.\"\n            )\n        obj = self._chain()\n        obj.query.set_limits(high=1)\n        obj.query.clear_ordering(force=True)\n        obj.query.add_ordering(*order_by)\n        return obj.get()\n\n    def earliest(self, *fields):\n        if self.query.is_sliced:\n            raise TypeError(\"Cannot change a query once a slice has been taken.\")\n        return self._earliest(*fields)\n\n    async def aearliest(self, *fields):\n        return await sync_to_async(self.earliest)(*fields)\n\n    def latest(self, *fields):\n        \"\"\"\n        Return the latest object according to fields (if given) or by the\n        model's Meta.get_latest_by.\n        \"\"\"\n        if self.query.is_sliced:\n            raise TypeError(\"Cannot change a query once a slice has been taken.\")\n        return self.reverse()._earliest(*fields)\n\n    async def alatest(self, *fields):\n        return await sync_to_async(self.latest)(*fields)\n\n    def first(self):\n        \"\"\"Return the first object of a query or None if no match is found.\"\"\"\n        if self.ordered:\n            queryset = self\n        else:\n            self._check_ordering_first_last_queryset_aggregation(method=\"first\")\n            queryset = self.order_by(\"pk\")\n        for obj in queryset[:1]:\n            return obj\n\n    async def afirst(self):\n        return await sync_to_async(self.first)()\n\n    def last(self):\n        \"\"\"Return the last object of a query or None if no match is found.\"\"\"\n        if self.ordered:\n            queryset = self.reverse()\n        else:\n            self._check_ordering_first_last_queryset_aggregation(method=\"last\")\n            queryset = self.order_by(\"-pk\")\n        for obj in queryset[:1]:\n            return obj\n\n    async def alast(self):\n        return await sync_to_async(self.last)()\n\n    def in_bulk(self, id_list=None, *, field_name=\"pk\"):\n        \"\"\"\n        Return a dictionary mapping each of the given IDs to the object with\n        that ID. If `id_list` isn't provided, evaluate the entire QuerySet.\n        \"\"\"\n        if self.query.is_sliced:\n            raise TypeError(\"Cannot use 'limit' or 'offset' with in_bulk().\")\n        opts = self.model._meta\n        unique_fields = [\n            constraint.fields[0]\n            for constraint in opts.total_unique_constraints\n            if len(constraint.fields) == 1\n        ]\n        if (\n            field_name != \"pk\"\n            and not opts.get_field(field_name).unique\n            and field_name not in unique_fields\n            and self.query.distinct_fields != (field_name,)\n        ):\n            raise ValueError(\n                \"in_bulk()'s field_name must be a unique field but %r isn't.\"\n                % field_name\n            )\n        if id_list is not None:\n            if not id_list:\n                return {}\n            filter_key = \"{}__in\".format(field_name)\n            batch_size = connections[self.db].features.max_query_params\n            id_list = tuple(id_list)\n            # If the database has a limit on the number of query parameters\n            # (e.g. SQLite), retrieve objects in batches if necessary.\n            if batch_size and batch_size < len(id_list):\n                qs = ()\n                for offset in range(0, len(id_list), batch_size):\n                    batch = id_list[offset : offset + batch_size]\n                    qs += tuple(self.filter(**{filter_key: batch}).order_by())\n            else:\n                qs = self.filter(**{filter_key: id_list}).order_by()\n        else:\n            qs = self._chain()\n        return {getattr(obj, field_name): obj for obj in qs}\n\n    async def ain_bulk(self, id_list=None, *, field_name=\"pk\"):\n        return await sync_to_async(self.in_bulk)(\n            id_list=id_list,\n            field_name=field_name,\n        )\n\n    def delete(self):\n        \"\"\"Delete the records in the current QuerySet.\"\"\"\n        self._not_support_combined_queries(\"delete\")\n        if self.query.is_sliced:\n            raise TypeError(\"Cannot use 'limit' or 'offset' with delete().\")\n        if self.query.distinct or self.query.distinct_fields:\n            raise TypeError(\"Cannot call delete() after .distinct().\")\n        if self._fields is not None:\n            raise TypeError(\"Cannot call delete() after .values() or .values_list()\")\n\n        del_query = self._chain()\n\n        # The delete is actually 2 queries - one to find related objects,\n        # and one to delete. Make sure that the discovery of related\n        # objects is performed on the same database as the deletion.\n        del_query._for_write = True\n\n        # Disable non-supported fields.\n        del_query.query.select_for_update = False\n        del_query.query.select_related = False\n        del_query.query.clear_ordering(force=True)\n\n        collector = Collector(using=del_query.db, origin=self)\n        collector.collect(del_query)\n        deleted, _rows_count = collector.delete()\n\n        # Clear the result cache, in case this QuerySet gets reused.\n        self._result_cache = None\n        return deleted, _rows_count\n\n    delete.alters_data = True\n    delete.queryset_only = True\n\n    async def adelete(self):\n        return await sync_to_async(self.delete)()\n\n    adelete.alters_data = True\n    adelete.queryset_only = True\n\n    def _raw_delete(self, using):\n        \"\"\"\n        Delete objects found from the given queryset in single direct SQL\n        query. No signals are sent and there is no protection for cascades.\n        \"\"\"\n        query = self.query.clone()\n        query.__class__ = sql.DeleteQuery\n        cursor = query.get_compiler(using).execute_sql(CURSOR)\n        if cursor:\n            with cursor:\n                return cursor.rowcount\n        return 0\n\n    _raw_delete.alters_data = True\n\n    def update(self, **kwargs):\n        \"\"\"\n        Update all elements in the current QuerySet, setting all the given\n        fields to the appropriate values.\n        \"\"\"\n        self._not_support_combined_queries(\"update\")\n        if self.query.is_sliced:\n            raise TypeError(\"Cannot update a query once a slice has been taken.\")\n        self._for_write = True\n        query = self.query.chain(sql.UpdateQuery)\n        query.add_update_values(kwargs)\n\n        # Inline annotations in order_by(), if possible.\n        new_order_by = []\n        for col in query.order_by:\n            if annotation := query.annotations.get(col):\n                if getattr(annotation, \"contains_aggregate\", False):\n                    raise exceptions.FieldError(\n                        f\"Cannot update when ordering by an aggregate: {annotation}\"\n                    )\n                new_order_by.append(annotation)\n            else:\n                new_order_by.append(col)\n        query.order_by = tuple(new_order_by)\n\n        # Clear any annotations so that they won't be present in subqueries.\n        query.annotations = {}\n        with transaction.mark_for_rollback_on_error(using=self.db):\n            rows = query.get_compiler(self.db).execute_sql(CURSOR)\n        self._result_cache = None\n        return rows\n\n    update.alters_data = True\n\n    async def aupdate(self, **kwargs):\n        return await sync_to_async(self.update)(**kwargs)\n\n    aupdate.alters_data = True\n\n    def _update(self, values):\n        \"\"\"\n        A version of update() that accepts field objects instead of field names.\n        Used primarily for model saving and not intended for use by general\n        code (it requires too much poking around at model internals to be\n        useful at that level).\n        \"\"\"\n        if self.query.is_sliced:\n            raise TypeError(\"Cannot update a query once a slice has been taken.\")\n        query = self.query.chain(sql.UpdateQuery)\n        query.add_update_fields(values)\n        # Clear any annotations so that they won't be present in subqueries.\n        query.annotations = {}\n        self._result_cache = None\n        return query.get_compiler(self.db).execute_sql(CURSOR)\n\n    _update.alters_data = True\n    _update.queryset_only = False\n\n    def exists(self):\n        \"\"\"\n        Return True if the QuerySet would have any results, False otherwise.\n        \"\"\"\n        if self._result_cache is None:\n            return self.query.has_results(using=self.db)\n        return bool(self._result_cache)\n\n    async def aexists(self):\n        return await sync_to_async(self.exists)()\n\n    def contains(self, obj):\n        \"\"\"\n        Return True if the QuerySet contains the provided obj,\n        False otherwise.\n        \"\"\"\n        self._not_support_combined_queries(\"contains\")\n        if self._fields is not None:\n            raise TypeError(\n                \"Cannot call QuerySet.contains() after .values() or .values_list().\"\n            )\n        try:\n            if obj._meta.concrete_model != self.model._meta.concrete_model:\n                return False\n        except AttributeError:\n            raise TypeError(\"'obj' must be a model instance.\")\n        if obj.pk is None:\n            raise ValueError(\"QuerySet.contains() cannot be used on unsaved objects.\")\n        if self._result_cache is not None:\n            return obj in self._result_cache\n        return self.filter(pk=obj.pk).exists()\n\n    async def acontains(self, obj):\n        return await sync_to_async(self.contains)(obj=obj)\n\n    def _prefetch_related_objects(self):\n        # This method can only be called once the result cache has been filled.\n        prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n        self._prefetch_done = True\n\n    def explain(self, *, format=None, **options):\n        \"\"\"\n        Runs an EXPLAIN on the SQL query this QuerySet would perform, and\n        returns the results.\n        \"\"\"\n        return self.query.explain(using=self.db, format=format, **options)\n\n    async def aexplain(self, *, format=None, **options):\n        return await sync_to_async(self.explain)(format=format, **options)\n\n    ##################################################\n    # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #\n    ##################################################\n\n    def raw(self, raw_query, params=(), translations=None, using=None):\n        if using is None:\n            using = self.db\n        qs = RawQuerySet(\n            raw_query,\n            model=self.model,\n            params=params,\n            translations=translations,\n            using=using,\n        )\n        qs._prefetch_related_lookups = self._prefetch_related_lookups[:]\n        return qs\n\n    def _values(self, *fields, **expressions):\n        clone = self._chain()\n        if expressions:\n            clone = clone.annotate(**expressions)\n        clone._fields = fields\n        clone.query.set_values(fields)\n        return clone\n\n    def values(self, *fields, **expressions):\n        fields += tuple(expressions)\n        clone = self._values(*fields, **expressions)\n        clone._iterable_class = ValuesIterable\n        return clone\n\n    def values_list(self, *fields, flat=False, named=False):\n        if flat and named:\n            raise TypeError(\"'flat' and 'named' can't be used together.\")\n        if flat and len(fields) > 1:\n            raise TypeError(\n                \"'flat' is not valid when values_list is called with more than one \"\n                \"field.\"\n            )\n\n        field_names = {f for f in fields if not hasattr(f, \"resolve_expression\")}\n        _fields = []\n        expressions = {}\n        counter = 1\n        for field in fields:\n            if hasattr(field, \"resolve_expression\"):\n                field_id_prefix = getattr(\n                    field, \"default_alias\", field.__class__.__name__.lower()\n                )\n                while True:\n                    field_id = field_id_prefix + str(counter)\n                    counter += 1\n                    if field_id not in field_names:\n                        break\n                expressions[field_id] = field\n                _fields.append(field_id)\n            else:\n                _fields.append(field)\n\n        clone = self._values(*_fields, **expressions)\n        clone._iterable_class = (\n            NamedValuesListIterable\n            if named\n            else FlatValuesListIterable\n            if flat\n            else ValuesListIterable\n        )\n        return clone\n\n    def dates(self, field_name, kind, order=\"ASC\"):\n        \"\"\"\n        Return a list of date objects representing all available dates for\n        the given field_name, scoped to 'kind'.\n        \"\"\"\n        if kind not in (\"year\", \"month\", \"week\", \"day\"):\n            raise ValueError(\"'kind' must be one of 'year', 'month', 'week', or 'day'.\")\n        if order not in (\"ASC\", \"DESC\"):\n            raise ValueError(\"'order' must be either 'ASC' or 'DESC'.\")\n        return (\n            self.annotate(\n                datefield=Trunc(field_name, kind, output_field=DateField()),\n                plain_field=F(field_name),\n            )\n            .values_list(\"datefield\", flat=True)\n            .distinct()\n            .filter(plain_field__isnull=False)\n            .order_by((\"-\" if order == \"DESC\" else \"\") + \"datefield\")\n        )\n\n    # RemovedInDjango50Warning: when the deprecation ends, remove is_dst\n    # argument.\n    def datetimes(\n        self, field_name, kind, order=\"ASC\", tzinfo=None, is_dst=timezone.NOT_PASSED\n    ):\n        \"\"\"\n        Return a list of datetime objects representing all available\n        datetimes for the given field_name, scoped to 'kind'.\n        \"\"\"\n        if kind not in (\"year\", \"month\", \"week\", \"day\", \"hour\", \"minute\", \"second\"):\n            raise ValueError(\n                \"'kind' must be one of 'year', 'month', 'week', 'day', \"\n                \"'hour', 'minute', or 'second'.\"\n            )\n        if order not in (\"ASC\", \"DESC\"):\n            raise ValueError(\"'order' must be either 'ASC' or 'DESC'.\")\n        if settings.USE_TZ:\n            if tzinfo is None:\n                tzinfo = timezone.get_current_timezone()\n        else:\n            tzinfo = None\n        return (\n            self.annotate(\n                datetimefield=Trunc(\n                    field_name,\n                    kind,\n                    output_field=DateTimeField(),\n                    tzinfo=tzinfo,\n                    is_dst=is_dst,\n                ),\n                plain_field=F(field_name),\n            )\n            .values_list(\"datetimefield\", flat=True)\n            .distinct()\n            .filter(plain_field__isnull=False)\n            .order_by((\"-\" if order == \"DESC\" else \"\") + \"datetimefield\")\n        )\n\n    def none(self):\n        \"\"\"Return an empty QuerySet.\"\"\"\n        clone = self._chain()\n        clone.query.set_empty()\n        return clone\n\n    ##################################################################\n    # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #\n    ##################################################################\n\n    def all(self):\n        \"\"\"\n        Return a new QuerySet that is a copy of the current one. This allows a\n        QuerySet to proxy for a model manager in some cases.\n        \"\"\"\n        return self._chain()\n\n    def filter(self, *args, **kwargs):\n        \"\"\"\n        Return a new QuerySet instance with the args ANDed to the existing\n        set.\n        \"\"\"\n        self._not_support_combined_queries(\"filter\")\n        return self._filter_or_exclude(False, args, kwargs)\n\n    def exclude(self, *args, **kwargs):\n        \"\"\"\n        Return a new QuerySet instance with NOT (args) ANDed to the existing\n        set.\n        \"\"\"\n        self._not_support_combined_queries(\"exclude\")\n        return self._filter_or_exclude(True, args, kwargs)\n\n    def _filter_or_exclude(self, negate, args, kwargs):\n        if (args or kwargs) and self.query.is_sliced:\n            raise TypeError(\"Cannot filter a query once a slice has been taken.\")\n        clone = self._chain()\n        if self._defer_next_filter:\n            self._defer_next_filter = False\n            clone._deferred_filter = negate, args, kwargs\n        else:\n            clone._filter_or_exclude_inplace(negate, args, kwargs)\n        return clone\n\n    def _filter_or_exclude_inplace(self, negate, args, kwargs):\n        if negate:\n            self._query.add_q(~Q(*args, **kwargs))\n        else:\n            self._query.add_q(Q(*args, **kwargs))\n\n    def complex_filter(self, filter_obj):\n        \"\"\"\n        Return a new QuerySet instance with filter_obj added to the filters.\n\n        filter_obj can be a Q object or a dictionary of keyword lookup\n        arguments.\n\n        This exists to support framework features such as 'limit_choices_to',\n        and usually it will be more natural to use other methods.\n        \"\"\"\n        if isinstance(filter_obj, Q):\n            clone = self._chain()\n            clone.query.add_q(filter_obj)\n            return clone\n        else:\n            return self._filter_or_exclude(False, args=(), kwargs=filter_obj)\n\n    def _combinator_query(self, combinator, *other_qs, all=False):\n        # Clone the query to inherit the select list and everything\n        clone = self._chain()\n        # Clear limits and ordering so they can be reapplied\n        clone.query.clear_ordering(force=True)\n        clone.query.clear_limits()\n        clone.query.combined_queries = (self.query,) + tuple(\n            qs.query for qs in other_qs\n        )\n        clone.query.combinator = combinator\n        clone.query.combinator_all = all\n        return clone\n\n    def union(self, *other_qs, all=False):\n        # If the query is an EmptyQuerySet, combine all nonempty querysets.\n        if isinstance(self, EmptyQuerySet):\n            qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]\n            if not qs:\n                return self\n            if len(qs) == 1:\n                return qs[0]\n            return qs[0]._combinator_query(\"union\", *qs[1:], all=all)\n        return self._combinator_query(\"union\", *other_qs, all=all)\n\n    def intersection(self, *other_qs):\n        # If any query is an EmptyQuerySet, return it.\n        if isinstance(self, EmptyQuerySet):\n            return self\n        for other in other_qs:\n            if isinstance(other, EmptyQuerySet):\n                return other\n        return self._combinator_query(\"intersection\", *other_qs)\n\n    def difference(self, *other_qs):\n        # If the query is an EmptyQuerySet, return it.\n        if isinstance(self, EmptyQuerySet):\n            return self\n        return self._combinator_query(\"difference\", *other_qs)\n\n    def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):\n        \"\"\"\n        Return a new QuerySet instance that will select objects with a\n        FOR UPDATE lock.\n        \"\"\"\n        if nowait and skip_locked:\n            raise ValueError(\"The nowait option cannot be used with skip_locked.\")\n        obj = self._chain()\n        obj._for_write = True\n        obj.query.select_for_update = True\n        obj.query.select_for_update_nowait = nowait\n        obj.query.select_for_update_skip_locked = skip_locked\n        obj.query.select_for_update_of = of\n        obj.query.select_for_no_key_update = no_key\n        return obj\n\n    def select_related(self, *fields):\n        \"\"\"\n        Return a new QuerySet instance that will select related objects.\n\n        If fields are specified, they must be ForeignKey fields and only those\n        related objects are included in the selection.\n\n        If select_related(None) is called, clear the list.\n        \"\"\"\n        self._not_support_combined_queries(\"select_related\")\n        if self._fields is not None:\n            raise TypeError(\n                \"Cannot call select_related() after .values() or .values_list()\"\n            )\n\n        obj = self._chain()\n        if fields == (None,):\n            obj.query.select_related = False\n        elif fields:\n            obj.query.add_select_related(fields)\n        else:\n            obj.query.select_related = True\n        return obj\n\n    def prefetch_related(self, *lookups):\n        \"\"\"\n        Return a new QuerySet instance that will prefetch the specified\n        Many-To-One and Many-To-Many related objects when the QuerySet is\n        evaluated.\n\n        When prefetch_related() is called more than once, append to the list of\n        prefetch lookups. If prefetch_related(None) is called, clear the list.\n        \"\"\"\n        self._not_support_combined_queries(\"prefetch_related\")\n        clone = self._chain()\n        if lookups == (None,):\n            clone._prefetch_related_lookups = ()\n        else:\n            for lookup in lookups:\n                if isinstance(lookup, Prefetch):\n                    lookup = lookup.prefetch_to\n                lookup = lookup.split(LOOKUP_SEP, 1)[0]\n                if lookup in self.query._filtered_relations:\n                    raise ValueError(\n                        \"prefetch_related() is not supported with FilteredRelation.\"\n                    )\n            clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n        return clone\n\n    def annotate(self, *args, **kwargs):\n        \"\"\"\n        Return a query set in which the returned objects have been annotated\n        with extra data or aggregations.\n        \"\"\"\n        self._not_support_combined_queries(\"annotate\")\n        return self._annotate(args, kwargs, select=True)\n\n    def alias(self, *args, **kwargs):\n        \"\"\"\n        Return a query set with added aliases for extra data or aggregations.\n        \"\"\"\n        self._not_support_combined_queries(\"alias\")\n        return self._annotate(args, kwargs, select=False)\n\n    def _annotate(self, args, kwargs, select=True):\n        self._validate_values_are_expressions(\n            args + tuple(kwargs.values()), method_name=\"annotate\"\n        )\n        annotations = {}\n        for arg in args:\n            # The default_alias property may raise a TypeError.\n            try:\n                if arg.default_alias in kwargs:\n                    raise ValueError(\n                        \"The named annotation '%s' conflicts with the \"\n                        \"default name for another annotation.\" % arg.default_alias\n                    )\n            except TypeError:\n                raise TypeError(\"Complex annotations require an alias\")\n            annotations[arg.default_alias] = arg\n        annotations.update(kwargs)\n\n        clone = self._chain()\n        names = self._fields\n        if names is None:\n            names = set(\n                chain.from_iterable(\n                    (field.name, field.attname)\n                    if hasattr(field, \"attname\")\n                    else (field.name,)\n                    for field in self.model._meta.get_fields()\n                )\n            )\n\n        for alias, annotation in annotations.items():\n            if alias in names:\n                raise ValueError(\n                    \"The annotation '%s' conflicts with a field on \"\n                    \"the model.\" % alias\n                )\n            if isinstance(annotation, FilteredRelation):\n                clone.query.add_filtered_relation(annotation, alias)\n            else:\n                clone.query.add_annotation(\n                    annotation,\n                    alias,\n                    is_summary=False,\n                    select=select,\n                )\n        for alias, annotation in clone.query.annotations.items():\n            if alias in annotations and annotation.contains_aggregate:\n                if clone._fields is None:\n                    clone.query.group_by = True\n                else:\n                    clone.query.set_group_by()\n                break\n\n        return clone\n\n    def order_by(self, *field_names):\n        \"\"\"Return a new QuerySet instance with the ordering changed.\"\"\"\n        if self.query.is_sliced:\n            raise TypeError(\"Cannot reorder a query once a slice has been taken.\")\n        obj = self._chain()\n        obj.query.clear_ordering(force=True, clear_default=False)\n        obj.query.add_ordering(*field_names)\n        return obj\n\n    def distinct(self, *field_names):\n        \"\"\"\n        Return a new QuerySet instance that will select only distinct results.\n        \"\"\"\n        self._not_support_combined_queries(\"distinct\")\n        if self.query.is_sliced:\n            raise TypeError(\n                \"Cannot create distinct fields once a slice has been taken.\"\n            )\n        obj = self._chain()\n        obj.query.add_distinct_fields(*field_names)\n        return obj\n\n    def extra(\n        self,\n        select=None,\n        where=None,\n        params=None,\n        tables=None,\n        order_by=None,\n        select_params=None,\n    ):\n        \"\"\"Add extra SQL fragments to the query.\"\"\"\n        self._not_support_combined_queries(\"extra\")\n        if self.query.is_sliced:\n            raise TypeError(\"Cannot change a query once a slice has been taken.\")\n        clone = self._chain()\n        clone.query.add_extra(select, select_params, where, params, tables, order_by)\n        return clone\n\n    def reverse(self):\n        \"\"\"Reverse the ordering of the QuerySet.\"\"\"\n        if self.query.is_sliced:\n            raise TypeError(\"Cannot reverse a query once a slice has been taken.\")\n        clone = self._chain()\n        clone.query.standard_ordering = not clone.query.standard_ordering\n        return clone\n\n    def defer(self, *fields):\n        \"\"\"\n        Defer the loading of data for certain fields until they are accessed.\n        Add the set of deferred fields to any existing set of deferred fields.\n        The only exception to this is if None is passed in as the only\n        parameter, in which case removal all deferrals.\n        \"\"\"\n        self._not_support_combined_queries(\"defer\")\n        if self._fields is not None:\n            raise TypeError(\"Cannot call defer() after .values() or .values_list()\")\n        clone = self._chain()\n        if fields == (None,):\n            clone.query.clear_deferred_loading()\n        else:\n            clone.query.add_deferred_loading(fields)\n        return clone\n\n    def only(self, *fields):\n        \"\"\"\n        Essentially, the opposite of defer(). Only the fields passed into this\n        method and that are not already specified as deferred are loaded\n        immediately when the queryset is evaluated.\n        \"\"\"\n        self._not_support_combined_queries(\"only\")\n        if self._fields is not None:\n            raise TypeError(\"Cannot call only() after .values() or .values_list()\")\n        if fields == (None,):\n            # Can only pass None to defer(), not only(), as the rest option.\n            # That won't stop people trying to do this, so let's be explicit.\n            raise TypeError(\"Cannot pass None as an argument to only().\")\n        for field in fields:\n            field = field.split(LOOKUP_SEP, 1)[0]\n            if field in self.query._filtered_relations:\n                raise ValueError(\"only() is not supported with FilteredRelation.\")\n        clone = self._chain()\n        clone.query.add_immediate_loading(fields)\n        return clone\n\n    def using(self, alias):\n        \"\"\"Select which database this QuerySet should execute against.\"\"\"\n        clone = self._chain()\n        clone._db = alias\n        return clone\n\n    ###################################\n    # PUBLIC INTROSPECTION ATTRIBUTES #\n    ###################################\n\n    @property\n    def ordered(self):\n        \"\"\"\n        Return True if the QuerySet is ordered -- i.e. has an order_by()\n        clause or a default ordering on the model (or is empty).\n        \"\"\"\n        if isinstance(self, EmptyQuerySet):\n            return True\n        if self.query.extra_order_by or self.query.order_by:\n            return True\n        elif (\n            self.query.default_ordering\n            and self.query.get_meta().ordering\n            and\n            # A default ordering doesn't affect GROUP BY queries.\n            not self.query.group_by\n        ):\n            return True\n        else:\n            return False\n\n    @property\n    def db(self):\n        \"\"\"Return the database used if this query is executed now.\"\"\"\n        if self._for_write:\n            return self._db or router.db_for_write(self.model, **self._hints)\n        return self._db or router.db_for_read(self.model, **self._hints)\n\n    ###################\n    # PRIVATE METHODS #\n    ###################\n\n    def _insert(\n        self,\n        objs,\n        fields,\n        returning_fields=None,\n        raw=False,\n        using=None,\n        on_conflict=None,\n        update_fields=None,\n        unique_fields=None,\n    ):\n        \"\"\"\n        Insert a new record for the given model. This provides an interface to\n        the InsertQuery class and is how Model.save() is implemented.\n        \"\"\"\n        self._for_write = True\n        if using is None:\n            using = self.db\n        query = sql.InsertQuery(\n            self.model,\n            on_conflict=on_conflict,\n            update_fields=update_fields,\n            unique_fields=unique_fields,\n        )\n        query.insert_values(fields, objs, raw=raw)\n        return query.get_compiler(using=using).execute_sql(returning_fields)\n\n    _insert.alters_data = True\n    _insert.queryset_only = False\n\n    def _batched_insert(\n        self,\n        objs,\n        fields,\n        batch_size,\n        on_conflict=None,\n        update_fields=None,\n        unique_fields=None,\n    ):\n        \"\"\"\n        Helper method for bulk_create() to insert objs one batch at a time.\n        \"\"\"\n        connection = connections[self.db]\n        ops = connection.ops\n        max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)\n        batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n        inserted_rows = []\n        bulk_return = connection.features.can_return_rows_from_bulk_insert\n        for item in [objs[i : i + batch_size] for i in range(0, len(objs), batch_size)]:\n            if bulk_return and on_conflict is None:\n                inserted_rows.extend(\n                    self._insert(\n                        item,\n                        fields=fields,\n                        using=self.db,\n                        returning_fields=self.model._meta.db_returning_fields,\n                    )\n                )\n            else:\n                self._insert(\n                    item,\n                    fields=fields,\n                    using=self.db,\n                    on_conflict=on_conflict,\n                    update_fields=update_fields,\n                    unique_fields=unique_fields,\n                )\n        return inserted_rows\n\n    def _chain(self):\n        \"\"\"\n        Return a copy of the current QuerySet that's ready for another\n        operation.\n        \"\"\"\n        obj = self._clone()\n        if obj._sticky_filter:\n            obj.query.filter_is_sticky = True\n            obj._sticky_filter = False\n        return obj\n\n    def _clone(self):\n        \"\"\"\n        Return a copy of the current QuerySet. A lightweight alternative\n        to deepcopy().\n        \"\"\"\n        c = self.__class__(\n            model=self.model,\n            query=self.query.chain(),\n            using=self._db,\n            hints=self._hints,\n        )\n        c._sticky_filter = self._sticky_filter\n        c._for_write = self._for_write\n        c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n        c._known_related_objects = self._known_related_objects\n        c._iterable_class = self._iterable_class\n        c._fields = self._fields\n        return c\n\n    def _fetch_all(self):\n        if self._result_cache is None:\n            self._result_cache = list(self._iterable_class(self))\n        if self._prefetch_related_lookups and not self._prefetch_done:\n            self._prefetch_related_objects()\n\n    def _next_is_sticky(self):\n        \"\"\"\n        Indicate that the next filter call and the one following that should\n        be treated as a single filter. This is only important when it comes to\n        determining when to reuse tables for many-to-many filters. Required so\n        that we can filter naturally on the results of related managers.\n\n        This doesn't return a clone of the current QuerySet (it returns\n        \"self\"). The method is only used internally and should be immediately\n        followed by a filter() that does create a clone.\n        \"\"\"\n        self._sticky_filter = True\n        return self\n\n    def _merge_sanity_check(self, other):\n        \"\"\"Check that two QuerySet classes may be merged.\"\"\"\n        if self._fields is not None and (\n            set(self.query.values_select) != set(other.query.values_select)\n            or set(self.query.extra_select) != set(other.query.extra_select)\n            or set(self.query.annotation_select) != set(other.query.annotation_select)\n        ):\n            raise TypeError(\n                \"Merging '%s' classes must involve the same values in each case.\"\n                % self.__class__.__name__\n            )\n\n    def _merge_known_related_objects(self, other):\n        \"\"\"\n        Keep track of all known related objects from either QuerySet instance.\n        \"\"\"\n        for field, objects in other._known_related_objects.items():\n            self._known_related_objects.setdefault(field, {}).update(objects)\n\n    def resolve_expression(self, *args, **kwargs):\n        if self._fields and len(self._fields) > 1:\n            # values() queryset can only be used as nested queries\n            # if they are set up to select only a single field.\n            raise TypeError(\"Cannot use multi-field values as a filter value.\")\n        query = self.query.resolve_expression(*args, **kwargs)\n        query._db = self._db\n        return query\n\n    resolve_expression.queryset_only = True\n\n    def _add_hints(self, **hints):\n        \"\"\"\n        Update hinting information for use by routers. Add new key/values or\n        overwrite existing key/values.\n        \"\"\"\n        self._hints.update(hints)\n\n    def _has_filters(self):\n        \"\"\"\n        Check if this QuerySet has any filtering going on. This isn't\n        equivalent with checking if all objects are present in results, for\n        example, qs[1:]._has_filters() -> False.\n        \"\"\"\n        return self.query.has_filters()\n\n    @staticmethod\n    def _validate_values_are_expressions(values, method_name):\n        invalid_args = sorted(\n            str(arg) for arg in values if not hasattr(arg, \"resolve_expression\")\n        )\n        if invalid_args:\n            raise TypeError(\n                \"QuerySet.%s() received non-expression(s): %s.\"\n                % (\n                    method_name,\n                    \", \".join(invalid_args),\n                )\n            )\n\n    def _not_support_combined_queries(self, operation_name):\n        if self.query.combinator:\n            raise NotSupportedError(\n                \"Calling QuerySet.%s() after %s() is not supported.\"\n                % (operation_name, self.query.combinator)\n            )\n\n    def _check_operator_queryset(self, other, operator_):\n        if self.query.combinator or other.query.combinator:\n            raise TypeError(f\"Cannot use {operator_} operator with combined queryset.\")\n\n    def _check_ordering_first_last_queryset_aggregation(self, method):\n        if isinstance(self.query.group_by, tuple) and not any(\n            col.output_field is self.model._meta.pk for col in self.query.group_by\n        ):\n            raise TypeError(\n                f\"Cannot use QuerySet.{method}() on an unordered queryset performing \"\n                f\"aggregation. Add an ordering with order_by().\"\n            )\n\n\nclass InstanceCheckMeta(type):\n    def __instancecheck__(self, instance):\n        return isinstance(instance, QuerySet) and instance.query.is_empty()\n\n\nclass EmptyQuerySet(metaclass=InstanceCheckMeta):\n    \"\"\"\n    Marker class to checking if a queryset is empty by .none():\n        isinstance(qs.none(), EmptyQuerySet) -> True\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        raise TypeError(\"EmptyQuerySet can't be instantiated\")\n\n\nclass RawQuerySet:\n    \"\"\"\n    Provide an iterator which converts the results of raw SQL queries into\n    annotated model instances.\n    \"\"\"\n\n    def __init__(\n        self,\n        raw_query,\n        model=None,\n        query=None,\n        params=(),\n        translations=None,\n        using=None,\n        hints=None,\n    ):\n        self.raw_query = raw_query\n        self.model = model\n        self._db = using\n        self._hints = hints or {}\n        self.query = query or sql.RawQuery(sql=raw_query, using=self.db, params=params)\n        self.params = params\n        self.translations = translations or {}\n        self._result_cache = None\n        self._prefetch_related_lookups = ()\n        self._prefetch_done = False\n\n    def resolve_model_init_order(self):\n        \"\"\"Resolve the init field names and value positions.\"\"\"\n        converter = connections[self.db].introspection.identifier_converter\n        model_init_fields = [\n            f for f in self.model._meta.fields if converter(f.column) in self.columns\n        ]\n        annotation_fields = [\n            (column, pos)\n            for pos, column in enumerate(self.columns)\n            if column not in self.model_fields\n        ]\n        model_init_order = [\n            self.columns.index(converter(f.column)) for f in model_init_fields\n        ]\n        model_init_names = [f.attname for f in model_init_fields]\n        return model_init_names, model_init_order, annotation_fields\n\n    def prefetch_related(self, *lookups):\n        \"\"\"Same as QuerySet.prefetch_related()\"\"\"\n        clone = self._clone()\n        if lookups == (None,):\n            clone._prefetch_related_lookups = ()\n        else:\n            clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n        return clone\n\n    def _prefetch_related_objects(self):\n        prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n        self._prefetch_done = True\n\n    def _clone(self):\n        \"\"\"Same as QuerySet._clone()\"\"\"\n        c = self.__class__(\n            self.raw_query,\n            model=self.model,\n            query=self.query,\n            params=self.params,\n            translations=self.translations,\n            using=self._db,\n            hints=self._hints,\n        )\n        c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n        return c\n\n    def _fetch_all(self):\n        if self._result_cache is None:\n            self._result_cache = list(self.iterator())\n        if self._prefetch_related_lookups and not self._prefetch_done:\n            self._prefetch_related_objects()\n\n    def __len__(self):\n        self._fetch_all()\n        return len(self._result_cache)\n\n    def __bool__(self):\n        self._fetch_all()\n        return bool(self._result_cache)\n\n    def __iter__(self):\n        self._fetch_all()\n        return iter(self._result_cache)\n\n    def __aiter__(self):\n        # Remember, __aiter__ itself is synchronous, it's the thing it returns\n        # that is async!\n        async def generator():\n            await sync_to_async(self._fetch_all)()\n            for item in self._result_cache:\n                yield item\n\n        return generator()\n\n    def iterator(self):\n        yield from RawModelIterable(self)\n\n    def __repr__(self):\n        return \"<%s: %s>\" % (self.__class__.__name__, self.query)\n\n    def __getitem__(self, k):\n        return list(self)[k]\n\n    @property\n    def db(self):\n        \"\"\"Return the database used if this query is executed now.\"\"\"\n        return self._db or router.db_for_read(self.model, **self._hints)\n\n    def using(self, alias):\n        \"\"\"Select the database this RawQuerySet should execute against.\"\"\"\n        return RawQuerySet(\n            self.raw_query,\n            model=self.model,\n            query=self.query.chain(using=alias),\n            params=self.params,\n            translations=self.translations,\n            using=alias,\n        )\n\n    @cached_property\n    def columns(self):\n        \"\"\"\n        A list of model field names in the order they'll appear in the\n        query results.\n        \"\"\"\n        columns = self.query.get_columns()\n        # Adjust any column names which don't match field names\n        for (query_name, model_name) in self.translations.items():\n            # Ignore translations for nonexistent column names\n            try:\n                index = columns.index(query_name)\n            except ValueError:\n                pass\n            else:\n                columns[index] = model_name\n        return columns\n\n    @cached_property\n    def model_fields(self):\n        \"\"\"A dict mapping column names to model field names.\"\"\"\n        converter = connections[self.db].introspection.identifier_converter\n        model_fields = {}\n        for field in self.model._meta.fields:\n            name, column = field.get_attname_column()\n            model_fields[converter(column)] = field\n        return model_fields\n\n\nclass Prefetch:\n    def __init__(self, lookup, queryset=None, to_attr=None):\n        # `prefetch_through` is the path we traverse to perform the prefetch.\n        self.prefetch_through = lookup\n        # `prefetch_to` is the path to the attribute that stores the result.\n        self.prefetch_to = lookup\n        if queryset is not None and (\n            isinstance(queryset, RawQuerySet)\n            or (\n                hasattr(queryset, \"_iterable_class\")\n                and not issubclass(queryset._iterable_class, ModelIterable)\n            )\n        ):\n            raise ValueError(\n                \"Prefetch querysets cannot use raw(), values(), and values_list().\"\n            )\n        if to_attr:\n            self.prefetch_to = LOOKUP_SEP.join(\n                lookup.split(LOOKUP_SEP)[:-1] + [to_attr]\n            )\n\n        self.queryset = queryset\n        self.to_attr = to_attr\n\n    def __getstate__(self):\n        obj_dict = self.__dict__.copy()\n        if self.queryset is not None:\n            queryset = self.queryset._chain()\n            # Prevent the QuerySet from being evaluated\n            queryset._result_cache = []\n            queryset._prefetch_done = True\n            obj_dict[\"queryset\"] = queryset\n        return obj_dict\n\n    def add_prefix(self, prefix):\n        self.prefetch_through = prefix + LOOKUP_SEP + self.prefetch_through\n        self.prefetch_to = prefix + LOOKUP_SEP + self.prefetch_to\n\n    def get_current_prefetch_to(self, level):\n        return LOOKUP_SEP.join(self.prefetch_to.split(LOOKUP_SEP)[: level + 1])\n\n    def get_current_to_attr(self, level):\n        parts = self.prefetch_to.split(LOOKUP_SEP)\n        to_attr = parts[level]\n        as_attr = self.to_attr and level == len(parts) - 1\n        return to_attr, as_attr\n\n    def get_current_queryset(self, level):\n        if self.get_current_prefetch_to(level) == self.prefetch_to:\n            return self.queryset\n        return None\n\n    def __eq__(self, other):\n        if not isinstance(other, Prefetch):\n            return NotImplemented\n        return self.prefetch_to == other.prefetch_to\n\n    def __hash__(self):\n        return hash((self.__class__, self.prefetch_to))\n\n\ndef normalize_prefetch_lookups(lookups, prefix=None):\n    \"\"\"Normalize lookups into Prefetch objects.\"\"\"\n    ret = []\n    for lookup in lookups:\n        if not isinstance(lookup, Prefetch):\n            lookup = Prefetch(lookup)\n        if prefix:\n            lookup.add_prefix(prefix)\n        ret.append(lookup)\n    return ret\n\n\ndef prefetch_related_objects(model_instances, *related_lookups):\n    \"\"\"\n    Populate prefetched object caches for a list of model instances based on\n    the lookups/Prefetch instances given.\n    \"\"\"\n    if not model_instances:\n        return  # nothing to do\n\n    # We need to be able to dynamically add to the list of prefetch_related\n    # lookups that we look up (see below).  So we need some book keeping to\n    # ensure we don't do duplicate work.\n    done_queries = {}  # dictionary of things like 'foo__bar': [results]\n\n    auto_lookups = set()  # we add to this as we go through.\n    followed_descriptors = set()  # recursion protection\n\n    all_lookups = normalize_prefetch_lookups(reversed(related_lookups))\n    while all_lookups:\n        lookup = all_lookups.pop()\n        if lookup.prefetch_to in done_queries:\n            if lookup.queryset is not None:\n                raise ValueError(\n                    \"'%s' lookup was already seen with a different queryset. \"\n                    \"You may need to adjust the ordering of your lookups.\"\n                    % lookup.prefetch_to\n                )\n\n            continue\n\n        # Top level, the list of objects to decorate is the result cache\n        # from the primary QuerySet. It won't be for deeper levels.\n        obj_list = model_instances\n\n        through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)\n        for level, through_attr in enumerate(through_attrs):\n            # Prepare main instances\n            if not obj_list:\n                break\n\n            prefetch_to = lookup.get_current_prefetch_to(level)\n            if prefetch_to in done_queries:\n                # Skip any prefetching, and any object preparation\n                obj_list = done_queries[prefetch_to]\n                continue\n\n            # Prepare objects:\n            good_objects = True\n            for obj in obj_list:\n                # Since prefetching can re-use instances, it is possible to have\n                # the same instance multiple times in obj_list, so obj might\n                # already be prepared.\n                if not hasattr(obj, \"_prefetched_objects_cache\"):\n                    try:\n                        obj._prefetched_objects_cache = {}\n                    except (AttributeError, TypeError):\n                        # Must be an immutable object from\n                        # values_list(flat=True), for example (TypeError) or\n                        # a QuerySet subclass that isn't returning Model\n                        # instances (AttributeError), either in Django or a 3rd\n                        # party. prefetch_related() doesn't make sense, so quit.\n                        good_objects = False\n                        break\n            if not good_objects:\n                break\n\n            # Descend down tree\n\n            # We assume that objects retrieved are homogeneous (which is the premise\n            # of prefetch_related), so what applies to first object applies to all.\n            first_obj = obj_list[0]\n            to_attr = lookup.get_current_to_attr(level)[0]\n            prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(\n                first_obj, through_attr, to_attr\n            )\n\n            if not attr_found:\n                raise AttributeError(\n                    \"Cannot find '%s' on %s object, '%s' is an invalid \"\n                    \"parameter to prefetch_related()\"\n                    % (\n                        through_attr,\n                        first_obj.__class__.__name__,\n                        lookup.prefetch_through,\n                    )\n                )\n\n            if level == len(through_attrs) - 1 and prefetcher is None:\n                # Last one, this *must* resolve to something that supports\n                # prefetching, otherwise there is no point adding it and the\n                # developer asking for it has made a mistake.\n                raise ValueError(\n                    \"'%s' does not resolve to an item that supports \"\n                    \"prefetching - this is an invalid parameter to \"\n                    \"prefetch_related().\" % lookup.prefetch_through\n                )\n\n            obj_to_fetch = None\n            if prefetcher is not None:\n                obj_to_fetch = [obj for obj in obj_list if not is_fetched(obj)]\n\n            if obj_to_fetch:\n                obj_list, additional_lookups = prefetch_one_level(\n                    obj_to_fetch,\n                    prefetcher,\n                    lookup,\n                    level,\n                )\n                # We need to ensure we don't keep adding lookups from the\n                # same relationships to stop infinite recursion. So, if we\n                # are already on an automatically added lookup, don't add\n                # the new lookups from relationships we've seen already.\n                if not (\n                    prefetch_to in done_queries\n                    and lookup in auto_lookups\n                    and descriptor in followed_descriptors\n                ):\n                    done_queries[prefetch_to] = obj_list\n                    new_lookups = normalize_prefetch_lookups(\n                        reversed(additional_lookups), prefetch_to\n                    )\n                    auto_lookups.update(new_lookups)\n                    all_lookups.extend(new_lookups)\n                followed_descriptors.add(descriptor)\n            else:\n                # Either a singly related object that has already been fetched\n                # (e.g. via select_related), or hopefully some other property\n                # that doesn't support prefetching but needs to be traversed.\n\n                # We replace the current list of parent objects with the list\n                # of related objects, filtering out empty or missing values so\n                # that we can continue with nullable or reverse relations.\n                new_obj_list = []\n                for obj in obj_list:\n                    if through_attr in getattr(obj, \"_prefetched_objects_cache\", ()):\n                        # If related objects have been prefetched, use the\n                        # cache rather than the object's through_attr.\n                        new_obj = list(obj._prefetched_objects_cache.get(through_attr))\n                    else:\n                        try:\n                            new_obj = getattr(obj, through_attr)\n                        except exceptions.ObjectDoesNotExist:\n                            continue\n                    if new_obj is None:\n                        continue\n                    # We special-case `list` rather than something more generic\n                    # like `Iterable` because we don't want to accidentally match\n                    # user models that define __iter__.\n                    if isinstance(new_obj, list):\n                        new_obj_list.extend(new_obj)\n                    else:\n                        new_obj_list.append(new_obj)\n                obj_list = new_obj_list\n\n\ndef get_prefetcher(instance, through_attr, to_attr):\n    \"\"\"\n    For the attribute 'through_attr' on the given instance, find\n    an object that has a get_prefetch_queryset().\n    Return a 4 tuple containing:\n    (the object with get_prefetch_queryset (or None),\n     the descriptor object representing this relationship (or None),\n     a boolean that is False if the attribute was not found at all,\n     a function that takes an instance and returns a boolean that is True if\n     the attribute has already been fetched for that instance)\n    \"\"\"\n\n    def has_to_attr_attribute(instance):\n        return hasattr(instance, to_attr)\n\n    prefetcher = None\n    is_fetched = has_to_attr_attribute\n\n    # For singly related objects, we have to avoid getting the attribute\n    # from the object, as this will trigger the query. So we first try\n    # on the class, in order to get the descriptor object.\n    rel_obj_descriptor = getattr(instance.__class__, through_attr, None)\n    if rel_obj_descriptor is None:\n        attr_found = hasattr(instance, through_attr)\n    else:\n        attr_found = True\n        if rel_obj_descriptor:\n            # singly related object, descriptor object has the\n            # get_prefetch_queryset() method.\n            if hasattr(rel_obj_descriptor, \"get_prefetch_queryset\"):\n                prefetcher = rel_obj_descriptor\n                is_fetched = rel_obj_descriptor.is_cached\n            else:\n                # descriptor doesn't support prefetching, so we go ahead and get\n                # the attribute on the instance rather than the class to\n                # support many related managers\n                rel_obj = getattr(instance, through_attr)\n                if hasattr(rel_obj, \"get_prefetch_queryset\"):\n                    prefetcher = rel_obj\n                if through_attr != to_attr:\n                    # Special case cached_property instances because hasattr\n                    # triggers attribute computation and assignment.\n                    if isinstance(\n                        getattr(instance.__class__, to_attr, None), cached_property\n                    ):\n\n                        def has_cached_property(instance):\n                            return to_attr in instance.__dict__\n\n                        is_fetched = has_cached_property\n                else:\n\n                    def in_prefetched_cache(instance):\n                        return through_attr in instance._prefetched_objects_cache\n\n                    is_fetched = in_prefetched_cache\n    return prefetcher, rel_obj_descriptor, attr_found, is_fetched\n\n\ndef prefetch_one_level(instances, prefetcher, lookup, level):\n    \"\"\"\n    Helper function for prefetch_related_objects().\n\n    Run prefetches on all instances using the prefetcher object,\n    assigning results to relevant caches in instance.\n\n    Return the prefetched objects along with any additional prefetches that\n    must be done due to prefetch_related lookups found from default managers.\n    \"\"\"\n    # prefetcher must have a method get_prefetch_queryset() which takes a list\n    # of instances, and returns a tuple:\n\n    # (queryset of instances of self.model that are related to passed in instances,\n    #  callable that gets value to be matched for returned instances,\n    #  callable that gets value to be matched for passed in instances,\n    #  boolean that is True for singly related objects,\n    #  cache or field name to assign to,\n    #  boolean that is True when the previous argument is a cache name vs a field name).\n\n    # The 'values to be matched' must be hashable as they will be used\n    # in a dictionary.\n\n    (\n        rel_qs,\n        rel_obj_attr,\n        instance_attr,\n        single,\n        cache_name,\n        is_descriptor,\n    ) = prefetcher.get_prefetch_queryset(instances, lookup.get_current_queryset(level))\n    # We have to handle the possibility that the QuerySet we just got back\n    # contains some prefetch_related lookups. We don't want to trigger the\n    # prefetch_related functionality by evaluating the query. Rather, we need\n    # to merge in the prefetch_related lookups.\n    # Copy the lookups in case it is a Prefetch object which could be reused\n    # later (happens in nested prefetch_related).\n    additional_lookups = [\n        copy.copy(additional_lookup)\n        for additional_lookup in getattr(rel_qs, \"_prefetch_related_lookups\", ())\n    ]\n    if additional_lookups:\n        # Don't need to clone because the manager should have given us a fresh\n        # instance, so we access an internal instead of using public interface\n        # for performance reasons.\n        rel_qs._prefetch_related_lookups = ()\n\n    all_related_objects = list(rel_qs)\n\n    rel_obj_cache = {}\n    for rel_obj in all_related_objects:\n        rel_attr_val = rel_obj_attr(rel_obj)\n        rel_obj_cache.setdefault(rel_attr_val, []).append(rel_obj)\n\n    to_attr, as_attr = lookup.get_current_to_attr(level)\n    # Make sure `to_attr` does not conflict with a field.\n    if as_attr and instances:\n        # We assume that objects retrieved are homogeneous (which is the premise\n        # of prefetch_related), so what applies to first object applies to all.\n        model = instances[0].__class__\n        try:\n            model._meta.get_field(to_attr)\n        except exceptions.FieldDoesNotExist:\n            pass\n        else:\n            msg = \"to_attr={} conflicts with a field on the {} model.\"\n            raise ValueError(msg.format(to_attr, model.__name__))\n\n    # Whether or not we're prefetching the last part of the lookup.\n    leaf = len(lookup.prefetch_through.split(LOOKUP_SEP)) - 1 == level\n\n    for obj in instances:\n        instance_attr_val = instance_attr(obj)\n        vals = rel_obj_cache.get(instance_attr_val, [])\n\n        if single:\n            val = vals[0] if vals else None\n            if as_attr:\n                # A to_attr has been given for the prefetch.\n                setattr(obj, to_attr, val)\n            elif is_descriptor:\n                # cache_name points to a field name in obj.\n                # This field is a descriptor for a related object.\n                setattr(obj, cache_name, val)\n            else:\n                # No to_attr has been given for this prefetch operation and the\n                # cache_name does not point to a descriptor. Store the value of\n                # the field in the object's field cache.\n                obj._state.fields_cache[cache_name] = val\n        else:\n            if as_attr:\n                setattr(obj, to_attr, vals)\n            else:\n                manager = getattr(obj, to_attr)\n                if leaf and lookup.queryset is not None:\n                    qs = manager._apply_rel_filters(lookup.queryset)\n                else:\n                    qs = manager.get_queryset()\n                qs._result_cache = vals\n                # We don't want the individual qs doing prefetch_related now,\n                # since we have merged this into the current work.\n                qs._prefetch_done = True\n                obj._prefetched_objects_cache[cache_name] = qs\n    return all_related_objects, additional_lookups\n\n\nclass RelatedPopulator:\n    \"\"\"\n    RelatedPopulator is used for select_related() object instantiation.\n\n    The idea is that each select_related() model will be populated by a\n    different RelatedPopulator instance. The RelatedPopulator instances get\n    klass_info and select (computed in SQLCompiler) plus the used db as\n    input for initialization. That data is used to compute which columns\n    to use, how to instantiate the model, and how to populate the links\n    between the objects.\n\n    The actual creation of the objects is done in populate() method. This\n    method gets row and from_obj as input and populates the select_related()\n    model instance.\n    \"\"\"\n\n    def __init__(self, klass_info, select, db):\n        self.db = db\n        # Pre-compute needed attributes. The attributes are:\n        #  - model_cls: the possibly deferred model class to instantiate\n        #  - either:\n        #    - cols_start, cols_end: usually the columns in the row are\n        #      in the same order model_cls.__init__ expects them, so we\n        #      can instantiate by model_cls(*row[cols_start:cols_end])\n        #    - reorder_for_init: When select_related descends to a child\n        #      class, then we want to reuse the already selected parent\n        #      data. However, in this case the parent data isn't necessarily\n        #      in the same order that Model.__init__ expects it to be, so\n        #      we have to reorder the parent data. The reorder_for_init\n        #      attribute contains a function used to reorder the field data\n        #      in the order __init__ expects it.\n        #  - pk_idx: the index of the primary key field in the reordered\n        #    model data. Used to check if a related object exists at all.\n        #  - init_list: the field attnames fetched from the database. For\n        #    deferred models this isn't the same as all attnames of the\n        #    model's fields.\n        #  - related_populators: a list of RelatedPopulator instances if\n        #    select_related() descends to related models from this model.\n        #  - local_setter, remote_setter: Methods to set cached values on\n        #    the object being populated and on the remote object. Usually\n        #    these are Field.set_cached_value() methods.\n        select_fields = klass_info[\"select_fields\"]\n        from_parent = klass_info[\"from_parent\"]\n        if not from_parent:\n            self.cols_start = select_fields[0]\n            self.cols_end = select_fields[-1] + 1\n            self.init_list = [\n                f[0].target.attname for f in select[self.cols_start : self.cols_end]\n            ]\n            self.reorder_for_init = None\n        else:\n            attname_indexes = {\n                select[idx][0].target.attname: idx for idx in select_fields\n            }\n            model_init_attnames = (\n                f.attname for f in klass_info[\"model\"]._meta.concrete_fields\n            )\n            self.init_list = [\n                attname for attname in model_init_attnames if attname in attname_indexes\n            ]\n            self.reorder_for_init = operator.itemgetter(\n                *[attname_indexes[attname] for attname in self.init_list]\n            )\n\n        self.model_cls = klass_info[\"model\"]\n        self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)\n        self.related_populators = get_related_populators(klass_info, select, self.db)\n        self.local_setter = klass_info[\"local_setter\"]\n        self.remote_setter = klass_info[\"remote_setter\"]\n\n    def populate(self, row, from_obj):\n        if self.reorder_for_init:\n            obj_data = self.reorder_for_init(row)\n        else:\n            obj_data = row[self.cols_start : self.cols_end]\n        if obj_data[self.pk_idx] is None:\n            obj = None\n        else:\n            obj = self.model_cls.from_db(self.db, self.init_list, obj_data)\n            for rel_iter in self.related_populators:\n                rel_iter.populate(row, obj)\n        self.local_setter(from_obj, obj)\n        if obj is not None:\n            self.remote_setter(obj, from_obj)\n\n\ndef get_related_populators(klass_info, select, db):\n    iterators = []\n    related_klass_infos = klass_info.get(\"related_klass_infos\", [])\n    for rel_klass_info in related_klass_infos:\n        rel_cls = RelatedPopulator(rel_klass_info, select, db)\n        iterators.append(rel_cls)\n    return iterators"}
{"model_name_or_path": "agentless", "instance_id": "django__django-16100", "model_patch": "", "raw_model_patch": "diff --git a/django/contrib/admin/views/main.py b/django/contrib/admin/views/main.py\nindex e6989d9..4879118 100644\n--- a/django/contrib/admin/views/main.py\n+++ b/django/contrib/admin/views/main.py\n@@ -271,7 +271,10 @@ class ChangeList:\n                 p[k] = v\n         return \"?%s\" % urlencode(sorted(p.items()))\n \n+    from django.db import transaction\n+    \n     def get_results(self, request):\n+        with transaction.atomic():\n         paginator = self.model_admin.get_paginator(\n             request, self.queryset, self.list_per_page\n         )\n", "original_file_content": "from datetime import datetime, timedelta\n\nfrom django import forms\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.admin import FieldListFilter\nfrom django.contrib.admin.exceptions import (\n    DisallowedModelAdminLookup,\n    DisallowedModelAdminToField,\n)\nfrom django.contrib.admin.options import (\n    IS_POPUP_VAR,\n    TO_FIELD_VAR,\n    IncorrectLookupParameters,\n)\nfrom django.contrib.admin.utils import (\n    get_fields_from_path,\n    lookup_spawns_duplicates,\n    prepare_lookup_value,\n    quote,\n)\nfrom django.core.exceptions import (\n    FieldDoesNotExist,\n    ImproperlyConfigured,\n    SuspiciousOperation,\n)\nfrom django.core.paginator import InvalidPage\nfrom django.db.models import Exists, F, Field, ManyToOneRel, OrderBy, OuterRef\nfrom django.db.models.expressions import Combinable\nfrom django.urls import reverse\nfrom django.utils.http import urlencode\nfrom django.utils.timezone import make_aware\nfrom django.utils.translation import gettext\n\n# Changelist settings\nALL_VAR = \"all\"\nORDER_VAR = \"o\"\nPAGE_VAR = \"p\"\nSEARCH_VAR = \"q\"\nERROR_FLAG = \"e\"\n\nIGNORED_PARAMS = (ALL_VAR, ORDER_VAR, SEARCH_VAR, IS_POPUP_VAR, TO_FIELD_VAR)\n\n\nclass ChangeListSearchForm(forms.Form):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Populate \"fields\" dynamically because SEARCH_VAR is a variable:\n        self.fields = {\n            SEARCH_VAR: forms.CharField(required=False, strip=False),\n        }\n\n\nclass ChangeList:\n    search_form_class = ChangeListSearchForm\n\n    def __init__(\n        self,\n        request,\n        model,\n        list_display,\n        list_display_links,\n        list_filter,\n        date_hierarchy,\n        search_fields,\n        list_select_related,\n        list_per_page,\n        list_max_show_all,\n        list_editable,\n        model_admin,\n        sortable_by,\n        search_help_text,\n    ):\n        self.model = model\n        self.opts = model._meta\n        self.lookup_opts = self.opts\n        self.root_queryset = model_admin.get_queryset(request)\n        self.list_display = list_display\n        self.list_display_links = list_display_links\n        self.list_filter = list_filter\n        self.has_filters = None\n        self.has_active_filters = None\n        self.clear_all_filters_qs = None\n        self.date_hierarchy = date_hierarchy\n        self.search_fields = search_fields\n        self.list_select_related = list_select_related\n        self.list_per_page = list_per_page\n        self.list_max_show_all = list_max_show_all\n        self.model_admin = model_admin\n        self.preserved_filters = model_admin.get_preserved_filters(request)\n        self.sortable_by = sortable_by\n        self.search_help_text = search_help_text\n\n        # Get search parameters from the query string.\n        _search_form = self.search_form_class(request.GET)\n        if not _search_form.is_valid():\n            for error in _search_form.errors.values():\n                messages.error(request, \", \".join(error))\n        self.query = _search_form.cleaned_data.get(SEARCH_VAR) or \"\"\n        try:\n            self.page_num = int(request.GET.get(PAGE_VAR, 1))\n        except ValueError:\n            self.page_num = 1\n        self.show_all = ALL_VAR in request.GET\n        self.is_popup = IS_POPUP_VAR in request.GET\n        to_field = request.GET.get(TO_FIELD_VAR)\n        if to_field and not model_admin.to_field_allowed(request, to_field):\n            raise DisallowedModelAdminToField(\n                \"The field %s cannot be referenced.\" % to_field\n            )\n        self.to_field = to_field\n        self.params = dict(request.GET.items())\n        if PAGE_VAR in self.params:\n            del self.params[PAGE_VAR]\n        if ERROR_FLAG in self.params:\n            del self.params[ERROR_FLAG]\n\n        if self.is_popup:\n            self.list_editable = ()\n        else:\n            self.list_editable = list_editable\n        self.queryset = self.get_queryset(request)\n        self.get_results(request)\n        if self.is_popup:\n            title = gettext(\"Select %s\")\n        elif self.model_admin.has_change_permission(request):\n            title = gettext(\"Select %s to change\")\n        else:\n            title = gettext(\"Select %s to view\")\n        self.title = title % self.opts.verbose_name\n        self.pk_attname = self.lookup_opts.pk.attname\n\n    def __repr__(self):\n        return \"<%s: model=%s model_admin=%s>\" % (\n            self.__class__.__qualname__,\n            self.model.__qualname__,\n            self.model_admin.__class__.__qualname__,\n        )\n\n    def get_filters_params(self, params=None):\n        \"\"\"\n        Return all params except IGNORED_PARAMS.\n        \"\"\"\n        params = params or self.params\n        lookup_params = params.copy()  # a dictionary of the query string\n        # Remove all the parameters that are globally and systematically\n        # ignored.\n        for ignored in IGNORED_PARAMS:\n            if ignored in lookup_params:\n                del lookup_params[ignored]\n        return lookup_params\n\n    def get_filters(self, request):\n        lookup_params = self.get_filters_params()\n        may_have_duplicates = False\n        has_active_filters = False\n\n        for key, value in lookup_params.items():\n            if not self.model_admin.lookup_allowed(key, value):\n                raise DisallowedModelAdminLookup(\"Filtering by %s not allowed\" % key)\n\n        filter_specs = []\n        for list_filter in self.list_filter:\n            lookup_params_count = len(lookup_params)\n            if callable(list_filter):\n                # This is simply a custom list filter class.\n                spec = list_filter(request, lookup_params, self.model, self.model_admin)\n            else:\n                field_path = None\n                if isinstance(list_filter, (tuple, list)):\n                    # This is a custom FieldListFilter class for a given field.\n                    field, field_list_filter_class = list_filter\n                else:\n                    # This is simply a field name, so use the default\n                    # FieldListFilter class that has been registered for the\n                    # type of the given field.\n                    field, field_list_filter_class = list_filter, FieldListFilter.create\n                if not isinstance(field, Field):\n                    field_path = field\n                    field = get_fields_from_path(self.model, field_path)[-1]\n\n                spec = field_list_filter_class(\n                    field,\n                    request,\n                    lookup_params,\n                    self.model,\n                    self.model_admin,\n                    field_path=field_path,\n                )\n                # field_list_filter_class removes any lookup_params it\n                # processes. If that happened, check if duplicates should be\n                # removed.\n                if lookup_params_count > len(lookup_params):\n                    may_have_duplicates |= lookup_spawns_duplicates(\n                        self.lookup_opts,\n                        field_path,\n                    )\n            if spec and spec.has_output():\n                filter_specs.append(spec)\n                if lookup_params_count > len(lookup_params):\n                    has_active_filters = True\n\n        if self.date_hierarchy:\n            # Create bounded lookup parameters so that the query is more\n            # efficient.\n            year = lookup_params.pop(\"%s__year\" % self.date_hierarchy, None)\n            if year is not None:\n                month = lookup_params.pop(\"%s__month\" % self.date_hierarchy, None)\n                day = lookup_params.pop(\"%s__day\" % self.date_hierarchy, None)\n                try:\n                    from_date = datetime(\n                        int(year),\n                        int(month if month is not None else 1),\n                        int(day if day is not None else 1),\n                    )\n                except ValueError as e:\n                    raise IncorrectLookupParameters(e) from e\n                if day:\n                    to_date = from_date + timedelta(days=1)\n                elif month:\n                    # In this branch, from_date will always be the first of a\n                    # month, so advancing 32 days gives the next month.\n                    to_date = (from_date + timedelta(days=32)).replace(day=1)\n                else:\n                    to_date = from_date.replace(year=from_date.year + 1)\n                if settings.USE_TZ:\n                    from_date = make_aware(from_date)\n                    to_date = make_aware(to_date)\n                lookup_params.update(\n                    {\n                        \"%s__gte\" % self.date_hierarchy: from_date,\n                        \"%s__lt\" % self.date_hierarchy: to_date,\n                    }\n                )\n\n        # At this point, all the parameters used by the various ListFilters\n        # have been removed from lookup_params, which now only contains other\n        # parameters passed via the query string. We now loop through the\n        # remaining parameters both to ensure that all the parameters are valid\n        # fields and to determine if at least one of them spawns duplicates. If\n        # the lookup parameters aren't real fields, then bail out.\n        try:\n            for key, value in lookup_params.items():\n                lookup_params[key] = prepare_lookup_value(key, value)\n                may_have_duplicates |= lookup_spawns_duplicates(self.lookup_opts, key)\n            return (\n                filter_specs,\n                bool(filter_specs),\n                lookup_params,\n                may_have_duplicates,\n                has_active_filters,\n            )\n        except FieldDoesNotExist as e:\n            raise IncorrectLookupParameters(e) from e\n\n    def get_query_string(self, new_params=None, remove=None):\n        if new_params is None:\n            new_params = {}\n        if remove is None:\n            remove = []\n        p = self.params.copy()\n        for r in remove:\n            for k in list(p):\n                if k.startswith(r):\n                    del p[k]\n        for k, v in new_params.items():\n            if v is None:\n                if k in p:\n                    del p[k]\n            else:\n                p[k] = v\n        return \"?%s\" % urlencode(sorted(p.items()))\n\n    def get_results(self, request):\n        paginator = self.model_admin.get_paginator(\n            request, self.queryset, self.list_per_page\n        )\n        # Get the number of objects, with admin filters applied.\n        result_count = paginator.count\n\n        # Get the total number of objects, with no admin filters applied.\n        if self.model_admin.show_full_result_count:\n            full_result_count = self.root_queryset.count()\n        else:\n            full_result_count = None\n        can_show_all = result_count <= self.list_max_show_all\n        multi_page = result_count > self.list_per_page\n\n        # Get the list of objects to display on this page.\n        if (self.show_all and can_show_all) or not multi_page:\n            result_list = self.queryset._clone()\n        else:\n            try:\n                result_list = paginator.page(self.page_num).object_list\n            except InvalidPage:\n                raise IncorrectLookupParameters\n\n        self.result_count = result_count\n        self.show_full_result_count = self.model_admin.show_full_result_count\n        # Admin actions are shown if there is at least one entry\n        # or if entries are not counted because show_full_result_count is disabled\n        self.show_admin_actions = not self.show_full_result_count or bool(\n            full_result_count\n        )\n        self.full_result_count = full_result_count\n        self.result_list = result_list\n        self.can_show_all = can_show_all\n        self.multi_page = multi_page\n        self.paginator = paginator\n\n    def _get_default_ordering(self):\n        ordering = []\n        if self.model_admin.ordering:\n            ordering = self.model_admin.ordering\n        elif self.lookup_opts.ordering:\n            ordering = self.lookup_opts.ordering\n        return ordering\n\n    def get_ordering_field(self, field_name):\n        \"\"\"\n        Return the proper model field name corresponding to the given\n        field_name to use for ordering. field_name may either be the name of a\n        proper model field or the name of a method (on the admin or model) or a\n        callable with the 'admin_order_field' attribute. Return None if no\n        proper model field name can be matched.\n        \"\"\"\n        try:\n            field = self.lookup_opts.get_field(field_name)\n            return field.name\n        except FieldDoesNotExist:\n            # See whether field_name is a name of a non-field\n            # that allows sorting.\n            if callable(field_name):\n                attr = field_name\n            elif hasattr(self.model_admin, field_name):\n                attr = getattr(self.model_admin, field_name)\n            else:\n                attr = getattr(self.model, field_name)\n            if isinstance(attr, property) and hasattr(attr, \"fget\"):\n                attr = attr.fget\n            return getattr(attr, \"admin_order_field\", None)\n\n    def get_ordering(self, request, queryset):\n        \"\"\"\n        Return the list of ordering fields for the change list.\n        First check the get_ordering() method in model admin, then check\n        the object's default ordering. Then, any manually-specified ordering\n        from the query string overrides anything. Finally, a deterministic\n        order is guaranteed by calling _get_deterministic_ordering() with the\n        constructed ordering.\n        \"\"\"\n        params = self.params\n        ordering = list(\n            self.model_admin.get_ordering(request) or self._get_default_ordering()\n        )\n        if ORDER_VAR in params:\n            # Clear ordering and used params\n            ordering = []\n            order_params = params[ORDER_VAR].split(\".\")\n            for p in order_params:\n                try:\n                    none, pfx, idx = p.rpartition(\"-\")\n                    field_name = self.list_display[int(idx)]\n                    order_field = self.get_ordering_field(field_name)\n                    if not order_field:\n                        continue  # No 'admin_order_field', skip it\n                    if isinstance(order_field, OrderBy):\n                        if pfx == \"-\":\n                            order_field = order_field.copy()\n                            order_field.reverse_ordering()\n                        ordering.append(order_field)\n                    elif hasattr(order_field, \"resolve_expression\"):\n                        # order_field is an expression.\n                        ordering.append(\n                            order_field.desc() if pfx == \"-\" else order_field.asc()\n                        )\n                    # reverse order if order_field has already \"-\" as prefix\n                    elif order_field.startswith(\"-\") and pfx == \"-\":\n                        ordering.append(order_field[1:])\n                    else:\n                        ordering.append(pfx + order_field)\n                except (IndexError, ValueError):\n                    continue  # Invalid ordering specified, skip it.\n\n        # Add the given query's ordering fields, if any.\n        ordering.extend(queryset.query.order_by)\n\n        return self._get_deterministic_ordering(ordering)\n\n    def _get_deterministic_ordering(self, ordering):\n        \"\"\"\n        Ensure a deterministic order across all database backends. Search for a\n        single field or unique together set of fields providing a total\n        ordering. If these are missing, augment the ordering with a descendant\n        primary key.\n        \"\"\"\n        ordering = list(ordering)\n        ordering_fields = set()\n        total_ordering_fields = {\"pk\"} | {\n            field.attname\n            for field in self.lookup_opts.fields\n            if field.unique and not field.null\n        }\n        for part in ordering:\n            # Search for single field providing a total ordering.\n            field_name = None\n            if isinstance(part, str):\n                field_name = part.lstrip(\"-\")\n            elif isinstance(part, F):\n                field_name = part.name\n            elif isinstance(part, OrderBy) and isinstance(part.expression, F):\n                field_name = part.expression.name\n            if field_name:\n                # Normalize attname references by using get_field().\n                try:\n                    field = self.lookup_opts.get_field(field_name)\n                except FieldDoesNotExist:\n                    # Could be \"?\" for random ordering or a related field\n                    # lookup. Skip this part of introspection for now.\n                    continue\n                # Ordering by a related field name orders by the referenced\n                # model's ordering. Skip this part of introspection for now.\n                if field.remote_field and field_name == field.name:\n                    continue\n                if field.attname in total_ordering_fields:\n                    break\n                ordering_fields.add(field.attname)\n        else:\n            # No single total ordering field, try unique_together and total\n            # unique constraints.\n            constraint_field_names = (\n                *self.lookup_opts.unique_together,\n                *(\n                    constraint.fields\n                    for constraint in self.lookup_opts.total_unique_constraints\n                ),\n            )\n            for field_names in constraint_field_names:\n                # Normalize attname references by using get_field().\n                fields = [\n                    self.lookup_opts.get_field(field_name) for field_name in field_names\n                ]\n                # Composite unique constraints containing a nullable column\n                # cannot ensure total ordering.\n                if any(field.null for field in fields):\n                    continue\n                if ordering_fields.issuperset(field.attname for field in fields):\n                    break\n            else:\n                # If no set of unique fields is present in the ordering, rely\n                # on the primary key to provide total ordering.\n                ordering.append(\"-pk\")\n        return ordering\n\n    def get_ordering_field_columns(self):\n        \"\"\"\n        Return a dictionary of ordering field column numbers and asc/desc.\n        \"\"\"\n        # We must cope with more than one column having the same underlying sort\n        # field, so we base things on column numbers.\n        ordering = self._get_default_ordering()\n        ordering_fields = {}\n        if ORDER_VAR not in self.params:\n            # for ordering specified on ModelAdmin or model Meta, we don't know\n            # the right column numbers absolutely, because there might be more\n            # than one column associated with that ordering, so we guess.\n            for field in ordering:\n                if isinstance(field, (Combinable, OrderBy)):\n                    if not isinstance(field, OrderBy):\n                        field = field.asc()\n                    if isinstance(field.expression, F):\n                        order_type = \"desc\" if field.descending else \"asc\"\n                        field = field.expression.name\n                    else:\n                        continue\n                elif field.startswith(\"-\"):\n                    field = field[1:]\n                    order_type = \"desc\"\n                else:\n                    order_type = \"asc\"\n                for index, attr in enumerate(self.list_display):\n                    if self.get_ordering_field(attr) == field:\n                        ordering_fields[index] = order_type\n                        break\n        else:\n            for p in self.params[ORDER_VAR].split(\".\"):\n                none, pfx, idx = p.rpartition(\"-\")\n                try:\n                    idx = int(idx)\n                except ValueError:\n                    continue  # skip it\n                ordering_fields[idx] = \"desc\" if pfx == \"-\" else \"asc\"\n        return ordering_fields\n\n    def get_queryset(self, request):\n        # First, we collect all the declared list filters.\n        (\n            self.filter_specs,\n            self.has_filters,\n            remaining_lookup_params,\n            filters_may_have_duplicates,\n            self.has_active_filters,\n        ) = self.get_filters(request)\n        # Then, we let every list filter modify the queryset to its liking.\n        qs = self.root_queryset\n        for filter_spec in self.filter_specs:\n            new_qs = filter_spec.queryset(request, qs)\n            if new_qs is not None:\n                qs = new_qs\n\n        try:\n            # Finally, we apply the remaining lookup parameters from the query\n            # string (i.e. those that haven't already been processed by the\n            # filters).\n            qs = qs.filter(**remaining_lookup_params)\n        except (SuspiciousOperation, ImproperlyConfigured):\n            # Allow certain types of errors to be re-raised as-is so that the\n            # caller can treat them in a special way.\n            raise\n        except Exception as e:\n            # Every other error is caught with a naked except, because we don't\n            # have any other way of validating lookup parameters. They might be\n            # invalid if the keyword arguments are incorrect, or if the values\n            # are not in the correct type, so we might get FieldError,\n            # ValueError, ValidationError, or ?.\n            raise IncorrectLookupParameters(e)\n\n        # Apply search results\n        qs, search_may_have_duplicates = self.model_admin.get_search_results(\n            request,\n            qs,\n            self.query,\n        )\n\n        # Set query string for clearing all filters.\n        self.clear_all_filters_qs = self.get_query_string(\n            new_params=remaining_lookup_params,\n            remove=self.get_filters_params(),\n        )\n        # Remove duplicates from results, if necessary\n        if filters_may_have_duplicates | search_may_have_duplicates:\n            qs = qs.filter(pk=OuterRef(\"pk\"))\n            qs = self.root_queryset.filter(Exists(qs))\n\n        # Set ordering.\n        ordering = self.get_ordering(request, qs)\n        qs = qs.order_by(*ordering)\n\n        if not qs.query.select_related:\n            qs = self.apply_select_related(qs)\n\n        return qs\n\n    def apply_select_related(self, qs):\n        if self.list_select_related is True:\n            return qs.select_related()\n\n        if self.list_select_related is False:\n            if self.has_related_field_in_list_display():\n                return qs.select_related()\n\n        if self.list_select_related:\n            return qs.select_related(*self.list_select_related)\n        return qs\n\n    def has_related_field_in_list_display(self):\n        for field_name in self.list_display:\n            try:\n                field = self.lookup_opts.get_field(field_name)\n            except FieldDoesNotExist:\n                pass\n            else:\n                if isinstance(field.remote_field, ManyToOneRel):\n                    # <FK>_id field names don't require a join.\n                    if field_name != field.get_attname():\n                        return True\n        return False\n\n    def url_for_result(self, result):\n        pk = getattr(result, self.pk_attname)\n        return reverse(\n            \"admin:%s_%s_change\" % (self.opts.app_label, self.opts.model_name),\n            args=(quote(pk),),\n            current_app=self.model_admin.admin_site.name,\n        )"}
{"model_name_or_path": "agentless", "instance_id": "django__django-16136", "model_patch": "", "raw_model_patch": "", "original_file_content": "import asyncio\nimport logging\n\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.http import (\n    HttpResponse,\n    HttpResponseGone,\n    HttpResponseNotAllowed,\n    HttpResponsePermanentRedirect,\n    HttpResponseRedirect,\n)\nfrom django.template.response import TemplateResponse\nfrom django.urls import reverse\nfrom django.utils.decorators import classonlymethod\nfrom django.utils.functional import classproperty\n\nlogger = logging.getLogger(\"django.request\")\n\n\nclass ContextMixin:\n    \"\"\"\n    A default context mixin that passes the keyword arguments received by\n    get_context_data() as the template context.\n    \"\"\"\n\n    extra_context = None\n\n    def get_context_data(self, **kwargs):\n        kwargs.setdefault(\"view\", self)\n        if self.extra_context is not None:\n            kwargs.update(self.extra_context)\n        return kwargs\n\n\nclass View:\n    \"\"\"\n    Intentionally simple parent class for all views. Only implements\n    dispatch-by-method and simple sanity checking.\n    \"\"\"\n\n    http_method_names = [\n        \"get\",\n        \"post\",\n        \"put\",\n        \"patch\",\n        \"delete\",\n        \"head\",\n        \"options\",\n        \"trace\",\n    ]\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Constructor. Called in the URLconf; can contain helpful extra\n        keyword arguments, and other things.\n        \"\"\"\n        # Go through keyword arguments, and either save their values to our\n        # instance, or raise an error.\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\n    @classproperty\n    def view_is_async(cls):\n        handlers = [\n            getattr(cls, method)\n            for method in cls.http_method_names\n            if (method != \"options\" and hasattr(cls, method))\n        ]\n        if not handlers:\n            return False\n        is_async = asyncio.iscoroutinefunction(handlers[0])\n        if not all(asyncio.iscoroutinefunction(h) == is_async for h in handlers[1:]):\n            raise ImproperlyConfigured(\n                f\"{cls.__qualname__} HTTP handlers must either be all sync or all \"\n                \"async.\"\n            )\n        return is_async\n\n    @classonlymethod\n    def as_view(cls, **initkwargs):\n        \"\"\"Main entry point for a request-response process.\"\"\"\n        for key in initkwargs:\n            if key in cls.http_method_names:\n                raise TypeError(\n                    \"The method name %s is not accepted as a keyword argument \"\n                    \"to %s().\" % (key, cls.__name__)\n                )\n            if not hasattr(cls, key):\n                raise TypeError(\n                    \"%s() received an invalid keyword %r. as_view \"\n                    \"only accepts arguments that are already \"\n                    \"attributes of the class.\" % (cls.__name__, key)\n                )\n\n        def view(request, *args, **kwargs):\n            self = cls(**initkwargs)\n            self.setup(request, *args, **kwargs)\n            if not hasattr(self, \"request\"):\n                raise AttributeError(\n                    \"%s instance has no 'request' attribute. Did you override \"\n                    \"setup() and forget to call super()?\" % cls.__name__\n                )\n            return self.dispatch(request, *args, **kwargs)\n\n        view.view_class = cls\n        view.view_initkwargs = initkwargs\n\n        # __name__ and __qualname__ are intentionally left unchanged as\n        # view_class should be used to robustly determine the name of the view\n        # instead.\n        view.__doc__ = cls.__doc__\n        view.__module__ = cls.__module__\n        view.__annotations__ = cls.dispatch.__annotations__\n        # Copy possible attributes set by decorators, e.g. @csrf_exempt, from\n        # the dispatch method.\n        view.__dict__.update(cls.dispatch.__dict__)\n\n        # Mark the callback if the view class is async.\n        if cls.view_is_async:\n            view._is_coroutine = asyncio.coroutines._is_coroutine\n\n        return view\n\n    def setup(self, request, *args, **kwargs):\n        \"\"\"Initialize attributes shared by all view methods.\"\"\"\n        if hasattr(self, \"get\") and not hasattr(self, \"head\"):\n            self.head = self.get\n        self.request = request\n        self.args = args\n        self.kwargs = kwargs\n\n    def dispatch(self, request, *args, **kwargs):\n        # Try to dispatch to the right method; if a method doesn't exist,\n        # defer to the error handler. Also defer to the error handler if the\n        # request method isn't on the approved list.\n        if request.method.lower() in self.http_method_names:\n            handler = getattr(\n                self, request.method.lower(), self.http_method_not_allowed\n            )\n        else:\n            handler = self.http_method_not_allowed\n        return handler(request, *args, **kwargs)\n\n    def http_method_not_allowed(self, request, *args, **kwargs):\n        logger.warning(\n            \"Method Not Allowed (%s): %s\",\n            request.method,\n            request.path,\n            extra={\"status_code\": 405, \"request\": request},\n        )\n        return HttpResponseNotAllowed(self._allowed_methods())\n\n    def options(self, request, *args, **kwargs):\n        \"\"\"Handle responding to requests for the OPTIONS HTTP verb.\"\"\"\n        response = HttpResponse()\n        response.headers[\"Allow\"] = \", \".join(self._allowed_methods())\n        response.headers[\"Content-Length\"] = \"0\"\n\n        if self.view_is_async:\n\n            async def func():\n                return response\n\n            return func()\n        else:\n            return response\n\n    def _allowed_methods(self):\n        return [m.upper() for m in self.http_method_names if hasattr(self, m)]\n\n\nclass TemplateResponseMixin:\n    \"\"\"A mixin that can be used to render a template.\"\"\"\n\n    template_name = None\n    template_engine = None\n    response_class = TemplateResponse\n    content_type = None\n\n    def render_to_response(self, context, **response_kwargs):\n        \"\"\"\n        Return a response, using the `response_class` for this view, with a\n        template rendered with the given context.\n\n        Pass response_kwargs to the constructor of the response class.\n        \"\"\"\n        response_kwargs.setdefault(\"content_type\", self.content_type)\n        return self.response_class(\n            request=self.request,\n            template=self.get_template_names(),\n            context=context,\n            using=self.template_engine,\n            **response_kwargs,\n        )\n\n    def get_template_names(self):\n        \"\"\"\n        Return a list of template names to be used for the request. Must return\n        a list. May not be called if render_to_response() is overridden.\n        \"\"\"\n        if self.template_name is None:\n            raise ImproperlyConfigured(\n                \"TemplateResponseMixin requires either a definition of \"\n                \"'template_name' or an implementation of 'get_template_names()'\"\n            )\n        else:\n            return [self.template_name]\n\n\nclass TemplateView(TemplateResponseMixin, ContextMixin, View):\n    \"\"\"\n    Render a template. Pass keyword arguments from the URLconf to the context.\n    \"\"\"\n\n    def get(self, request, *args, **kwargs):\n        context = self.get_context_data(**kwargs)\n        return self.render_to_response(context)\n\n\nclass RedirectView(View):\n    \"\"\"Provide a redirect on any GET request.\"\"\"\n\n    permanent = False\n    url = None\n    pattern_name = None\n    query_string = False\n\n    def get_redirect_url(self, *args, **kwargs):\n        \"\"\"\n        Return the URL redirect to. Keyword arguments from the URL pattern\n        match generating the redirect request are provided as kwargs to this\n        method.\n        \"\"\"\n        if self.url:\n            url = self.url % kwargs\n        elif self.pattern_name:\n            url = reverse(self.pattern_name, args=args, kwargs=kwargs)\n        else:\n            return None\n\n        args = self.request.META.get(\"QUERY_STRING\", \"\")\n        if args and self.query_string:\n            url = \"%s?%s\" % (url, args)\n        return url\n\n    def get(self, request, *args, **kwargs):\n        url = self.get_redirect_url(*args, **kwargs)\n        if url:\n            if self.permanent:\n                return HttpResponsePermanentRedirect(url)\n            else:\n                return HttpResponseRedirect(url)\n        else:\n            logger.warning(\n                \"Gone: %s\", request.path, extra={\"status_code\": 410, \"request\": request}\n            )\n            return HttpResponseGone()\n\n    def head(self, request, *args, **kwargs):\n        return self.get(request, *args, **kwargs)\n\n    def post(self, request, *args, **kwargs):\n        return self.get(request, *args, **kwargs)\n\n    def options(self, request, *args, **kwargs):\n        return self.get(request, *args, **kwargs)\n\n    def delete(self, request, *args, **kwargs):\n        return self.get(request, *args, **kwargs)\n\n    def put(self, request, *args, **kwargs):\n        return self.get(request, *args, **kwargs)\n\n    def patch(self, request, *args, **kwargs):\n        return self.get(request, *args, **kwargs)"}
{"model_name_or_path": "agentless", "instance_id": "django__django-16485", "model_patch": "diff --git a/django/template/defaultfilters.py b/django/template/defaultfilters.py\nindex edd25c0..a0eee8c 100644\n--- a/django/template/defaultfilters.py\n+++ b/django/template/defaultfilters.py\n@@ -178,6 +178,9 @@ def floatformat(text, arg=-1):\n             )\n         )\n \n+    if d == Decimal('0.00'):\n+        return \"0.00\"\n+        \n     exp = Decimal(1).scaleb(-abs(p))\n     # Set the precision high enough to avoid an exception (#15789).\n     tupl = d.as_tuple()\n", "raw_model_patch": "diff --git a/django/template/defaultfilters.py b/django/template/defaultfilters.py\nindex edd25c0..a0eee8c 100644\n--- a/django/template/defaultfilters.py\n+++ b/django/template/defaultfilters.py\n@@ -178,6 +178,9 @@ def floatformat(text, arg=-1):\n             )\n         )\n \n+    if d == Decimal('0.00'):\n+        return \"0.00\"\n+        \n     exp = Decimal(1).scaleb(-abs(p))\n     # Set the precision high enough to avoid an exception (#15789).\n     tupl = d.as_tuple()\n", "original_file_content": "\"\"\"Default variable filters.\"\"\"\nimport random as random_module\nimport re\nimport types\nimport warnings\nfrom decimal import ROUND_HALF_UP, Context, Decimal, InvalidOperation\nfrom functools import wraps\nfrom inspect import unwrap\nfrom operator import itemgetter\nfrom pprint import pformat\nfrom urllib.parse import quote\n\nfrom django.utils import formats\nfrom django.utils.dateformat import format, time_format\nfrom django.utils.deprecation import RemovedInDjango51Warning\nfrom django.utils.encoding import iri_to_uri\nfrom django.utils.html import avoid_wrapping, conditional_escape, escape, escapejs\nfrom django.utils.html import json_script as _json_script\nfrom django.utils.html import linebreaks, strip_tags\nfrom django.utils.html import urlize as _urlize\nfrom django.utils.safestring import SafeData, mark_safe\nfrom django.utils.text import Truncator, normalize_newlines, phone2numeric\nfrom django.utils.text import slugify as _slugify\nfrom django.utils.text import wrap\nfrom django.utils.timesince import timesince, timeuntil\nfrom django.utils.translation import gettext, ngettext\n\nfrom .base import VARIABLE_ATTRIBUTE_SEPARATOR\nfrom .library import Library\n\nregister = Library()\n\n\n#######################\n# STRING DECORATOR    #\n#######################\n\n\ndef stringfilter(func):\n    \"\"\"\n    Decorator for filters which should only receive strings. The object\n    passed as the first positional argument will be converted to a string.\n    \"\"\"\n\n    @wraps(func)\n    def _dec(first, *args, **kwargs):\n        first = str(first)\n        result = func(first, *args, **kwargs)\n        if isinstance(first, SafeData) and getattr(unwrap(func), \"is_safe\", False):\n            result = mark_safe(result)\n        return result\n\n    return _dec\n\n\n###################\n# STRINGS         #\n###################\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef addslashes(value):\n    \"\"\"\n    Add slashes before quotes. Useful for escaping strings in CSV, for\n    example. Less useful for escaping JavaScript; use the ``escapejs``\n    filter instead.\n    \"\"\"\n    return value.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef capfirst(value):\n    \"\"\"Capitalize the first character of the value.\"\"\"\n    return value and value[0].upper() + value[1:]\n\n\n@register.filter(\"escapejs\")\n@stringfilter\ndef escapejs_filter(value):\n    \"\"\"Hex encode characters for use in JavaScript strings.\"\"\"\n    return escapejs(value)\n\n\n@register.filter(is_safe=True)\ndef json_script(value, element_id=None):\n    \"\"\"\n    Output value JSON-encoded, wrapped in a <script type=\"application/json\">\n    tag (with an optional id).\n    \"\"\"\n    return _json_script(value, element_id)\n\n\n@register.filter(is_safe=True)\ndef floatformat(text, arg=-1):\n    \"\"\"\n    Display a float to a specified number of decimal places.\n\n    If called without an argument, display the floating point number with one\n    decimal place -- but only if there's a decimal place to be displayed:\n\n    * num1 = 34.23234\n    * num2 = 34.00000\n    * num3 = 34.26000\n    * {{ num1|floatformat }} displays \"34.2\"\n    * {{ num2|floatformat }} displays \"34\"\n    * {{ num3|floatformat }} displays \"34.3\"\n\n    If arg is positive, always display exactly arg number of decimal places:\n\n    * {{ num1|floatformat:3 }} displays \"34.232\"\n    * {{ num2|floatformat:3 }} displays \"34.000\"\n    * {{ num3|floatformat:3 }} displays \"34.260\"\n\n    If arg is negative, display arg number of decimal places -- but only if\n    there are places to be displayed:\n\n    * {{ num1|floatformat:\"-3\" }} displays \"34.232\"\n    * {{ num2|floatformat:\"-3\" }} displays \"34\"\n    * {{ num3|floatformat:\"-3\" }} displays \"34.260\"\n\n    If arg has the 'g' suffix, force the result to be grouped by the\n    THOUSAND_SEPARATOR for the active locale. When the active locale is\n    en (English):\n\n    * {{ 6666.6666|floatformat:\"2g\" }} displays \"6,666.67\"\n    * {{ 10000|floatformat:\"g\" }} displays \"10,000\"\n\n    If arg has the 'u' suffix, force the result to be unlocalized. When the\n    active locale is pl (Polish):\n\n    * {{ 66666.6666|floatformat:\"2\" }} displays \"66666,67\"\n    * {{ 66666.6666|floatformat:\"2u\" }} displays \"66666.67\"\n\n    If the input float is infinity or NaN, display the string representation\n    of that value.\n    \"\"\"\n    force_grouping = False\n    use_l10n = True\n    if isinstance(arg, str):\n        last_char = arg[-1]\n        if arg[-2:] in {\"gu\", \"ug\"}:\n            force_grouping = True\n            use_l10n = False\n            arg = arg[:-2] or -1\n        elif last_char == \"g\":\n            force_grouping = True\n            arg = arg[:-1] or -1\n        elif last_char == \"u\":\n            use_l10n = False\n            arg = arg[:-1] or -1\n    try:\n        input_val = str(text)\n        d = Decimal(input_val)\n    except InvalidOperation:\n        try:\n            d = Decimal(str(float(text)))\n        except (ValueError, InvalidOperation, TypeError):\n            return \"\"\n    try:\n        p = int(arg)\n    except ValueError:\n        return input_val\n\n    try:\n        m = int(d) - d\n    except (ValueError, OverflowError, InvalidOperation):\n        return input_val\n\n    if not m and p < 0:\n        return mark_safe(\n            formats.number_format(\n                \"%d\" % (int(d)),\n                0,\n                use_l10n=use_l10n,\n                force_grouping=force_grouping,\n            )\n        )\n\n    exp = Decimal(1).scaleb(-abs(p))\n    # Set the precision high enough to avoid an exception (#15789).\n    tupl = d.as_tuple()\n    units = len(tupl[1])\n    units += -tupl[2] if m else tupl[2]\n    prec = abs(p) + units + 1\n\n    # Avoid conversion to scientific notation by accessing `sign`, `digits`,\n    # and `exponent` from Decimal.as_tuple() directly.\n    rounded_d = d.quantize(exp, ROUND_HALF_UP, Context(prec=prec))\n    sign, digits, exponent = rounded_d.as_tuple()\n    digits = [str(digit) for digit in reversed(digits)]\n    while len(digits) <= abs(exponent):\n        digits.append(\"0\")\n    digits.insert(-exponent, \".\")\n    if sign and rounded_d:\n        digits.append(\"-\")\n    number = \"\".join(reversed(digits))\n    return mark_safe(\n        formats.number_format(\n            number,\n            abs(p),\n            use_l10n=use_l10n,\n            force_grouping=force_grouping,\n        )\n    )\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef iriencode(value):\n    \"\"\"Escape an IRI value for use in a URL.\"\"\"\n    return iri_to_uri(value)\n\n\n@register.filter(is_safe=True, needs_autoescape=True)\n@stringfilter\ndef linenumbers(value, autoescape=True):\n    \"\"\"Display text with line numbers.\"\"\"\n    lines = value.split(\"\\n\")\n    # Find the maximum width of the line count, for use with zero padding\n    # string format command\n    width = str(len(str(len(lines))))\n    if not autoescape or isinstance(value, SafeData):\n        for i, line in enumerate(lines):\n            lines[i] = (\"%0\" + width + \"d. %s\") % (i + 1, line)\n    else:\n        for i, line in enumerate(lines):\n            lines[i] = (\"%0\" + width + \"d. %s\") % (i + 1, escape(line))\n    return mark_safe(\"\\n\".join(lines))\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef lower(value):\n    \"\"\"Convert a string into all lowercase.\"\"\"\n    return value.lower()\n\n\n@register.filter(is_safe=False)\n@stringfilter\ndef make_list(value):\n    \"\"\"\n    Return the value turned into a list.\n\n    For an integer, it's a list of digits.\n    For a string, it's a list of characters.\n    \"\"\"\n    return list(value)\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef slugify(value):\n    \"\"\"\n    Convert to ASCII. Convert spaces to hyphens. Remove characters that aren't\n    alphanumerics, underscores, or hyphens. Convert to lowercase. Also strip\n    leading and trailing whitespace.\n    \"\"\"\n    return _slugify(value)\n\n\n@register.filter(is_safe=True)\ndef stringformat(value, arg):\n    \"\"\"\n    Format the variable according to the arg, a string formatting specifier.\n\n    This specifier uses Python string formatting syntax, with the exception\n    that the leading \"%\" is dropped.\n\n    See https://docs.python.org/library/stdtypes.html#printf-style-string-formatting\n    for documentation of Python string formatting.\n    \"\"\"\n    if isinstance(value, tuple):\n        value = str(value)\n    try:\n        return (\"%\" + str(arg)) % value\n    except (ValueError, TypeError):\n        return \"\"\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef title(value):\n    \"\"\"Convert a string into titlecase.\"\"\"\n    t = re.sub(\"([a-z])'([A-Z])\", lambda m: m[0].lower(), value.title())\n    return re.sub(r\"\\d([A-Z])\", lambda m: m[0].lower(), t)\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef truncatechars(value, arg):\n    \"\"\"Truncate a string after `arg` number of characters.\"\"\"\n    try:\n        length = int(arg)\n    except ValueError:  # Invalid literal for int().\n        return value  # Fail silently.\n    return Truncator(value).chars(length)\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef truncatechars_html(value, arg):\n    \"\"\"\n    Truncate HTML after `arg` number of chars.\n    Preserve newlines in the HTML.\n    \"\"\"\n    try:\n        length = int(arg)\n    except ValueError:  # invalid literal for int()\n        return value  # Fail silently.\n    return Truncator(value).chars(length, html=True)\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef truncatewords(value, arg):\n    \"\"\"\n    Truncate a string after `arg` number of words.\n    Remove newlines within the string.\n    \"\"\"\n    try:\n        length = int(arg)\n    except ValueError:  # Invalid literal for int().\n        return value  # Fail silently.\n    return Truncator(value).words(length, truncate=\" \u2026\")\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef truncatewords_html(value, arg):\n    \"\"\"\n    Truncate HTML after `arg` number of words.\n    Preserve newlines in the HTML.\n    \"\"\"\n    try:\n        length = int(arg)\n    except ValueError:  # invalid literal for int()\n        return value  # Fail silently.\n    return Truncator(value).words(length, html=True, truncate=\" \u2026\")\n\n\n@register.filter(is_safe=False)\n@stringfilter\ndef upper(value):\n    \"\"\"Convert a string into all uppercase.\"\"\"\n    return value.upper()\n\n\n@register.filter(is_safe=False)\n@stringfilter\ndef urlencode(value, safe=None):\n    \"\"\"\n    Escape a value for use in a URL.\n\n    The ``safe`` parameter determines the characters which should not be\n    escaped by Python's quote() function. If not provided, use the default safe\n    characters (but an empty string can be provided when *all* characters\n    should be escaped).\n    \"\"\"\n    kwargs = {}\n    if safe is not None:\n        kwargs[\"safe\"] = safe\n    return quote(value, **kwargs)\n\n\n@register.filter(is_safe=True, needs_autoescape=True)\n@stringfilter\ndef urlize(value, autoescape=True):\n    \"\"\"Convert URLs in plain text into clickable links.\"\"\"\n    return mark_safe(_urlize(value, nofollow=True, autoescape=autoescape))\n\n\n@register.filter(is_safe=True, needs_autoescape=True)\n@stringfilter\ndef urlizetrunc(value, limit, autoescape=True):\n    \"\"\"\n    Convert URLs into clickable links, truncating URLs to the given character\n    limit, and adding 'rel=nofollow' attribute to discourage spamming.\n\n    Argument: Length to truncate URLs to.\n    \"\"\"\n    return mark_safe(\n        _urlize(value, trim_url_limit=int(limit), nofollow=True, autoescape=autoescape)\n    )\n\n\n@register.filter(is_safe=False)\n@stringfilter\ndef wordcount(value):\n    \"\"\"Return the number of words.\"\"\"\n    return len(value.split())\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef wordwrap(value, arg):\n    \"\"\"Wrap words at `arg` line length.\"\"\"\n    return wrap(value, int(arg))\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef ljust(value, arg):\n    \"\"\"Left-align the value in a field of a given width.\"\"\"\n    return value.ljust(int(arg))\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef rjust(value, arg):\n    \"\"\"Right-align the value in a field of a given width.\"\"\"\n    return value.rjust(int(arg))\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef center(value, arg):\n    \"\"\"Center the value in a field of a given width.\"\"\"\n    return value.center(int(arg))\n\n\n@register.filter\n@stringfilter\ndef cut(value, arg):\n    \"\"\"Remove all values of arg from the given string.\"\"\"\n    safe = isinstance(value, SafeData)\n    value = value.replace(arg, \"\")\n    if safe and arg != \";\":\n        return mark_safe(value)\n    return value\n\n\n###################\n# HTML STRINGS    #\n###################\n\n\n@register.filter(\"escape\", is_safe=True)\n@stringfilter\ndef escape_filter(value):\n    \"\"\"Mark the value as a string that should be auto-escaped.\"\"\"\n    return conditional_escape(value)\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef force_escape(value):\n    \"\"\"\n    Escape a string's HTML. Return a new string containing the escaped\n    characters (as opposed to \"escape\", which marks the content for later\n    possible escaping).\n    \"\"\"\n    return escape(value)\n\n\n@register.filter(\"linebreaks\", is_safe=True, needs_autoescape=True)\n@stringfilter\ndef linebreaks_filter(value, autoescape=True):\n    \"\"\"\n    Replace line breaks in plain text with appropriate HTML; a single\n    newline becomes an HTML line break (``<br>``) and a new line\n    followed by a blank line becomes a paragraph break (``</p>``).\n    \"\"\"\n    autoescape = autoescape and not isinstance(value, SafeData)\n    return mark_safe(linebreaks(value, autoescape))\n\n\n@register.filter(is_safe=True, needs_autoescape=True)\n@stringfilter\ndef linebreaksbr(value, autoescape=True):\n    \"\"\"\n    Convert all newlines in a piece of plain text to HTML line breaks\n    (``<br>``).\n    \"\"\"\n    autoescape = autoescape and not isinstance(value, SafeData)\n    value = normalize_newlines(value)\n    if autoescape:\n        value = escape(value)\n    return mark_safe(value.replace(\"\\n\", \"<br>\"))\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef safe(value):\n    \"\"\"Mark the value as a string that should not be auto-escaped.\"\"\"\n    return mark_safe(value)\n\n\n@register.filter(is_safe=True)\ndef safeseq(value):\n    \"\"\"\n    A \"safe\" filter for sequences. Mark each element in the sequence,\n    individually, as safe, after converting them to strings. Return a list\n    with the results.\n    \"\"\"\n    return [mark_safe(obj) for obj in value]\n\n\n@register.filter(is_safe=True)\n@stringfilter\ndef striptags(value):\n    \"\"\"Strip all [X]HTML tags.\"\"\"\n    return strip_tags(value)\n\n\n###################\n# LISTS           #\n###################\n\n\ndef _property_resolver(arg):\n    \"\"\"\n    When arg is convertible to float, behave like operator.itemgetter(arg)\n    Otherwise, chain __getitem__() and getattr().\n\n    >>> _property_resolver(1)('abc')\n    'b'\n    >>> _property_resolver('1')('abc')\n    Traceback (most recent call last):\n    ...\n    TypeError: string indices must be integers\n    >>> class Foo:\n    ...     a = 42\n    ...     b = 3.14\n    ...     c = 'Hey!'\n    >>> _property_resolver('b')(Foo())\n    3.14\n    \"\"\"\n    try:\n        float(arg)\n    except ValueError:\n        if VARIABLE_ATTRIBUTE_SEPARATOR + \"_\" in arg or arg[0] == \"_\":\n            raise AttributeError(\"Access to private variables is forbidden.\")\n        parts = arg.split(VARIABLE_ATTRIBUTE_SEPARATOR)\n\n        def resolve(value):\n            for part in parts:\n                try:\n                    value = value[part]\n                except (AttributeError, IndexError, KeyError, TypeError, ValueError):\n                    value = getattr(value, part)\n            return value\n\n        return resolve\n    else:\n        return itemgetter(arg)\n\n\n@register.filter(is_safe=False)\ndef dictsort(value, arg):\n    \"\"\"\n    Given a list of dicts, return that list sorted by the property given in\n    the argument.\n    \"\"\"\n    try:\n        return sorted(value, key=_property_resolver(arg))\n    except (AttributeError, TypeError):\n        return \"\"\n\n\n@register.filter(is_safe=False)\ndef dictsortreversed(value, arg):\n    \"\"\"\n    Given a list of dicts, return that list sorted in reverse order by the\n    property given in the argument.\n    \"\"\"\n    try:\n        return sorted(value, key=_property_resolver(arg), reverse=True)\n    except (AttributeError, TypeError):\n        return \"\"\n\n\n@register.filter(is_safe=False)\ndef first(value):\n    \"\"\"Return the first item in a list.\"\"\"\n    try:\n        return value[0]\n    except IndexError:\n        return \"\"\n\n\n@register.filter(is_safe=True, needs_autoescape=True)\ndef join(value, arg, autoescape=True):\n    \"\"\"Join a list with a string, like Python's ``str.join(list)``.\"\"\"\n    try:\n        if autoescape:\n            value = [conditional_escape(v) for v in value]\n        data = conditional_escape(arg).join(value)\n    except TypeError:  # Fail silently if arg isn't iterable.\n        return value\n    return mark_safe(data)\n\n\n@register.filter(is_safe=True)\ndef last(value):\n    \"\"\"Return the last item in a list.\"\"\"\n    try:\n        return value[-1]\n    except IndexError:\n        return \"\"\n\n\n@register.filter(is_safe=False)\ndef length(value):\n    \"\"\"Return the length of the value - useful for lists.\"\"\"\n    try:\n        return len(value)\n    except (ValueError, TypeError):\n        return 0\n\n\n@register.filter(is_safe=False)\ndef length_is(value, arg):\n    \"\"\"Return a boolean of whether the value's length is the argument.\"\"\"\n    warnings.warn(\n        \"The length_is template filter is deprecated in favor of the length template \"\n        \"filter and the == operator within an {% if %} tag.\",\n        RemovedInDjango51Warning,\n    )\n    try:\n        return len(value) == int(arg)\n    except (ValueError, TypeError):\n        return \"\"\n\n\n@register.filter(is_safe=True)\ndef random(value):\n    \"\"\"Return a random item from the list.\"\"\"\n    return random_module.choice(value)\n\n\n@register.filter(\"slice\", is_safe=True)\ndef slice_filter(value, arg):\n    \"\"\"\n    Return a slice of the list using the same syntax as Python's list slicing.\n    \"\"\"\n    try:\n        bits = []\n        for x in str(arg).split(\":\"):\n            if not x:\n                bits.append(None)\n            else:\n                bits.append(int(x))\n        return value[slice(*bits)]\n\n    except (ValueError, TypeError):\n        return value  # Fail silently.\n\n\n@register.filter(is_safe=True, needs_autoescape=True)\ndef unordered_list(value, autoescape=True):\n    \"\"\"\n    Recursively take a self-nested list and return an HTML unordered list --\n    WITHOUT opening and closing <ul> tags.\n\n    Assume the list is in the proper format. For example, if ``var`` contains:\n    ``['States', ['Kansas', ['Lawrence', 'Topeka'], 'Illinois']]``, then\n    ``{{ var|unordered_list }}`` returns::\n\n        <li>States\n        <ul>\n                <li>Kansas\n                <ul>\n                        <li>Lawrence</li>\n                        <li>Topeka</li>\n                </ul>\n                </li>\n                <li>Illinois</li>\n        </ul>\n        </li>\n    \"\"\"\n    if autoescape:\n        escaper = conditional_escape\n    else:\n\n        def escaper(x):\n            return x\n\n    def walk_items(item_list):\n        item_iterator = iter(item_list)\n        try:\n            item = next(item_iterator)\n            while True:\n                try:\n                    next_item = next(item_iterator)\n                except StopIteration:\n                    yield item, None\n                    break\n                if isinstance(next_item, (list, tuple, types.GeneratorType)):\n                    try:\n                        iter(next_item)\n                    except TypeError:\n                        pass\n                    else:\n                        yield item, next_item\n                        item = next(item_iterator)\n                        continue\n                yield item, None\n                item = next_item\n        except StopIteration:\n            pass\n\n    def list_formatter(item_list, tabs=1):\n        indent = \"\\t\" * tabs\n        output = []\n        for item, children in walk_items(item_list):\n            sublist = \"\"\n            if children:\n                sublist = \"\\n%s<ul>\\n%s\\n%s</ul>\\n%s\" % (\n                    indent,\n                    list_formatter(children, tabs + 1),\n                    indent,\n                    indent,\n                )\n            output.append(\"%s<li>%s%s</li>\" % (indent, escaper(item), sublist))\n        return \"\\n\".join(output)\n\n    return mark_safe(list_formatter(value))\n\n\n###################\n# INTEGERS        #\n###################\n\n\n@register.filter(is_safe=False)\ndef add(value, arg):\n    \"\"\"Add the arg to the value.\"\"\"\n    try:\n        return int(value) + int(arg)\n    except (ValueError, TypeError):\n        try:\n            return value + arg\n        except Exception:\n            return \"\"\n\n\n@register.filter(is_safe=False)\ndef get_digit(value, arg):\n    \"\"\"\n    Given a whole number, return the requested digit of it, where 1 is the\n    right-most digit, 2 is the second-right-most digit, etc. Return the\n    original value for invalid input (if input or argument is not an integer,\n    or if argument is less than 1). Otherwise, output is always an integer.\n    \"\"\"\n    try:\n        arg = int(arg)\n        value = int(value)\n    except ValueError:\n        return value  # Fail silently for an invalid argument\n    if arg < 1:\n        return value\n    try:\n        return int(str(value)[-arg])\n    except IndexError:\n        return 0\n\n\n###################\n# DATES           #\n###################\n\n\n@register.filter(expects_localtime=True, is_safe=False)\ndef date(value, arg=None):\n    \"\"\"Format a date according to the given format.\"\"\"\n    if value in (None, \"\"):\n        return \"\"\n    try:\n        return formats.date_format(value, arg)\n    except AttributeError:\n        try:\n            return format(value, arg)\n        except AttributeError:\n            return \"\"\n\n\n@register.filter(expects_localtime=True, is_safe=False)\ndef time(value, arg=None):\n    \"\"\"Format a time according to the given format.\"\"\"\n    if value in (None, \"\"):\n        return \"\"\n    try:\n        return formats.time_format(value, arg)\n    except (AttributeError, TypeError):\n        try:\n            return time_format(value, arg)\n        except (AttributeError, TypeError):\n            return \"\"\n\n\n@register.filter(\"timesince\", is_safe=False)\ndef timesince_filter(value, arg=None):\n    \"\"\"Format a date as the time since that date (i.e. \"4 days, 6 hours\").\"\"\"\n    if not value:\n        return \"\"\n    try:\n        if arg:\n            return timesince(value, arg)\n        return timesince(value)\n    except (ValueError, TypeError):\n        return \"\"\n\n\n@register.filter(\"timeuntil\", is_safe=False)\ndef timeuntil_filter(value, arg=None):\n    \"\"\"Format a date as the time until that date (i.e. \"4 days, 6 hours\").\"\"\"\n    if not value:\n        return \"\"\n    try:\n        return timeuntil(value, arg)\n    except (ValueError, TypeError):\n        return \"\"\n\n\n###################\n# LOGIC           #\n###################\n\n\n@register.filter(is_safe=False)\ndef default(value, arg):\n    \"\"\"If value is unavailable, use given default.\"\"\"\n    return value or arg\n\n\n@register.filter(is_safe=False)\ndef default_if_none(value, arg):\n    \"\"\"If value is None, use given default.\"\"\"\n    if value is None:\n        return arg\n    return value\n\n\n@register.filter(is_safe=False)\ndef divisibleby(value, arg):\n    \"\"\"Return True if the value is divisible by the argument.\"\"\"\n    return int(value) % int(arg) == 0\n\n\n@register.filter(is_safe=False)\ndef yesno(value, arg=None):\n    \"\"\"\n    Given a string mapping values for true, false, and (optionally) None,\n    return one of those strings according to the value:\n\n    ==========  ======================  ==================================\n    Value       Argument                Outputs\n    ==========  ======================  ==================================\n    ``True``    ``\"yeah,no,maybe\"``     ``yeah``\n    ``False``   ``\"yeah,no,maybe\"``     ``no``\n    ``None``    ``\"yeah,no,maybe\"``     ``maybe``\n    ``None``    ``\"yeah,no\"``           ``\"no\"`` (converts None to False\n                                        if no mapping for None is given.\n    ==========  ======================  ==================================\n    \"\"\"\n    if arg is None:\n        # Translators: Please do not add spaces around commas.\n        arg = gettext(\"yes,no,maybe\")\n    bits = arg.split(\",\")\n    if len(bits) < 2:\n        return value  # Invalid arg.\n    try:\n        yes, no, maybe = bits\n    except ValueError:\n        # Unpack list of wrong size (no \"maybe\" value provided).\n        yes, no, maybe = bits[0], bits[1], bits[1]\n    if value is None:\n        return maybe\n    if value:\n        return yes\n    return no\n\n\n###################\n# MISC            #\n###################\n\n\n@register.filter(is_safe=True)\ndef filesizeformat(bytes_):\n    \"\"\"\n    Format the value like a 'human-readable' file size (i.e. 13 KB, 4.1 MB,\n    102 bytes, etc.).\n    \"\"\"\n    try:\n        bytes_ = int(bytes_)\n    except (TypeError, ValueError, UnicodeDecodeError):\n        value = ngettext(\"%(size)d byte\", \"%(size)d bytes\", 0) % {\"size\": 0}\n        return avoid_wrapping(value)\n\n    def filesize_number_format(value):\n        return formats.number_format(round(value, 1), 1)\n\n    KB = 1 << 10\n    MB = 1 << 20\n    GB = 1 << 30\n    TB = 1 << 40\n    PB = 1 << 50\n\n    negative = bytes_ < 0\n    if negative:\n        bytes_ = -bytes_  # Allow formatting of negative numbers.\n\n    if bytes_ < KB:\n        value = ngettext(\"%(size)d byte\", \"%(size)d bytes\", bytes_) % {\"size\": bytes_}\n    elif bytes_ < MB:\n        value = gettext(\"%s KB\") % filesize_number_format(bytes_ / KB)\n    elif bytes_ < GB:\n        value = gettext(\"%s MB\") % filesize_number_format(bytes_ / MB)\n    elif bytes_ < TB:\n        value = gettext(\"%s GB\") % filesize_number_format(bytes_ / GB)\n    elif bytes_ < PB:\n        value = gettext(\"%s TB\") % filesize_number_format(bytes_ / TB)\n    else:\n        value = gettext(\"%s PB\") % filesize_number_format(bytes_ / PB)\n\n    if negative:\n        value = \"-%s\" % value\n    return avoid_wrapping(value)\n\n\n@register.filter(is_safe=False)\ndef pluralize(value, arg=\"s\"):\n    \"\"\"\n    Return a plural suffix if the value is not 1, '1', or an object of\n    length 1. By default, use 's' as the suffix:\n\n    * If value is 0, vote{{ value|pluralize }} display \"votes\".\n    * If value is 1, vote{{ value|pluralize }} display \"vote\".\n    * If value is 2, vote{{ value|pluralize }} display \"votes\".\n\n    If an argument is provided, use that string instead:\n\n    * If value is 0, class{{ value|pluralize:\"es\" }} display \"classes\".\n    * If value is 1, class{{ value|pluralize:\"es\" }} display \"class\".\n    * If value is 2, class{{ value|pluralize:\"es\" }} display \"classes\".\n\n    If the provided argument contains a comma, use the text before the comma\n    for the singular case and the text after the comma for the plural case:\n\n    * If value is 0, cand{{ value|pluralize:\"y,ies\" }} display \"candies\".\n    * If value is 1, cand{{ value|pluralize:\"y,ies\" }} display \"candy\".\n    * If value is 2, cand{{ value|pluralize:\"y,ies\" }} display \"candies\".\n    \"\"\"\n    if \",\" not in arg:\n        arg = \",\" + arg\n    bits = arg.split(\",\")\n    if len(bits) > 2:\n        return \"\"\n    singular_suffix, plural_suffix = bits[:2]\n\n    try:\n        return singular_suffix if float(value) == 1 else plural_suffix\n    except ValueError:  # Invalid string that's not a number.\n        pass\n    except TypeError:  # Value isn't a string or a number; maybe it's a list?\n        try:\n            return singular_suffix if len(value) == 1 else plural_suffix\n        except TypeError:  # len() of unsized object.\n            pass\n    return \"\"\n\n\n@register.filter(\"phone2numeric\", is_safe=True)\ndef phone2numeric_filter(value):\n    \"\"\"Take a phone number and converts it in to its numerical equivalent.\"\"\"\n    return phone2numeric(value)\n\n\n@register.filter(is_safe=True)\ndef pprint(value):\n    \"\"\"A wrapper around pprint.pprint -- for debugging, really.\"\"\"\n    try:\n        return pformat(value)\n    except Exception as e:\n        return \"Error in formatting: %s: %s\" % (e.__class__.__name__, e)"}
{"model_name_or_path": "agentless", "instance_id": "django__django-16642", "model_patch": "diff --git a/django/http/response.py b/django/http/response.py\nindex b918697..983b221 100644\n--- a/django/http/response.py\n+++ b/django/http/response.py\n@@ -613,6 +613,10 @@ class FileResponse(StreamingHttpResponse):\n                     \"gzip\": \"application/gzip\",\n                     \"xz\": \"application/x-xz\",\n                 }.get(encoding, content_type)\n+                content_type = {\n+                    \"br\": \"application/brotli\",\n+                    \"Z\": \"application/x-compress\",\n+                }.get(filename.split('.')[-1], content_type)\n                 self.headers[\"Content-Type\"] = (\n                     content_type or \"application/octet-stream\"\n                 )\n", "raw_model_patch": "diff --git a/django/http/response.py b/django/http/response.py\nindex b918697..983b221 100644\n--- a/django/http/response.py\n+++ b/django/http/response.py\n@@ -613,6 +613,10 @@ class FileResponse(StreamingHttpResponse):\n                     \"gzip\": \"application/gzip\",\n                     \"xz\": \"application/x-xz\",\n                 }.get(encoding, content_type)\n+                content_type = {\n+                    \"br\": \"application/brotli\",\n+                    \"Z\": \"application/x-compress\",\n+                }.get(filename.split('.')[-1], content_type)\n                 self.headers[\"Content-Type\"] = (\n                     content_type or \"application/octet-stream\"\n                 )\n", "original_file_content": "import datetime\nimport io\nimport json\nimport mimetypes\nimport os\nimport re\nimport sys\nimport time\nimport warnings\nfrom email.header import Header\nfrom http.client import responses\nfrom urllib.parse import urlparse\n\nfrom asgiref.sync import async_to_sync, sync_to_async\n\nfrom django.conf import settings\nfrom django.core import signals, signing\nfrom django.core.exceptions import DisallowedRedirect\nfrom django.core.serializers.json import DjangoJSONEncoder\nfrom django.http.cookie import SimpleCookie\nfrom django.utils import timezone\nfrom django.utils.datastructures import CaseInsensitiveMapping\nfrom django.utils.encoding import iri_to_uri\nfrom django.utils.http import content_disposition_header, http_date\nfrom django.utils.regex_helper import _lazy_re_compile\n\n_charset_from_content_type_re = _lazy_re_compile(\n    r\";\\s*charset=(?P<charset>[^\\s;]+)\", re.I\n)\n\n\nclass ResponseHeaders(CaseInsensitiveMapping):\n    def __init__(self, data):\n        \"\"\"\n        Populate the initial data using __setitem__ to ensure values are\n        correctly encoded.\n        \"\"\"\n        self._store = {}\n        if data:\n            for header, value in self._unpack_items(data):\n                self[header] = value\n\n    def _convert_to_charset(self, value, charset, mime_encode=False):\n        \"\"\"\n        Convert headers key/value to ascii/latin-1 native strings.\n        `charset` must be 'ascii' or 'latin-1'. If `mime_encode` is True and\n        `value` can't be represented in the given charset, apply MIME-encoding.\n        \"\"\"\n        try:\n            if isinstance(value, str):\n                # Ensure string is valid in given charset\n                value.encode(charset)\n            elif isinstance(value, bytes):\n                # Convert bytestring using given charset\n                value = value.decode(charset)\n            else:\n                value = str(value)\n                # Ensure string is valid in given charset.\n                value.encode(charset)\n            if \"\\n\" in value or \"\\r\" in value:\n                raise BadHeaderError(\n                    f\"Header values can't contain newlines (got {value!r})\"\n                )\n        except UnicodeError as e:\n            # Encoding to a string of the specified charset failed, but we\n            # don't know what type that value was, or if it contains newlines,\n            # which we may need to check for before sending it to be\n            # encoded for multiple character sets.\n            if (isinstance(value, bytes) and (b\"\\n\" in value or b\"\\r\" in value)) or (\n                isinstance(value, str) and (\"\\n\" in value or \"\\r\" in value)\n            ):\n                raise BadHeaderError(\n                    f\"Header values can't contain newlines (got {value!r})\"\n                ) from e\n            if mime_encode:\n                value = Header(value, \"utf-8\", maxlinelen=sys.maxsize).encode()\n            else:\n                e.reason += \", HTTP response headers must be in %s format\" % charset\n                raise\n        return value\n\n    def __delitem__(self, key):\n        self.pop(key)\n\n    def __setitem__(self, key, value):\n        key = self._convert_to_charset(key, \"ascii\")\n        value = self._convert_to_charset(value, \"latin-1\", mime_encode=True)\n        self._store[key.lower()] = (key, value)\n\n    def pop(self, key, default=None):\n        return self._store.pop(key.lower(), default)\n\n    def setdefault(self, key, value):\n        if key not in self:\n            self[key] = value\n\n\nclass BadHeaderError(ValueError):\n    pass\n\n\nclass HttpResponseBase:\n    \"\"\"\n    An HTTP response base class with dictionary-accessed headers.\n\n    This class doesn't handle content. It should not be used directly.\n    Use the HttpResponse and StreamingHttpResponse subclasses instead.\n    \"\"\"\n\n    status_code = 200\n\n    def __init__(\n        self, content_type=None, status=None, reason=None, charset=None, headers=None\n    ):\n        self.headers = ResponseHeaders(headers)\n        self._charset = charset\n        if \"Content-Type\" not in self.headers:\n            if content_type is None:\n                content_type = f\"text/html; charset={self.charset}\"\n            self.headers[\"Content-Type\"] = content_type\n        elif content_type:\n            raise ValueError(\n                \"'headers' must not contain 'Content-Type' when the \"\n                \"'content_type' parameter is provided.\"\n            )\n        self._resource_closers = []\n        # This parameter is set by the handler. It's necessary to preserve the\n        # historical behavior of request_finished.\n        self._handler_class = None\n        self.cookies = SimpleCookie()\n        self.closed = False\n        if status is not None:\n            try:\n                self.status_code = int(status)\n            except (ValueError, TypeError):\n                raise TypeError(\"HTTP status code must be an integer.\")\n\n            if not 100 <= self.status_code <= 599:\n                raise ValueError(\"HTTP status code must be an integer from 100 to 599.\")\n        self._reason_phrase = reason\n\n    @property\n    def reason_phrase(self):\n        if self._reason_phrase is not None:\n            return self._reason_phrase\n        # Leave self._reason_phrase unset in order to use the default\n        # reason phrase for status code.\n        return responses.get(self.status_code, \"Unknown Status Code\")\n\n    @reason_phrase.setter\n    def reason_phrase(self, value):\n        self._reason_phrase = value\n\n    @property\n    def charset(self):\n        if self._charset is not None:\n            return self._charset\n        # The Content-Type header may not yet be set, because the charset is\n        # being inserted *into* it.\n        if content_type := self.headers.get(\"Content-Type\"):\n            if matched := _charset_from_content_type_re.search(content_type):\n                # Extract the charset and strip its double quotes.\n                # Note that having parsed it from the Content-Type, we don't\n                # store it back into the _charset for later intentionally, to\n                # allow for the Content-Type to be switched again later.\n                return matched[\"charset\"].replace('\"', \"\")\n        return settings.DEFAULT_CHARSET\n\n    @charset.setter\n    def charset(self, value):\n        self._charset = value\n\n    def serialize_headers(self):\n        \"\"\"HTTP headers as a bytestring.\"\"\"\n        return b\"\\r\\n\".join(\n            [\n                key.encode(\"ascii\") + b\": \" + value.encode(\"latin-1\")\n                for key, value in self.headers.items()\n            ]\n        )\n\n    __bytes__ = serialize_headers\n\n    @property\n    def _content_type_for_repr(self):\n        return (\n            ', \"%s\"' % self.headers[\"Content-Type\"]\n            if \"Content-Type\" in self.headers\n            else \"\"\n        )\n\n    def __setitem__(self, header, value):\n        self.headers[header] = value\n\n    def __delitem__(self, header):\n        del self.headers[header]\n\n    def __getitem__(self, header):\n        return self.headers[header]\n\n    def has_header(self, header):\n        \"\"\"Case-insensitive check for a header.\"\"\"\n        return header in self.headers\n\n    __contains__ = has_header\n\n    def items(self):\n        return self.headers.items()\n\n    def get(self, header, alternate=None):\n        return self.headers.get(header, alternate)\n\n    def set_cookie(\n        self,\n        key,\n        value=\"\",\n        max_age=None,\n        expires=None,\n        path=\"/\",\n        domain=None,\n        secure=False,\n        httponly=False,\n        samesite=None,\n    ):\n        \"\"\"\n        Set a cookie.\n\n        ``expires`` can be:\n        - a string in the correct format,\n        - a naive ``datetime.datetime`` object in UTC,\n        - an aware ``datetime.datetime`` object in any time zone.\n        If it is a ``datetime.datetime`` object then calculate ``max_age``.\n\n        ``max_age`` can be:\n        - int/float specifying seconds,\n        - ``datetime.timedelta`` object.\n        \"\"\"\n        self.cookies[key] = value\n        if expires is not None:\n            if isinstance(expires, datetime.datetime):\n                if timezone.is_naive(expires):\n                    expires = timezone.make_aware(expires, datetime.timezone.utc)\n                delta = expires - datetime.datetime.now(tz=datetime.timezone.utc)\n                # Add one second so the date matches exactly (a fraction of\n                # time gets lost between converting to a timedelta and\n                # then the date string).\n                delta += datetime.timedelta(seconds=1)\n                # Just set max_age - the max_age logic will set expires.\n                expires = None\n                if max_age is not None:\n                    raise ValueError(\"'expires' and 'max_age' can't be used together.\")\n                max_age = max(0, delta.days * 86400 + delta.seconds)\n            else:\n                self.cookies[key][\"expires\"] = expires\n        else:\n            self.cookies[key][\"expires\"] = \"\"\n        if max_age is not None:\n            if isinstance(max_age, datetime.timedelta):\n                max_age = max_age.total_seconds()\n            self.cookies[key][\"max-age\"] = int(max_age)\n            # IE requires expires, so set it if hasn't been already.\n            if not expires:\n                self.cookies[key][\"expires\"] = http_date(time.time() + max_age)\n        if path is not None:\n            self.cookies[key][\"path\"] = path\n        if domain is not None:\n            self.cookies[key][\"domain\"] = domain\n        if secure:\n            self.cookies[key][\"secure\"] = True\n        if httponly:\n            self.cookies[key][\"httponly\"] = True\n        if samesite:\n            if samesite.lower() not in (\"lax\", \"none\", \"strict\"):\n                raise ValueError('samesite must be \"lax\", \"none\", or \"strict\".')\n            self.cookies[key][\"samesite\"] = samesite\n\n    def setdefault(self, key, value):\n        \"\"\"Set a header unless it has already been set.\"\"\"\n        self.headers.setdefault(key, value)\n\n    def set_signed_cookie(self, key, value, salt=\"\", **kwargs):\n        value = signing.get_cookie_signer(salt=key + salt).sign(value)\n        return self.set_cookie(key, value, **kwargs)\n\n    def delete_cookie(self, key, path=\"/\", domain=None, samesite=None):\n        # Browsers can ignore the Set-Cookie header if the cookie doesn't use\n        # the secure flag and:\n        # - the cookie name starts with \"__Host-\" or \"__Secure-\", or\n        # - the samesite is \"none\".\n        secure = key.startswith((\"__Secure-\", \"__Host-\")) or (\n            samesite and samesite.lower() == \"none\"\n        )\n        self.set_cookie(\n            key,\n            max_age=0,\n            path=path,\n            domain=domain,\n            secure=secure,\n            expires=\"Thu, 01 Jan 1970 00:00:00 GMT\",\n            samesite=samesite,\n        )\n\n    # Common methods used by subclasses\n\n    def make_bytes(self, value):\n        \"\"\"Turn a value into a bytestring encoded in the output charset.\"\"\"\n        # Per PEP 3333, this response body must be bytes. To avoid returning\n        # an instance of a subclass, this function returns `bytes(value)`.\n        # This doesn't make a copy when `value` already contains bytes.\n\n        # Handle string types -- we can't rely on force_bytes here because:\n        # - Python attempts str conversion first\n        # - when self._charset != 'utf-8' it re-encodes the content\n        if isinstance(value, (bytes, memoryview)):\n            return bytes(value)\n        if isinstance(value, str):\n            return bytes(value.encode(self.charset))\n        # Handle non-string types.\n        return str(value).encode(self.charset)\n\n    # These methods partially implement the file-like object interface.\n    # See https://docs.python.org/library/io.html#io.IOBase\n\n    # The WSGI server must call this method upon completion of the request.\n    # See http://blog.dscpl.com.au/2012/10/obligations-for-calling-close-on.html\n    def close(self):\n        for closer in self._resource_closers:\n            try:\n                closer()\n            except Exception:\n                pass\n        # Free resources that were still referenced.\n        self._resource_closers.clear()\n        self.closed = True\n        signals.request_finished.send(sender=self._handler_class)\n\n    def write(self, content):\n        raise OSError(\"This %s instance is not writable\" % self.__class__.__name__)\n\n    def flush(self):\n        pass\n\n    def tell(self):\n        raise OSError(\n            \"This %s instance cannot tell its position\" % self.__class__.__name__\n        )\n\n    # These methods partially implement a stream-like object interface.\n    # See https://docs.python.org/library/io.html#io.IOBase\n\n    def readable(self):\n        return False\n\n    def seekable(self):\n        return False\n\n    def writable(self):\n        return False\n\n    def writelines(self, lines):\n        raise OSError(\"This %s instance is not writable\" % self.__class__.__name__)\n\n\nclass HttpResponse(HttpResponseBase):\n    \"\"\"\n    An HTTP response class with a string as content.\n\n    This content can be read, appended to, or replaced.\n    \"\"\"\n\n    streaming = False\n    non_picklable_attrs = frozenset(\n        [\n            \"resolver_match\",\n            # Non-picklable attributes added by test clients.\n            \"client\",\n            \"context\",\n            \"json\",\n            \"templates\",\n        ]\n    )\n\n    def __init__(self, content=b\"\", *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Content is a bytestring. See the `content` property methods.\n        self.content = content\n\n    def __getstate__(self):\n        obj_dict = self.__dict__.copy()\n        for attr in self.non_picklable_attrs:\n            if attr in obj_dict:\n                del obj_dict[attr]\n        return obj_dict\n\n    def __repr__(self):\n        return \"<%(cls)s status_code=%(status_code)d%(content_type)s>\" % {\n            \"cls\": self.__class__.__name__,\n            \"status_code\": self.status_code,\n            \"content_type\": self._content_type_for_repr,\n        }\n\n    def serialize(self):\n        \"\"\"Full HTTP message, including headers, as a bytestring.\"\"\"\n        return self.serialize_headers() + b\"\\r\\n\\r\\n\" + self.content\n\n    __bytes__ = serialize\n\n    @property\n    def content(self):\n        return b\"\".join(self._container)\n\n    @content.setter\n    def content(self, value):\n        # Consume iterators upon assignment to allow repeated iteration.\n        if hasattr(value, \"__iter__\") and not isinstance(\n            value, (bytes, memoryview, str)\n        ):\n            content = b\"\".join(self.make_bytes(chunk) for chunk in value)\n            if hasattr(value, \"close\"):\n                try:\n                    value.close()\n                except Exception:\n                    pass\n        else:\n            content = self.make_bytes(value)\n        # Create a list of properly encoded bytestrings to support write().\n        self._container = [content]\n\n    def __iter__(self):\n        return iter(self._container)\n\n    def write(self, content):\n        self._container.append(self.make_bytes(content))\n\n    def tell(self):\n        return len(self.content)\n\n    def getvalue(self):\n        return self.content\n\n    def writable(self):\n        return True\n\n    def writelines(self, lines):\n        for line in lines:\n            self.write(line)\n\n\nclass StreamingHttpResponse(HttpResponseBase):\n    \"\"\"\n    A streaming HTTP response class with an iterator as content.\n\n    This should only be iterated once, when the response is streamed to the\n    client. However, it can be appended to or replaced with a new iterator\n    that wraps the original content (or yields entirely new content).\n    \"\"\"\n\n    streaming = True\n\n    def __init__(self, streaming_content=(), *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # `streaming_content` should be an iterable of bytestrings.\n        # See the `streaming_content` property methods.\n        self.streaming_content = streaming_content\n\n    def __repr__(self):\n        return \"<%(cls)s status_code=%(status_code)d%(content_type)s>\" % {\n            \"cls\": self.__class__.__qualname__,\n            \"status_code\": self.status_code,\n            \"content_type\": self._content_type_for_repr,\n        }\n\n    @property\n    def content(self):\n        raise AttributeError(\n            \"This %s instance has no `content` attribute. Use \"\n            \"`streaming_content` instead.\" % self.__class__.__name__\n        )\n\n    @property\n    def streaming_content(self):\n        if self.is_async:\n            # pull to lexical scope to capture fixed reference in case\n            # streaming_content is set again later.\n            _iterator = self._iterator\n\n            async def awrapper():\n                async for part in _iterator:\n                    yield self.make_bytes(part)\n\n            return awrapper()\n        else:\n            return map(self.make_bytes, self._iterator)\n\n    @streaming_content.setter\n    def streaming_content(self, value):\n        self._set_streaming_content(value)\n\n    def _set_streaming_content(self, value):\n        # Ensure we can never iterate on \"value\" more than once.\n        try:\n            self._iterator = iter(value)\n            self.is_async = False\n        except TypeError:\n            self._iterator = aiter(value)\n            self.is_async = True\n        if hasattr(value, \"close\"):\n            self._resource_closers.append(value.close)\n\n    def __iter__(self):\n        try:\n            return iter(self.streaming_content)\n        except TypeError:\n            warnings.warn(\n                \"StreamingHttpResponse must consume asynchronous iterators in order to \"\n                \"serve them synchronously. Use a synchronous iterator instead.\",\n                Warning,\n            )\n\n            # async iterator. Consume in async_to_sync and map back.\n            async def to_list(_iterator):\n                as_list = []\n                async for chunk in _iterator:\n                    as_list.append(chunk)\n                return as_list\n\n            return map(self.make_bytes, iter(async_to_sync(to_list)(self._iterator)))\n\n    async def __aiter__(self):\n        try:\n            async for part in self.streaming_content:\n                yield part\n        except TypeError:\n            warnings.warn(\n                \"StreamingHttpResponse must consume synchronous iterators in order to \"\n                \"serve them asynchronously. Use an asynchronous iterator instead.\",\n                Warning,\n            )\n            # sync iterator. Consume via sync_to_async and yield via async\n            # generator.\n            for part in await sync_to_async(list)(self.streaming_content):\n                yield part\n\n    def getvalue(self):\n        return b\"\".join(self.streaming_content)\n\n\nclass FileResponse(StreamingHttpResponse):\n    \"\"\"\n    A streaming HTTP response class optimized for files.\n    \"\"\"\n\n    block_size = 4096\n\n    def __init__(self, *args, as_attachment=False, filename=\"\", **kwargs):\n        self.as_attachment = as_attachment\n        self.filename = filename\n        self._no_explicit_content_type = (\n            \"content_type\" not in kwargs or kwargs[\"content_type\"] is None\n        )\n        super().__init__(*args, **kwargs)\n\n    def _set_streaming_content(self, value):\n        if not hasattr(value, \"read\"):\n            self.file_to_stream = None\n            return super()._set_streaming_content(value)\n\n        self.file_to_stream = filelike = value\n        if hasattr(filelike, \"close\"):\n            self._resource_closers.append(filelike.close)\n        value = iter(lambda: filelike.read(self.block_size), b\"\")\n        self.set_headers(filelike)\n        super()._set_streaming_content(value)\n\n    def set_headers(self, filelike):\n        \"\"\"\n        Set some common response headers (Content-Length, Content-Type, and\n        Content-Disposition) based on the `filelike` response content.\n        \"\"\"\n        filename = getattr(filelike, \"name\", \"\")\n        filename = filename if isinstance(filename, str) else \"\"\n        seekable = hasattr(filelike, \"seek\") and (\n            not hasattr(filelike, \"seekable\") or filelike.seekable()\n        )\n        if hasattr(filelike, \"tell\"):\n            if seekable:\n                initial_position = filelike.tell()\n                filelike.seek(0, io.SEEK_END)\n                self.headers[\"Content-Length\"] = filelike.tell() - initial_position\n                filelike.seek(initial_position)\n            elif hasattr(filelike, \"getbuffer\"):\n                self.headers[\"Content-Length\"] = (\n                    filelike.getbuffer().nbytes - filelike.tell()\n                )\n            elif os.path.exists(filename):\n                self.headers[\"Content-Length\"] = (\n                    os.path.getsize(filename) - filelike.tell()\n                )\n        elif seekable:\n            self.headers[\"Content-Length\"] = sum(\n                iter(lambda: len(filelike.read(self.block_size)), 0)\n            )\n            filelike.seek(-int(self.headers[\"Content-Length\"]), io.SEEK_END)\n\n        filename = os.path.basename(self.filename or filename)\n        if self._no_explicit_content_type:\n            if filename:\n                content_type, encoding = mimetypes.guess_type(filename)\n                # Encoding isn't set to prevent browsers from automatically\n                # uncompressing files.\n                content_type = {\n                    \"bzip2\": \"application/x-bzip\",\n                    \"gzip\": \"application/gzip\",\n                    \"xz\": \"application/x-xz\",\n                }.get(encoding, content_type)\n                self.headers[\"Content-Type\"] = (\n                    content_type or \"application/octet-stream\"\n                )\n            else:\n                self.headers[\"Content-Type\"] = \"application/octet-stream\"\n\n        if content_disposition := content_disposition_header(\n            self.as_attachment, filename\n        ):\n            self.headers[\"Content-Disposition\"] = content_disposition\n\n\nclass HttpResponseRedirectBase(HttpResponse):\n    allowed_schemes = [\"http\", \"https\", \"ftp\"]\n\n    def __init__(self, redirect_to, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self[\"Location\"] = iri_to_uri(redirect_to)\n        parsed = urlparse(str(redirect_to))\n        if parsed.scheme and parsed.scheme not in self.allowed_schemes:\n            raise DisallowedRedirect(\n                \"Unsafe redirect to URL with protocol '%s'\" % parsed.scheme\n            )\n\n    url = property(lambda self: self[\"Location\"])\n\n    def __repr__(self):\n        return (\n            '<%(cls)s status_code=%(status_code)d%(content_type)s, url=\"%(url)s\">'\n            % {\n                \"cls\": self.__class__.__name__,\n                \"status_code\": self.status_code,\n                \"content_type\": self._content_type_for_repr,\n                \"url\": self.url,\n            }\n        )\n\n\nclass HttpResponseRedirect(HttpResponseRedirectBase):\n    status_code = 302\n\n\nclass HttpResponsePermanentRedirect(HttpResponseRedirectBase):\n    status_code = 301\n\n\nclass HttpResponseNotModified(HttpResponse):\n    status_code = 304\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        del self[\"content-type\"]\n\n    @HttpResponse.content.setter\n    def content(self, value):\n        if value:\n            raise AttributeError(\n                \"You cannot set content to a 304 (Not Modified) response\"\n            )\n        self._container = []\n\n\nclass HttpResponseBadRequest(HttpResponse):\n    status_code = 400\n\n\nclass HttpResponseNotFound(HttpResponse):\n    status_code = 404\n\n\nclass HttpResponseForbidden(HttpResponse):\n    status_code = 403\n\n\nclass HttpResponseNotAllowed(HttpResponse):\n    status_code = 405\n\n    def __init__(self, permitted_methods, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self[\"Allow\"] = \", \".join(permitted_methods)\n\n    def __repr__(self):\n        return \"<%(cls)s [%(methods)s] status_code=%(status_code)d%(content_type)s>\" % {\n            \"cls\": self.__class__.__name__,\n            \"status_code\": self.status_code,\n            \"content_type\": self._content_type_for_repr,\n            \"methods\": self[\"Allow\"],\n        }\n\n\nclass HttpResponseGone(HttpResponse):\n    status_code = 410\n\n\nclass HttpResponseServerError(HttpResponse):\n    status_code = 500\n\n\nclass Http404(Exception):\n    pass\n\n\nclass JsonResponse(HttpResponse):\n    \"\"\"\n    An HTTP response class that consumes data to be serialized to JSON.\n\n    :param data: Data to be dumped into json. By default only ``dict`` objects\n      are allowed to be passed due to a security flaw before ECMAScript 5. See\n      the ``safe`` parameter for more information.\n    :param encoder: Should be a json encoder class. Defaults to\n      ``django.core.serializers.json.DjangoJSONEncoder``.\n    :param safe: Controls if only ``dict`` objects may be serialized. Defaults\n      to ``True``.\n    :param json_dumps_params: A dictionary of kwargs passed to json.dumps().\n    \"\"\"\n\n    def __init__(\n        self,\n        data,\n        encoder=DjangoJSONEncoder,\n        safe=True,\n        json_dumps_params=None,\n        **kwargs,\n    ):\n        if safe and not isinstance(data, dict):\n            raise TypeError(\n                \"In order to allow non-dict objects to be serialized set the \"\n                \"safe parameter to False.\"\n            )\n        if json_dumps_params is None:\n            json_dumps_params = {}\n        kwargs.setdefault(\"content_type\", \"application/json\")\n        data = json.dumps(data, cls=encoder, **json_dumps_params)\n        super().__init__(content=data, **kwargs)"}
{"model_name_or_path": "agentless", "instance_id": "django__django-16819", "model_patch": "diff --git a/django/db/migrations/optimizer.py b/django/db/migrations/optimizer.py\nindex a9ec258..a7833c6 100644\n--- a/django/db/migrations/optimizer.py\n+++ b/django/db/migrations/optimizer.py\n@@ -42,6 +42,15 @@ class MigrationOptimizer:\n         new_operations = []\n         for i, operation in enumerate(operations):\n             right = True  # Should we reduce on the right or on the left.\n+            # Optimize AddIndex and RemoveIndex operations\n+            if isinstance(operation, AddIndex) and isinstance(other, RemoveIndex):\n+                if operation.index == other.index:\n+                    # If we have an AddIndex followed by its corresponding RemoveIndex, we can optimize it out.\n+                    continue\n+            elif isinstance(operation, RemoveIndex) and isinstance(other, AddIndex):\n+                if operation.index == other.index:\n+                    # If we have a RemoveIndex followed by its corresponding AddIndex, we can also optimize it out.\n+                    continue\n             # Compare it to each operation after it\n             for j, other in enumerate(operations[i + 1 :]):\n                 result = operation.reduce(other, app_label)\n@@ -66,4 +75,4 @@ class MigrationOptimizer:\n                     right = False\n             else:\n                 new_operations.append(operation)\n-        return new_operations\n+        return new_operations\n", "raw_model_patch": "diff --git a/django/db/migrations/optimizer.py b/django/db/migrations/optimizer.py\nindex a9ec258..a7833c6 100644\n--- a/django/db/migrations/optimizer.py\n+++ b/django/db/migrations/optimizer.py\n@@ -42,6 +42,15 @@ class MigrationOptimizer:\n         new_operations = []\n         for i, operation in enumerate(operations):\n             right = True  # Should we reduce on the right or on the left.\n+            # Optimize AddIndex and RemoveIndex operations\n+            if isinstance(operation, AddIndex) and isinstance(other, RemoveIndex):\n+                if operation.index == other.index:\n+                    # If we have an AddIndex followed by its corresponding RemoveIndex, we can optimize it out.\n+                    continue\n+            elif isinstance(operation, RemoveIndex) and isinstance(other, AddIndex):\n+                if operation.index == other.index:\n+                    # If we have a RemoveIndex followed by its corresponding AddIndex, we can also optimize it out.\n+                    continue\n             # Compare it to each operation after it\n             for j, other in enumerate(operations[i + 1 :]):\n                 result = operation.reduce(other, app_label)\n@@ -66,4 +75,4 @@ class MigrationOptimizer:\n                     right = False\n             else:\n                 new_operations.append(operation)\n-        return new_operations\n+        return new_operations\n", "original_file_content": "class MigrationOptimizer:\n    \"\"\"\n    Power the optimization process, where you provide a list of Operations\n    and you are returned a list of equal or shorter length - operations\n    are merged into one if possible.\n\n    For example, a CreateModel and an AddField can be optimized into a\n    new CreateModel, and CreateModel and DeleteModel can be optimized into\n    nothing.\n    \"\"\"\n\n    def optimize(self, operations, app_label):\n        \"\"\"\n        Main optimization entry point. Pass in a list of Operation instances,\n        get out a new list of Operation instances.\n\n        Unfortunately, due to the scope of the optimization (two combinable\n        operations might be separated by several hundred others), this can't be\n        done as a peephole optimization with checks/output implemented on\n        the Operations themselves; instead, the optimizer looks at each\n        individual operation and scans forwards in the list to see if there\n        are any matches, stopping at boundaries - operations which can't\n        be optimized over (RunSQL, operations on the same field/model, etc.)\n\n        The inner loop is run until the starting list is the same as the result\n        list, and then the result is returned. This means that operation\n        optimization must be stable and always return an equal or shorter list.\n        \"\"\"\n        # Internal tracking variable for test assertions about # of loops\n        if app_label is None:\n            raise TypeError(\"app_label must be a str.\")\n        self._iterations = 0\n        while True:\n            result = self.optimize_inner(operations, app_label)\n            self._iterations += 1\n            if result == operations:\n                return result\n            operations = result\n\n    def optimize_inner(self, operations, app_label):\n        \"\"\"Inner optimization loop.\"\"\"\n        new_operations = []\n        for i, operation in enumerate(operations):\n            right = True  # Should we reduce on the right or on the left.\n            # Compare it to each operation after it\n            for j, other in enumerate(operations[i + 1 :]):\n                result = operation.reduce(other, app_label)\n                if isinstance(result, list):\n                    in_between = operations[i + 1 : i + j + 1]\n                    if right:\n                        new_operations.extend(in_between)\n                        new_operations.extend(result)\n                    elif all(op.reduce(other, app_label) is True for op in in_between):\n                        # Perform a left reduction if all of the in-between\n                        # operations can optimize through other.\n                        new_operations.extend(result)\n                        new_operations.extend(in_between)\n                    else:\n                        # Otherwise keep trying.\n                        new_operations.append(operation)\n                        break\n                    new_operations.extend(operations[i + j + 2 :])\n                    return new_operations\n                elif not result:\n                    # Can't perform a right reduction.\n                    right = False\n            else:\n                new_operations.append(operation)\n        return new_operations"}
{"model_name_or_path": "agentless", "instance_id": "matplotlib__matplotlib-23412", "model_patch": "diff --git a/lib/matplotlib/patches.py b/lib/matplotlib/patches.py\nindex 1eefb1e..27f3d84 100644\n--- a/lib/matplotlib/patches.py\n+++ b/lib/matplotlib/patches.py\n@@ -549,7 +549,12 @@ class Patch(artist.Artist):\n         if self._edgecolor[3] == 0 or self._linestyle == 'None':\n             lw = 0\n         gc.set_linewidth(lw)\n-        gc.set_dashes(*self._dash_pattern)\n+        if len(self._dash_pattern) == 2:\n+            offset = self._dash_pattern[0]\n+            dash_pattern = self._dash_pattern[1]\n+            gc.set_dashes(offset, *dash_pattern)\n+        else:\n+            gc.set_dashes(*self._dash_pattern)\n         gc.set_capstyle(self._capstyle)\n         gc.set_joinstyle(self._joinstyle)\n \n", "raw_model_patch": "diff --git a/lib/matplotlib/patches.py b/lib/matplotlib/patches.py\nindex 1eefb1e..27f3d84 100644\n--- a/lib/matplotlib/patches.py\n+++ b/lib/matplotlib/patches.py\n@@ -549,7 +549,12 @@ class Patch(artist.Artist):\n         if self._edgecolor[3] == 0 or self._linestyle == 'None':\n             lw = 0\n         gc.set_linewidth(lw)\n-        gc.set_dashes(*self._dash_pattern)\n+        if len(self._dash_pattern) == 2:\n+            offset = self._dash_pattern[0]\n+            dash_pattern = self._dash_pattern[1]\n+            gc.set_dashes(offset, *dash_pattern)\n+        else:\n+            gc.set_dashes(*self._dash_pattern)\n         gc.set_capstyle(self._capstyle)\n         gc.set_joinstyle(self._joinstyle)\n \n", "original_file_content": "r\"\"\"\nPatches are `.Artist`\\s with a face color and an edge color.\n\"\"\"\n\nimport contextlib\nimport functools\nimport inspect\nimport math\nfrom numbers import Number\nimport textwrap\nfrom collections import namedtuple\n\nimport numpy as np\n\nimport matplotlib as mpl\nfrom . import (_api, artist, cbook, colors, _docstring, hatch as mhatch,\n               lines as mlines, transforms)\nfrom .bezier import (\n    NonIntersectingPathException, get_cos_sin, get_intersection,\n    get_parallels, inside_circle, make_wedged_bezier2,\n    split_bezier_intersecting_with_closedpath, split_path_inout)\nfrom .path import Path\nfrom ._enums import JoinStyle, CapStyle\n\n\n@_docstring.interpd\n@_api.define_aliases({\n    \"antialiased\": [\"aa\"],\n    \"edgecolor\": [\"ec\"],\n    \"facecolor\": [\"fc\"],\n    \"linestyle\": [\"ls\"],\n    \"linewidth\": [\"lw\"],\n})\nclass Patch(artist.Artist):\n    \"\"\"\n    A patch is a 2D artist with a face color and an edge color.\n\n    If any of *edgecolor*, *facecolor*, *linewidth*, or *antialiased*\n    are *None*, they default to their rc params setting.\n    \"\"\"\n    zorder = 1\n\n    # Whether to draw an edge by default.  Set on a\n    # subclass-by-subclass basis.\n    _edge_default = False\n\n    @_api.make_keyword_only(\"3.6\", name=\"edgecolor\")\n    def __init__(self,\n                 edgecolor=None,\n                 facecolor=None,\n                 color=None,\n                 linewidth=None,\n                 linestyle=None,\n                 antialiased=None,\n                 hatch=None,\n                 fill=True,\n                 capstyle=None,\n                 joinstyle=None,\n                 **kwargs):\n        \"\"\"\n        The following kwarg properties are supported\n\n        %(Patch:kwdoc)s\n        \"\"\"\n        super().__init__()\n\n        if linestyle is None:\n            linestyle = \"solid\"\n        if capstyle is None:\n            capstyle = CapStyle.butt\n        if joinstyle is None:\n            joinstyle = JoinStyle.miter\n\n        self._hatch_color = colors.to_rgba(mpl.rcParams['hatch.color'])\n        self._fill = True  # needed for set_facecolor call\n        if color is not None:\n            if edgecolor is not None or facecolor is not None:\n                _api.warn_external(\n                    \"Setting the 'color' property will override \"\n                    \"the edgecolor or facecolor properties.\")\n            self.set_color(color)\n        else:\n            self.set_edgecolor(edgecolor)\n            self.set_facecolor(facecolor)\n\n        self._linewidth = 0\n        self._unscaled_dash_pattern = (0, None)  # offset, dash\n        self._dash_pattern = (0, None)  # offset, dash (scaled by linewidth)\n\n        self.set_fill(fill)\n        self.set_linestyle(linestyle)\n        self.set_linewidth(linewidth)\n        self.set_antialiased(antialiased)\n        self.set_hatch(hatch)\n        self.set_capstyle(capstyle)\n        self.set_joinstyle(joinstyle)\n\n        if len(kwargs):\n            self._internal_update(kwargs)\n\n    def get_verts(self):\n        \"\"\"\n        Return a copy of the vertices used in this patch.\n\n        If the patch contains Bezier curves, the curves will be interpolated by\n        line segments.  To access the curves as curves, use `get_path`.\n        \"\"\"\n        trans = self.get_transform()\n        path = self.get_path()\n        polygons = path.to_polygons(trans)\n        if len(polygons):\n            return polygons[0]\n        return []\n\n    def _process_radius(self, radius):\n        if radius is not None:\n            return radius\n        if isinstance(self._picker, Number):\n            _radius = self._picker\n        else:\n            if self.get_edgecolor()[3] == 0:\n                _radius = 0\n            else:\n                _radius = self.get_linewidth()\n        return _radius\n\n    def contains(self, mouseevent, radius=None):\n        \"\"\"\n        Test whether the mouse event occurred in the patch.\n\n        Returns\n        -------\n        (bool, empty dict)\n        \"\"\"\n        inside, info = self._default_contains(mouseevent)\n        if inside is not None:\n            return inside, info\n        radius = self._process_radius(radius)\n        codes = self.get_path().codes\n        if codes is not None:\n            vertices = self.get_path().vertices\n            # if the current path is concatenated by multiple sub paths.\n            # get the indexes of the starting code(MOVETO) of all sub paths\n            idxs, = np.where(codes == Path.MOVETO)\n            # Don't split before the first MOVETO.\n            idxs = idxs[1:]\n            subpaths = map(\n                Path, np.split(vertices, idxs), np.split(codes, idxs))\n        else:\n            subpaths = [self.get_path()]\n        inside = any(\n            subpath.contains_point(\n                (mouseevent.x, mouseevent.y), self.get_transform(), radius)\n            for subpath in subpaths)\n        return inside, {}\n\n    def contains_point(self, point, radius=None):\n        \"\"\"\n        Return whether the given point is inside the patch.\n\n        Parameters\n        ----------\n        point : (float, float)\n            The point (x, y) to check, in target coordinates of\n            ``self.get_transform()``. These are display coordinates for patches\n            that are added to a figure or axes.\n        radius : float, optional\n            Add an additional margin on the patch in target coordinates of\n            ``self.get_transform()``. See `.Path.contains_point` for further\n            details.\n\n        Returns\n        -------\n        bool\n\n        Notes\n        -----\n        The proper use of this method depends on the transform of the patch.\n        Isolated patches do not have a transform. In this case, the patch\n        creation coordinates and the point coordinates match. The following\n        example checks that the center of a circle is within the circle\n\n        >>> center = 0, 0\n        >>> c = Circle(center, radius=1)\n        >>> c.contains_point(center)\n        True\n\n        The convention of checking against the transformed patch stems from\n        the fact that this method is predominantly used to check if display\n        coordinates (e.g. from mouse events) are within the patch. If you want\n        to do the above check with data coordinates, you have to properly\n        transform them first:\n\n        >>> center = 0, 0\n        >>> c = Circle(center, radius=1)\n        >>> plt.gca().add_patch(c)\n        >>> transformed_center = c.get_transform().transform(center)\n        >>> c.contains_point(transformed_center)\n        True\n\n        \"\"\"\n        radius = self._process_radius(radius)\n        return self.get_path().contains_point(point,\n                                              self.get_transform(),\n                                              radius)\n\n    def contains_points(self, points, radius=None):\n        \"\"\"\n        Return whether the given points are inside the patch.\n\n        Parameters\n        ----------\n        points : (N, 2) array\n            The points to check, in target coordinates of\n            ``self.get_transform()``. These are display coordinates for patches\n            that are added to a figure or axes. Columns contain x and y values.\n        radius : float, optional\n            Add an additional margin on the patch in target coordinates of\n            ``self.get_transform()``. See `.Path.contains_point` for further\n            details.\n\n        Returns\n        -------\n        length-N bool array\n\n        Notes\n        -----\n        The proper use of this method depends on the transform of the patch.\n        See the notes on `.Patch.contains_point`.\n        \"\"\"\n        radius = self._process_radius(radius)\n        return self.get_path().contains_points(points,\n                                               self.get_transform(),\n                                               radius)\n\n    def update_from(self, other):\n        # docstring inherited.\n        super().update_from(other)\n        # For some properties we don't need or don't want to go through the\n        # getters/setters, so we just copy them directly.\n        self._edgecolor = other._edgecolor\n        self._facecolor = other._facecolor\n        self._original_edgecolor = other._original_edgecolor\n        self._original_facecolor = other._original_facecolor\n        self._fill = other._fill\n        self._hatch = other._hatch\n        self._hatch_color = other._hatch_color\n        self._unscaled_dash_pattern = other._unscaled_dash_pattern\n        self.set_linewidth(other._linewidth)  # also sets scaled dashes\n        self.set_transform(other.get_data_transform())\n        # If the transform of other needs further initialization, then it will\n        # be the case for this artist too.\n        self._transformSet = other.is_transform_set()\n\n    def get_extents(self):\n        \"\"\"\n        Return the `Patch`'s axis-aligned extents as a `~.transforms.Bbox`.\n        \"\"\"\n        return self.get_path().get_extents(self.get_transform())\n\n    def get_transform(self):\n        \"\"\"Return the `~.transforms.Transform` applied to the `Patch`.\"\"\"\n        return self.get_patch_transform() + artist.Artist.get_transform(self)\n\n    def get_data_transform(self):\n        \"\"\"\n        Return the `~.transforms.Transform` mapping data coordinates to\n        physical coordinates.\n        \"\"\"\n        return artist.Artist.get_transform(self)\n\n    def get_patch_transform(self):\n        \"\"\"\n        Return the `~.transforms.Transform` instance mapping patch coordinates\n        to data coordinates.\n\n        For example, one may define a patch of a circle which represents a\n        radius of 5 by providing coordinates for a unit circle, and a\n        transform which scales the coordinates (the patch coordinate) by 5.\n        \"\"\"\n        return transforms.IdentityTransform()\n\n    def get_antialiased(self):\n        \"\"\"Return whether antialiasing is used for drawing.\"\"\"\n        return self._antialiased\n\n    def get_edgecolor(self):\n        \"\"\"Return the edge color.\"\"\"\n        return self._edgecolor\n\n    def get_facecolor(self):\n        \"\"\"Return the face color.\"\"\"\n        return self._facecolor\n\n    def get_linewidth(self):\n        \"\"\"Return the line width in points.\"\"\"\n        return self._linewidth\n\n    def get_linestyle(self):\n        \"\"\"Return the linestyle.\"\"\"\n        return self._linestyle\n\n    def set_antialiased(self, aa):\n        \"\"\"\n        Set whether to use antialiased rendering.\n\n        Parameters\n        ----------\n        aa : bool or None\n        \"\"\"\n        if aa is None:\n            aa = mpl.rcParams['patch.antialiased']\n        self._antialiased = aa\n        self.stale = True\n\n    def _set_edgecolor(self, color):\n        set_hatch_color = True\n        if color is None:\n            if (mpl.rcParams['patch.force_edgecolor'] or\n                    not self._fill or self._edge_default):\n                color = mpl.rcParams['patch.edgecolor']\n            else:\n                color = 'none'\n                set_hatch_color = False\n\n        self._edgecolor = colors.to_rgba(color, self._alpha)\n        if set_hatch_color:\n            self._hatch_color = self._edgecolor\n        self.stale = True\n\n    def set_edgecolor(self, color):\n        \"\"\"\n        Set the patch edge color.\n\n        Parameters\n        ----------\n        color : color or None\n        \"\"\"\n        self._original_edgecolor = color\n        self._set_edgecolor(color)\n\n    def _set_facecolor(self, color):\n        if color is None:\n            color = mpl.rcParams['patch.facecolor']\n        alpha = self._alpha if self._fill else 0\n        self._facecolor = colors.to_rgba(color, alpha)\n        self.stale = True\n\n    def set_facecolor(self, color):\n        \"\"\"\n        Set the patch face color.\n\n        Parameters\n        ----------\n        color : color or None\n        \"\"\"\n        self._original_facecolor = color\n        self._set_facecolor(color)\n\n    def set_color(self, c):\n        \"\"\"\n        Set both the edgecolor and the facecolor.\n\n        Parameters\n        ----------\n        c : color\n\n        See Also\n        --------\n        Patch.set_facecolor, Patch.set_edgecolor\n            For setting the edge or face color individually.\n        \"\"\"\n        self.set_facecolor(c)\n        self.set_edgecolor(c)\n\n    def set_alpha(self, alpha):\n        # docstring inherited\n        super().set_alpha(alpha)\n        self._set_facecolor(self._original_facecolor)\n        self._set_edgecolor(self._original_edgecolor)\n        # stale is already True\n\n    def set_linewidth(self, w):\n        \"\"\"\n        Set the patch linewidth in points.\n\n        Parameters\n        ----------\n        w : float or None\n        \"\"\"\n        if w is None:\n            w = mpl.rcParams['patch.linewidth']\n        self._linewidth = float(w)\n        self._dash_pattern = mlines._scale_dashes(\n            *self._unscaled_dash_pattern, w)\n        self.stale = True\n\n    def set_linestyle(self, ls):\n        \"\"\"\n        Set the patch linestyle.\n\n        ==========================================  =================\n        linestyle                                   description\n        ==========================================  =================\n        ``'-'`` or ``'solid'``                      solid line\n        ``'--'`` or  ``'dashed'``                   dashed line\n        ``'-.'`` or  ``'dashdot'``                  dash-dotted line\n        ``':'`` or ``'dotted'``                     dotted line\n        ``'none'``, ``'None'``, ``' '``, or ``''``  draw nothing\n        ==========================================  =================\n\n        Alternatively a dash tuple of the following form can be provided::\n\n            (offset, onoffseq)\n\n        where ``onoffseq`` is an even length tuple of on and off ink in points.\n\n        Parameters\n        ----------\n        ls : {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n            The line style.\n        \"\"\"\n        if ls is None:\n            ls = \"solid\"\n        if ls in [' ', '', 'none']:\n            ls = 'None'\n        self._linestyle = ls\n        self._unscaled_dash_pattern = mlines._get_dash_pattern(ls)\n        self._dash_pattern = mlines._scale_dashes(\n            *self._unscaled_dash_pattern, self._linewidth)\n        self.stale = True\n\n    def set_fill(self, b):\n        \"\"\"\n        Set whether to fill the patch.\n\n        Parameters\n        ----------\n        b : bool\n        \"\"\"\n        self._fill = bool(b)\n        self._set_facecolor(self._original_facecolor)\n        self._set_edgecolor(self._original_edgecolor)\n        self.stale = True\n\n    def get_fill(self):\n        \"\"\"Return whether the patch is filled.\"\"\"\n        return self._fill\n\n    # Make fill a property so as to preserve the long-standing\n    # but somewhat inconsistent behavior in which fill was an\n    # attribute.\n    fill = property(get_fill, set_fill)\n\n    @_docstring.interpd\n    def set_capstyle(self, s):\n        \"\"\"\n        Set the `.CapStyle`.\n\n        The default capstyle is 'round' for `.FancyArrowPatch` and 'butt' for\n        all other patches.\n\n        Parameters\n        ----------\n        s : `.CapStyle` or %(CapStyle)s\n        \"\"\"\n        cs = CapStyle(s)\n        self._capstyle = cs\n        self.stale = True\n\n    def get_capstyle(self):\n        \"\"\"Return the capstyle.\"\"\"\n        return self._capstyle.name\n\n    @_docstring.interpd\n    def set_joinstyle(self, s):\n        \"\"\"\n        Set the `.JoinStyle`.\n\n        The default joinstyle is 'round' for `.FancyArrowPatch` and 'miter' for\n        all other patches.\n\n        Parameters\n        ----------\n        s : `.JoinStyle` or %(JoinStyle)s\n        \"\"\"\n        js = JoinStyle(s)\n        self._joinstyle = js\n        self.stale = True\n\n    def get_joinstyle(self):\n        \"\"\"Return the joinstyle.\"\"\"\n        return self._joinstyle.name\n\n    def set_hatch(self, hatch):\n        r\"\"\"\n        Set the hatching pattern.\n\n        *hatch* can be one of::\n\n          /   - diagonal hatching\n          \\   - back diagonal\n          |   - vertical\n          -   - horizontal\n          +   - crossed\n          x   - crossed diagonal\n          o   - small circle\n          O   - large circle\n          .   - dots\n          *   - stars\n\n        Letters can be combined, in which case all the specified\n        hatchings are done.  If same letter repeats, it increases the\n        density of hatching of that pattern.\n\n        Hatching is supported in the PostScript, PDF, SVG and Agg\n        backends only.\n\n        Parameters\n        ----------\n        hatch : {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n        \"\"\"\n        # Use validate_hatch(list) after deprecation.\n        mhatch._validate_hatch_pattern(hatch)\n        self._hatch = hatch\n        self.stale = True\n\n    def get_hatch(self):\n        \"\"\"Return the hatching pattern.\"\"\"\n        return self._hatch\n\n    @contextlib.contextmanager\n    def _bind_draw_path_function(self, renderer):\n        \"\"\"\n        ``draw()`` helper factored out for sharing with `FancyArrowPatch`.\n\n        Yields a callable ``dp`` such that calling ``dp(*args, **kwargs)`` is\n        equivalent to calling ``renderer1.draw_path(gc, *args, **kwargs)``\n        where ``renderer1`` and ``gc`` have been suitably set from ``renderer``\n        and the artist's properties.\n        \"\"\"\n\n        renderer.open_group('patch', self.get_gid())\n        gc = renderer.new_gc()\n\n        gc.set_foreground(self._edgecolor, isRGBA=True)\n\n        lw = self._linewidth\n        if self._edgecolor[3] == 0 or self._linestyle == 'None':\n            lw = 0\n        gc.set_linewidth(lw)\n        gc.set_dashes(*self._dash_pattern)\n        gc.set_capstyle(self._capstyle)\n        gc.set_joinstyle(self._joinstyle)\n\n        gc.set_antialiased(self._antialiased)\n        self._set_gc_clip(gc)\n        gc.set_url(self._url)\n        gc.set_snap(self.get_snap())\n\n        gc.set_alpha(self._alpha)\n\n        if self._hatch:\n            gc.set_hatch(self._hatch)\n            gc.set_hatch_color(self._hatch_color)\n\n        if self.get_sketch_params() is not None:\n            gc.set_sketch_params(*self.get_sketch_params())\n\n        if self.get_path_effects():\n            from matplotlib.patheffects import PathEffectRenderer\n            renderer = PathEffectRenderer(self.get_path_effects(), renderer)\n\n        # In `with _bind_draw_path_function(renderer) as draw_path: ...`\n        # (in the implementations of `draw()` below), calls to `draw_path(...)`\n        # will occur as if they took place here with `gc` inserted as\n        # additional first argument.\n        yield functools.partial(renderer.draw_path, gc)\n\n        gc.restore()\n        renderer.close_group('patch')\n        self.stale = False\n\n    @artist.allow_rasterization\n    def draw(self, renderer):\n        # docstring inherited\n        if not self.get_visible():\n            return\n        # Patch has traditionally ignored the dashoffset.\n        with cbook._setattr_cm(\n                 self, _dash_pattern=(0, self._dash_pattern[1])), \\\n             self._bind_draw_path_function(renderer) as draw_path:\n            path = self.get_path()\n            transform = self.get_transform()\n            tpath = transform.transform_path_non_affine(path)\n            affine = transform.get_affine()\n            draw_path(tpath, affine,\n                      # Work around a bug in the PDF and SVG renderers, which\n                      # do not draw the hatches if the facecolor is fully\n                      # transparent, but do if it is None.\n                      self._facecolor if self._facecolor[3] else None)\n\n    def get_path(self):\n        \"\"\"Return the path of this patch.\"\"\"\n        raise NotImplementedError('Derived must override')\n\n    def get_window_extent(self, renderer=None):\n        return self.get_path().get_extents(self.get_transform())\n\n    def _convert_xy_units(self, xy):\n        \"\"\"Convert x and y units for a tuple (x, y).\"\"\"\n        x = self.convert_xunits(xy[0])\n        y = self.convert_yunits(xy[1])\n        return x, y\n\n\nclass Shadow(Patch):\n    def __str__(self):\n        return \"Shadow(%s)\" % (str(self.patch))\n\n    @_docstring.dedent_interpd\n    def __init__(self, patch, ox, oy, **kwargs):\n        \"\"\"\n        Create a shadow of the given *patch*.\n\n        By default, the shadow will have the same face color as the *patch*,\n        but darkened.\n\n        Parameters\n        ----------\n        patch : `.Patch`\n            The patch to create the shadow for.\n        ox, oy : float\n            The shift of the shadow in data coordinates, scaled by a factor\n            of dpi/72.\n        **kwargs\n            Properties of the shadow patch. Supported keys are:\n\n            %(Patch:kwdoc)s\n        \"\"\"\n        super().__init__()\n        self.patch = patch\n        self._ox, self._oy = ox, oy\n        self._shadow_transform = transforms.Affine2D()\n\n        self.update_from(self.patch)\n        color = .3 * np.asarray(colors.to_rgb(self.patch.get_facecolor()))\n        self.update({'facecolor': color, 'edgecolor': color, 'alpha': 0.5,\n                     # Place shadow patch directly behind the inherited patch.\n                     'zorder': np.nextafter(self.patch.zorder, -np.inf),\n                     **kwargs})\n\n    def _update_transform(self, renderer):\n        ox = renderer.points_to_pixels(self._ox)\n        oy = renderer.points_to_pixels(self._oy)\n        self._shadow_transform.clear().translate(ox, oy)\n\n    def get_path(self):\n        return self.patch.get_path()\n\n    def get_patch_transform(self):\n        return self.patch.get_patch_transform() + self._shadow_transform\n\n    def draw(self, renderer):\n        self._update_transform(renderer)\n        super().draw(renderer)\n\n\nclass Rectangle(Patch):\n    \"\"\"\n    A rectangle defined via an anchor point *xy* and its *width* and *height*.\n\n    The rectangle extends from ``xy[0]`` to ``xy[0] + width`` in x-direction\n    and from ``xy[1]`` to ``xy[1] + height`` in y-direction. ::\n\n      :                +------------------+\n      :                |                  |\n      :              height               |\n      :                |                  |\n      :               (xy)---- width -----+\n\n    One may picture *xy* as the bottom left corner, but which corner *xy* is\n    actually depends on the direction of the axis and the sign of *width*\n    and *height*; e.g. *xy* would be the bottom right corner if the x-axis\n    was inverted or if *width* was negative.\n    \"\"\"\n\n    def __str__(self):\n        pars = self._x0, self._y0, self._width, self._height, self.angle\n        fmt = \"Rectangle(xy=(%g, %g), width=%g, height=%g, angle=%g)\"\n        return fmt % pars\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"angle\")\n    def __init__(self, xy, width, height, angle=0.0, *,\n                 rotation_point='xy', **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        xy : (float, float)\n            The anchor point.\n        width : float\n            Rectangle width.\n        height : float\n            Rectangle height.\n        angle : float, default: 0\n            Rotation in degrees anti-clockwise about the rotation point.\n        rotation_point : {'xy', 'center', (number, number)}, default: 'xy'\n            If ``'xy'``, rotate around the anchor point. If ``'center'`` rotate\n            around the center. If 2-tuple of number, rotate around this\n            coordinate.\n\n        Other Parameters\n        ----------------\n        **kwargs : `.Patch` properties\n            %(Patch:kwdoc)s\n        \"\"\"\n        super().__init__(**kwargs)\n        self._x0 = xy[0]\n        self._y0 = xy[1]\n        self._width = width\n        self._height = height\n        self.angle = float(angle)\n        self.rotation_point = rotation_point\n        # Required for RectangleSelector with axes aspect ratio != 1\n        # The patch is defined in data coordinates and when changing the\n        # selector with square modifier and not in data coordinates, we need\n        # to correct for the aspect ratio difference between the data and\n        # display coordinate systems. Its value is typically provide by\n        # Axes._get_aspect_ratio()\n        self._aspect_ratio_correction = 1.0\n        self._convert_units()  # Validate the inputs.\n\n    def get_path(self):\n        \"\"\"Return the vertices of the rectangle.\"\"\"\n        return Path.unit_rectangle()\n\n    def _convert_units(self):\n        \"\"\"Convert bounds of the rectangle.\"\"\"\n        x0 = self.convert_xunits(self._x0)\n        y0 = self.convert_yunits(self._y0)\n        x1 = self.convert_xunits(self._x0 + self._width)\n        y1 = self.convert_yunits(self._y0 + self._height)\n        return x0, y0, x1, y1\n\n    def get_patch_transform(self):\n        # Note: This cannot be called until after this has been added to\n        # an Axes, otherwise unit conversion will fail. This makes it very\n        # important to call the accessor method and not directly access the\n        # transformation member variable.\n        bbox = self.get_bbox()\n        if self.rotation_point == 'center':\n            width, height = bbox.x1 - bbox.x0, bbox.y1 - bbox.y0\n            rotation_point = bbox.x0 + width / 2., bbox.y0 + height / 2.\n        elif self.rotation_point == 'xy':\n            rotation_point = bbox.x0, bbox.y0\n        else:\n            rotation_point = self.rotation_point\n        return transforms.BboxTransformTo(bbox) \\\n                + transforms.Affine2D() \\\n                .translate(-rotation_point[0], -rotation_point[1]) \\\n                .scale(1, self._aspect_ratio_correction) \\\n                .rotate_deg(self.angle) \\\n                .scale(1, 1 / self._aspect_ratio_correction) \\\n                .translate(*rotation_point)\n\n    @property\n    def rotation_point(self):\n        \"\"\"The rotation point of the patch.\"\"\"\n        return self._rotation_point\n\n    @rotation_point.setter\n    def rotation_point(self, value):\n        if value in ['center', 'xy'] or (\n                isinstance(value, tuple) and len(value) == 2 and\n                isinstance(value[0], Number) and isinstance(value[1], Number)\n                ):\n            self._rotation_point = value\n        else:\n            raise ValueError(\"`rotation_point` must be one of \"\n                             \"{'xy', 'center', (number, number)}.\")\n\n    def get_x(self):\n        \"\"\"Return the left coordinate of the rectangle.\"\"\"\n        return self._x0\n\n    def get_y(self):\n        \"\"\"Return the bottom coordinate of the rectangle.\"\"\"\n        return self._y0\n\n    def get_xy(self):\n        \"\"\"Return the left and bottom coords of the rectangle as a tuple.\"\"\"\n        return self._x0, self._y0\n\n    def get_corners(self):\n        \"\"\"\n        Return the corners of the rectangle, moving anti-clockwise from\n        (x0, y0).\n        \"\"\"\n        return self.get_patch_transform().transform(\n            [(0, 0), (1, 0), (1, 1), (0, 1)])\n\n    def get_center(self):\n        \"\"\"Return the centre of the rectangle.\"\"\"\n        return self.get_patch_transform().transform((0.5, 0.5))\n\n    def get_width(self):\n        \"\"\"Return the width of the rectangle.\"\"\"\n        return self._width\n\n    def get_height(self):\n        \"\"\"Return the height of the rectangle.\"\"\"\n        return self._height\n\n    def get_angle(self):\n        \"\"\"Get the rotation angle in degrees.\"\"\"\n        return self.angle\n\n    def set_x(self, x):\n        \"\"\"Set the left coordinate of the rectangle.\"\"\"\n        self._x0 = x\n        self.stale = True\n\n    def set_y(self, y):\n        \"\"\"Set the bottom coordinate of the rectangle.\"\"\"\n        self._y0 = y\n        self.stale = True\n\n    def set_angle(self, angle):\n        \"\"\"\n        Set the rotation angle in degrees.\n\n        The rotation is performed anti-clockwise around *xy*.\n        \"\"\"\n        self.angle = angle\n        self.stale = True\n\n    def set_xy(self, xy):\n        \"\"\"\n        Set the left and bottom coordinates of the rectangle.\n\n        Parameters\n        ----------\n        xy : (float, float)\n        \"\"\"\n        self._x0, self._y0 = xy\n        self.stale = True\n\n    def set_width(self, w):\n        \"\"\"Set the width of the rectangle.\"\"\"\n        self._width = w\n        self.stale = True\n\n    def set_height(self, h):\n        \"\"\"Set the height of the rectangle.\"\"\"\n        self._height = h\n        self.stale = True\n\n    def set_bounds(self, *args):\n        \"\"\"\n        Set the bounds of the rectangle as *left*, *bottom*, *width*, *height*.\n\n        The values may be passed as separate parameters or as a tuple::\n\n            set_bounds(left, bottom, width, height)\n            set_bounds((left, bottom, width, height))\n\n        .. ACCEPTS: (left, bottom, width, height)\n        \"\"\"\n        if len(args) == 1:\n            l, b, w, h = args[0]\n        else:\n            l, b, w, h = args\n        self._x0 = l\n        self._y0 = b\n        self._width = w\n        self._height = h\n        self.stale = True\n\n    def get_bbox(self):\n        \"\"\"Return the `.Bbox`.\"\"\"\n        x0, y0, x1, y1 = self._convert_units()\n        return transforms.Bbox.from_extents(x0, y0, x1, y1)\n\n    xy = property(get_xy, set_xy)\n\n\nclass RegularPolygon(Patch):\n    \"\"\"A regular polygon patch.\"\"\"\n\n    def __str__(self):\n        s = \"RegularPolygon((%g, %g), %d, radius=%g, orientation=%g)\"\n        return s % (self.xy[0], self.xy[1], self.numvertices, self.radius,\n                    self.orientation)\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"radius\")\n    def __init__(self, xy, numVertices, radius=5, orientation=0,\n                 **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        xy : (float, float)\n            The center position.\n\n        numVertices : int\n            The number of vertices.\n\n        radius : float\n            The distance from the center to each of the vertices.\n\n        orientation : float\n            The polygon rotation angle (in radians).\n\n        **kwargs\n            `Patch` properties:\n\n            %(Patch:kwdoc)s\n        \"\"\"\n        self.xy = xy\n        self.numvertices = numVertices\n        self.orientation = orientation\n        self.radius = radius\n        self._path = Path.unit_regular_polygon(numVertices)\n        self._patch_transform = transforms.Affine2D()\n        super().__init__(**kwargs)\n\n    def get_path(self):\n        return self._path\n\n    def get_patch_transform(self):\n        return self._patch_transform.clear() \\\n            .scale(self.radius) \\\n            .rotate(self.orientation) \\\n            .translate(*self.xy)\n\n\nclass PathPatch(Patch):\n    \"\"\"A general polycurve path patch.\"\"\"\n\n    _edge_default = True\n\n    def __str__(self):\n        s = \"PathPatch%d((%g, %g) ...)\"\n        return s % (len(self._path.vertices), *tuple(self._path.vertices[0]))\n\n    @_docstring.dedent_interpd\n    def __init__(self, path, **kwargs):\n        \"\"\"\n        *path* is a `~.path.Path` object.\n\n        Valid keyword arguments are:\n\n        %(Patch:kwdoc)s\n        \"\"\"\n        super().__init__(**kwargs)\n        self._path = path\n\n    def get_path(self):\n        return self._path\n\n    def set_path(self, path):\n        self._path = path\n\n\nclass StepPatch(PathPatch):\n    \"\"\"\n    A path patch describing a stepwise constant function.\n\n    By default the path is not closed and starts and stops at\n    baseline value.\n    \"\"\"\n\n    _edge_default = False\n\n    @_docstring.dedent_interpd\n    def __init__(self, values, edges, *,\n                 orientation='vertical', baseline=0, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        values : array-like\n            The step heights.\n\n        edges : array-like\n            The edge positions, with ``len(edges) == len(vals) + 1``,\n            between which the curve takes on vals values.\n\n        orientation : {'vertical', 'horizontal'}, default: 'vertical'\n            The direction of the steps. Vertical means that *values* are\n            along the y-axis, and edges are along the x-axis.\n\n        baseline : float, array-like or None, default: 0\n            The bottom value of the bounding edges or when\n            ``fill=True``, position of lower edge. If *fill* is\n            True or an array is passed to *baseline*, a closed\n            path is drawn.\n\n        Other valid keyword arguments are:\n\n        %(Patch:kwdoc)s\n        \"\"\"\n        self.orientation = orientation\n        self._edges = np.asarray(edges)\n        self._values = np.asarray(values)\n        self._baseline = np.asarray(baseline) if baseline is not None else None\n        self._update_path()\n        super().__init__(self._path, **kwargs)\n\n    def _update_path(self):\n        if np.isnan(np.sum(self._edges)):\n            raise ValueError('Nan values in \"edges\" are disallowed')\n        if self._edges.size - 1 != self._values.size:\n            raise ValueError('Size mismatch between \"values\" and \"edges\". '\n                             \"Expected `len(values) + 1 == len(edges)`, but \"\n                             f\"`len(values) = {self._values.size}` and \"\n                             f\"`len(edges) = {self._edges.size}`.\")\n        # Initializing with empty arrays allows supporting empty stairs.\n        verts, codes = [np.empty((0, 2))], [np.empty(0, dtype=Path.code_type)]\n\n        _nan_mask = np.isnan(self._values)\n        if self._baseline is not None:\n            _nan_mask |= np.isnan(self._baseline)\n        for idx0, idx1 in cbook.contiguous_regions(~_nan_mask):\n            x = np.repeat(self._edges[idx0:idx1+1], 2)\n            y = np.repeat(self._values[idx0:idx1], 2)\n            if self._baseline is None:\n                y = np.concatenate([y[:1], y, y[-1:]])\n            elif self._baseline.ndim == 0:  # single baseline value\n                y = np.concatenate([[self._baseline], y, [self._baseline]])\n            elif self._baseline.ndim == 1:  # baseline array\n                base = np.repeat(self._baseline[idx0:idx1], 2)[::-1]\n                x = np.concatenate([x, x[::-1]])\n                y = np.concatenate([base[-1:], y, base[:1],\n                                    base[:1], base, base[-1:]])\n            else:  # no baseline\n                raise ValueError('Invalid `baseline` specified')\n            if self.orientation == 'vertical':\n                xy = np.column_stack([x, y])\n            else:\n                xy = np.column_stack([y, x])\n            verts.append(xy)\n            codes.append([Path.MOVETO] + [Path.LINETO]*(len(xy)-1))\n        self._path = Path(np.concatenate(verts), np.concatenate(codes))\n\n    def get_data(self):\n        \"\"\"Get `.StepPatch` values, edges and baseline as namedtuple.\"\"\"\n        StairData = namedtuple('StairData', 'values edges baseline')\n        return StairData(self._values, self._edges, self._baseline)\n\n    def set_data(self, values=None, edges=None, baseline=None):\n        \"\"\"\n        Set `.StepPatch` values, edges and baseline.\n\n        Parameters\n        ----------\n        values : 1D array-like or None\n            Will not update values, if passing None\n        edges : 1D array-like, optional\n        baseline : float, 1D array-like or None\n        \"\"\"\n        if values is None and edges is None and baseline is None:\n            raise ValueError(\"Must set *values*, *edges* or *baseline*.\")\n        if values is not None:\n            self._values = np.asarray(values)\n        if edges is not None:\n            self._edges = np.asarray(edges)\n        if baseline is not None:\n            self._baseline = np.asarray(baseline)\n        self._update_path()\n        self.stale = True\n\n\nclass Polygon(Patch):\n    \"\"\"A general polygon patch.\"\"\"\n\n    def __str__(self):\n        if len(self._path.vertices):\n            s = \"Polygon%d((%g, %g) ...)\"\n            return s % (len(self._path.vertices), *self._path.vertices[0])\n        else:\n            return \"Polygon0()\"\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"closed\")\n    def __init__(self, xy, closed=True, **kwargs):\n        \"\"\"\n        *xy* is a numpy array with shape Nx2.\n\n        If *closed* is *True*, the polygon will be closed so the\n        starting and ending points are the same.\n\n        Valid keyword arguments are:\n\n        %(Patch:kwdoc)s\n        \"\"\"\n        super().__init__(**kwargs)\n        self._closed = closed\n        self.set_xy(xy)\n\n    def get_path(self):\n        \"\"\"Get the `.Path` of the polygon.\"\"\"\n        return self._path\n\n    def get_closed(self):\n        \"\"\"Return whether the polygon is closed.\"\"\"\n        return self._closed\n\n    def set_closed(self, closed):\n        \"\"\"\n        Set whether the polygon is closed.\n\n        Parameters\n        ----------\n        closed : bool\n           True if the polygon is closed\n        \"\"\"\n        if self._closed == bool(closed):\n            return\n        self._closed = bool(closed)\n        self.set_xy(self.get_xy())\n        self.stale = True\n\n    def get_xy(self):\n        \"\"\"\n        Get the vertices of the path.\n\n        Returns\n        -------\n        (N, 2) numpy array\n            The coordinates of the vertices.\n        \"\"\"\n        return self._path.vertices\n\n    def set_xy(self, xy):\n        \"\"\"\n        Set the vertices of the polygon.\n\n        Parameters\n        ----------\n        xy : (N, 2) array-like\n            The coordinates of the vertices.\n\n        Notes\n        -----\n        Unlike `~.path.Path`, we do not ignore the last input vertex. If the\n        polygon is meant to be closed, and the last point of the polygon is not\n        equal to the first, we assume that the user has not explicitly passed a\n        ``CLOSEPOLY`` vertex, and add it ourselves.\n        \"\"\"\n        xy = np.asarray(xy)\n        nverts, _ = xy.shape\n        if self._closed:\n            # if the first and last vertex are the \"same\", then we assume that\n            # the user explicitly passed the CLOSEPOLY vertex. Otherwise, we\n            # have to append one since the last vertex will be \"ignored\" by\n            # Path\n            if nverts == 1 or nverts > 1 and (xy[0] != xy[-1]).any():\n                xy = np.concatenate([xy, [xy[0]]])\n        else:\n            # if we aren't closed, and the last vertex matches the first, then\n            # we assume we have an unnecessary CLOSEPOLY vertex and remove it\n            if nverts > 2 and (xy[0] == xy[-1]).all():\n                xy = xy[:-1]\n        self._path = Path(xy, closed=self._closed)\n        self.stale = True\n\n    xy = property(get_xy, set_xy,\n                  doc='The vertices of the path as (N, 2) numpy array.')\n\n\nclass Wedge(Patch):\n    \"\"\"Wedge shaped patch.\"\"\"\n\n    def __str__(self):\n        pars = (self.center[0], self.center[1], self.r,\n                self.theta1, self.theta2, self.width)\n        fmt = \"Wedge(center=(%g, %g), r=%g, theta1=%g, theta2=%g, width=%s)\"\n        return fmt % pars\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"width\")\n    def __init__(self, center, r, theta1, theta2, width=None, **kwargs):\n        \"\"\"\n        A wedge centered at *x*, *y* center with radius *r* that\n        sweeps *theta1* to *theta2* (in degrees).  If *width* is given,\n        then a partial wedge is drawn from inner radius *r* - *width*\n        to outer radius *r*.\n\n        Valid keyword arguments are:\n\n        %(Patch:kwdoc)s\n        \"\"\"\n        super().__init__(**kwargs)\n        self.center = center\n        self.r, self.width = r, width\n        self.theta1, self.theta2 = theta1, theta2\n        self._patch_transform = transforms.IdentityTransform()\n        self._recompute_path()\n\n    def _recompute_path(self):\n        # Inner and outer rings are connected unless the annulus is complete\n        if abs((self.theta2 - self.theta1) - 360) <= 1e-12:\n            theta1, theta2 = 0, 360\n            connector = Path.MOVETO\n        else:\n            theta1, theta2 = self.theta1, self.theta2\n            connector = Path.LINETO\n\n        # Form the outer ring\n        arc = Path.arc(theta1, theta2)\n\n        if self.width is not None:\n            # Partial annulus needs to draw the outer ring\n            # followed by a reversed and scaled inner ring\n            v1 = arc.vertices\n            v2 = arc.vertices[::-1] * (self.r - self.width) / self.r\n            v = np.concatenate([v1, v2, [v1[0, :], (0, 0)]])\n            c = np.concatenate([\n                arc.codes, arc.codes, [connector, Path.CLOSEPOLY]])\n            c[len(arc.codes)] = connector\n        else:\n            # Wedge doesn't need an inner ring\n            v = np.concatenate([\n                arc.vertices, [(0, 0), arc.vertices[0, :], (0, 0)]])\n            c = np.concatenate([\n                arc.codes, [connector, connector, Path.CLOSEPOLY]])\n\n        # Shift and scale the wedge to the final location.\n        v *= self.r\n        v += np.asarray(self.center)\n        self._path = Path(v, c)\n\n    def set_center(self, center):\n        self._path = None\n        self.center = center\n        self.stale = True\n\n    def set_radius(self, radius):\n        self._path = None\n        self.r = radius\n        self.stale = True\n\n    def set_theta1(self, theta1):\n        self._path = None\n        self.theta1 = theta1\n        self.stale = True\n\n    def set_theta2(self, theta2):\n        self._path = None\n        self.theta2 = theta2\n        self.stale = True\n\n    def set_width(self, width):\n        self._path = None\n        self.width = width\n        self.stale = True\n\n    def get_path(self):\n        if self._path is None:\n            self._recompute_path()\n        return self._path\n\n\n# COVERAGE NOTE: Not used internally or from examples\nclass Arrow(Patch):\n    \"\"\"An arrow patch.\"\"\"\n\n    def __str__(self):\n        return \"Arrow()\"\n\n    _path = Path._create_closed([\n        [0.0, 0.1], [0.0, -0.1], [0.8, -0.1], [0.8, -0.3], [1.0, 0.0],\n        [0.8, 0.3], [0.8, 0.1]])\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"width\")\n    def __init__(self, x, y, dx, dy, width=1.0, **kwargs):\n        \"\"\"\n        Draws an arrow from (*x*, *y*) to (*x* + *dx*, *y* + *dy*).\n        The width of the arrow is scaled by *width*.\n\n        Parameters\n        ----------\n        x : float\n            x coordinate of the arrow tail.\n        y : float\n            y coordinate of the arrow tail.\n        dx : float\n            Arrow length in the x direction.\n        dy : float\n            Arrow length in the y direction.\n        width : float, default: 1\n            Scale factor for the width of the arrow. With a default value of 1,\n            the tail width is 0.2 and head width is 0.6.\n        **kwargs\n            Keyword arguments control the `Patch` properties:\n\n            %(Patch:kwdoc)s\n\n        See Also\n        --------\n        FancyArrow\n            Patch that allows independent control of the head and tail\n            properties.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._patch_transform = (\n            transforms.Affine2D()\n            .scale(np.hypot(dx, dy), width)\n            .rotate(np.arctan2(dy, dx))\n            .translate(x, y)\n            .frozen())\n\n    def get_path(self):\n        return self._path\n\n    def get_patch_transform(self):\n        return self._patch_transform\n\n\nclass FancyArrow(Polygon):\n    \"\"\"\n    Like Arrow, but lets you set head width and head height independently.\n    \"\"\"\n\n    _edge_default = True\n\n    def __str__(self):\n        return \"FancyArrow()\"\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"width\")\n    def __init__(self, x, y, dx, dy, width=0.001, length_includes_head=False,\n                 head_width=None, head_length=None, shape='full', overhang=0,\n                 head_starts_at_zero=False, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        x, y : float\n            The x and y coordinates of the arrow base.\n\n        dx, dy : float\n            The length of the arrow along x and y direction.\n\n        width : float, default: 0.001\n            Width of full arrow tail.\n\n        length_includes_head : bool, default: False\n            True if head is to be counted in calculating the length.\n\n        head_width : float or None, default: 3*width\n            Total width of the full arrow head.\n\n        head_length : float or None, default: 1.5*head_width\n            Length of arrow head.\n\n        shape : {'full', 'left', 'right'}, default: 'full'\n            Draw the left-half, right-half, or full arrow.\n\n        overhang : float, default: 0\n            Fraction that the arrow is swept back (0 overhang means\n            triangular shape). Can be negative or greater than one.\n\n        head_starts_at_zero : bool, default: False\n            If True, the head starts being drawn at coordinate 0\n            instead of ending at coordinate 0.\n\n        **kwargs\n            `.Patch` properties:\n\n            %(Patch:kwdoc)s\n        \"\"\"\n        self._x = x\n        self._y = y\n        self._dx = dx\n        self._dy = dy\n        self._width = width\n        self._length_includes_head = length_includes_head\n        self._head_width = head_width\n        self._head_length = head_length\n        self._shape = shape\n        self._overhang = overhang\n        self._head_starts_at_zero = head_starts_at_zero\n        self._make_verts()\n        super().__init__(self.verts, closed=True, **kwargs)\n\n    def set_data(self, *, x=None, y=None, dx=None, dy=None, width=None,\n                 head_width=None, head_length=None):\n        \"\"\"\n        Set `.FancyArrow` x, y, dx, dy, width, head_with, and head_length.\n        Values left as None will not be updated.\n\n        Parameters\n        ----------\n        x, y : float or None, default: None\n            The x and y coordinates of the arrow base.\n\n        dx, dy : float or None, default: None\n            The length of the arrow along x and y direction.\n\n        width : float or None, default: None\n            Width of full arrow tail.\n\n        head_width : float or None, default: None\n            Total width of the full arrow head.\n\n        head_length : float or None, default: None\n            Length of arrow head.\n        \"\"\"\n        if x is not None:\n            self._x = x\n        if y is not None:\n            self._y = y\n        if dx is not None:\n            self._dx = dx\n        if dy is not None:\n            self._dy = dy\n        if width is not None:\n            self._width = width\n        if head_width is not None:\n            self._head_width = head_width\n        if head_length is not None:\n            self._head_length = head_length\n        self._make_verts()\n        self.set_xy(self.verts)\n\n    def _make_verts(self):\n        if self._head_width is None:\n            head_width = 3 * self._width\n        else:\n            head_width = self._head_width\n        if self._head_length is None:\n            head_length = 1.5 * head_width\n        else:\n            head_length = self._head_length\n\n        distance = np.hypot(self._dx, self._dy)\n\n        if self._length_includes_head:\n            length = distance\n        else:\n            length = distance + head_length\n        if not length:\n            self.verts = np.empty([0, 2])  # display nothing if empty\n        else:\n            # start by drawing horizontal arrow, point at (0, 0)\n            hw, hl = head_width, head_length\n            hs, lw = self._overhang, self._width\n            left_half_arrow = np.array([\n                [0.0, 0.0],                 # tip\n                [-hl, -hw / 2],             # leftmost\n                [-hl * (1 - hs), -lw / 2],  # meets stem\n                [-length, -lw / 2],         # bottom left\n                [-length, 0],\n            ])\n            # if we're not including the head, shift up by head length\n            if not self._length_includes_head:\n                left_half_arrow += [head_length, 0]\n            # if the head starts at 0, shift up by another head length\n            if self._head_starts_at_zero:\n                left_half_arrow += [head_length / 2, 0]\n            # figure out the shape, and complete accordingly\n            if self._shape == 'left':\n                coords = left_half_arrow\n            else:\n                right_half_arrow = left_half_arrow * [1, -1]\n                if self._shape == 'right':\n                    coords = right_half_arrow\n                elif self._shape == 'full':\n                    # The half-arrows contain the midpoint of the stem,\n                    # which we can omit from the full arrow. Including it\n                    # twice caused a problem with xpdf.\n                    coords = np.concatenate([left_half_arrow[:-1],\n                                             right_half_arrow[-2::-1]])\n                else:\n                    raise ValueError(f\"Got unknown shape: {self._shape!r}\")\n            if distance != 0:\n                cx = self._dx / distance\n                sx = self._dy / distance\n            else:\n                # Account for division by zero\n                cx, sx = 0, 1\n            M = [[cx, sx], [-sx, cx]]\n            self.verts = np.dot(coords, M) + [\n                self._x + self._dx,\n                self._y + self._dy,\n            ]\n\n\n_docstring.interpd.update(\n    FancyArrow=\"\\n\".join(\n        (inspect.getdoc(FancyArrow.__init__) or \"\").splitlines()[2:]))\n\n\nclass CirclePolygon(RegularPolygon):\n    \"\"\"A polygon-approximation of a circle patch.\"\"\"\n\n    def __str__(self):\n        s = \"CirclePolygon((%g, %g), radius=%g, resolution=%d)\"\n        return s % (self.xy[0], self.xy[1], self.radius, self.numvertices)\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"resolution\")\n    def __init__(self, xy, radius=5,\n                 resolution=20,  # the number of vertices\n                 ** kwargs):\n        \"\"\"\n        Create a circle at *xy* = (*x*, *y*) with given *radius*.\n\n        This circle is approximated by a regular polygon with *resolution*\n        sides.  For a smoother circle drawn with splines, see `Circle`.\n\n        Valid keyword arguments are:\n\n        %(Patch:kwdoc)s\n        \"\"\"\n        super().__init__(\n            xy, resolution, radius=radius, orientation=0, **kwargs)\n\n\nclass Ellipse(Patch):\n    \"\"\"A scale-free ellipse.\"\"\"\n\n    def __str__(self):\n        pars = (self._center[0], self._center[1],\n                self.width, self.height, self.angle)\n        fmt = \"Ellipse(xy=(%s, %s), width=%s, height=%s, angle=%s)\"\n        return fmt % pars\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"angle\")\n    def __init__(self, xy, width, height, angle=0, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        xy : (float, float)\n            xy coordinates of ellipse centre.\n        width : float\n            Total length (diameter) of horizontal axis.\n        height : float\n            Total length (diameter) of vertical axis.\n        angle : float, default: 0\n            Rotation in degrees anti-clockwise.\n\n        Notes\n        -----\n        Valid keyword arguments are:\n\n        %(Patch:kwdoc)s\n        \"\"\"\n        super().__init__(**kwargs)\n\n        self._center = xy\n        self._width, self._height = width, height\n        self._angle = angle\n        self._path = Path.unit_circle()\n        # Required for EllipseSelector with axes aspect ratio != 1\n        # The patch is defined in data coordinates and when changing the\n        # selector with square modifier and not in data coordinates, we need\n        # to correct for the aspect ratio difference between the data and\n        # display coordinate systems.\n        self._aspect_ratio_correction = 1.0\n        # Note: This cannot be calculated until this is added to an Axes\n        self._patch_transform = transforms.IdentityTransform()\n\n    def _recompute_transform(self):\n        \"\"\"\n        Notes\n        -----\n        This cannot be called until after this has been added to an Axes,\n        otherwise unit conversion will fail. This makes it very important to\n        call the accessor method and not directly access the transformation\n        member variable.\n        \"\"\"\n        center = (self.convert_xunits(self._center[0]),\n                  self.convert_yunits(self._center[1]))\n        width = self.convert_xunits(self._width)\n        height = self.convert_yunits(self._height)\n        self._patch_transform = transforms.Affine2D() \\\n            .scale(width * 0.5, height * 0.5 * self._aspect_ratio_correction) \\\n            .rotate_deg(self.angle) \\\n            .scale(1, 1 / self._aspect_ratio_correction) \\\n            .translate(*center)\n\n    def get_path(self):\n        \"\"\"Return the path of the ellipse.\"\"\"\n        return self._path\n\n    def get_patch_transform(self):\n        self._recompute_transform()\n        return self._patch_transform\n\n    def set_center(self, xy):\n        \"\"\"\n        Set the center of the ellipse.\n\n        Parameters\n        ----------\n        xy : (float, float)\n        \"\"\"\n        self._center = xy\n        self.stale = True\n\n    def get_center(self):\n        \"\"\"Return the center of the ellipse.\"\"\"\n        return self._center\n\n    center = property(get_center, set_center)\n\n    def set_width(self, width):\n        \"\"\"\n        Set the width of the ellipse.\n\n        Parameters\n        ----------\n        width : float\n        \"\"\"\n        self._width = width\n        self.stale = True\n\n    def get_width(self):\n        \"\"\"\n        Return the width of the ellipse.\n        \"\"\"\n        return self._width\n\n    width = property(get_width, set_width)\n\n    def set_height(self, height):\n        \"\"\"\n        Set the height of the ellipse.\n\n        Parameters\n        ----------\n        height : float\n        \"\"\"\n        self._height = height\n        self.stale = True\n\n    def get_height(self):\n        \"\"\"Return the height of the ellipse.\"\"\"\n        return self._height\n\n    height = property(get_height, set_height)\n\n    def set_angle(self, angle):\n        \"\"\"\n        Set the angle of the ellipse.\n\n        Parameters\n        ----------\n        angle : float\n        \"\"\"\n        self._angle = angle\n        self.stale = True\n\n    def get_angle(self):\n        \"\"\"Return the angle of the ellipse.\"\"\"\n        return self._angle\n\n    angle = property(get_angle, set_angle)\n\n    def get_corners(self):\n        \"\"\"\n        Return the corners of the ellipse bounding box.\n\n        The bounding box orientation is moving anti-clockwise from the\n        lower left corner defined before rotation.\n        \"\"\"\n        return self.get_patch_transform().transform(\n            [(-1, -1), (1, -1), (1, 1), (-1, 1)])\n\n\nclass Annulus(Patch):\n    \"\"\"\n    An elliptical annulus.\n    \"\"\"\n\n    @_docstring.dedent_interpd\n    def __init__(self, xy, r, width, angle=0.0, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        xy : (float, float)\n            xy coordinates of annulus centre.\n        r : float or (float, float)\n            The radius, or semi-axes:\n\n            - If float: radius of the outer circle.\n            - If two floats: semi-major and -minor axes of outer ellipse.\n        width : float\n            Width (thickness) of the annular ring. The width is measured inward\n            from the outer ellipse so that for the inner ellipse the semi-axes\n            are given by ``r - width``. *width* must be less than or equal to\n            the semi-minor axis.\n        angle : float, default: 0\n            Rotation angle in degrees (anti-clockwise from the positive\n            x-axis). Ignored for circular annuli (i.e., if *r* is a scalar).\n        **kwargs\n            Keyword arguments control the `Patch` properties:\n\n            %(Patch:kwdoc)s\n        \"\"\"\n        super().__init__(**kwargs)\n\n        self.set_radii(r)\n        self.center = xy\n        self.width = width\n        self.angle = angle\n        self._path = None\n\n    def __str__(self):\n        if self.a == self.b:\n            r = self.a\n        else:\n            r = (self.a, self.b)\n\n        return \"Annulus(xy=(%s, %s), r=%s, width=%s, angle=%s)\" % \\\n                (*self.center, r, self.width, self.angle)\n\n    def set_center(self, xy):\n        \"\"\"\n        Set the center of the annulus.\n\n        Parameters\n        ----------\n        xy : (float, float)\n        \"\"\"\n        self._center = xy\n        self._path = None\n        self.stale = True\n\n    def get_center(self):\n        \"\"\"Return the center of the annulus.\"\"\"\n        return self._center\n\n    center = property(get_center, set_center)\n\n    def set_width(self, width):\n        \"\"\"\n        Set the width (thickness) of the annulus ring.\n\n        The width is measured inwards from the outer ellipse.\n\n        Parameters\n        ----------\n        width : float\n        \"\"\"\n        if min(self.a, self.b) <= width:\n            raise ValueError(\n                'Width of annulus must be less than or equal semi-minor axis')\n\n        self._width = width\n        self._path = None\n        self.stale = True\n\n    def get_width(self):\n        \"\"\"Return the width (thickness) of the annulus ring.\"\"\"\n        return self._width\n\n    width = property(get_width, set_width)\n\n    def set_angle(self, angle):\n        \"\"\"\n        Set the tilt angle of the annulus.\n\n        Parameters\n        ----------\n        angle : float\n        \"\"\"\n        self._angle = angle\n        self._path = None\n        self.stale = True\n\n    def get_angle(self):\n        \"\"\"Return the angle of the annulus.\"\"\"\n        return self._angle\n\n    angle = property(get_angle, set_angle)\n\n    def set_semimajor(self, a):\n        \"\"\"\n        Set the semi-major axis *a* of the annulus.\n\n        Parameters\n        ----------\n        a : float\n        \"\"\"\n        self.a = float(a)\n        self._path = None\n        self.stale = True\n\n    def set_semiminor(self, b):\n        \"\"\"\n        Set the semi-minor axis *b* of the annulus.\n\n        Parameters\n        ----------\n        b : float\n        \"\"\"\n        self.b = float(b)\n        self._path = None\n        self.stale = True\n\n    def set_radii(self, r):\n        \"\"\"\n        Set the semi-major (*a*) and semi-minor radii (*b*) of the annulus.\n\n        Parameters\n        ----------\n        r : float or (float, float)\n            The radius, or semi-axes:\n\n            - If float: radius of the outer circle.\n            - If two floats: semi-major and -minor axes of outer ellipse.\n        \"\"\"\n        if np.shape(r) == (2,):\n            self.a, self.b = r\n        elif np.shape(r) == ():\n            self.a = self.b = float(r)\n        else:\n            raise ValueError(\"Parameter 'r' must be one or two floats.\")\n\n        self._path = None\n        self.stale = True\n\n    def get_radii(self):\n        \"\"\"Return the semi-major and semi-minor radii of the annulus.\"\"\"\n        return self.a, self.b\n\n    radii = property(get_radii, set_radii)\n\n    def _transform_verts(self, verts, a, b):\n        return transforms.Affine2D() \\\n            .scale(*self._convert_xy_units((a, b))) \\\n            .rotate_deg(self.angle) \\\n            .translate(*self._convert_xy_units(self.center)) \\\n            .transform(verts)\n\n    def _recompute_path(self):\n        # circular arc\n        arc = Path.arc(0, 360)\n\n        # annulus needs to draw an outer ring\n        # followed by a reversed and scaled inner ring\n        a, b, w = self.a, self.b, self.width\n        v1 = self._transform_verts(arc.vertices, a, b)\n        v2 = self._transform_verts(arc.vertices[::-1], a - w, b - w)\n        v = np.vstack([v1, v2, v1[0, :], (0, 0)])\n        c = np.hstack([arc.codes, Path.MOVETO,\n                       arc.codes[1:], Path.MOVETO,\n                       Path.CLOSEPOLY])\n        self._path = Path(v, c)\n\n    def get_path(self):\n        if self._path is None:\n            self._recompute_path()\n        return self._path\n\n\nclass Circle(Ellipse):\n    \"\"\"\n    A circle patch.\n    \"\"\"\n    def __str__(self):\n        pars = self.center[0], self.center[1], self.radius\n        fmt = \"Circle(xy=(%g, %g), radius=%g)\"\n        return fmt % pars\n\n    @_docstring.dedent_interpd\n    def __init__(self, xy, radius=5, **kwargs):\n        \"\"\"\n        Create a true circle at center *xy* = (*x*, *y*) with given *radius*.\n\n        Unlike `CirclePolygon` which is a polygonal approximation, this uses\n        Bezier splines and is much closer to a scale-free circle.\n\n        Valid keyword arguments are:\n\n        %(Patch:kwdoc)s\n        \"\"\"\n        super().__init__(xy, radius * 2, radius * 2, **kwargs)\n        self.radius = radius\n\n    def set_radius(self, radius):\n        \"\"\"\n        Set the radius of the circle.\n\n        Parameters\n        ----------\n        radius : float\n        \"\"\"\n        self.width = self.height = 2 * radius\n        self.stale = True\n\n    def get_radius(self):\n        \"\"\"Return the radius of the circle.\"\"\"\n        return self.width / 2.\n\n    radius = property(get_radius, set_radius)\n\n\nclass Arc(Ellipse):\n    \"\"\"\n    An elliptical arc, i.e. a segment of an ellipse.\n\n    Due to internal optimizations, the arc cannot be filled.\n    \"\"\"\n\n    def __str__(self):\n        pars = (self.center[0], self.center[1], self.width,\n                self.height, self.angle, self.theta1, self.theta2)\n        fmt = (\"Arc(xy=(%g, %g), width=%g, \"\n               \"height=%g, angle=%g, theta1=%g, theta2=%g)\")\n        return fmt % pars\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"angle\")\n    def __init__(self, xy, width, height, angle=0.0,\n                 theta1=0.0, theta2=360.0, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        xy : (float, float)\n            The center of the ellipse.\n\n        width : float\n            The length of the horizontal axis.\n\n        height : float\n            The length of the vertical axis.\n\n        angle : float\n            Rotation of the ellipse in degrees (counterclockwise).\n\n        theta1, theta2 : float, default: 0, 360\n            Starting and ending angles of the arc in degrees. These values\n            are relative to *angle*, e.g. if *angle* = 45 and *theta1* = 90\n            the absolute starting angle is 135.\n            Default *theta1* = 0, *theta2* = 360, i.e. a complete ellipse.\n            The arc is drawn in the counterclockwise direction.\n            Angles greater than or equal to 360, or smaller than 0, are\n            represented by an equivalent angle in the range [0, 360), by\n            taking the input value mod 360.\n\n        Other Parameters\n        ----------------\n        **kwargs : `.Patch` properties\n            Most `.Patch` properties are supported as keyword arguments,\n            with the exception of *fill* and *facecolor* because filling is\n            not supported.\n\n        %(Patch:kwdoc)s\n        \"\"\"\n        fill = kwargs.setdefault('fill', False)\n        if fill:\n            raise ValueError(\"Arc objects can not be filled\")\n\n        super().__init__(xy, width, height, angle=angle, **kwargs)\n\n        self.theta1 = theta1\n        self.theta2 = theta2\n\n    @artist.allow_rasterization\n    def draw(self, renderer):\n        \"\"\"\n        Draw the arc to the given *renderer*.\n\n        Notes\n        -----\n        Ellipses are normally drawn using an approximation that uses\n        eight cubic Bezier splines.  The error of this approximation\n        is 1.89818e-6, according to this unverified source:\n\n          Lancaster, Don.  *Approximating a Circle or an Ellipse Using\n          Four Bezier Cubic Splines.*\n\n          https://www.tinaja.com/glib/ellipse4.pdf\n\n        There is a use case where very large ellipses must be drawn\n        with very high accuracy, and it is too expensive to render the\n        entire ellipse with enough segments (either splines or line\n        segments).  Therefore, in the case where either radius of the\n        ellipse is large enough that the error of the spline\n        approximation will be visible (greater than one pixel offset\n        from the ideal), a different technique is used.\n\n        In that case, only the visible parts of the ellipse are drawn,\n        with each visible arc using a fixed number of spline segments\n        (8).  The algorithm proceeds as follows:\n\n        1. The points where the ellipse intersects the axes (or figure)\n           bounding box are located.  (This is done by performing an inverse\n           transformation on the bbox such that it is relative to the unit\n           circle -- this makes the intersection calculation much easier than\n           doing rotated ellipse intersection directly.)\n\n           This uses the \"line intersecting a circle\" algorithm from:\n\n               Vince, John.  *Geometry for Computer Graphics: Formulae,\n               Examples & Proofs.*  London: Springer-Verlag, 2005.\n\n        2. The angles of each of the intersection points are calculated.\n\n        3. Proceeding counterclockwise starting in the positive\n           x-direction, each of the visible arc-segments between the\n           pairs of vertices are drawn using the Bezier arc\n           approximation technique implemented in `.Path.arc`.\n        \"\"\"\n        if not self.get_visible():\n            return\n\n        self._recompute_transform()\n\n        width = self.convert_xunits(self.width)\n        height = self.convert_yunits(self.height)\n\n        # If the width and height of ellipse are not equal, take into account\n        # stretching when calculating angles to draw between\n        def theta_stretch(theta, scale):\n            theta = np.deg2rad(theta)\n            x = np.cos(theta)\n            y = np.sin(theta)\n            stheta = np.rad2deg(np.arctan2(scale * y, x))\n            # arctan2 has the range [-pi, pi], we expect [0, 2*pi]\n            return (stheta + 360) % 360\n\n        theta1 = self.theta1\n        theta2 = self.theta2\n\n        if (\n            # if we need to stretch the angles because we are distorted\n            width != height\n            # and we are not doing a full circle.\n            #\n            # 0 and 360 do not exactly round-trip through the angle\n            # stretching (due to both float precision limitations and\n            # the difference between the range of arctan2 [-pi, pi] and\n            # this method [0, 360]) so avoid doing it if we don't have to.\n            and not (theta1 != theta2 and theta1 % 360 == theta2 % 360)\n        ):\n            theta1 = theta_stretch(self.theta1, width / height)\n            theta2 = theta_stretch(self.theta2, width / height)\n\n        # Get width and height in pixels we need to use\n        # `self.get_data_transform` rather than `self.get_transform`\n        # because we want the transform from dataspace to the\n        # screen space to estimate how big the arc will be in physical\n        # units when rendered (the transform that we get via\n        # `self.get_transform()` goes from an idealized unit-radius\n        # space to screen space).\n        data_to_screen_trans = self.get_data_transform()\n        pwidth, pheight = (data_to_screen_trans.transform((width, height)) -\n                           data_to_screen_trans.transform((0, 0)))\n        inv_error = (1.0 / 1.89818e-6) * 0.5\n\n        if pwidth < inv_error and pheight < inv_error:\n            self._path = Path.arc(theta1, theta2)\n            return Patch.draw(self, renderer)\n\n        def line_circle_intersect(x0, y0, x1, y1):\n            dx = x1 - x0\n            dy = y1 - y0\n            dr2 = dx * dx + dy * dy\n            D = x0 * y1 - x1 * y0\n            D2 = D * D\n            discrim = dr2 - D2\n            if discrim >= 0.0:\n                sign_dy = np.copysign(1, dy)  # +/-1, never 0.\n                sqrt_discrim = np.sqrt(discrim)\n                return np.array(\n                    [[(D * dy + sign_dy * dx * sqrt_discrim) / dr2,\n                      (-D * dx + abs(dy) * sqrt_discrim) / dr2],\n                     [(D * dy - sign_dy * dx * sqrt_discrim) / dr2,\n                      (-D * dx - abs(dy) * sqrt_discrim) / dr2]])\n            else:\n                return np.empty((0, 2))\n\n        def segment_circle_intersect(x0, y0, x1, y1):\n            epsilon = 1e-9\n            if x1 < x0:\n                x0e, x1e = x1, x0\n            else:\n                x0e, x1e = x0, x1\n            if y1 < y0:\n                y0e, y1e = y1, y0\n            else:\n                y0e, y1e = y0, y1\n            xys = line_circle_intersect(x0, y0, x1, y1)\n            xs, ys = xys.T\n            return xys[\n                (x0e - epsilon < xs) & (xs < x1e + epsilon)\n                & (y0e - epsilon < ys) & (ys < y1e + epsilon)\n            ]\n\n        # Transform the axes (or figure) box_path so that it is relative to\n        # the unit circle in the same way that it is relative to the desired\n        # ellipse.\n        box_path_transform = (\n            transforms.BboxTransformTo((self.axes or self.figure).bbox)\n            - self.get_transform())\n        box_path = Path.unit_rectangle().transformed(box_path_transform)\n\n        thetas = set()\n        # For each of the point pairs, there is a line segment\n        for p0, p1 in zip(box_path.vertices[:-1], box_path.vertices[1:]):\n            xy = segment_circle_intersect(*p0, *p1)\n            x, y = xy.T\n            # arctan2 return [-pi, pi), the rest of our angles are in\n            # [0, 360], adjust as needed.\n            theta = (np.rad2deg(np.arctan2(y, x)) + 360) % 360\n            thetas.update(theta[(theta1 < theta) & (theta < theta2)])\n        thetas = sorted(thetas) + [theta2]\n        last_theta = theta1\n        theta1_rad = np.deg2rad(theta1)\n        inside = box_path.contains_point(\n            (np.cos(theta1_rad), np.sin(theta1_rad))\n        )\n\n        # save original path\n        path_original = self._path\n        for theta in thetas:\n            if inside:\n                self._path = Path.arc(last_theta, theta, 8)\n                Patch.draw(self, renderer)\n                inside = False\n            else:\n                inside = True\n            last_theta = theta\n\n        # restore original path\n        self._path = path_original\n\n\ndef bbox_artist(artist, renderer, props=None, fill=True):\n    \"\"\"\n    A debug function to draw a rectangle around the bounding\n    box returned by an artist's `.Artist.get_window_extent`\n    to test whether the artist is returning the correct bbox.\n\n    *props* is a dict of rectangle props with the additional property\n    'pad' that sets the padding around the bbox in points.\n    \"\"\"\n    if props is None:\n        props = {}\n    props = props.copy()  # don't want to alter the pad externally\n    pad = props.pop('pad', 4)\n    pad = renderer.points_to_pixels(pad)\n    bbox = artist.get_window_extent(renderer)\n    r = Rectangle(\n        xy=(bbox.x0 - pad / 2, bbox.y0 - pad / 2),\n        width=bbox.width + pad, height=bbox.height + pad,\n        fill=fill, transform=transforms.IdentityTransform(), clip_on=False)\n    r.update(props)\n    r.draw(renderer)\n\n\ndef draw_bbox(bbox, renderer, color='k', trans=None):\n    \"\"\"\n    A debug function to draw a rectangle around the bounding\n    box returned by an artist's `.Artist.get_window_extent`\n    to test whether the artist is returning the correct bbox.\n    \"\"\"\n    r = Rectangle(xy=bbox.p0, width=bbox.width, height=bbox.height,\n                  edgecolor=color, fill=False, clip_on=False)\n    if trans is not None:\n        r.set_transform(trans)\n    r.draw(renderer)\n\n\ndef _simpleprint_styles(_styles):\n    \"\"\"\n    A helper function for the _Style class.  Given the dictionary of\n    {stylename: styleclass}, return a string rep of the list of keys.\n    Used to update the documentation.\n    \"\"\"\n    return \"[{}]\".format(\"|\".join(map(\" '{}' \".format, _styles)))\n\n\nclass _Style:\n    \"\"\"\n    A base class for the Styles. It is meant to be a container class,\n    where actual styles are declared as subclass of it, and it\n    provides some helper functions.\n    \"\"\"\n\n    def __new__(cls, stylename, **kwargs):\n        \"\"\"Return the instance of the subclass with the given style name.\"\"\"\n        # The \"class\" should have the _style_list attribute, which is a mapping\n        # of style names to style classes.\n        _list = stylename.replace(\" \", \"\").split(\",\")\n        _name = _list[0].lower()\n        try:\n            _cls = cls._style_list[_name]\n        except KeyError as err:\n            raise ValueError(f\"Unknown style: {stylename!r}\") from err\n        try:\n            _args_pair = [cs.split(\"=\") for cs in _list[1:]]\n            _args = {k: float(v) for k, v in _args_pair}\n        except ValueError as err:\n            raise ValueError(\n                f\"Incorrect style argument: {stylename!r}\") from err\n        return _cls(**{**_args, **kwargs})\n\n    @classmethod\n    def get_styles(cls):\n        \"\"\"Return a dictionary of available styles.\"\"\"\n        return cls._style_list\n\n    @classmethod\n    def pprint_styles(cls):\n        \"\"\"Return the available styles as pretty-printed string.\"\"\"\n        table = [('Class', 'Name', 'Attrs'),\n                 *[(cls.__name__,\n                    # Add backquotes, as - and | have special meaning in reST.\n                    f'``{name}``',\n                    # [1:-1] drops the surrounding parentheses.\n                    str(inspect.signature(cls))[1:-1] or 'None')\n                   for name, cls in cls._style_list.items()]]\n        # Convert to rst table.\n        col_len = [max(len(cell) for cell in column) for column in zip(*table)]\n        table_formatstr = '  '.join('=' * cl for cl in col_len)\n        rst_table = '\\n'.join([\n            '',\n            table_formatstr,\n            '  '.join(cell.ljust(cl) for cell, cl in zip(table[0], col_len)),\n            table_formatstr,\n            *['  '.join(cell.ljust(cl) for cell, cl in zip(row, col_len))\n              for row in table[1:]],\n            table_formatstr,\n            '',\n        ])\n        return textwrap.indent(rst_table, prefix=' ' * 4)\n\n    @classmethod\n    def register(cls, name, style):\n        \"\"\"Register a new style.\"\"\"\n        if not issubclass(style, cls._Base):\n            raise ValueError(\"%s must be a subclass of %s\" % (style,\n                                                              cls._Base))\n        cls._style_list[name] = style\n\n\ndef _register_style(style_list, cls=None, *, name=None):\n    \"\"\"Class decorator that stashes a class in a (style) dictionary.\"\"\"\n    if cls is None:\n        return functools.partial(_register_style, style_list, name=name)\n    style_list[name or cls.__name__.lower()] = cls\n    return cls\n\n\nclass BoxStyle(_Style):\n    \"\"\"\n    `BoxStyle` is a container class which defines several\n    boxstyle classes, which are used for `FancyBboxPatch`.\n\n    A style object can be created as::\n\n           BoxStyle.Round(pad=0.2)\n\n    or::\n\n           BoxStyle(\"Round\", pad=0.2)\n\n    or::\n\n           BoxStyle(\"Round, pad=0.2\")\n\n    The following boxstyle classes are defined.\n\n    %(AvailableBoxstyles)s\n\n    An instance of a boxstyle class is a callable object, with the signature ::\n\n       __call__(self, x0, y0, width, height, mutation_size) -> Path\n\n    *x0*, *y0*, *width* and *height* specify the location and size of the box\n    to be drawn; *mutation_size* scales the outline properties such as padding.\n    \"\"\"\n\n    _style_list = {}\n\n    @_register_style(_style_list)\n    class Square:\n        \"\"\"A square box.\"\"\"\n\n        def __init__(self, pad=0.3):\n            \"\"\"\n            Parameters\n            ----------\n            pad : float, default: 0.3\n                The amount of padding around the original box.\n            \"\"\"\n            self.pad = pad\n\n        def __call__(self, x0, y0, width, height, mutation_size):\n            pad = mutation_size * self.pad\n            # width and height with padding added.\n            width, height = width + 2 * pad, height + 2 * pad\n            # boundary of the padded box\n            x0, y0 = x0 - pad, y0 - pad\n            x1, y1 = x0 + width, y0 + height\n            return Path._create_closed(\n                [(x0, y0), (x1, y0), (x1, y1), (x0, y1)])\n\n    @_register_style(_style_list)\n    class Circle:\n        \"\"\"A circular box.\"\"\"\n\n        def __init__(self, pad=0.3):\n            \"\"\"\n            Parameters\n            ----------\n            pad : float, default: 0.3\n                The amount of padding around the original box.\n            \"\"\"\n            self.pad = pad\n\n        def __call__(self, x0, y0, width, height, mutation_size):\n            pad = mutation_size * self.pad\n            width, height = width + 2 * pad, height + 2 * pad\n            # boundary of the padded box\n            x0, y0 = x0 - pad, y0 - pad\n            return Path.circle((x0 + width / 2, y0 + height / 2),\n                               max(width, height) / 2)\n\n    @_register_style(_style_list)\n    class LArrow:\n        \"\"\"A box in the shape of a left-pointing arrow.\"\"\"\n\n        def __init__(self, pad=0.3):\n            \"\"\"\n            Parameters\n            ----------\n            pad : float, default: 0.3\n                The amount of padding around the original box.\n            \"\"\"\n            self.pad = pad\n\n        def __call__(self, x0, y0, width, height, mutation_size):\n            # padding\n            pad = mutation_size * self.pad\n            # width and height with padding added.\n            width, height = width + 2 * pad, height + 2 * pad\n            # boundary of the padded box\n            x0, y0 = x0 - pad, y0 - pad,\n            x1, y1 = x0 + width, y0 + height\n\n            dx = (y1 - y0) / 2\n            dxx = dx / 2\n            x0 = x0 + pad / 1.4  # adjust by ~sqrt(2)\n\n            return Path._create_closed(\n                [(x0 + dxx, y0), (x1, y0), (x1, y1), (x0 + dxx, y1),\n                 (x0 + dxx, y1 + dxx), (x0 - dx, y0 + dx),\n                 (x0 + dxx, y0 - dxx),  # arrow\n                 (x0 + dxx, y0)])\n\n    @_register_style(_style_list)\n    class RArrow(LArrow):\n        \"\"\"A box in the shape of a right-pointing arrow.\"\"\"\n\n        def __call__(self, x0, y0, width, height, mutation_size):\n            p = BoxStyle.LArrow.__call__(\n                self, x0, y0, width, height, mutation_size)\n            p.vertices[:, 0] = 2 * x0 + width - p.vertices[:, 0]\n            return p\n\n    @_register_style(_style_list)\n    class DArrow:\n        \"\"\"A box in the shape of a two-way arrow.\"\"\"\n        # Modified from LArrow to add a right arrow to the bbox.\n\n        def __init__(self, pad=0.3):\n            \"\"\"\n            Parameters\n            ----------\n            pad : float, default: 0.3\n                The amount of padding around the original box.\n            \"\"\"\n            self.pad = pad\n\n        def __call__(self, x0, y0, width, height, mutation_size):\n            # padding\n            pad = mutation_size * self.pad\n            # width and height with padding added.\n            # The width is padded by the arrows, so we don't need to pad it.\n            height = height + 2 * pad\n            # boundary of the padded box\n            x0, y0 = x0 - pad, y0 - pad\n            x1, y1 = x0 + width, y0 + height\n\n            dx = (y1 - y0) / 2\n            dxx = dx / 2\n            x0 = x0 + pad / 1.4  # adjust by ~sqrt(2)\n\n            return Path._create_closed([\n                (x0 + dxx, y0), (x1, y0),  # bot-segment\n                (x1, y0 - dxx), (x1 + dx + dxx, y0 + dx),\n                (x1, y1 + dxx),  # right-arrow\n                (x1, y1), (x0 + dxx, y1),  # top-segment\n                (x0 + dxx, y1 + dxx), (x0 - dx, y0 + dx),\n                (x0 + dxx, y0 - dxx),  # left-arrow\n                (x0 + dxx, y0)])\n\n    @_register_style(_style_list)\n    class Round:\n        \"\"\"A box with round corners.\"\"\"\n\n        def __init__(self, pad=0.3, rounding_size=None):\n            \"\"\"\n            Parameters\n            ----------\n            pad : float, default: 0.3\n                The amount of padding around the original box.\n            rounding_size : float, default: *pad*\n                Radius of the corners.\n            \"\"\"\n            self.pad = pad\n            self.rounding_size = rounding_size\n\n        def __call__(self, x0, y0, width, height, mutation_size):\n\n            # padding\n            pad = mutation_size * self.pad\n\n            # size of the rounding corner\n            if self.rounding_size:\n                dr = mutation_size * self.rounding_size\n            else:\n                dr = pad\n\n            width, height = width + 2 * pad, height + 2 * pad\n\n            x0, y0 = x0 - pad, y0 - pad,\n            x1, y1 = x0 + width, y0 + height\n\n            # Round corners are implemented as quadratic Bezier, e.g.,\n            # [(x0, y0-dr), (x0, y0), (x0+dr, y0)] for lower left corner.\n            cp = [(x0 + dr, y0),\n                  (x1 - dr, y0),\n                  (x1, y0), (x1, y0 + dr),\n                  (x1, y1 - dr),\n                  (x1, y1), (x1 - dr, y1),\n                  (x0 + dr, y1),\n                  (x0, y1), (x0, y1 - dr),\n                  (x0, y0 + dr),\n                  (x0, y0), (x0 + dr, y0),\n                  (x0 + dr, y0)]\n\n            com = [Path.MOVETO,\n                   Path.LINETO,\n                   Path.CURVE3, Path.CURVE3,\n                   Path.LINETO,\n                   Path.CURVE3, Path.CURVE3,\n                   Path.LINETO,\n                   Path.CURVE3, Path.CURVE3,\n                   Path.LINETO,\n                   Path.CURVE3, Path.CURVE3,\n                   Path.CLOSEPOLY]\n\n            path = Path(cp, com)\n\n            return path\n\n    @_register_style(_style_list)\n    class Round4:\n        \"\"\"A box with rounded edges.\"\"\"\n\n        def __init__(self, pad=0.3, rounding_size=None):\n            \"\"\"\n            Parameters\n            ----------\n            pad : float, default: 0.3\n                The amount of padding around the original box.\n            rounding_size : float, default: *pad*/2\n                Rounding of edges.\n            \"\"\"\n            self.pad = pad\n            self.rounding_size = rounding_size\n\n        def __call__(self, x0, y0, width, height, mutation_size):\n\n            # padding\n            pad = mutation_size * self.pad\n\n            # Rounding size; defaults to half of the padding.\n            if self.rounding_size:\n                dr = mutation_size * self.rounding_size\n            else:\n                dr = pad / 2.\n\n            width = width + 2 * pad - 2 * dr\n            height = height + 2 * pad - 2 * dr\n\n            x0, y0 = x0 - pad + dr, y0 - pad + dr,\n            x1, y1 = x0 + width, y0 + height\n\n            cp = [(x0, y0),\n                  (x0 + dr, y0 - dr), (x1 - dr, y0 - dr), (x1, y0),\n                  (x1 + dr, y0 + dr), (x1 + dr, y1 - dr), (x1, y1),\n                  (x1 - dr, y1 + dr), (x0 + dr, y1 + dr), (x0, y1),\n                  (x0 - dr, y1 - dr), (x0 - dr, y0 + dr), (x0, y0),\n                  (x0, y0)]\n\n            com = [Path.MOVETO,\n                   Path.CURVE4, Path.CURVE4, Path.CURVE4,\n                   Path.CURVE4, Path.CURVE4, Path.CURVE4,\n                   Path.CURVE4, Path.CURVE4, Path.CURVE4,\n                   Path.CURVE4, Path.CURVE4, Path.CURVE4,\n                   Path.CLOSEPOLY]\n\n            path = Path(cp, com)\n\n            return path\n\n    @_register_style(_style_list)\n    class Sawtooth:\n        \"\"\"A box with a sawtooth outline.\"\"\"\n\n        def __init__(self, pad=0.3, tooth_size=None):\n            \"\"\"\n            Parameters\n            ----------\n            pad : float, default: 0.3\n                The amount of padding around the original box.\n            tooth_size : float, default: *pad*/2\n                Size of the sawtooth.\n            \"\"\"\n            self.pad = pad\n            self.tooth_size = tooth_size\n\n        def _get_sawtooth_vertices(self, x0, y0, width, height, mutation_size):\n\n            # padding\n            pad = mutation_size * self.pad\n\n            # size of sawtooth\n            if self.tooth_size is None:\n                tooth_size = self.pad * .5 * mutation_size\n            else:\n                tooth_size = self.tooth_size * mutation_size\n\n            tooth_size2 = tooth_size / 2\n            width = width + 2 * pad - tooth_size\n            height = height + 2 * pad - tooth_size\n\n            # the sizes of the vertical and horizontal sawtooth are\n            # separately adjusted to fit the given box size.\n            dsx_n = int(round((width - tooth_size) / (tooth_size * 2))) * 2\n            dsx = (width - tooth_size) / dsx_n\n            dsy_n = int(round((height - tooth_size) / (tooth_size * 2))) * 2\n            dsy = (height - tooth_size) / dsy_n\n\n            x0, y0 = x0 - pad + tooth_size2, y0 - pad + tooth_size2\n            x1, y1 = x0 + width, y0 + height\n\n            bottom_saw_x = [\n                x0,\n                *(x0 + tooth_size2 + dsx * .5 * np.arange(dsx_n * 2)),\n                x1 - tooth_size2,\n            ]\n            bottom_saw_y = [\n                y0,\n                *([y0 - tooth_size2, y0, y0 + tooth_size2, y0] * dsx_n),\n                y0 - tooth_size2,\n            ]\n            right_saw_x = [\n                x1,\n                *([x1 + tooth_size2, x1, x1 - tooth_size2, x1] * dsx_n),\n                x1 + tooth_size2,\n            ]\n            right_saw_y = [\n                y0,\n                *(y0 + tooth_size2 + dsy * .5 * np.arange(dsy_n * 2)),\n                y1 - tooth_size2,\n            ]\n            top_saw_x = [\n                x1,\n                *(x1 - tooth_size2 - dsx * .5 * np.arange(dsx_n * 2)),\n                x0 + tooth_size2,\n            ]\n            top_saw_y = [\n                y1,\n                *([y1 + tooth_size2, y1, y1 - tooth_size2, y1] * dsx_n),\n                y1 + tooth_size2,\n            ]\n            left_saw_x = [\n                x0,\n                *([x0 - tooth_size2, x0, x0 + tooth_size2, x0] * dsy_n),\n                x0 - tooth_size2,\n            ]\n            left_saw_y = [\n                y1,\n                *(y1 - tooth_size2 - dsy * .5 * np.arange(dsy_n * 2)),\n                y0 + tooth_size2,\n            ]\n\n            saw_vertices = [*zip(bottom_saw_x, bottom_saw_y),\n                            *zip(right_saw_x, right_saw_y),\n                            *zip(top_saw_x, top_saw_y),\n                            *zip(left_saw_x, left_saw_y),\n                            (bottom_saw_x[0], bottom_saw_y[0])]\n\n            return saw_vertices\n\n        def __call__(self, x0, y0, width, height, mutation_size):\n            saw_vertices = self._get_sawtooth_vertices(x0, y0, width,\n                                                       height, mutation_size)\n            path = Path(saw_vertices, closed=True)\n            return path\n\n    @_register_style(_style_list)\n    class Roundtooth(Sawtooth):\n        \"\"\"A box with a rounded sawtooth outline.\"\"\"\n\n        def __call__(self, x0, y0, width, height, mutation_size):\n            saw_vertices = self._get_sawtooth_vertices(x0, y0,\n                                                       width, height,\n                                                       mutation_size)\n            # Add a trailing vertex to allow us to close the polygon correctly\n            saw_vertices = np.concatenate([saw_vertices, [saw_vertices[0]]])\n            codes = ([Path.MOVETO] +\n                     [Path.CURVE3, Path.CURVE3] * ((len(saw_vertices)-1)//2) +\n                     [Path.CLOSEPOLY])\n            return Path(saw_vertices, codes)\n\n\nclass ConnectionStyle(_Style):\n    \"\"\"\n    `ConnectionStyle` is a container class which defines\n    several connectionstyle classes, which is used to create a path\n    between two points.  These are mainly used with `FancyArrowPatch`.\n\n    A connectionstyle object can be either created as::\n\n           ConnectionStyle.Arc3(rad=0.2)\n\n    or::\n\n           ConnectionStyle(\"Arc3\", rad=0.2)\n\n    or::\n\n           ConnectionStyle(\"Arc3, rad=0.2\")\n\n    The following classes are defined\n\n    %(AvailableConnectorstyles)s\n\n    An instance of any connection style class is an callable object,\n    whose call signature is::\n\n        __call__(self, posA, posB,\n                 patchA=None, patchB=None,\n                 shrinkA=2., shrinkB=2.)\n\n    and it returns a `.Path` instance. *posA* and *posB* are\n    tuples of (x, y) coordinates of the two points to be\n    connected. *patchA* (or *patchB*) is given, the returned path is\n    clipped so that it start (or end) from the boundary of the\n    patch. The path is further shrunk by *shrinkA* (or *shrinkB*)\n    which is given in points.\n    \"\"\"\n\n    _style_list = {}\n\n    class _Base:\n        \"\"\"\n        A base class for connectionstyle classes. The subclass needs\n        to implement a *connect* method whose call signature is::\n\n          connect(posA, posB)\n\n        where posA and posB are tuples of x, y coordinates to be\n        connected.  The method needs to return a path connecting two\n        points. This base class defines a __call__ method, and a few\n        helper methods.\n        \"\"\"\n\n        class SimpleEvent:\n            def __init__(self, xy):\n                self.x, self.y = xy\n\n        def _clip(self, path, patchA, patchB):\n            \"\"\"\n            Clip the path to the boundary of the patchA and patchB.\n            The starting point of the path needed to be inside of the\n            patchA and the end point inside the patch B. The *contains*\n            methods of each patch object is utilized to test if the point\n            is inside the path.\n            \"\"\"\n\n            if patchA:\n                def insideA(xy_display):\n                    xy_event = ConnectionStyle._Base.SimpleEvent(xy_display)\n                    return patchA.contains(xy_event)[0]\n\n                try:\n                    left, right = split_path_inout(path, insideA)\n                except ValueError:\n                    right = path\n\n                path = right\n\n            if patchB:\n                def insideB(xy_display):\n                    xy_event = ConnectionStyle._Base.SimpleEvent(xy_display)\n                    return patchB.contains(xy_event)[0]\n\n                try:\n                    left, right = split_path_inout(path, insideB)\n                except ValueError:\n                    left = path\n\n                path = left\n\n            return path\n\n        def _shrink(self, path, shrinkA, shrinkB):\n            \"\"\"\n            Shrink the path by fixed size (in points) with shrinkA and shrinkB.\n            \"\"\"\n            if shrinkA:\n                insideA = inside_circle(*path.vertices[0], shrinkA)\n                try:\n                    left, path = split_path_inout(path, insideA)\n                except ValueError:\n                    pass\n            if shrinkB:\n                insideB = inside_circle(*path.vertices[-1], shrinkB)\n                try:\n                    path, right = split_path_inout(path, insideB)\n                except ValueError:\n                    pass\n            return path\n\n        def __call__(self, posA, posB,\n                     shrinkA=2., shrinkB=2., patchA=None, patchB=None):\n            \"\"\"\n            Call the *connect* method to create a path between *posA* and\n            *posB*; then clip and shrink the path.\n            \"\"\"\n            path = self.connect(posA, posB)\n            clipped_path = self._clip(path, patchA, patchB)\n            shrunk_path = self._shrink(clipped_path, shrinkA, shrinkB)\n            return shrunk_path\n\n    @_register_style(_style_list)\n    class Arc3(_Base):\n        \"\"\"\n        Creates a simple quadratic Bezier curve between two\n        points. The curve is created so that the middle control point\n        (C1) is located at the same distance from the start (C0) and\n        end points(C2) and the distance of the C1 to the line\n        connecting C0-C2 is *rad* times the distance of C0-C2.\n        \"\"\"\n\n        def __init__(self, rad=0.):\n            \"\"\"\n            *rad*\n              curvature of the curve.\n            \"\"\"\n            self.rad = rad\n\n        def connect(self, posA, posB):\n            x1, y1 = posA\n            x2, y2 = posB\n            x12, y12 = (x1 + x2) / 2., (y1 + y2) / 2.\n            dx, dy = x2 - x1, y2 - y1\n\n            f = self.rad\n\n            cx, cy = x12 + f * dy, y12 - f * dx\n\n            vertices = [(x1, y1),\n                        (cx, cy),\n                        (x2, y2)]\n            codes = [Path.MOVETO,\n                     Path.CURVE3,\n                     Path.CURVE3]\n\n            return Path(vertices, codes)\n\n    @_register_style(_style_list)\n    class Angle3(_Base):\n        \"\"\"\n        Creates a simple quadratic Bezier curve between two\n        points. The middle control points is placed at the\n        intersecting point of two lines which cross the start and\n        end point, and have a slope of angleA and angleB, respectively.\n        \"\"\"\n\n        def __init__(self, angleA=90, angleB=0):\n            \"\"\"\n            *angleA*\n              starting angle of the path\n\n            *angleB*\n              ending angle of the path\n            \"\"\"\n\n            self.angleA = angleA\n            self.angleB = angleB\n\n        def connect(self, posA, posB):\n            x1, y1 = posA\n            x2, y2 = posB\n\n            cosA = math.cos(math.radians(self.angleA))\n            sinA = math.sin(math.radians(self.angleA))\n            cosB = math.cos(math.radians(self.angleB))\n            sinB = math.sin(math.radians(self.angleB))\n\n            cx, cy = get_intersection(x1, y1, cosA, sinA,\n                                      x2, y2, cosB, sinB)\n\n            vertices = [(x1, y1), (cx, cy), (x2, y2)]\n            codes = [Path.MOVETO, Path.CURVE3, Path.CURVE3]\n\n            return Path(vertices, codes)\n\n    @_register_style(_style_list)\n    class Angle(_Base):\n        \"\"\"\n        Creates a piecewise continuous quadratic Bezier path between\n        two points. The path has a one passing-through point placed at\n        the intersecting point of two lines which cross the start\n        and end point, and have a slope of angleA and angleB, respectively.\n        The connecting edges are rounded with *rad*.\n        \"\"\"\n\n        def __init__(self, angleA=90, angleB=0, rad=0.):\n            \"\"\"\n            *angleA*\n              starting angle of the path\n\n            *angleB*\n              ending angle of the path\n\n            *rad*\n              rounding radius of the edge\n            \"\"\"\n\n            self.angleA = angleA\n            self.angleB = angleB\n\n            self.rad = rad\n\n        def connect(self, posA, posB):\n            x1, y1 = posA\n            x2, y2 = posB\n\n            cosA = math.cos(math.radians(self.angleA))\n            sinA = math.sin(math.radians(self.angleA))\n            cosB = math.cos(math.radians(self.angleB))\n            sinB = math.sin(math.radians(self.angleB))\n\n            cx, cy = get_intersection(x1, y1, cosA, sinA,\n                                      x2, y2, cosB, sinB)\n\n            vertices = [(x1, y1)]\n            codes = [Path.MOVETO]\n\n            if self.rad == 0.:\n                vertices.append((cx, cy))\n                codes.append(Path.LINETO)\n            else:\n                dx1, dy1 = x1 - cx, y1 - cy\n                d1 = np.hypot(dx1, dy1)\n                f1 = self.rad / d1\n                dx2, dy2 = x2 - cx, y2 - cy\n                d2 = np.hypot(dx2, dy2)\n                f2 = self.rad / d2\n                vertices.extend([(cx + dx1 * f1, cy + dy1 * f1),\n                                 (cx, cy),\n                                 (cx + dx2 * f2, cy + dy2 * f2)])\n                codes.extend([Path.LINETO, Path.CURVE3, Path.CURVE3])\n\n            vertices.append((x2, y2))\n            codes.append(Path.LINETO)\n\n            return Path(vertices, codes)\n\n    @_register_style(_style_list)\n    class Arc(_Base):\n        \"\"\"\n        Creates a piecewise continuous quadratic Bezier path between\n        two points. The path can have two passing-through points, a\n        point placed at the distance of armA and angle of angleA from\n        point A, another point with respect to point B. The edges are\n        rounded with *rad*.\n        \"\"\"\n\n        def __init__(self, angleA=0, angleB=0, armA=None, armB=None, rad=0.):\n            \"\"\"\n            *angleA* :\n              starting angle of the path\n\n            *angleB* :\n              ending angle of the path\n\n            *armA* :\n              length of the starting arm\n\n            *armB* :\n              length of the ending arm\n\n            *rad* :\n              rounding radius of the edges\n            \"\"\"\n\n            self.angleA = angleA\n            self.angleB = angleB\n            self.armA = armA\n            self.armB = armB\n\n            self.rad = rad\n\n        def connect(self, posA, posB):\n            x1, y1 = posA\n            x2, y2 = posB\n\n            vertices = [(x1, y1)]\n            rounded = []\n            codes = [Path.MOVETO]\n\n            if self.armA:\n                cosA = math.cos(math.radians(self.angleA))\n                sinA = math.sin(math.radians(self.angleA))\n                # x_armA, y_armB\n                d = self.armA - self.rad\n                rounded.append((x1 + d * cosA, y1 + d * sinA))\n                d = self.armA\n                rounded.append((x1 + d * cosA, y1 + d * sinA))\n\n            if self.armB:\n                cosB = math.cos(math.radians(self.angleB))\n                sinB = math.sin(math.radians(self.angleB))\n                x_armB, y_armB = x2 + self.armB * cosB, y2 + self.armB * sinB\n\n                if rounded:\n                    xp, yp = rounded[-1]\n                    dx, dy = x_armB - xp, y_armB - yp\n                    dd = (dx * dx + dy * dy) ** .5\n\n                    rounded.append((xp + self.rad * dx / dd,\n                                    yp + self.rad * dy / dd))\n                    vertices.extend(rounded)\n                    codes.extend([Path.LINETO,\n                                  Path.CURVE3,\n                                  Path.CURVE3])\n                else:\n                    xp, yp = vertices[-1]\n                    dx, dy = x_armB - xp, y_armB - yp\n                    dd = (dx * dx + dy * dy) ** .5\n\n                d = dd - self.rad\n                rounded = [(xp + d * dx / dd, yp + d * dy / dd),\n                           (x_armB, y_armB)]\n\n            if rounded:\n                xp, yp = rounded[-1]\n                dx, dy = x2 - xp, y2 - yp\n                dd = (dx * dx + dy * dy) ** .5\n\n                rounded.append((xp + self.rad * dx / dd,\n                                yp + self.rad * dy / dd))\n                vertices.extend(rounded)\n                codes.extend([Path.LINETO,\n                              Path.CURVE3,\n                              Path.CURVE3])\n\n            vertices.append((x2, y2))\n            codes.append(Path.LINETO)\n\n            return Path(vertices, codes)\n\n    @_register_style(_style_list)\n    class Bar(_Base):\n        \"\"\"\n        A line with *angle* between A and B with *armA* and\n        *armB*. One of the arms is extended so that they are connected in\n        a right angle. The length of armA is determined by (*armA*\n        + *fraction* x AB distance). Same for armB.\n        \"\"\"\n\n        def __init__(self, armA=0., armB=0., fraction=0.3, angle=None):\n            \"\"\"\n            Parameters\n            ----------\n            armA : float\n                minimum length of armA\n\n            armB : float\n                minimum length of armB\n\n            fraction : float\n                a fraction of the distance between two points that\n                will be added to armA and armB.\n\n            angle : float or None\n                angle of the connecting line (if None, parallel\n                to A and B)\n            \"\"\"\n            self.armA = armA\n            self.armB = armB\n            self.fraction = fraction\n            self.angle = angle\n\n        def connect(self, posA, posB):\n            x1, y1 = posA\n            x20, y20 = x2, y2 = posB\n\n            theta1 = math.atan2(y2 - y1, x2 - x1)\n            dx, dy = x2 - x1, y2 - y1\n            dd = (dx * dx + dy * dy) ** .5\n            ddx, ddy = dx / dd, dy / dd\n\n            armA, armB = self.armA, self.armB\n\n            if self.angle is not None:\n                theta0 = np.deg2rad(self.angle)\n                dtheta = theta1 - theta0\n                dl = dd * math.sin(dtheta)\n                dL = dd * math.cos(dtheta)\n                x2, y2 = x1 + dL * math.cos(theta0), y1 + dL * math.sin(theta0)\n                armB = armB - dl\n\n                # update\n                dx, dy = x2 - x1, y2 - y1\n                dd2 = (dx * dx + dy * dy) ** .5\n                ddx, ddy = dx / dd2, dy / dd2\n\n            arm = max(armA, armB)\n            f = self.fraction * dd + arm\n\n            cx1, cy1 = x1 + f * ddy, y1 - f * ddx\n            cx2, cy2 = x2 + f * ddy, y2 - f * ddx\n\n            vertices = [(x1, y1),\n                        (cx1, cy1),\n                        (cx2, cy2),\n                        (x20, y20)]\n            codes = [Path.MOVETO,\n                     Path.LINETO,\n                     Path.LINETO,\n                     Path.LINETO]\n\n            return Path(vertices, codes)\n\n\ndef _point_along_a_line(x0, y0, x1, y1, d):\n    \"\"\"\n    Return the point on the line connecting (*x0*, *y0*) -- (*x1*, *y1*) whose\n    distance from (*x0*, *y0*) is *d*.\n    \"\"\"\n    dx, dy = x0 - x1, y0 - y1\n    ff = d / (dx * dx + dy * dy) ** .5\n    x2, y2 = x0 - ff * dx, y0 - ff * dy\n\n    return x2, y2\n\n\nclass ArrowStyle(_Style):\n    \"\"\"\n    `ArrowStyle` is a container class which defines several\n    arrowstyle classes, which is used to create an arrow path along a\n    given path.  These are mainly used with `FancyArrowPatch`.\n\n    A arrowstyle object can be either created as::\n\n           ArrowStyle.Fancy(head_length=.4, head_width=.4, tail_width=.4)\n\n    or::\n\n           ArrowStyle(\"Fancy\", head_length=.4, head_width=.4, tail_width=.4)\n\n    or::\n\n           ArrowStyle(\"Fancy, head_length=.4, head_width=.4, tail_width=.4\")\n\n    The following classes are defined\n\n    %(AvailableArrowstyles)s\n\n    An instance of any arrow style class is a callable object,\n    whose call signature is::\n\n        __call__(self, path, mutation_size, linewidth, aspect_ratio=1.)\n\n    and it returns a tuple of a `.Path` instance and a boolean\n    value. *path* is a `.Path` instance along which the arrow\n    will be drawn. *mutation_size* and *aspect_ratio* have the same\n    meaning as in `BoxStyle`. *linewidth* is a line width to be\n    stroked. This is meant to be used to correct the location of the\n    head so that it does not overshoot the destination point, but not all\n    classes support it.\n    \"\"\"\n\n    _style_list = {}\n\n    class _Base:\n        \"\"\"\n        Arrow Transmuter Base class\n\n        ArrowTransmuterBase and its derivatives are used to make a fancy\n        arrow around a given path. The __call__ method returns a path\n        (which will be used to create a PathPatch instance) and a boolean\n        value indicating the path is open therefore is not fillable.  This\n        class is not an artist and actual drawing of the fancy arrow is\n        done by the FancyArrowPatch class.\n        \"\"\"\n\n        # The derived classes are required to be able to be initialized\n        # w/o arguments, i.e., all its argument (except self) must have\n        # the default values.\n\n        @staticmethod\n        def ensure_quadratic_bezier(path):\n            \"\"\"\n            Some ArrowStyle classes only works with a simple quadratic\n            Bezier curve (created with `.ConnectionStyle.Arc3` or\n            `.ConnectionStyle.Angle3`). This static method checks if the\n            provided path is a simple quadratic Bezier curve and returns its\n            control points if true.\n            \"\"\"\n            segments = list(path.iter_segments())\n            if (len(segments) != 2 or segments[0][1] != Path.MOVETO or\n                    segments[1][1] != Path.CURVE3):\n                raise ValueError(\n                    \"'path' is not a valid quadratic Bezier curve\")\n            return [*segments[0][0], *segments[1][0]]\n\n        def transmute(self, path, mutation_size, linewidth):\n            \"\"\"\n            The transmute method is the very core of the ArrowStyle class and\n            must be overridden in the subclasses. It receives the path object\n            along which the arrow will be drawn, and the mutation_size, with\n            which the arrow head etc. will be scaled. The linewidth may be\n            used to adjust the path so that it does not pass beyond the given\n            points. It returns a tuple of a Path instance and a boolean. The\n            boolean value indicate whether the path can be filled or not. The\n            return value can also be a list of paths and list of booleans of a\n            same length.\n            \"\"\"\n            raise NotImplementedError('Derived must override')\n\n        def __call__(self, path, mutation_size, linewidth,\n                     aspect_ratio=1.):\n            \"\"\"\n            The __call__ method is a thin wrapper around the transmute method\n            and takes care of the aspect ratio.\n            \"\"\"\n\n            if aspect_ratio is not None:\n                # Squeeze the given height by the aspect_ratio\n                vertices = path.vertices / [1, aspect_ratio]\n                path_shrunk = Path(vertices, path.codes)\n                # call transmute method with squeezed height.\n                path_mutated, fillable = self.transmute(path_shrunk,\n                                                        mutation_size,\n                                                        linewidth)\n                if np.iterable(fillable):\n                    # Restore the height\n                    path_list = [Path(p.vertices * [1, aspect_ratio], p.codes)\n                                 for p in path_mutated]\n                    return path_list, fillable\n                else:\n                    return path_mutated, fillable\n            else:\n                return self.transmute(path, mutation_size, linewidth)\n\n    class _Curve(_Base):\n        \"\"\"\n        A simple arrow which will work with any path instance. The\n        returned path is the concatenation of the original path, and at\n        most two paths representing the arrow head or bracket at the begin\n        point and at the end point. The arrow heads can be either open\n        or closed.\n        \"\"\"\n\n        beginarrow = endarrow = None  # Whether arrows are drawn.\n        arrow = \"-\"\n        fillbegin = fillend = False  # Whether arrows are filled.\n\n        def __init__(self, head_length=.4, head_width=.2, widthA=1., widthB=1.,\n                     lengthA=0.2, lengthB=0.2, angleA=0, angleB=0, scaleA=None,\n                     scaleB=None):\n            \"\"\"\n            Parameters\n            ----------\n            head_length : float, default: 0.4\n                Length of the arrow head, relative to *mutation_scale*.\n            head_width : float, default: 0.2\n                Width of the arrow head, relative to *mutation_scale*.\n            widthA : float, default: 1.0\n                Width of the bracket at the beginning of the arrow\n            widthB : float, default: 1.0\n                Width of the bracket at the end of the arrow\n            lengthA : float, default: 0.2\n                Length of the bracket at the beginning of the arrow\n            lengthB : float, default: 0.2\n                Length of the bracket at the end of the arrow\n            angleA : float, default 0\n                Orientation of the bracket at the beginning, as a\n                counterclockwise angle. 0 degrees means perpendicular\n                to the line.\n            angleB : float, default 0\n                Orientation of the bracket at the beginning, as a\n                counterclockwise angle. 0 degrees means perpendicular\n                to the line.\n            scaleA : float, default *mutation_size*\n                The mutation_size for the beginning bracket\n            scaleB : float, default *mutation_size*\n                The mutation_size for the end bracket\n            \"\"\"\n\n            self.head_length, self.head_width = head_length, head_width\n            self.widthA, self.widthB = widthA, widthB\n            self.lengthA, self.lengthB = lengthA, lengthB\n            self.angleA, self.angleB = angleA, angleB\n            self.scaleA, self.scaleB = scaleA, scaleB\n\n            self._beginarrow_head = False\n            self._beginarrow_bracket = False\n            self._endarrow_head = False\n            self._endarrow_bracket = False\n\n            if \"-\" not in self.arrow:\n                raise ValueError(\"arrow must have the '-' between \"\n                                 \"the two heads\")\n\n            beginarrow, endarrow = self.arrow.split(\"-\", 1)\n\n            if beginarrow == \"<\":\n                self._beginarrow_head = True\n                self._beginarrow_bracket = False\n            elif beginarrow == \"<|\":\n                self._beginarrow_head = True\n                self._beginarrow_bracket = False\n                self.fillbegin = True\n            elif beginarrow in (\"]\", \"|\"):\n                self._beginarrow_head = False\n                self._beginarrow_bracket = True\n            elif self.beginarrow is True:\n                self._beginarrow_head = True\n                self._beginarrow_bracket = False\n\n                _api.warn_deprecated('3.5', name=\"beginarrow\",\n                                     alternative=\"arrow\")\n            elif self.beginarrow is False:\n                self._beginarrow_head = False\n                self._beginarrow_bracket = False\n\n                _api.warn_deprecated('3.5', name=\"beginarrow\",\n                                     alternative=\"arrow\")\n\n            if endarrow == \">\":\n                self._endarrow_head = True\n                self._endarrow_bracket = False\n            elif endarrow == \"|>\":\n                self._endarrow_head = True\n                self._endarrow_bracket = False\n                self.fillend = True\n            elif endarrow in (\"[\", \"|\"):\n                self._endarrow_head = False\n                self._endarrow_bracket = True\n            elif self.endarrow is True:\n                self._endarrow_head = True\n                self._endarrow_bracket = False\n\n                _api.warn_deprecated('3.5', name=\"endarrow\",\n                                     alternative=\"arrow\")\n            elif self.endarrow is False:\n                self._endarrow_head = False\n                self._endarrow_bracket = False\n\n                _api.warn_deprecated('3.5', name=\"endarrow\",\n                                     alternative=\"arrow\")\n\n            super().__init__()\n\n        def _get_arrow_wedge(self, x0, y0, x1, y1,\n                             head_dist, cos_t, sin_t, linewidth):\n            \"\"\"\n            Return the paths for arrow heads. Since arrow lines are\n            drawn with capstyle=projected, The arrow goes beyond the\n            desired point. This method also returns the amount of the path\n            to be shrunken so that it does not overshoot.\n            \"\"\"\n\n            # arrow from x0, y0 to x1, y1\n            dx, dy = x0 - x1, y0 - y1\n\n            cp_distance = np.hypot(dx, dy)\n\n            # pad_projected : amount of pad to account the\n            # overshooting of the projection of the wedge\n            pad_projected = (.5 * linewidth / sin_t)\n\n            # Account for division by zero\n            if cp_distance == 0:\n                cp_distance = 1\n\n            # apply pad for projected edge\n            ddx = pad_projected * dx / cp_distance\n            ddy = pad_projected * dy / cp_distance\n\n            # offset for arrow wedge\n            dx = dx / cp_distance * head_dist\n            dy = dy / cp_distance * head_dist\n\n            dx1, dy1 = cos_t * dx + sin_t * dy, -sin_t * dx + cos_t * dy\n            dx2, dy2 = cos_t * dx - sin_t * dy, sin_t * dx + cos_t * dy\n\n            vertices_arrow = [(x1 + ddx + dx1, y1 + ddy + dy1),\n                              (x1 + ddx, y1 + ddy),\n                              (x1 + ddx + dx2, y1 + ddy + dy2)]\n            codes_arrow = [Path.MOVETO,\n                           Path.LINETO,\n                           Path.LINETO]\n\n            return vertices_arrow, codes_arrow, ddx, ddy\n\n        def _get_bracket(self, x0, y0,\n                         x1, y1, width, length, angle):\n\n            cos_t, sin_t = get_cos_sin(x1, y1, x0, y0)\n\n            # arrow from x0, y0 to x1, y1\n            from matplotlib.bezier import get_normal_points\n            x1, y1, x2, y2 = get_normal_points(x0, y0, cos_t, sin_t, width)\n\n            dx, dy = length * cos_t, length * sin_t\n\n            vertices_arrow = [(x1 + dx, y1 + dy),\n                              (x1, y1),\n                              (x2, y2),\n                              (x2 + dx, y2 + dy)]\n            codes_arrow = [Path.MOVETO,\n                           Path.LINETO,\n                           Path.LINETO,\n                           Path.LINETO]\n\n            if angle:\n                trans = transforms.Affine2D().rotate_deg_around(x0, y0, angle)\n                vertices_arrow = trans.transform(vertices_arrow)\n\n            return vertices_arrow, codes_arrow\n\n        def transmute(self, path, mutation_size, linewidth):\n\n            if self._beginarrow_head or self._endarrow_head:\n                head_length = self.head_length * mutation_size\n                head_width = self.head_width * mutation_size\n                head_dist = np.hypot(head_length, head_width)\n                cos_t, sin_t = head_length / head_dist, head_width / head_dist\n\n            scaleA = mutation_size if self.scaleA is None else self.scaleA\n            scaleB = mutation_size if self.scaleB is None else self.scaleB\n\n            # begin arrow\n            x0, y0 = path.vertices[0]\n            x1, y1 = path.vertices[1]\n\n            # If there is no room for an arrow and a line, then skip the arrow\n            has_begin_arrow = self._beginarrow_head and (x0, y0) != (x1, y1)\n            verticesA, codesA, ddxA, ddyA = (\n                self._get_arrow_wedge(x1, y1, x0, y0,\n                                      head_dist, cos_t, sin_t, linewidth)\n                if has_begin_arrow\n                else ([], [], 0, 0)\n            )\n\n            # end arrow\n            x2, y2 = path.vertices[-2]\n            x3, y3 = path.vertices[-1]\n\n            # If there is no room for an arrow and a line, then skip the arrow\n            has_end_arrow = self._endarrow_head and (x2, y2) != (x3, y3)\n            verticesB, codesB, ddxB, ddyB = (\n                self._get_arrow_wedge(x2, y2, x3, y3,\n                                      head_dist, cos_t, sin_t, linewidth)\n                if has_end_arrow\n                else ([], [], 0, 0)\n            )\n\n            # This simple code will not work if ddx, ddy is greater than the\n            # separation between vertices.\n            _path = [Path(np.concatenate([[(x0 + ddxA, y0 + ddyA)],\n                                          path.vertices[1:-1],\n                                          [(x3 + ddxB, y3 + ddyB)]]),\n                          path.codes)]\n            _fillable = [False]\n\n            if has_begin_arrow:\n                if self.fillbegin:\n                    p = np.concatenate([verticesA, [verticesA[0],\n                                                    verticesA[0]], ])\n                    c = np.concatenate([codesA, [Path.LINETO, Path.CLOSEPOLY]])\n                    _path.append(Path(p, c))\n                    _fillable.append(True)\n                else:\n                    _path.append(Path(verticesA, codesA))\n                    _fillable.append(False)\n            elif self._beginarrow_bracket:\n                x0, y0 = path.vertices[0]\n                x1, y1 = path.vertices[1]\n                verticesA, codesA = self._get_bracket(x0, y0, x1, y1,\n                                                      self.widthA * scaleA,\n                                                      self.lengthA * scaleA,\n                                                      self.angleA)\n\n                _path.append(Path(verticesA, codesA))\n                _fillable.append(False)\n\n            if has_end_arrow:\n                if self.fillend:\n                    _fillable.append(True)\n                    p = np.concatenate([verticesB, [verticesB[0],\n                                                    verticesB[0]], ])\n                    c = np.concatenate([codesB, [Path.LINETO, Path.CLOSEPOLY]])\n                    _path.append(Path(p, c))\n                else:\n                    _fillable.append(False)\n                    _path.append(Path(verticesB, codesB))\n            elif self._endarrow_bracket:\n                x0, y0 = path.vertices[-1]\n                x1, y1 = path.vertices[-2]\n                verticesB, codesB = self._get_bracket(x0, y0, x1, y1,\n                                                      self.widthB * scaleB,\n                                                      self.lengthB * scaleB,\n                                                      self.angleB)\n\n                _path.append(Path(verticesB, codesB))\n                _fillable.append(False)\n\n            return _path, _fillable\n\n    @_register_style(_style_list, name=\"-\")\n    class Curve(_Curve):\n        \"\"\"A simple curve without any arrow head.\"\"\"\n\n        def __init__(self):  # hide head_length, head_width\n            # These attributes (whose values come from backcompat) only matter\n            # if someone modifies beginarrow/etc. on an ArrowStyle instance.\n            super().__init__(head_length=.2, head_width=.1)\n\n    @_register_style(_style_list, name=\"<-\")\n    class CurveA(_Curve):\n        \"\"\"An arrow with a head at its begin point.\"\"\"\n        arrow = \"<-\"\n\n    @_register_style(_style_list, name=\"->\")\n    class CurveB(_Curve):\n        \"\"\"An arrow with a head at its end point.\"\"\"\n        arrow = \"->\"\n\n    @_register_style(_style_list, name=\"<->\")\n    class CurveAB(_Curve):\n        \"\"\"An arrow with heads both at the begin and the end point.\"\"\"\n        arrow = \"<->\"\n\n    @_register_style(_style_list, name=\"<|-\")\n    class CurveFilledA(_Curve):\n        \"\"\"An arrow with filled triangle head at the begin.\"\"\"\n        arrow = \"<|-\"\n\n    @_register_style(_style_list, name=\"-|>\")\n    class CurveFilledB(_Curve):\n        \"\"\"An arrow with filled triangle head at the end.\"\"\"\n        arrow = \"-|>\"\n\n    @_register_style(_style_list, name=\"<|-|>\")\n    class CurveFilledAB(_Curve):\n        \"\"\"An arrow with filled triangle heads at both ends.\"\"\"\n        arrow = \"<|-|>\"\n\n    @_register_style(_style_list, name=\"]-\")\n    class BracketA(_Curve):\n        \"\"\"An arrow with an outward square bracket at its start.\"\"\"\n        arrow = \"]-\"\n\n        def __init__(self, widthA=1., lengthA=0.2, angleA=0):\n            \"\"\"\n            Parameters\n            ----------\n            widthA : float, default: 1.0\n                Width of the bracket.\n            lengthA : float, default: 0.2\n                Length of the bracket.\n            angleA : float, default: 0 degrees\n                Orientation of the bracket, as a counterclockwise angle.\n                0 degrees means perpendicular to the line.\n            \"\"\"\n            super().__init__(widthA=widthA, lengthA=lengthA, angleA=angleA)\n\n    @_register_style(_style_list, name=\"-[\")\n    class BracketB(_Curve):\n        \"\"\"An arrow with an outward square bracket at its end.\"\"\"\n        arrow = \"-[\"\n\n        def __init__(self, widthB=1., lengthB=0.2, angleB=0):\n            \"\"\"\n            Parameters\n            ----------\n            widthB : float, default: 1.0\n                Width of the bracket.\n            lengthB : float, default: 0.2\n                Length of the bracket.\n            angleB : float, default: 0 degrees\n                Orientation of the bracket, as a counterclockwise angle.\n                0 degrees means perpendicular to the line.\n            \"\"\"\n            super().__init__(widthB=widthB, lengthB=lengthB, angleB=angleB)\n\n    @_register_style(_style_list, name=\"]-[\")\n    class BracketAB(_Curve):\n        \"\"\"An arrow with outward square brackets at both ends.\"\"\"\n        arrow = \"]-[\"\n\n        def __init__(self,\n                     widthA=1., lengthA=0.2, angleA=0,\n                     widthB=1., lengthB=0.2, angleB=0):\n            \"\"\"\n            Parameters\n            ----------\n            widthA, widthB : float, default: 1.0\n                Width of the bracket.\n            lengthA, lengthB : float, default: 0.2\n                Length of the bracket.\n            angleA, angleB : float, default: 0 degrees\n                Orientation of the bracket, as a counterclockwise angle.\n                0 degrees means perpendicular to the line.\n            \"\"\"\n            super().__init__(widthA=widthA, lengthA=lengthA, angleA=angleA,\n                             widthB=widthB, lengthB=lengthB, angleB=angleB)\n\n    @_register_style(_style_list, name=\"|-|\")\n    class BarAB(_Curve):\n        \"\"\"An arrow with vertical bars ``|`` at both ends.\"\"\"\n        arrow = \"|-|\"\n\n        def __init__(self, widthA=1., angleA=0, widthB=1., angleB=0):\n            \"\"\"\n            Parameters\n            ----------\n            widthA, widthB : float, default: 1.0\n                Width of the bracket.\n            angleA, angleB : float, default: 0 degrees\n                Orientation of the bracket, as a counterclockwise angle.\n                0 degrees means perpendicular to the line.\n            \"\"\"\n            super().__init__(widthA=widthA, lengthA=0, angleA=angleA,\n                             widthB=widthB, lengthB=0, angleB=angleB)\n\n    @_register_style(_style_list, name=']->')\n    class BracketCurve(_Curve):\n        \"\"\"\n        An arrow with an outward square bracket at its start and a head at\n        the end.\n        \"\"\"\n        arrow = \"]->\"\n\n        def __init__(self, widthA=1., lengthA=0.2, angleA=None):\n            \"\"\"\n            Parameters\n            ----------\n            widthA : float, default: 1.0\n                Width of the bracket.\n            lengthA : float, default: 0.2\n                Length of the bracket.\n            angleA : float, default: 0 degrees\n                Orientation of the bracket, as a counterclockwise angle.\n                0 degrees means perpendicular to the line.\n            \"\"\"\n            super().__init__(widthA=widthA, lengthA=lengthA, angleA=angleA)\n\n    @_register_style(_style_list, name='<-[')\n    class CurveBracket(_Curve):\n        \"\"\"\n        An arrow with an outward square bracket at its end and a head at\n        the start.\n        \"\"\"\n        arrow = \"<-[\"\n\n        def __init__(self, widthB=1., lengthB=0.2, angleB=None):\n            \"\"\"\n            Parameters\n            ----------\n            widthB : float, default: 1.0\n                Width of the bracket.\n            lengthB : float, default: 0.2\n                Length of the bracket.\n            angleB : float, default: 0 degrees\n                Orientation of the bracket, as a counterclockwise angle.\n                0 degrees means perpendicular to the line.\n            \"\"\"\n            super().__init__(widthB=widthB, lengthB=lengthB, angleB=angleB)\n\n    @_register_style(_style_list)\n    class Simple(_Base):\n        \"\"\"A simple arrow. Only works with a quadratic Bezier curve.\"\"\"\n\n        def __init__(self, head_length=.5, head_width=.5, tail_width=.2):\n            \"\"\"\n            Parameters\n            ----------\n            head_length : float, default: 0.5\n                Length of the arrow head.\n\n            head_width : float, default: 0.5\n                Width of the arrow head.\n\n            tail_width : float, default: 0.2\n                Width of the arrow tail.\n            \"\"\"\n            self.head_length, self.head_width, self.tail_width = \\\n                head_length, head_width, tail_width\n            super().__init__()\n\n        def transmute(self, path, mutation_size, linewidth):\n\n            x0, y0, x1, y1, x2, y2 = self.ensure_quadratic_bezier(path)\n\n            # divide the path into a head and a tail\n            head_length = self.head_length * mutation_size\n            in_f = inside_circle(x2, y2, head_length)\n            arrow_path = [(x0, y0), (x1, y1), (x2, y2)]\n\n            try:\n                arrow_out, arrow_in = \\\n                    split_bezier_intersecting_with_closedpath(\n                        arrow_path, in_f, tolerance=0.01)\n            except NonIntersectingPathException:\n                # if this happens, make a straight line of the head_length\n                # long.\n                x0, y0 = _point_along_a_line(x2, y2, x1, y1, head_length)\n                x1n, y1n = 0.5 * (x0 + x2), 0.5 * (y0 + y2)\n                arrow_in = [(x0, y0), (x1n, y1n), (x2, y2)]\n                arrow_out = None\n\n            # head\n            head_width = self.head_width * mutation_size\n            head_left, head_right = make_wedged_bezier2(arrow_in,\n                                                        head_width / 2., wm=.5)\n\n            # tail\n            if arrow_out is not None:\n                tail_width = self.tail_width * mutation_size\n                tail_left, tail_right = get_parallels(arrow_out,\n                                                      tail_width / 2.)\n\n                patch_path = [(Path.MOVETO, tail_right[0]),\n                              (Path.CURVE3, tail_right[1]),\n                              (Path.CURVE3, tail_right[2]),\n                              (Path.LINETO, head_right[0]),\n                              (Path.CURVE3, head_right[1]),\n                              (Path.CURVE3, head_right[2]),\n                              (Path.CURVE3, head_left[1]),\n                              (Path.CURVE3, head_left[0]),\n                              (Path.LINETO, tail_left[2]),\n                              (Path.CURVE3, tail_left[1]),\n                              (Path.CURVE3, tail_left[0]),\n                              (Path.LINETO, tail_right[0]),\n                              (Path.CLOSEPOLY, tail_right[0]),\n                              ]\n            else:\n                patch_path = [(Path.MOVETO, head_right[0]),\n                              (Path.CURVE3, head_right[1]),\n                              (Path.CURVE3, head_right[2]),\n                              (Path.CURVE3, head_left[1]),\n                              (Path.CURVE3, head_left[0]),\n                              (Path.CLOSEPOLY, head_left[0]),\n                              ]\n\n            path = Path([p for c, p in patch_path], [c for c, p in patch_path])\n\n            return path, True\n\n    @_register_style(_style_list)\n    class Fancy(_Base):\n        \"\"\"A fancy arrow. Only works with a quadratic Bezier curve.\"\"\"\n\n        def __init__(self, head_length=.4, head_width=.4, tail_width=.4):\n            \"\"\"\n            Parameters\n            ----------\n            head_length : float, default: 0.4\n                Length of the arrow head.\n\n            head_width : float, default: 0.4\n                Width of the arrow head.\n\n            tail_width : float, default: 0.4\n                Width of the arrow tail.\n            \"\"\"\n            self.head_length, self.head_width, self.tail_width = \\\n                head_length, head_width, tail_width\n            super().__init__()\n\n        def transmute(self, path, mutation_size, linewidth):\n\n            x0, y0, x1, y1, x2, y2 = self.ensure_quadratic_bezier(path)\n\n            # divide the path into a head and a tail\n            head_length = self.head_length * mutation_size\n            arrow_path = [(x0, y0), (x1, y1), (x2, y2)]\n\n            # path for head\n            in_f = inside_circle(x2, y2, head_length)\n            try:\n                path_out, path_in = split_bezier_intersecting_with_closedpath(\n                    arrow_path, in_f, tolerance=0.01)\n            except NonIntersectingPathException:\n                # if this happens, make a straight line of the head_length\n                # long.\n                x0, y0 = _point_along_a_line(x2, y2, x1, y1, head_length)\n                x1n, y1n = 0.5 * (x0 + x2), 0.5 * (y0 + y2)\n                arrow_path = [(x0, y0), (x1n, y1n), (x2, y2)]\n                path_head = arrow_path\n            else:\n                path_head = path_in\n\n            # path for head\n            in_f = inside_circle(x2, y2, head_length * .8)\n            path_out, path_in = split_bezier_intersecting_with_closedpath(\n                arrow_path, in_f, tolerance=0.01)\n            path_tail = path_out\n\n            # head\n            head_width = self.head_width * mutation_size\n            head_l, head_r = make_wedged_bezier2(path_head,\n                                                 head_width / 2.,\n                                                 wm=.6)\n\n            # tail\n            tail_width = self.tail_width * mutation_size\n            tail_left, tail_right = make_wedged_bezier2(path_tail,\n                                                        tail_width * .5,\n                                                        w1=1., wm=0.6, w2=0.3)\n\n            # path for head\n            in_f = inside_circle(x0, y0, tail_width * .3)\n            path_in, path_out = split_bezier_intersecting_with_closedpath(\n                arrow_path, in_f, tolerance=0.01)\n            tail_start = path_in[-1]\n\n            head_right, head_left = head_r, head_l\n            patch_path = [(Path.MOVETO, tail_start),\n                          (Path.LINETO, tail_right[0]),\n                          (Path.CURVE3, tail_right[1]),\n                          (Path.CURVE3, tail_right[2]),\n                          (Path.LINETO, head_right[0]),\n                          (Path.CURVE3, head_right[1]),\n                          (Path.CURVE3, head_right[2]),\n                          (Path.CURVE3, head_left[1]),\n                          (Path.CURVE3, head_left[0]),\n                          (Path.LINETO, tail_left[2]),\n                          (Path.CURVE3, tail_left[1]),\n                          (Path.CURVE3, tail_left[0]),\n                          (Path.LINETO, tail_start),\n                          (Path.CLOSEPOLY, tail_start),\n                          ]\n            path = Path([p for c, p in patch_path], [c for c, p in patch_path])\n\n            return path, True\n\n    @_register_style(_style_list)\n    class Wedge(_Base):\n        \"\"\"\n        Wedge(?) shape. Only works with a quadratic Bezier curve.  The\n        begin point has a width of the tail_width and the end point has a\n        width of 0. At the middle, the width is shrink_factor*tail_width.\n        \"\"\"\n\n        def __init__(self, tail_width=.3, shrink_factor=0.5):\n            \"\"\"\n            Parameters\n            ----------\n            tail_width : float, default: 0.3\n                Width of the tail.\n\n            shrink_factor : float, default: 0.5\n                Fraction of the arrow width at the middle point.\n            \"\"\"\n            self.tail_width = tail_width\n            self.shrink_factor = shrink_factor\n            super().__init__()\n\n        def transmute(self, path, mutation_size, linewidth):\n\n            x0, y0, x1, y1, x2, y2 = self.ensure_quadratic_bezier(path)\n\n            arrow_path = [(x0, y0), (x1, y1), (x2, y2)]\n            b_plus, b_minus = make_wedged_bezier2(\n                                    arrow_path,\n                                    self.tail_width * mutation_size / 2.,\n                                    wm=self.shrink_factor)\n\n            patch_path = [(Path.MOVETO, b_plus[0]),\n                          (Path.CURVE3, b_plus[1]),\n                          (Path.CURVE3, b_plus[2]),\n                          (Path.LINETO, b_minus[2]),\n                          (Path.CURVE3, b_minus[1]),\n                          (Path.CURVE3, b_minus[0]),\n                          (Path.CLOSEPOLY, b_minus[0]),\n                          ]\n            path = Path([p for c, p in patch_path], [c for c, p in patch_path])\n\n            return path, True\n\n\n_docstring.interpd.update(\n    AvailableBoxstyles=BoxStyle.pprint_styles(),\n    ListBoxstyles=_simpleprint_styles(BoxStyle._style_list),\n    AvailableArrowstyles=ArrowStyle.pprint_styles(),\n    AvailableConnectorstyles=ConnectionStyle.pprint_styles(),\n)\n_docstring.dedent_interpd(BoxStyle)\n_docstring.dedent_interpd(ArrowStyle)\n_docstring.dedent_interpd(ConnectionStyle)\n\n\nclass FancyBboxPatch(Patch):\n    \"\"\"\n    A fancy box around a rectangle with lower left at *xy* = (*x*, *y*)\n    with specified width and height.\n\n    `.FancyBboxPatch` is similar to `.Rectangle`, but it draws a fancy box\n    around the rectangle. The transformation of the rectangle box to the\n    fancy box is delegated to the style classes defined in `.BoxStyle`.\n    \"\"\"\n\n    _edge_default = True\n\n    def __str__(self):\n        s = self.__class__.__name__ + \"((%g, %g), width=%g, height=%g)\"\n        return s % (self._x, self._y, self._width, self._height)\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"mutation_scale\")\n    @_api.delete_parameter(\"3.4\", \"bbox_transmuter\", alternative=\"boxstyle\")\n    def __init__(self, xy, width, height,\n                 boxstyle=\"round\", bbox_transmuter=None,\n                 mutation_scale=1, mutation_aspect=1,\n                 **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        xy : float, float\n          The lower left corner of the box.\n\n        width : float\n            The width of the box.\n\n        height : float\n            The height of the box.\n\n        boxstyle : str or `matplotlib.patches.BoxStyle`\n            The style of the fancy box. This can either be a `.BoxStyle`\n            instance or a string of the style name and optionally comma\n            separated attributes (e.g. \"Round, pad=0.2\"). This string is\n            passed to `.BoxStyle` to construct a `.BoxStyle` object. See\n            there for a full documentation.\n\n            The following box styles are available:\n\n            %(AvailableBoxstyles)s\n\n        mutation_scale : float, default: 1\n            Scaling factor applied to the attributes of the box style\n            (e.g. pad or rounding_size).\n\n        mutation_aspect : float, default: 1\n            The height of the rectangle will be squeezed by this value before\n            the mutation and the mutated box will be stretched by the inverse\n            of it. For example, this allows different horizontal and vertical\n            padding.\n\n        Other Parameters\n        ----------------\n        **kwargs : `.Patch` properties\n\n        %(Patch:kwdoc)s\n        \"\"\"\n\n        super().__init__(**kwargs)\n\n        self._x = xy[0]\n        self._y = xy[1]\n        self._width = width\n        self._height = height\n\n        if boxstyle == \"custom\":\n            _api.warn_deprecated(\n                \"3.4\", message=\"Support for boxstyle='custom' is deprecated \"\n                \"since %(since)s and will be removed %(removal)s; directly \"\n                \"pass a boxstyle instance as the boxstyle parameter instead.\")\n            if bbox_transmuter is None:\n                raise ValueError(\"bbox_transmuter argument is needed with \"\n                                 \"custom boxstyle\")\n            self._bbox_transmuter = bbox_transmuter\n        else:\n            self.set_boxstyle(boxstyle)\n\n        self._mutation_scale = mutation_scale\n        self._mutation_aspect = mutation_aspect\n\n        self.stale = True\n\n    @_docstring.dedent_interpd\n    def set_boxstyle(self, boxstyle=None, **kwargs):\n        \"\"\"\n        Set the box style.\n\n        Most box styles can be further configured using attributes.\n        Attributes from the previous box style are not reused.\n\n        Without argument (or with ``boxstyle=None``), the available box styles\n        are returned as a human-readable string.\n\n        Parameters\n        ----------\n        boxstyle : str or `matplotlib.patches.BoxStyle`\n            The style of the fancy box. This can either be a `.BoxStyle`\n            instance or a string of the style name and optionally comma\n            separated attributes (e.g. \"Round, pad=0.2\"). This string is\n            passed to `.BoxStyle` to construct a `.BoxStyle` object. See\n            there for a full documentation.\n\n            The following box styles are available:\n\n            %(AvailableBoxstyles)s\n\n            .. ACCEPTS: %(ListBoxstyles)s\n\n        **kwargs\n            Additional attributes for the box style. See the table above for\n            supported parameters.\n\n        Examples\n        --------\n        ::\n\n            set_boxstyle(\"round,pad=0.2\")\n            set_boxstyle(\"round\", pad=0.2)\n\n        \"\"\"\n        if boxstyle is None:\n            return BoxStyle.pprint_styles()\n        self._bbox_transmuter = (\n            BoxStyle(boxstyle, **kwargs) if isinstance(boxstyle, str)\n            else boxstyle)\n        self.stale = True\n\n    def set_mutation_scale(self, scale):\n        \"\"\"\n        Set the mutation scale.\n\n        Parameters\n        ----------\n        scale : float\n        \"\"\"\n        self._mutation_scale = scale\n        self.stale = True\n\n    def get_mutation_scale(self):\n        \"\"\"Return the mutation scale.\"\"\"\n        return self._mutation_scale\n\n    def set_mutation_aspect(self, aspect):\n        \"\"\"\n        Set the aspect ratio of the bbox mutation.\n\n        Parameters\n        ----------\n        aspect : float\n        \"\"\"\n        self._mutation_aspect = aspect\n        self.stale = True\n\n    def get_mutation_aspect(self):\n        \"\"\"Return the aspect ratio of the bbox mutation.\"\"\"\n        return (self._mutation_aspect if self._mutation_aspect is not None\n                else 1)  # backcompat.\n\n    def get_boxstyle(self):\n        \"\"\"Return the boxstyle object.\"\"\"\n        return self._bbox_transmuter\n\n    def get_path(self):\n        \"\"\"Return the mutated path of the rectangle.\"\"\"\n        boxstyle = self.get_boxstyle()\n        x = self._x\n        y = self._y\n        width = self._width\n        height = self._height\n        m_scale = self.get_mutation_scale()\n        m_aspect = self.get_mutation_aspect()\n        # Squeeze the given height by the aspect_ratio.\n        y, height = y / m_aspect, height / m_aspect\n        # Call boxstyle with squeezed height.\n        try:\n            inspect.signature(boxstyle).bind(x, y, width, height, m_scale)\n        except TypeError:\n            # Don't apply aspect twice.\n            path = boxstyle(x, y, width, height, m_scale, 1)\n            _api.warn_deprecated(\n                \"3.4\", message=\"boxstyles must be callable without the \"\n                \"'mutation_aspect' parameter since %(since)s; support for the \"\n                \"old call signature will be removed %(removal)s.\")\n        else:\n            path = boxstyle(x, y, width, height, m_scale)\n        vertices, codes = path.vertices, path.codes\n        # Restore the height.\n        vertices[:, 1] = vertices[:, 1] * m_aspect\n        return Path(vertices, codes)\n\n    # Following methods are borrowed from the Rectangle class.\n\n    def get_x(self):\n        \"\"\"Return the left coord of the rectangle.\"\"\"\n        return self._x\n\n    def get_y(self):\n        \"\"\"Return the bottom coord of the rectangle.\"\"\"\n        return self._y\n\n    def get_width(self):\n        \"\"\"Return the width of the rectangle.\"\"\"\n        return self._width\n\n    def get_height(self):\n        \"\"\"Return the height of the rectangle.\"\"\"\n        return self._height\n\n    def set_x(self, x):\n        \"\"\"\n        Set the left coord of the rectangle.\n\n        Parameters\n        ----------\n        x : float\n        \"\"\"\n        self._x = x\n        self.stale = True\n\n    def set_y(self, y):\n        \"\"\"\n        Set the bottom coord of the rectangle.\n\n        Parameters\n        ----------\n        y : float\n        \"\"\"\n        self._y = y\n        self.stale = True\n\n    def set_width(self, w):\n        \"\"\"\n        Set the rectangle width.\n\n        Parameters\n        ----------\n        w : float\n        \"\"\"\n        self._width = w\n        self.stale = True\n\n    def set_height(self, h):\n        \"\"\"\n        Set the rectangle height.\n\n        Parameters\n        ----------\n        h : float\n        \"\"\"\n        self._height = h\n        self.stale = True\n\n    def set_bounds(self, *args):\n        \"\"\"\n        Set the bounds of the rectangle.\n\n        Call signatures::\n\n            set_bounds(left, bottom, width, height)\n            set_bounds((left, bottom, width, height))\n\n        Parameters\n        ----------\n        left, bottom : float\n            The coordinates of the bottom left corner of the rectangle.\n        width, height : float\n            The width/height of the rectangle.\n        \"\"\"\n        if len(args) == 1:\n            l, b, w, h = args[0]\n        else:\n            l, b, w, h = args\n        self._x = l\n        self._y = b\n        self._width = w\n        self._height = h\n        self.stale = True\n\n    def get_bbox(self):\n        \"\"\"Return the `.Bbox`.\"\"\"\n        return transforms.Bbox.from_bounds(self._x, self._y,\n                                           self._width, self._height)\n\n\nclass FancyArrowPatch(Patch):\n    \"\"\"\n    A fancy arrow patch. It draws an arrow using the `ArrowStyle`.\n\n    The head and tail positions are fixed at the specified start and end points\n    of the arrow, but the size and shape (in display coordinates) of the arrow\n    does not change when the axis is moved or zoomed.\n    \"\"\"\n    _edge_default = True\n\n    def __str__(self):\n        if self._posA_posB is not None:\n            (x1, y1), (x2, y2) = self._posA_posB\n            return f\"{type(self).__name__}(({x1:g}, {y1:g})->({x2:g}, {y2:g}))\"\n        else:\n            return f\"{type(self).__name__}({self._path_original})\"\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"path\")\n    def __init__(self, posA=None, posB=None, path=None,\n                 arrowstyle=\"simple\", connectionstyle=\"arc3\",\n                 patchA=None, patchB=None,\n                 shrinkA=2, shrinkB=2,\n                 mutation_scale=1, mutation_aspect=1,\n                 **kwargs):\n        \"\"\"\n        There are two ways for defining an arrow:\n\n        - If *posA* and *posB* are given, a path connecting two points is\n          created according to *connectionstyle*. The path will be\n          clipped with *patchA* and *patchB* and further shrunken by\n          *shrinkA* and *shrinkB*. An arrow is drawn along this\n          resulting path using the *arrowstyle* parameter.\n\n        - Alternatively if *path* is provided, an arrow is drawn along this\n          path and *patchA*, *patchB*, *shrinkA*, and *shrinkB* are ignored.\n\n        Parameters\n        ----------\n        posA, posB : (float, float), default: None\n            (x, y) coordinates of arrow tail and arrow head respectively.\n\n        path : `~matplotlib.path.Path`, default: None\n            If provided, an arrow is drawn along this path and *patchA*,\n            *patchB*, *shrinkA*, and *shrinkB* are ignored.\n\n        arrowstyle : str or `.ArrowStyle`, default: 'simple'\n            The `.ArrowStyle` with which the fancy arrow is drawn.  If a\n            string, it should be one of the available arrowstyle names, with\n            optional comma-separated attributes.  The optional attributes are\n            meant to be scaled with the *mutation_scale*.  The following arrow\n            styles are available:\n\n            %(AvailableArrowstyles)s\n\n        connectionstyle : str or `.ConnectionStyle` or None, optional, \\\ndefault: 'arc3'\n            The `.ConnectionStyle` with which *posA* and *posB* are connected.\n            If a string, it should be one of the available connectionstyle\n            names, with optional comma-separated attributes.  The following\n            connection styles are available:\n\n            %(AvailableConnectorstyles)s\n\n        patchA, patchB : `.Patch`, default: None\n            Head and tail patches, respectively.\n\n        shrinkA, shrinkB : float, default: 2\n            Shrinking factor of the tail and head of the arrow respectively.\n\n        mutation_scale : float, default: 1\n            Value with which attributes of *arrowstyle* (e.g., *head_length*)\n            will be scaled.\n\n        mutation_aspect : None or float, default: None\n            The height of the rectangle will be squeezed by this value before\n            the mutation and the mutated box will be stretched by the inverse\n            of it.\n\n        Other Parameters\n        ----------------\n        **kwargs : `.Patch` properties, optional\n            Here is a list of available `.Patch` properties:\n\n        %(Patch:kwdoc)s\n\n            In contrast to other patches, the default ``capstyle`` and\n            ``joinstyle`` for `FancyArrowPatch` are set to ``\"round\"``.\n        \"\"\"\n        # Traditionally, the cap- and joinstyle for FancyArrowPatch are round\n        kwargs.setdefault(\"joinstyle\", JoinStyle.round)\n        kwargs.setdefault(\"capstyle\", CapStyle.round)\n\n        super().__init__(**kwargs)\n\n        if posA is not None and posB is not None and path is None:\n            self._posA_posB = [posA, posB]\n\n            if connectionstyle is None:\n                connectionstyle = \"arc3\"\n            self.set_connectionstyle(connectionstyle)\n\n        elif posA is None and posB is None and path is not None:\n            self._posA_posB = None\n        else:\n            raise ValueError(\"Either posA and posB, or path need to provided\")\n\n        self.patchA = patchA\n        self.patchB = patchB\n        self.shrinkA = shrinkA\n        self.shrinkB = shrinkB\n\n        self._path_original = path\n\n        self.set_arrowstyle(arrowstyle)\n\n        self._mutation_scale = mutation_scale\n        self._mutation_aspect = mutation_aspect\n\n        self._dpi_cor = 1.0\n\n    def set_positions(self, posA, posB):\n        \"\"\"\n        Set the begin and end positions of the connecting path.\n\n        Parameters\n        ----------\n        posA, posB : None, tuple\n            (x, y) coordinates of arrow tail and arrow head respectively. If\n            `None` use current value.\n        \"\"\"\n        if posA is not None:\n            self._posA_posB[0] = posA\n        if posB is not None:\n            self._posA_posB[1] = posB\n        self.stale = True\n\n    def set_patchA(self, patchA):\n        \"\"\"\n        Set the tail patch.\n\n        Parameters\n        ----------\n        patchA : `.patches.Patch`\n        \"\"\"\n        self.patchA = patchA\n        self.stale = True\n\n    def set_patchB(self, patchB):\n        \"\"\"\n        Set the head patch.\n\n        Parameters\n        ----------\n        patchB : `.patches.Patch`\n        \"\"\"\n        self.patchB = patchB\n        self.stale = True\n\n    def set_connectionstyle(self, connectionstyle, **kwargs):\n        \"\"\"\n        Set the connection style. Old attributes are forgotten.\n\n        Parameters\n        ----------\n        connectionstyle : str or `.ConnectionStyle` or None, optional\n            Can be a string with connectionstyle name with\n            optional comma-separated attributes, e.g.::\n\n                set_connectionstyle(\"arc,angleA=0,armA=30,rad=10\")\n\n            Alternatively, the attributes can be provided as keywords, e.g.::\n\n                set_connectionstyle(\"arc\", angleA=0,armA=30,rad=10)\n\n            Without any arguments (or with ``connectionstyle=None``), return\n            available styles as a list of strings.\n        \"\"\"\n\n        if connectionstyle is None:\n            return ConnectionStyle.pprint_styles()\n\n        if (isinstance(connectionstyle, ConnectionStyle._Base) or\n                callable(connectionstyle)):\n            self._connector = connectionstyle\n        else:\n            self._connector = ConnectionStyle(connectionstyle, **kwargs)\n        self.stale = True\n\n    def get_connectionstyle(self):\n        \"\"\"Return the `ConnectionStyle` used.\"\"\"\n        return self._connector\n\n    def set_arrowstyle(self, arrowstyle=None, **kwargs):\n        \"\"\"\n        Set the arrow style. Old attributes are forgotten. Without arguments\n        (or with ``arrowstyle=None``) returns available box styles as a list of\n        strings.\n\n        Parameters\n        ----------\n        arrowstyle : None or ArrowStyle or str, default: None\n            Can be a string with arrowstyle name with optional comma-separated\n            attributes, e.g.::\n\n                set_arrowstyle(\"Fancy,head_length=0.2\")\n\n            Alternatively attributes can be provided as keywords, e.g.::\n\n                set_arrowstyle(\"fancy\", head_length=0.2)\n\n        \"\"\"\n\n        if arrowstyle is None:\n            return ArrowStyle.pprint_styles()\n\n        if isinstance(arrowstyle, ArrowStyle._Base):\n            self._arrow_transmuter = arrowstyle\n        else:\n            self._arrow_transmuter = ArrowStyle(arrowstyle, **kwargs)\n        self.stale = True\n\n    def get_arrowstyle(self):\n        \"\"\"Return the arrowstyle object.\"\"\"\n        return self._arrow_transmuter\n\n    def set_mutation_scale(self, scale):\n        \"\"\"\n        Set the mutation scale.\n\n        Parameters\n        ----------\n        scale : float\n        \"\"\"\n        self._mutation_scale = scale\n        self.stale = True\n\n    def get_mutation_scale(self):\n        \"\"\"\n        Return the mutation scale.\n\n        Returns\n        -------\n        scalar\n        \"\"\"\n        return self._mutation_scale\n\n    def set_mutation_aspect(self, aspect):\n        \"\"\"\n        Set the aspect ratio of the bbox mutation.\n\n        Parameters\n        ----------\n        aspect : float\n        \"\"\"\n        self._mutation_aspect = aspect\n        self.stale = True\n\n    def get_mutation_aspect(self):\n        \"\"\"Return the aspect ratio of the bbox mutation.\"\"\"\n        return (self._mutation_aspect if self._mutation_aspect is not None\n                else 1)  # backcompat.\n\n    def get_path(self):\n        \"\"\"Return the path of the arrow in the data coordinates.\"\"\"\n        # The path is generated in display coordinates, then converted back to\n        # data coordinates.\n        _path, fillable = self._get_path_in_displaycoord()\n        if np.iterable(fillable):\n            _path = Path.make_compound_path(*_path)\n        return self.get_transform().inverted().transform_path(_path)\n\n    def _get_path_in_displaycoord(self):\n        \"\"\"Return the mutated path of the arrow in display coordinates.\"\"\"\n        dpi_cor = self._dpi_cor\n\n        if self._posA_posB is not None:\n            posA = self._convert_xy_units(self._posA_posB[0])\n            posB = self._convert_xy_units(self._posA_posB[1])\n            (posA, posB) = self.get_transform().transform((posA, posB))\n            _path = self.get_connectionstyle()(posA, posB,\n                                               patchA=self.patchA,\n                                               patchB=self.patchB,\n                                               shrinkA=self.shrinkA * dpi_cor,\n                                               shrinkB=self.shrinkB * dpi_cor\n                                               )\n        else:\n            _path = self.get_transform().transform_path(self._path_original)\n\n        _path, fillable = self.get_arrowstyle()(\n            _path,\n            self.get_mutation_scale() * dpi_cor,\n            self.get_linewidth() * dpi_cor,\n            self.get_mutation_aspect())\n\n        return _path, fillable\n\n    get_path_in_displaycoord = _api.deprecate_privatize_attribute(\n        \"3.5\",\n        alternative=\"self.get_transform().transform_path(self.get_path())\")\n\n    def draw(self, renderer):\n        if not self.get_visible():\n            return\n\n        with self._bind_draw_path_function(renderer) as draw_path:\n\n            # FIXME : dpi_cor is for the dpi-dependency of the linewidth. There\n            # could be room for improvement.  Maybe _get_path_in_displaycoord\n            # could take a renderer argument, but get_path should be adapted\n            # too.\n            self._dpi_cor = renderer.points_to_pixels(1.)\n            path, fillable = self._get_path_in_displaycoord()\n\n            if not np.iterable(fillable):\n                path = [path]\n                fillable = [fillable]\n\n            affine = transforms.IdentityTransform()\n\n            for p, f in zip(path, fillable):\n                draw_path(\n                    p, affine,\n                    self._facecolor if f and self._facecolor[3] else None)\n\n\nclass ConnectionPatch(FancyArrowPatch):\n    \"\"\"A patch that connects two points (possibly in different axes).\"\"\"\n\n    def __str__(self):\n        return \"ConnectionPatch((%g, %g), (%g, %g))\" % \\\n               (self.xy1[0], self.xy1[1], self.xy2[0], self.xy2[1])\n\n    @_docstring.dedent_interpd\n    @_api.make_keyword_only(\"3.6\", name=\"axesA\")\n    def __init__(self, xyA, xyB, coordsA, coordsB=None,\n                 axesA=None, axesB=None,\n                 arrowstyle=\"-\",\n                 connectionstyle=\"arc3\",\n                 patchA=None,\n                 patchB=None,\n                 shrinkA=0.,\n                 shrinkB=0.,\n                 mutation_scale=10.,\n                 mutation_aspect=None,\n                 clip_on=False,\n                 **kwargs):\n        \"\"\"\n        Connect point *xyA* in *coordsA* with point *xyB* in *coordsB*.\n\n        Valid keys are\n\n        ===============  ======================================================\n        Key              Description\n        ===============  ======================================================\n        arrowstyle       the arrow style\n        connectionstyle  the connection style\n        relpos           default is (0.5, 0.5)\n        patchA           default is bounding box of the text\n        patchB           default is None\n        shrinkA          default is 2 points\n        shrinkB          default is 2 points\n        mutation_scale   default is text size (in points)\n        mutation_aspect  default is 1.\n        ?                any key for `matplotlib.patches.PathPatch`\n        ===============  ======================================================\n\n        *coordsA* and *coordsB* are strings that indicate the\n        coordinates of *xyA* and *xyB*.\n\n        ==================== ==================================================\n        Property             Description\n        ==================== ==================================================\n        'figure points'      points from the lower left corner of the figure\n        'figure pixels'      pixels from the lower left corner of the figure\n        'figure fraction'    0, 0 is lower left of figure and 1, 1 is upper\n                             right\n        'subfigure points'   points from the lower left corner of the subfigure\n        'subfigure pixels'   pixels from the lower left corner of the subfigure\n        'subfigure fraction' fraction of the subfigure, 0, 0 is lower left.\n        'axes points'        points from lower left corner of axes\n        'axes pixels'        pixels from lower left corner of axes\n        'axes fraction'      0, 0 is lower left of axes and 1, 1 is upper right\n        'data'               use the coordinate system of the object being\n                             annotated (default)\n        'offset points'      offset (in points) from the *xy* value\n        'polar'              you can specify *theta*, *r* for the annotation,\n                             even in cartesian plots.  Note that if you are\n                             using a polar axes, you do not need to specify\n                             polar for the coordinate system since that is the\n                             native \"data\" coordinate system.\n        ==================== ==================================================\n\n        Alternatively they can be set to any valid\n        `~matplotlib.transforms.Transform`.\n\n        Note that 'subfigure pixels' and 'figure pixels' are the same\n        for the parent figure, so users who want code that is usable in\n        a subfigure can use 'subfigure pixels'.\n\n        .. note::\n\n           Using `ConnectionPatch` across two `~.axes.Axes` instances\n           is not directly compatible with :doc:`constrained layout\n           </tutorials/intermediate/constrainedlayout_guide>`. Add the artist\n           directly to the `.Figure` instead of adding it to a specific Axes,\n           or exclude it from the layout using ``con.set_in_layout(False)``.\n\n           .. code-block:: default\n\n              fig, ax = plt.subplots(1, 2, constrained_layout=True)\n              con = ConnectionPatch(..., axesA=ax[0], axesB=ax[1])\n              fig.add_artist(con)\n\n        \"\"\"\n        if coordsB is None:\n            coordsB = coordsA\n        # we'll draw ourself after the artist we annotate by default\n        self.xy1 = xyA\n        self.xy2 = xyB\n        self.coords1 = coordsA\n        self.coords2 = coordsB\n\n        self.axesA = axesA\n        self.axesB = axesB\n\n        super().__init__(posA=(0, 0), posB=(1, 1),\n                         arrowstyle=arrowstyle,\n                         connectionstyle=connectionstyle,\n                         patchA=patchA, patchB=patchB,\n                         shrinkA=shrinkA, shrinkB=shrinkB,\n                         mutation_scale=mutation_scale,\n                         mutation_aspect=mutation_aspect,\n                         clip_on=clip_on,\n                         **kwargs)\n        # if True, draw annotation only if self.xy is inside the axes\n        self._annotation_clip = None\n\n    def _get_xy(self, xy, s, axes=None):\n        \"\"\"Calculate the pixel position of given point.\"\"\"\n        s0 = s  # For the error message, if needed.\n        if axes is None:\n            axes = self.axes\n        xy = np.array(xy)\n        if s in [\"figure points\", \"axes points\"]:\n            xy *= self.figure.dpi / 72\n            s = s.replace(\"points\", \"pixels\")\n        elif s == \"figure fraction\":\n            s = self.figure.transFigure\n        elif s == \"subfigure fraction\":\n            s = self.figure.transSubfigure\n        elif s == \"axes fraction\":\n            s = axes.transAxes\n        x, y = xy\n\n        if s == 'data':\n            trans = axes.transData\n            x = float(self.convert_xunits(x))\n            y = float(self.convert_yunits(y))\n            return trans.transform((x, y))\n        elif s == 'offset points':\n            if self.xycoords == 'offset points':  # prevent recursion\n                return self._get_xy(self.xy, 'data')\n            return (\n                self._get_xy(self.xy, self.xycoords)  # converted data point\n                + xy * self.figure.dpi / 72)  # converted offset\n        elif s == 'polar':\n            theta, r = x, y\n            x = r * np.cos(theta)\n            y = r * np.sin(theta)\n            trans = axes.transData\n            return trans.transform((x, y))\n        elif s == 'figure pixels':\n            # pixels from the lower left corner of the figure\n            bb = self.figure.figbbox\n            x = bb.x0 + x if x >= 0 else bb.x1 + x\n            y = bb.y0 + y if y >= 0 else bb.y1 + y\n            return x, y\n        elif s == 'subfigure pixels':\n            # pixels from the lower left corner of the figure\n            bb = self.figure.bbox\n            x = bb.x0 + x if x >= 0 else bb.x1 + x\n            y = bb.y0 + y if y >= 0 else bb.y1 + y\n            return x, y\n        elif s == 'axes pixels':\n            # pixels from the lower left corner of the axes\n            bb = axes.bbox\n            x = bb.x0 + x if x >= 0 else bb.x1 + x\n            y = bb.y0 + y if y >= 0 else bb.y1 + y\n            return x, y\n        elif isinstance(s, transforms.Transform):\n            return s.transform(xy)\n        else:\n            raise ValueError(f\"{s0} is not a valid coordinate transformation\")\n\n    def set_annotation_clip(self, b):\n        \"\"\"\n        Set the annotation's clipping behavior.\n\n        Parameters\n        ----------\n        b : bool or None\n            - True: The annotation will be clipped when ``self.xy`` is\n              outside the axes.\n            - False: The annotation will always be drawn.\n            - None: The annotation will be clipped when ``self.xy`` is\n              outside the axes and ``self.xycoords == \"data\"``.\n        \"\"\"\n        self._annotation_clip = b\n        self.stale = True\n\n    def get_annotation_clip(self):\n        \"\"\"\n        Return the clipping behavior.\n\n        See `.set_annotation_clip` for the meaning of the return value.\n        \"\"\"\n        return self._annotation_clip\n\n    def _get_path_in_displaycoord(self):\n        \"\"\"Return the mutated path of the arrow in display coordinates.\"\"\"\n        dpi_cor = self._dpi_cor\n        posA = self._get_xy(self.xy1, self.coords1, self.axesA)\n        posB = self._get_xy(self.xy2, self.coords2, self.axesB)\n        path = self.get_connectionstyle()(\n            posA, posB,\n            patchA=self.patchA, patchB=self.patchB,\n            shrinkA=self.shrinkA * dpi_cor, shrinkB=self.shrinkB * dpi_cor,\n        )\n        path, fillable = self.get_arrowstyle()(\n            path,\n            self.get_mutation_scale() * dpi_cor,\n            self.get_linewidth() * dpi_cor,\n            self.get_mutation_aspect()\n        )\n        return path, fillable\n\n    def _check_xy(self, renderer):\n        \"\"\"Check whether the annotation needs to be drawn.\"\"\"\n\n        b = self.get_annotation_clip()\n\n        if b or (b is None and self.coords1 == \"data\"):\n            xy_pixel = self._get_xy(self.xy1, self.coords1, self.axesA)\n            if self.axesA is None:\n                axes = self.axes\n            else:\n                axes = self.axesA\n            if not axes.contains_point(xy_pixel):\n                return False\n\n        if b or (b is None and self.coords2 == \"data\"):\n            xy_pixel = self._get_xy(self.xy2, self.coords2, self.axesB)\n            if self.axesB is None:\n                axes = self.axes\n            else:\n                axes = self.axesB\n            if not axes.contains_point(xy_pixel):\n                return False\n\n        return True\n\n    def draw(self, renderer):\n        if renderer is not None:\n            self._renderer = renderer\n        if not self.get_visible() or not self._check_xy(renderer):\n            return\n        super().draw(renderer)"}
{"model_name_or_path": "agentless", "instance_id": "matplotlib__matplotlib-23476", "model_patch": "diff --git a/lib/matplotlib/backends/backend_macosx.py b/lib/matplotlib/backends/backend_macosx.py\nindex 4ab352a..b087be8 100644\n--- a/lib/matplotlib/backends/backend_macosx.py\n+++ b/lib/matplotlib/backends/backend_macosx.py\n@@ -97,6 +97,8 @@ class FigureCanvasMac(FigureCanvasAgg, _macosx.FigureCanvas, FigureCanvasBase):\n         scale = self.figure.dpi / self.device_pixel_ratio\n         width /= scale\n         height /= scale\n+        # Ensure DPI is not doubled upon unpickling\n+        self.figure.dpi = original_dpi  # Set the original DPI to prevent doubling\n         self.figure.set_size_inches(width, height, forward=False)\n         ResizeEvent(\"resize_event\", self)._process()\n         self.draw_idle()\n", "raw_model_patch": "diff --git a/lib/matplotlib/backends/backend_macosx.py b/lib/matplotlib/backends/backend_macosx.py\nindex 4ab352a..b087be8 100644\n--- a/lib/matplotlib/backends/backend_macosx.py\n+++ b/lib/matplotlib/backends/backend_macosx.py\n@@ -97,6 +97,8 @@ class FigureCanvasMac(FigureCanvasAgg, _macosx.FigureCanvas, FigureCanvasBase):\n         scale = self.figure.dpi / self.device_pixel_ratio\n         width /= scale\n         height /= scale\n+        # Ensure DPI is not doubled upon unpickling\n+        self.figure.dpi = original_dpi  # Set the original DPI to prevent doubling\n         self.figure.set_size_inches(width, height, forward=False)\n         ResizeEvent(\"resize_event\", self)._process()\n         self.draw_idle()\n", "original_file_content": "import os\n\nimport matplotlib as mpl\nfrom matplotlib import _api, cbook\nfrom matplotlib._pylab_helpers import Gcf\nfrom . import _macosx\nfrom .backend_agg import FigureCanvasAgg\nfrom matplotlib.backend_bases import (\n    _Backend, FigureCanvasBase, FigureManagerBase, NavigationToolbar2,\n    ResizeEvent, TimerBase)\nfrom matplotlib.figure import Figure\nfrom matplotlib.widgets import SubplotTool\n\n\nclass TimerMac(_macosx.Timer, TimerBase):\n    \"\"\"Subclass of `.TimerBase` using CFRunLoop timer events.\"\"\"\n    # completely implemented at the C-level (in _macosx.Timer)\n\n\nclass FigureCanvasMac(FigureCanvasAgg, _macosx.FigureCanvas, FigureCanvasBase):\n    # docstring inherited\n\n    # Ideally this class would be `class FCMacAgg(FCAgg, FCMac)`\n    # (FC=FigureCanvas) where FCMac would be an ObjC-implemented mac-specific\n    # class also inheriting from FCBase (this is the approach with other GUI\n    # toolkits).  However, writing an extension type inheriting from a Python\n    # base class is slightly tricky (the extension type must be a heap type),\n    # and we can just as well lift the FCBase base up one level, keeping it *at\n    # the end* to have the right method resolution order.\n\n    # Events such as button presses, mouse movements, and key presses are\n    # handled in C and events (MouseEvent, etc.) are triggered from there.\n\n    required_interactive_framework = \"macosx\"\n    _timer_cls = TimerMac\n    manager_class = _api.classproperty(lambda cls: FigureManagerMac)\n\n    def __init__(self, figure):\n        super().__init__(figure=figure)\n        self._draw_pending = False\n        self._is_drawing = False\n\n    def draw(self):\n        \"\"\"Render the figure and update the macosx canvas.\"\"\"\n        # The renderer draw is done here; delaying causes problems with code\n        # that uses the result of the draw() to update plot elements.\n        if self._is_drawing:\n            return\n        with cbook._setattr_cm(self, _is_drawing=True):\n            super().draw()\n        self.update()\n\n    def draw_idle(self):\n        # docstring inherited\n        if not (getattr(self, '_draw_pending', False) or\n                getattr(self, '_is_drawing', False)):\n            self._draw_pending = True\n            # Add a singleshot timer to the eventloop that will call back\n            # into the Python method _draw_idle to take care of the draw\n            self._single_shot_timer(self._draw_idle)\n\n    def _single_shot_timer(self, callback):\n        \"\"\"Add a single shot timer with the given callback\"\"\"\n        # We need to explicitly stop (called from delete) the timer after\n        # firing, otherwise segfaults will occur when trying to deallocate\n        # the singleshot timers.\n        def callback_func(callback, timer):\n            callback()\n            del timer\n        timer = self.new_timer(interval=0)\n        timer.add_callback(callback_func, callback, timer)\n        timer.start()\n\n    def _draw_idle(self):\n        \"\"\"\n        Draw method for singleshot timer\n\n        This draw method can be added to a singleshot timer, which can\n        accumulate draws while the eventloop is spinning. This method will\n        then only draw the first time and short-circuit the others.\n        \"\"\"\n        with self._idle_draw_cntx():\n            if not self._draw_pending:\n                # Short-circuit because our draw request has already been\n                # taken care of\n                return\n            self._draw_pending = False\n            self.draw()\n\n    def blit(self, bbox=None):\n        # docstring inherited\n        super().blit(bbox)\n        self.update()\n\n    def resize(self, width, height):\n        # Size from macOS is logical pixels, dpi is physical.\n        scale = self.figure.dpi / self.device_pixel_ratio\n        width /= scale\n        height /= scale\n        self.figure.set_size_inches(width, height, forward=False)\n        ResizeEvent(\"resize_event\", self)._process()\n        self.draw_idle()\n\n\nclass NavigationToolbar2Mac(_macosx.NavigationToolbar2, NavigationToolbar2):\n\n    def __init__(self, canvas):\n        data_path = cbook._get_data_path('images')\n        _, tooltips, image_names, _ = zip(*NavigationToolbar2.toolitems)\n        _macosx.NavigationToolbar2.__init__(\n            self, canvas,\n            tuple(str(data_path / image_name) + \".pdf\"\n                  for image_name in image_names if image_name is not None),\n            tuple(tooltip for tooltip in tooltips if tooltip is not None))\n        NavigationToolbar2.__init__(self, canvas)\n\n    def draw_rubberband(self, event, x0, y0, x1, y1):\n        self.canvas.set_rubberband(int(x0), int(y0), int(x1), int(y1))\n\n    def remove_rubberband(self):\n        self.canvas.remove_rubberband()\n\n    def save_figure(self, *args):\n        directory = os.path.expanduser(mpl.rcParams['savefig.directory'])\n        filename = _macosx.choose_save_file('Save the figure',\n                                            directory,\n                                            self.canvas.get_default_filename())\n        if filename is None:  # Cancel\n            return\n        # Save dir for next time, unless empty str (which means use cwd).\n        if mpl.rcParams['savefig.directory']:\n            mpl.rcParams['savefig.directory'] = os.path.dirname(filename)\n        self.canvas.figure.savefig(filename)\n\n    def prepare_configure_subplots(self):\n        toolfig = Figure(figsize=(6, 3))\n        canvas = FigureCanvasMac(toolfig)\n        toolfig.subplots_adjust(top=0.9)\n        # Need to keep a reference to the tool.\n        _tool = SubplotTool(self.canvas.figure, toolfig)\n        return canvas\n\n\nclass FigureManagerMac(_macosx.FigureManager, FigureManagerBase):\n    _toolbar2_class = NavigationToolbar2Mac\n\n    def __init__(self, canvas, num):\n        self._shown = False\n        _macosx.FigureManager.__init__(self, canvas)\n        icon_path = str(cbook._get_data_path('images/matplotlib.pdf'))\n        _macosx.FigureManager.set_icon(icon_path)\n        FigureManagerBase.__init__(self, canvas, num)\n        if self.toolbar is not None:\n            self.toolbar.update()\n        if mpl.is_interactive():\n            self.show()\n            self.canvas.draw_idle()\n\n    def _close_button_pressed(self):\n        Gcf.destroy(self)\n        self.canvas.flush_events()\n\n    @_api.deprecated(\"3.6\")\n    def close(self):\n        return self._close_button_pressed()\n\n    def show(self):\n        if not self._shown:\n            self._show()\n            self._shown = True\n        if mpl.rcParams[\"figure.raise_window\"]:\n            self._raise()\n\n\n@_Backend.export\nclass _BackendMac(_Backend):\n    FigureCanvas = FigureCanvasMac\n    FigureManager = FigureManagerMac\n\n    @staticmethod\n    def mainloop():\n        _macosx.show()"}
{"model_name_or_path": "agentless", "instance_id": "pydata__xarray-2905", "model_patch": "diff --git a/xarray/core/variable.py b/xarray/core/variable.py\nindex d82c4a0..62f4bd8 100644\n--- a/xarray/core/variable.py\n+++ b/xarray/core/variable.py\n@@ -218,7 +218,8 @@ def as_compatible_data(data, fastpath=False):\n         data = np.timedelta64(getattr(data, \"value\", data), \"ns\")\n \n     # we don't want nested self-described arrays\n-    data = getattr(data, \"values\", data)\n+    if hasattr(data, \"values\") and not isinstance(data, (list, set, dict)):\n+        data = getattr(data, \"values\")\n \n     if isinstance(data, np.ma.MaskedArray):\n         mask = np.ma.getmaskarray(data)\n", "raw_model_patch": "diff --git a/xarray/core/variable.py b/xarray/core/variable.py\nindex d82c4a0..62f4bd8 100644\n--- a/xarray/core/variable.py\n+++ b/xarray/core/variable.py\n@@ -218,7 +218,8 @@ def as_compatible_data(data, fastpath=False):\n         data = np.timedelta64(getattr(data, \"value\", data), \"ns\")\n \n     # we don't want nested self-described arrays\n-    data = getattr(data, \"values\", data)\n+    if hasattr(data, \"values\") and not isinstance(data, (list, set, dict)):\n+        data = getattr(data, \"values\")\n \n     if isinstance(data, np.ma.MaskedArray):\n         mask = np.ma.getmaskarray(data)\n", "original_file_content": "import copy\nimport functools\nimport itertools\nimport numbers\nimport warnings\nfrom collections import defaultdict\nfrom datetime import timedelta\nfrom distutils.version import LooseVersion\nfrom typing import (\n    Any,\n    Dict,\n    Hashable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    TypeVar,\n    Union,\n)\n\nimport numpy as np\nimport pandas as pd\n\nimport xarray as xr  # only for Dataset and DataArray\n\nfrom . import arithmetic, common, dtypes, duck_array_ops, indexing, nputils, ops, utils\nfrom .indexing import (\n    BasicIndexer,\n    OuterIndexer,\n    PandasIndexAdapter,\n    VectorizedIndexer,\n    as_indexable,\n)\nfrom .npcompat import IS_NEP18_ACTIVE\nfrom .options import _get_keep_attrs\nfrom .pycompat import (\n    cupy_array_type,\n    dask_array_type,\n    integer_types,\n    is_duck_dask_array,\n)\nfrom .utils import (\n    OrderedSet,\n    _default,\n    decode_numpy_dict_values,\n    drop_dims_from_indexers,\n    either_dict_or_kwargs,\n    ensure_us_time_resolution,\n    infix_dims,\n    is_duck_array,\n    maybe_coerce_to_str,\n)\n\nNON_NUMPY_SUPPORTED_ARRAY_TYPES = (\n    (\n        indexing.ExplicitlyIndexed,\n        pd.Index,\n    )\n    + dask_array_type\n    + cupy_array_type\n)\n# https://github.com/python/mypy/issues/224\nBASIC_INDEXING_TYPES = integer_types + (slice,)  # type: ignore\n\nVariableType = TypeVar(\"VariableType\", bound=\"Variable\")\n\"\"\"Type annotation to be used when methods of Variable return self or a copy of self.\nWhen called from an instance of a subclass, e.g. IndexVariable, mypy identifies the\noutput as an instance of the subclass.\n\nUsage::\n\n   class Variable:\n       def f(self: VariableType, ...) -> VariableType:\n           ...\n\"\"\"\n\n\nclass MissingDimensionsError(ValueError):\n    \"\"\"Error class used when we can't safely guess a dimension name.\"\"\"\n\n    # inherits from ValueError for backward compatibility\n    # TODO: move this to an xarray.exceptions module?\n\n\ndef as_variable(obj, name=None) -> \"Union[Variable, IndexVariable]\":\n    \"\"\"Convert an object into a Variable.\n\n    Parameters\n    ----------\n    obj : object\n        Object to convert into a Variable.\n\n        - If the object is already a Variable, return a shallow copy.\n        - Otherwise, if the object has 'dims' and 'data' attributes, convert\n          it into a new Variable.\n        - If all else fails, attempt to convert the object into a Variable by\n          unpacking it into the arguments for creating a new Variable.\n    name : str, optional\n        If provided:\n\n        - `obj` can be a 1D array, which is assumed to label coordinate values\n          along a dimension of this given name.\n        - Variables with name matching one of their dimensions are converted\n          into `IndexVariable` objects.\n\n    Returns\n    -------\n    var : Variable\n        The newly created variable.\n\n    \"\"\"\n    from .dataarray import DataArray\n\n    # TODO: consider extending this method to automatically handle Iris and\n    if isinstance(obj, DataArray):\n        # extract the primary Variable from DataArrays\n        obj = obj.variable\n\n    if isinstance(obj, Variable):\n        obj = obj.copy(deep=False)\n    elif isinstance(obj, tuple):\n        try:\n            obj = Variable(*obj)\n        except (TypeError, ValueError) as error:\n            # use .format() instead of % because it handles tuples consistently\n            raise error.__class__(\n                \"Could not convert tuple of form \"\n                \"(dims, data[, attrs, encoding]): \"\n                \"{} to Variable.\".format(obj)\n            )\n    elif utils.is_scalar(obj):\n        obj = Variable([], obj)\n    elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n        obj = Variable(obj.name, obj)\n    elif isinstance(obj, (set, dict)):\n        raise TypeError(\"variable {!r} has invalid type {!r}\".format(name, type(obj)))\n    elif name is not None:\n        data = as_compatible_data(obj)\n        if data.ndim != 1:\n            raise MissingDimensionsError(\n                \"cannot set variable %r with %r-dimensional data \"\n                \"without explicit dimension names. Pass a tuple of \"\n                \"(dims, data) instead.\" % (name, data.ndim)\n            )\n        obj = Variable(name, data, fastpath=True)\n    else:\n        raise TypeError(\n            \"unable to convert object into a variable without an \"\n            \"explicit list of dimensions: %r\" % obj\n        )\n\n    if name is not None and name in obj.dims:\n        # convert the Variable into an Index\n        if obj.ndim != 1:\n            raise MissingDimensionsError(\n                \"%r has more than 1-dimension and the same name as one of its \"\n                \"dimensions %r. xarray disallows such variables because they \"\n                \"conflict with the coordinates used to label \"\n                \"dimensions.\" % (name, obj.dims)\n            )\n        obj = obj.to_index_variable()\n\n    return obj\n\n\ndef _maybe_wrap_data(data):\n    \"\"\"\n    Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure\n    they can be indexed properly.\n\n    NumpyArrayAdapter, PandasIndexAdapter and LazilyOuterIndexedArray should\n    all pass through unmodified.\n    \"\"\"\n    if isinstance(data, pd.Index):\n        return PandasIndexAdapter(data)\n    return data\n\n\ndef _possibly_convert_objects(values):\n    \"\"\"Convert arrays of datetime.datetime and datetime.timedelta objects into\n    datetime64 and timedelta64, according to the pandas convention. Also used for\n    validating that datetime64 and timedelta64 objects are within the valid date\n    range for ns precision, as pandas will raise an error if they are not.\n    \"\"\"\n    return np.asarray(pd.Series(values.ravel())).reshape(values.shape)\n\n\ndef as_compatible_data(data, fastpath=False):\n    \"\"\"Prepare and wrap data to put in a Variable.\n\n    - If data does not have the necessary attributes, convert it to ndarray.\n    - If data has dtype=datetime64, ensure that it has ns precision. If it's a\n      pandas.Timestamp, convert it to datetime64.\n    - If data is already a pandas or xarray object (other than an Index), just\n      use the values.\n\n    Finally, wrap it up with an adapter if necessary.\n    \"\"\"\n    if fastpath and getattr(data, \"ndim\", 0) > 0:\n        # can't use fastpath (yet) for scalars\n        return _maybe_wrap_data(data)\n\n    if isinstance(data, Variable):\n        return data.data\n\n    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n        return _maybe_wrap_data(data)\n\n    if isinstance(data, tuple):\n        data = utils.to_0d_object_array(data)\n\n    if isinstance(data, pd.Timestamp):\n        # TODO: convert, handle datetime objects, too\n        data = np.datetime64(data.value, \"ns\")\n\n    if isinstance(data, timedelta):\n        data = np.timedelta64(getattr(data, \"value\", data), \"ns\")\n\n    # we don't want nested self-described arrays\n    data = getattr(data, \"values\", data)\n\n    if isinstance(data, np.ma.MaskedArray):\n        mask = np.ma.getmaskarray(data)\n        if mask.any():\n            dtype, fill_value = dtypes.maybe_promote(data.dtype)\n            data = np.asarray(data, dtype=dtype)\n            data[mask] = fill_value\n        else:\n            data = np.asarray(data)\n\n    if not isinstance(data, np.ndarray):\n        if hasattr(data, \"__array_function__\"):\n            if IS_NEP18_ACTIVE:\n                return data\n            else:\n                raise TypeError(\n                    \"Got an NumPy-like array type providing the \"\n                    \"__array_function__ protocol but NEP18 is not enabled. \"\n                    \"Check that numpy >= v1.16 and that the environment \"\n                    'variable \"NUMPY_EXPERIMENTAL_ARRAY_FUNCTION\" is set to '\n                    '\"1\"'\n                )\n\n    # validate whether the data is valid data types.\n    data = np.asarray(data)\n\n    if isinstance(data, np.ndarray):\n        if data.dtype.kind == \"O\":\n            data = _possibly_convert_objects(data)\n        elif data.dtype.kind == \"M\":\n            data = _possibly_convert_objects(data)\n        elif data.dtype.kind == \"m\":\n            data = _possibly_convert_objects(data)\n\n    return _maybe_wrap_data(data)\n\n\ndef _as_array_or_item(data):\n    \"\"\"Return the given values as a numpy array, or as an individual item if\n    it's a 0d datetime64 or timedelta64 array.\n\n    Importantly, this function does not copy data if it is already an ndarray -\n    otherwise, it will not be possible to update Variable values in place.\n\n    This function mostly exists because 0-dimensional ndarrays with\n    dtype=datetime64 are broken :(\n    https://github.com/numpy/numpy/issues/4337\n    https://github.com/numpy/numpy/issues/7619\n\n    TODO: remove this (replace with np.asarray) once these issues are fixed\n    \"\"\"\n    if isinstance(data, cupy_array_type):\n        data = data.get()\n    else:\n        data = np.asarray(data)\n    if data.ndim == 0:\n        if data.dtype.kind == \"M\":\n            data = np.datetime64(data, \"ns\")\n        elif data.dtype.kind == \"m\":\n            data = np.timedelta64(data, \"ns\")\n    return data\n\n\nclass Variable(\n    common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin\n):\n    \"\"\"A netcdf-like variable consisting of dimensions, data and attributes\n    which describe a single Array. A single Variable object is not fully\n    described outside the context of its parent Dataset (if you want such a\n    fully described object, use a DataArray instead).\n\n    The main functional difference between Variables and numpy arrays is that\n    numerical operations on Variables implement array broadcasting by dimension\n    name. For example, adding an Variable with dimensions `('time',)` to\n    another Variable with dimensions `('space',)` results in a new Variable\n    with dimensions `('time', 'space')`. Furthermore, numpy reduce operations\n    like ``mean`` or ``sum`` are overwritten to take a \"dimension\" argument\n    instead of an \"axis\".\n\n    Variables are light-weight objects used as the building block for datasets.\n    They are more primitive objects, so operations with them provide marginally\n    higher performance than using DataArrays. However, manipulating data in the\n    form of a Dataset or DataArray should almost always be preferred, because\n    they can use more complete metadata in context of coordinate labels.\n    \"\"\"\n\n    __slots__ = (\"_dims\", \"_data\", \"_attrs\", \"_encoding\")\n\n    def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n        \"\"\"\n        Parameters\n        ----------\n        dims : str or sequence of str\n            Name(s) of the the data dimension(s). Must be either a string (only\n            for 1D data) or a sequence of strings with length equal to the\n            number of dimensions.\n        data : array_like\n            Data array which supports numpy-like data access.\n        attrs : dict_like or None, optional\n            Attributes to assign to the new variable. If None (default), an\n            empty attribute dictionary is initialized.\n        encoding : dict_like or None, optional\n            Dictionary specifying how to encode this array's data into a\n            serialized format like netCDF4. Currently used keys (for netCDF)\n            include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.\n            Well-behaved code to serialize a Variable should ignore\n            unrecognized encoding items.\n        \"\"\"\n        self._data = as_compatible_data(data, fastpath=fastpath)\n        self._dims = self._parse_dimensions(dims)\n        self._attrs = None\n        self._encoding = None\n        if attrs is not None:\n            self.attrs = attrs\n        if encoding is not None:\n            self.encoding = encoding\n\n    @property\n    def dtype(self):\n        return self._data.dtype\n\n    @property\n    def shape(self):\n        return self._data.shape\n\n    @property\n    def nbytes(self):\n        return self.size * self.dtype.itemsize\n\n    @property\n    def _in_memory(self):\n        return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (\n            isinstance(self._data, indexing.MemoryCachedArray)\n            and isinstance(self._data.array, indexing.NumpyIndexingAdapter)\n        )\n\n    @property\n    def data(self):\n        if is_duck_array(self._data):\n            return self._data\n        else:\n            return self.values\n\n    @data.setter\n    def data(self, data):\n        data = as_compatible_data(data)\n        if data.shape != self.shape:\n            raise ValueError(\n                f\"replacement data must match the Variable's shape. \"\n                f\"replacement data has shape {data.shape}; Variable has shape {self.shape}\"\n            )\n        self._data = data\n\n    def astype(\n        self: VariableType,\n        dtype,\n        *,\n        order=None,\n        casting=None,\n        subok=None,\n        copy=None,\n        keep_attrs=True,\n    ) -> VariableType:\n        \"\"\"\n        Copy of the Variable object, with data cast to a specified type.\n\n        Parameters\n        ----------\n        dtype : str or dtype\n            Typecode or data-type to which the array is cast.\n        order : {'C', 'F', 'A', 'K'}, optional\n            Controls the memory layout order of the result. \u2018C\u2019 means C order,\n            \u2018F\u2019 means Fortran order, \u2018A\u2019 means \u2018F\u2019 order if all the arrays are\n            Fortran contiguous, \u2018C\u2019 order otherwise, and \u2018K\u2019 means as close to\n            the order the array elements appear in memory as possible.\n        casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n            Controls what kind of data casting may occur.\n\n            * 'no' means the data types should not be cast at all.\n            * 'equiv' means only byte-order changes are allowed.\n            * 'safe' means only casts which can preserve values are allowed.\n            * 'same_kind' means only safe casts or casts within a kind,\n              like float64 to float32, are allowed.\n            * 'unsafe' means any data conversions may be done.\n        subok : bool, optional\n            If True, then sub-classes will be passed-through, otherwise the\n            returned array will be forced to be a base-class array.\n        copy : bool, optional\n            By default, astype always returns a newly allocated array. If this\n            is set to False and the `dtype` requirement is satisfied, the input\n            array is returned instead of a copy.\n        keep_attrs : bool, optional\n            By default, astype keeps attributes. Set to False to remove\n            attributes in the returned object.\n\n        Returns\n        -------\n        out : same as object\n            New object with data cast to the specified type.\n\n        Notes\n        -----\n        The ``order``, ``casting``, ``subok`` and ``copy`` arguments are only passed\n        through to the ``astype`` method of the underlying array when a value\n        different than ``None`` is supplied.\n        Make sure to only supply these arguments if the underlying array class\n        supports them.\n\n        See Also\n        --------\n        numpy.ndarray.astype\n        dask.array.Array.astype\n        sparse.COO.astype\n        \"\"\"\n        from .computation import apply_ufunc\n\n        kwargs = dict(order=order, casting=casting, subok=subok, copy=copy)\n        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n\n        return apply_ufunc(\n            duck_array_ops.astype,\n            self,\n            dtype,\n            kwargs=kwargs,\n            keep_attrs=keep_attrs,\n            dask=\"allowed\",\n        )\n\n    def load(self, **kwargs):\n        \"\"\"Manually trigger loading of this variable's data from disk or a\n        remote source into memory and return this variable.\n\n        Normally, it should not be necessary to call this method in user code,\n        because all xarray functions should either work on deferred data or\n        load data automatically.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.array.compute``.\n\n        See Also\n        --------\n        dask.array.compute\n        \"\"\"\n        if is_duck_dask_array(self._data):\n            self._data = as_compatible_data(self._data.compute(**kwargs))\n        elif not is_duck_array(self._data):\n            self._data = np.asarray(self._data)\n        return self\n\n    def compute(self, **kwargs):\n        \"\"\"Manually trigger loading of this variable's data from disk or a\n        remote source into memory and return a new variable. The original is\n        left unaltered.\n\n        Normally, it should not be necessary to call this method in user code,\n        because all xarray functions should either work on deferred data or\n        load data automatically.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments passed on to ``dask.array.compute``.\n\n        See Also\n        --------\n        dask.array.compute\n        \"\"\"\n        new = self.copy(deep=False)\n        return new.load(**kwargs)\n\n    def __dask_tokenize__(self):\n        # Use v.data, instead of v._data, in order to cope with the wrappers\n        # around NetCDF and the like\n        from dask.base import normalize_token\n\n        return normalize_token((type(self), self._dims, self.data, self._attrs))\n\n    def __dask_graph__(self):\n        if is_duck_dask_array(self._data):\n            return self._data.__dask_graph__()\n        else:\n            return None\n\n    def __dask_keys__(self):\n        return self._data.__dask_keys__()\n\n    def __dask_layers__(self):\n        return self._data.__dask_layers__()\n\n    @property\n    def __dask_optimize__(self):\n        return self._data.__dask_optimize__\n\n    @property\n    def __dask_scheduler__(self):\n        return self._data.__dask_scheduler__\n\n    def __dask_postcompute__(self):\n        array_func, array_args = self._data.__dask_postcompute__()\n        return (\n            self._dask_finalize,\n            (array_func, array_args, self._dims, self._attrs, self._encoding),\n        )\n\n    def __dask_postpersist__(self):\n        array_func, array_args = self._data.__dask_postpersist__()\n        return (\n            self._dask_finalize,\n            (array_func, array_args, self._dims, self._attrs, self._encoding),\n        )\n\n    @staticmethod\n    def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):\n        data = array_func(results, *array_args)\n        return Variable(dims, data, attrs=attrs, encoding=encoding)\n\n    @property\n    def values(self):\n        \"\"\"The variable's data as a numpy.ndarray\"\"\"\n        return _as_array_or_item(self._data)\n\n    @values.setter\n    def values(self, values):\n        self.data = values\n\n    def to_base_variable(self):\n        \"\"\"Return this variable as a base xarray.Variable\"\"\"\n        return Variable(\n            self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n        )\n\n    to_variable = utils.alias(to_base_variable, \"to_variable\")\n\n    def to_index_variable(self):\n        \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n        return IndexVariable(\n            self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n        )\n\n    to_coord = utils.alias(to_index_variable, \"to_coord\")\n\n    def to_index(self):\n        \"\"\"Convert this variable to a pandas.Index\"\"\"\n        return self.to_index_variable().to_index()\n\n    def to_dict(self, data=True):\n        \"\"\"Dictionary representation of variable.\"\"\"\n        item = {\"dims\": self.dims, \"attrs\": decode_numpy_dict_values(self.attrs)}\n        if data:\n            item[\"data\"] = ensure_us_time_resolution(self.values).tolist()\n        else:\n            item.update({\"dtype\": str(self.dtype), \"shape\": self.shape})\n        return item\n\n    @property\n    def dims(self):\n        \"\"\"Tuple of dimension names with which this variable is associated.\"\"\"\n        return self._dims\n\n    @dims.setter\n    def dims(self, value):\n        self._dims = self._parse_dimensions(value)\n\n    def _parse_dimensions(self, dims):\n        if isinstance(dims, str):\n            dims = (dims,)\n        dims = tuple(dims)\n        if len(dims) != self.ndim:\n            raise ValueError(\n                \"dimensions %s must have the same length as the \"\n                \"number of data dimensions, ndim=%s\" % (dims, self.ndim)\n            )\n        return dims\n\n    def _item_key_to_tuple(self, key):\n        if utils.is_dict_like(key):\n            return tuple(key.get(dim, slice(None)) for dim in self.dims)\n        else:\n            return key\n\n    def _broadcast_indexes(self, key):\n        \"\"\"Prepare an indexing key for an indexing operation.\n\n        Parameters\n        ----------\n        key : int, slice, array-like, dict or tuple of integer, slice and array-like\n            Any valid input for indexing.\n\n        Returns\n        -------\n        dims : tuple\n            Dimension of the resultant variable.\n        indexers : IndexingTuple subclass\n            Tuple of integer, array-like, or slices to use when indexing\n            self._data. The type of this argument indicates the type of\n            indexing to perform, either basic, outer or vectorized.\n        new_order : Optional[Sequence[int]]\n            Optional reordering to do on the result of indexing. If not None,\n            the first len(new_order) indexing should be moved to these\n            positions.\n        \"\"\"\n        key = self._item_key_to_tuple(key)  # key is a tuple\n        # key is a tuple of full size\n        key = indexing.expanded_indexer(key, self.ndim)\n        # Convert a scalar Variable to an integer\n        key = tuple(\n            k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key\n        )\n        # Convert a 0d-array to an integer\n        key = tuple(\n            k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key\n        )\n\n        if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):\n            return self._broadcast_indexes_basic(key)\n\n        self._validate_indexers(key)\n        # Detect it can be mapped as an outer indexer\n        # If all key is unlabeled, or\n        # key can be mapped as an OuterIndexer.\n        if all(not isinstance(k, Variable) for k in key):\n            return self._broadcast_indexes_outer(key)\n\n        # If all key is 1-dimensional and there are no duplicate labels,\n        # key can be mapped as an OuterIndexer.\n        dims = []\n        for k, d in zip(key, self.dims):\n            if isinstance(k, Variable):\n                if len(k.dims) > 1:\n                    return self._broadcast_indexes_vectorized(key)\n                dims.append(k.dims[0])\n            elif not isinstance(k, integer_types):\n                dims.append(d)\n        if len(set(dims)) == len(dims):\n            return self._broadcast_indexes_outer(key)\n\n        return self._broadcast_indexes_vectorized(key)\n\n    def _broadcast_indexes_basic(self, key):\n        dims = tuple(\n            dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)\n        )\n        return dims, BasicIndexer(key), None\n\n    def _validate_indexers(self, key):\n        \"\"\" Make sanity checks \"\"\"\n        for dim, k in zip(self.dims, key):\n            if isinstance(k, BASIC_INDEXING_TYPES):\n                pass\n            else:\n                if not isinstance(k, Variable):\n                    k = np.asarray(k)\n                    if k.ndim > 1:\n                        raise IndexError(\n                            \"Unlabeled multi-dimensional array cannot be \"\n                            \"used for indexing: {}\".format(k)\n                        )\n                if k.dtype.kind == \"b\":\n                    if self.shape[self.get_axis_num(dim)] != len(k):\n                        raise IndexError(\n                            \"Boolean array size {:d} is used to index array \"\n                            \"with shape {:s}.\".format(len(k), str(self.shape))\n                        )\n                    if k.ndim > 1:\n                        raise IndexError(\n                            \"{}-dimensional boolean indexing is \"\n                            \"not supported. \".format(k.ndim)\n                        )\n                    if getattr(k, \"dims\", (dim,)) != (dim,):\n                        raise IndexError(\n                            \"Boolean indexer should be unlabeled or on the \"\n                            \"same dimension to the indexed array. Indexer is \"\n                            \"on {:s} but the target dimension is {:s}.\".format(\n                                str(k.dims), dim\n                            )\n                        )\n\n    def _broadcast_indexes_outer(self, key):\n        dims = tuple(\n            k.dims[0] if isinstance(k, Variable) else dim\n            for k, dim in zip(key, self.dims)\n            if not isinstance(k, integer_types)\n        )\n\n        new_key = []\n        for k in key:\n            if isinstance(k, Variable):\n                k = k.data\n            if not isinstance(k, BASIC_INDEXING_TYPES):\n                k = np.asarray(k)\n                if k.size == 0:\n                    # Slice by empty list; numpy could not infer the dtype\n                    k = k.astype(int)\n                elif k.dtype.kind == \"b\":\n                    (k,) = np.nonzero(k)\n            new_key.append(k)\n\n        return dims, OuterIndexer(tuple(new_key)), None\n\n    def _nonzero(self):\n        \"\"\" Equivalent numpy's nonzero but returns a tuple of Varibles. \"\"\"\n        # TODO we should replace dask's native nonzero\n        # after https://github.com/dask/dask/issues/1076 is implemented.\n        nonzeros = np.nonzero(self.data)\n        return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))\n\n    def _broadcast_indexes_vectorized(self, key):\n        variables = []\n        out_dims_set = OrderedSet()\n        for dim, value in zip(self.dims, key):\n            if isinstance(value, slice):\n                out_dims_set.add(dim)\n            else:\n                variable = (\n                    value\n                    if isinstance(value, Variable)\n                    else as_variable(value, name=dim)\n                )\n                if variable.dtype.kind == \"b\":  # boolean indexing case\n                    (variable,) = variable._nonzero()\n\n                variables.append(variable)\n                out_dims_set.update(variable.dims)\n\n        variable_dims = set()\n        for variable in variables:\n            variable_dims.update(variable.dims)\n\n        slices = []\n        for i, (dim, value) in enumerate(zip(self.dims, key)):\n            if isinstance(value, slice):\n                if dim in variable_dims:\n                    # We only convert slice objects to variables if they share\n                    # a dimension with at least one other variable. Otherwise,\n                    # we can equivalently leave them as slices aknd transpose\n                    # the result. This is significantly faster/more efficient\n                    # for most array backends.\n                    values = np.arange(*value.indices(self.sizes[dim]))\n                    variables.insert(i - len(slices), Variable((dim,), values))\n                else:\n                    slices.append((i, value))\n\n        try:\n            variables = _broadcast_compat_variables(*variables)\n        except ValueError:\n            raise IndexError(f\"Dimensions of indexers mismatch: {key}\")\n\n        out_key = [variable.data for variable in variables]\n        out_dims = tuple(out_dims_set)\n        slice_positions = set()\n        for i, value in slices:\n            out_key.insert(i, value)\n            new_position = out_dims.index(self.dims[i])\n            slice_positions.add(new_position)\n\n        if slice_positions:\n            new_order = [i for i in range(len(out_dims)) if i not in slice_positions]\n        else:\n            new_order = None\n\n        return out_dims, VectorizedIndexer(tuple(out_key)), new_order\n\n    def __getitem__(self: VariableType, key) -> VariableType:\n        \"\"\"Return a new Variable object whose contents are consistent with\n        getting the provided key from the underlying data.\n\n        NB. __getitem__ and __setitem__ implement xarray-style indexing,\n        where if keys are unlabeled arrays, we index the array orthogonally\n        with them. If keys are labeled array (such as Variables), they are\n        broadcasted with our usual scheme and then the array is indexed with\n        the broadcasted key, like numpy's fancy indexing.\n\n        If you really want to do indexing like `x[x > 0]`, manipulate the numpy\n        array `x.values` directly.\n        \"\"\"\n        dims, indexer, new_order = self._broadcast_indexes(key)\n        data = as_indexable(self._data)[indexer]\n        if new_order:\n            data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n        return self._finalize_indexing_result(dims, data)\n\n    def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n        \"\"\"Used by IndexVariable to return IndexVariable objects when possible.\"\"\"\n        return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n\n    def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n        \"\"\"Index this Variable with -1 remapped to fill_value.\"\"\"\n        # TODO(shoyer): expose this method in public API somewhere (isel?) and\n        # use it for reindex.\n        # TODO(shoyer): add a sanity check that all other integers are\n        # non-negative\n        # TODO(shoyer): add an optimization, remapping -1 to an adjacent value\n        # that is actually indexed rather than mapping it to the last value\n        # along each axis.\n\n        if fill_value is dtypes.NA:\n            fill_value = dtypes.get_fill_value(self.dtype)\n\n        dims, indexer, new_order = self._broadcast_indexes(key)\n\n        if self.size:\n            if is_duck_dask_array(self._data):\n                # dask's indexing is faster this way; also vindex does not\n                # support negative indices yet:\n                # https://github.com/dask/dask/pull/2967\n                actual_indexer = indexing.posify_mask_indexer(indexer)\n            else:\n                actual_indexer = indexer\n\n            data = as_indexable(self._data)[actual_indexer]\n            mask = indexing.create_mask(indexer, self.shape, data)\n            # we need to invert the mask in order to pass data first. This helps\n            # pint to choose the correct unit\n            # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\n            data = duck_array_ops.where(np.logical_not(mask), data, fill_value)\n        else:\n            # array cannot be indexed along dimensions of size 0, so just\n            # build the mask directly instead.\n            mask = indexing.create_mask(indexer, self.shape)\n            data = np.broadcast_to(fill_value, getattr(mask, \"shape\", ()))\n\n        if new_order:\n            data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n        return self._finalize_indexing_result(dims, data)\n\n    def __setitem__(self, key, value):\n        \"\"\"__setitem__ is overloaded to access the underlying numpy values with\n        orthogonal indexing.\n\n        See __getitem__ for more details.\n        \"\"\"\n        dims, index_tuple, new_order = self._broadcast_indexes(key)\n\n        if not isinstance(value, Variable):\n            value = as_compatible_data(value)\n            if value.ndim > len(dims):\n                raise ValueError(\n                    \"shape mismatch: value array of shape %s could not be \"\n                    \"broadcast to indexing result with %s dimensions\"\n                    % (value.shape, len(dims))\n                )\n            if value.ndim == 0:\n                value = Variable((), value)\n            else:\n                value = Variable(dims[-value.ndim :], value)\n        # broadcast to become assignable\n        value = value.set_dims(dims).data\n\n        if new_order:\n            value = duck_array_ops.asarray(value)\n            value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]\n            value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))\n\n        indexable = as_indexable(self._data)\n        indexable[index_tuple] = value\n\n    @property\n    def attrs(self) -> Dict[Hashable, Any]:\n        \"\"\"Dictionary of local attributes on this variable.\"\"\"\n        if self._attrs is None:\n            self._attrs = {}\n        return self._attrs\n\n    @attrs.setter\n    def attrs(self, value: Mapping[Hashable, Any]) -> None:\n        self._attrs = dict(value)\n\n    @property\n    def encoding(self):\n        \"\"\"Dictionary of encodings on this variable.\"\"\"\n        if self._encoding is None:\n            self._encoding = {}\n        return self._encoding\n\n    @encoding.setter\n    def encoding(self, value):\n        try:\n            self._encoding = dict(value)\n        except ValueError:\n            raise ValueError(\"encoding must be castable to a dictionary\")\n\n    def copy(self, deep=True, data=None):\n        \"\"\"Returns a copy of this object.\n\n        If `deep=True`, the data array is loaded into memory and copied onto\n        the new object. Dimensions, attributes and encodings are always copied.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Whether the data array is loaded into memory and copied onto\n            the new object. Default is True.\n        data : array_like, optional\n            Data to use in the new object. Must have same shape as original.\n            When `data` is used, `deep` is ignored.\n\n        Returns\n        -------\n        object : Variable\n            New object with dimensions, attributes, encodings, and optionally\n            data copied from original.\n\n        Examples\n        --------\n        Shallow copy versus deep copy\n\n        >>> var = xr.Variable(data=[1, 2, 3], dims=\"x\")\n        >>> var.copy()\n        <xarray.Variable (x: 3)>\n        array([1, 2, 3])\n        >>> var_0 = var.copy(deep=False)\n        >>> var_0[0] = 7\n        >>> var_0\n        <xarray.Variable (x: 3)>\n        array([7, 2, 3])\n        >>> var\n        <xarray.Variable (x: 3)>\n        array([7, 2, 3])\n\n        Changing the data using the ``data`` argument maintains the\n        structure of the original object, but with the new data. Original\n        object is unaffected.\n\n        >>> var.copy(data=[0.1, 0.2, 0.3])\n        <xarray.Variable (x: 3)>\n        array([0.1, 0.2, 0.3])\n        >>> var\n        <xarray.Variable (x: 3)>\n        array([7, 2, 3])\n\n        See Also\n        --------\n        pandas.DataFrame.copy\n        \"\"\"\n        if data is None:\n            data = self._data\n\n            if isinstance(data, indexing.MemoryCachedArray):\n                # don't share caching between copies\n                data = indexing.MemoryCachedArray(data.array)\n\n            if deep:\n                data = copy.deepcopy(data)\n\n        else:\n            data = as_compatible_data(data)\n            if self.shape != data.shape:\n                raise ValueError(\n                    \"Data shape {} must match shape of object {}\".format(\n                        data.shape, self.shape\n                    )\n                )\n\n        # note:\n        # dims is already an immutable tuple\n        # attributes and encoding will be copied when the new Array is created\n        return self._replace(data=data)\n\n    def _replace(\n        self, dims=_default, data=_default, attrs=_default, encoding=_default\n    ) -> \"Variable\":\n        if dims is _default:\n            dims = copy.copy(self._dims)\n        if data is _default:\n            data = copy.copy(self.data)\n        if attrs is _default:\n            attrs = copy.copy(self._attrs)\n        if encoding is _default:\n            encoding = copy.copy(self._encoding)\n        return type(self)(dims, data, attrs, encoding, fastpath=True)\n\n    def __copy__(self):\n        return self.copy(deep=False)\n\n    def __deepcopy__(self, memo=None):\n        # memo does nothing but is required for compatibility with\n        # copy.deepcopy\n        return self.copy(deep=True)\n\n    # mutable objects should not be hashable\n    # https://github.com/python/mypy/issues/4266\n    __hash__ = None  # type: ignore\n\n    @property\n    def chunks(self):\n        \"\"\"Block dimensions for this array's data or None if it's not a dask\n        array.\n        \"\"\"\n        return getattr(self._data, \"chunks\", None)\n\n    _array_counter = itertools.count()\n\n    def chunk(self, chunks={}, name=None, lock=False):\n        \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n\n        If this variable is a non-dask array, it will be converted to dask\n        array. If it's a dask array, it will be rechunked to the given chunk\n        sizes.\n\n        If neither chunks is not provided for one or more dimensions, chunk\n        sizes along that dimension will not be updated; non-dask arrays will be\n        converted into dask arrays with a single block.\n\n        Parameters\n        ----------\n        chunks : int, tuple or dict, optional\n            Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n            ``{'x': 5, 'y': 5}``.\n        name : str, optional\n            Used to generate the name for this array in the internal dask\n            graph. Does not need not be unique.\n        lock : optional\n            Passed on to :py:func:`dask.array.from_array`, if the array is not\n            already as dask array.\n\n        Returns\n        -------\n        chunked : xarray.Variable\n        \"\"\"\n        import dask\n        import dask.array as da\n\n        if chunks is None:\n            warnings.warn(\n                \"None value for 'chunks' is deprecated. \"\n                \"It will raise an error in the future. Use instead '{}'\",\n                category=FutureWarning,\n            )\n            chunks = {}\n\n        if utils.is_dict_like(chunks):\n            chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}\n\n        data = self._data\n        if is_duck_dask_array(data):\n            data = data.rechunk(chunks)\n        else:\n            if isinstance(data, indexing.ExplicitlyIndexed):\n                # Unambiguously handle array storage backends (like NetCDF4 and h5py)\n                # that can't handle general array indexing. For example, in netCDF4 you\n                # can do \"outer\" indexing along two dimensions independent, which works\n                # differently from how NumPy handles it.\n                # da.from_array works by using lazy indexing with a tuple of slices.\n                # Using OuterIndexer is a pragmatic choice: dask does not yet handle\n                # different indexing types in an explicit way:\n                # https://github.com/dask/dask/issues/2883\n                data = indexing.ImplicitToExplicitIndexingAdapter(\n                    data, indexing.OuterIndexer\n                )\n                if LooseVersion(dask.__version__) < \"2.0.0\":\n                    kwargs = {}\n                else:\n                    # All of our lazily loaded backend array classes should use NumPy\n                    # array operations.\n                    kwargs = {\"meta\": np.ndarray}\n            else:\n                kwargs = {}\n\n            if utils.is_dict_like(chunks):\n                chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))\n\n            data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)\n\n        return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n\n    def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):\n        \"\"\"\n        use sparse-array as backend.\n        \"\"\"\n        import sparse\n\n        # TODO: what to do if dask-backended?\n        if fill_value is dtypes.NA:\n            dtype, fill_value = dtypes.maybe_promote(self.dtype)\n        else:\n            dtype = dtypes.result_type(self.dtype, fill_value)\n\n        if sparse_format is _default:\n            sparse_format = \"coo\"\n        try:\n            as_sparse = getattr(sparse, f\"as_{sparse_format.lower()}\")\n        except AttributeError:\n            raise ValueError(f\"{sparse_format} is not a valid sparse format\")\n\n        data = as_sparse(self.data.astype(dtype), fill_value=fill_value)\n        return self._replace(data=data)\n\n    def _to_dense(self):\n        \"\"\"\n        Change backend from sparse to np.array\n        \"\"\"\n        if hasattr(self._data, \"todense\"):\n            return self._replace(data=self._data.todense())\n        return self.copy(deep=False)\n\n    def isel(\n        self: VariableType,\n        indexers: Mapping[Hashable, Any] = None,\n        missing_dims: str = \"raise\",\n        **indexers_kwargs: Any,\n    ) -> VariableType:\n        \"\"\"Return a new array indexed along the specified dimension(s).\n\n        Parameters\n        ----------\n        **indexers : {dim: indexer, ...}\n            Keyword arguments with names matching dimensions and values given\n            by integers, slice objects or arrays.\n        missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n            What to do if dimensions that should be selected from are not present in the\n            DataArray:\n            - \"raise\": raise an exception\n            - \"warning\": raise a warning, and ignore the missing dimensions\n            - \"ignore\": ignore the missing dimensions\n\n        Returns\n        -------\n        obj : Array object\n            A new Array with the selected data and dimensions. In general,\n            the new variable's data will be a view of this variable's data,\n            unless numpy fancy indexing was triggered by using an array\n            indexer, in which case the data will be a copy.\n        \"\"\"\n        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n\n        indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n\n        key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)\n        return self[key]\n\n    def squeeze(self, dim=None):\n        \"\"\"Return a new object with squeezed data.\n\n        Parameters\n        ----------\n        dim : None or str or tuple of str, optional\n            Selects a subset of the length one dimensions. If a dimension is\n            selected with length greater than one, an error is raised. If\n            None, all length one dimensions are squeezed.\n\n        Returns\n        -------\n        squeezed : same type as caller\n            This object, but with with all or a subset of the dimensions of\n            length 1 removed.\n\n        See Also\n        --------\n        numpy.squeeze\n        \"\"\"\n        dims = common.get_squeeze_dims(self, dim)\n        return self.isel({d: 0 for d in dims})\n\n    def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):\n        axis = self.get_axis_num(dim)\n\n        if count > 0:\n            keep = slice(None, -count)\n        elif count < 0:\n            keep = slice(-count, None)\n        else:\n            keep = slice(None)\n\n        trimmed_data = self[(slice(None),) * axis + (keep,)].data\n\n        if fill_value is dtypes.NA:\n            dtype, fill_value = dtypes.maybe_promote(self.dtype)\n        else:\n            dtype = self.dtype\n\n        width = min(abs(count), self.shape[axis])\n        dim_pad = (width, 0) if count >= 0 else (0, width)\n        pads = [(0, 0) if d != dim else dim_pad for d in self.dims]\n\n        data = duck_array_ops.pad(\n            trimmed_data.astype(dtype),\n            pads,\n            mode=\"constant\",\n            constant_values=fill_value,\n        )\n\n        if is_duck_dask_array(data):\n            # chunked data should come out with the same chunks; this makes\n            # it feasible to combine shifted and unshifted data\n            # TODO: remove this once dask.array automatically aligns chunks\n            data = data.rechunk(self.data.chunks)\n\n        return type(self)(self.dims, data, self._attrs, fastpath=True)\n\n    def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n        \"\"\"\n        Return a new Variable with shifted data.\n\n        Parameters\n        ----------\n        shifts : mapping of the form {dim: offset}\n            Integer offset to shift along each of the given dimensions.\n            Positive offsets shift to the right; negative offsets shift to the\n            left.\n        fill_value : scalar, optional\n            Value to use for newly missing values\n        **shifts_kwargs\n            The keyword arguments form of ``shifts``.\n            One of shifts or shifts_kwargs must be provided.\n\n        Returns\n        -------\n        shifted : Variable\n            Variable with the same dimensions and attributes but shifted data.\n        \"\"\"\n        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n        result = self\n        for dim, count in shifts.items():\n            result = result._shift_one_dim(dim, count, fill_value=fill_value)\n        return result\n\n    def _pad_options_dim_to_index(\n        self,\n        pad_option: Mapping[Hashable, Union[int, Tuple[int, int]]],\n        fill_with_shape=False,\n    ):\n        if fill_with_shape:\n            return [\n                (n, n) if d not in pad_option else pad_option[d]\n                for d, n in zip(self.dims, self.data.shape)\n            ]\n        return [(0, 0) if d not in pad_option else pad_option[d] for d in self.dims]\n\n    def pad(\n        self,\n        pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n        mode: str = \"constant\",\n        stat_length: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        constant_values: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        end_values: Union[\n            int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n        ] = None,\n        reflect_type: str = None,\n        **pad_width_kwargs: Any,\n    ):\n        \"\"\"\n        Return a new Variable with padded data.\n\n        Parameters\n        ----------\n        pad_width : mapping of hashable to tuple of int\n            Mapping with the form of {dim: (pad_before, pad_after)}\n            describing the number of values padded along each dimension.\n            {dim: pad} is a shortcut for pad_before = pad_after = pad\n        mode : str, default: \"constant\"\n            See numpy / Dask docs\n        stat_length : int, tuple or mapping of hashable to tuple\n            Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n            values at edge of each axis used to calculate the statistic value.\n        constant_values : scalar, tuple or mapping of hashable to tuple\n            Used in 'constant'.  The values to set the padded values for each\n            axis.\n        end_values : scalar, tuple or mapping of hashable to tuple\n            Used in 'linear_ramp'.  The values used for the ending value of the\n            linear_ramp and that will form the edge of the padded array.\n        reflect_type : {\"even\", \"odd\"}, optional\n            Used in \"reflect\", and \"symmetric\".  The \"even\" style is the\n            default with an unaltered reflection around the edge value.  For\n            the \"odd\" style, the extended part of the array is created by\n            subtracting the reflected values from two times the edge value.\n        **pad_width_kwargs\n            One of pad_width or pad_width_kwargs must be provided.\n\n        Returns\n        -------\n        padded : Variable\n            Variable with the same dimensions and attributes but padded data.\n        \"\"\"\n        pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, \"pad\")\n\n        # change default behaviour of pad with mode constant\n        if mode == \"constant\" and (\n            constant_values is None or constant_values is dtypes.NA\n        ):\n            dtype, constant_values = dtypes.maybe_promote(self.dtype)\n        else:\n            dtype = self.dtype\n\n        # create pad_options_kwargs, numpy requires only relevant kwargs to be nonempty\n        if isinstance(stat_length, dict):\n            stat_length = self._pad_options_dim_to_index(\n                stat_length, fill_with_shape=True\n            )\n        if isinstance(constant_values, dict):\n            constant_values = self._pad_options_dim_to_index(constant_values)\n        if isinstance(end_values, dict):\n            end_values = self._pad_options_dim_to_index(end_values)\n\n        # workaround for bug in Dask's default value of stat_length https://github.com/dask/dask/issues/5303\n        if stat_length is None and mode in [\"maximum\", \"mean\", \"median\", \"minimum\"]:\n            stat_length = [(n, n) for n in self.data.shape]  # type: ignore\n\n        # change integer values to a tuple of two of those values and change pad_width to index\n        for k, v in pad_width.items():\n            if isinstance(v, numbers.Number):\n                pad_width[k] = (v, v)\n        pad_width_by_index = self._pad_options_dim_to_index(pad_width)\n\n        # create pad_options_kwargs, numpy/dask requires only relevant kwargs to be nonempty\n        pad_option_kwargs = {}\n        if stat_length is not None:\n            pad_option_kwargs[\"stat_length\"] = stat_length\n        if constant_values is not None:\n            pad_option_kwargs[\"constant_values\"] = constant_values\n        if end_values is not None:\n            pad_option_kwargs[\"end_values\"] = end_values\n        if reflect_type is not None:\n            pad_option_kwargs[\"reflect_type\"] = reflect_type  # type: ignore\n\n        array = duck_array_ops.pad(\n            self.data.astype(dtype, copy=False),\n            pad_width_by_index,\n            mode=mode,\n            **pad_option_kwargs,\n        )\n\n        return type(self)(self.dims, array)\n\n    def _roll_one_dim(self, dim, count):\n        axis = self.get_axis_num(dim)\n\n        count %= self.shape[axis]\n        if count != 0:\n            indices = [slice(-count, None), slice(None, -count)]\n        else:\n            indices = [slice(None)]\n\n        arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]\n\n        data = duck_array_ops.concatenate(arrays, axis)\n\n        if is_duck_dask_array(data):\n            # chunked data should come out with the same chunks; this makes\n            # it feasible to combine shifted and unshifted data\n            # TODO: remove this once dask.array automatically aligns chunks\n            data = data.rechunk(self.data.chunks)\n\n        return type(self)(self.dims, data, self._attrs, fastpath=True)\n\n    def roll(self, shifts=None, **shifts_kwargs):\n        \"\"\"\n        Return a new Variable with rolld data.\n\n        Parameters\n        ----------\n        shifts : mapping of hashable to int\n            Integer offset to roll along each of the given dimensions.\n            Positive offsets roll to the right; negative offsets roll to the\n            left.\n        **shifts_kwargs\n            The keyword arguments form of ``shifts``.\n            One of shifts or shifts_kwargs must be provided.\n\n        Returns\n        -------\n        shifted : Variable\n            Variable with the same dimensions and attributes but rolled data.\n        \"\"\"\n        shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n\n        result = self\n        for dim, count in shifts.items():\n            result = result._roll_one_dim(dim, count)\n        return result\n\n    def transpose(self, *dims) -> \"Variable\":\n        \"\"\"Return a new Variable object with transposed dimensions.\n\n        Parameters\n        ----------\n        *dims : str, optional\n            By default, reverse the dimensions. Otherwise, reorder the\n            dimensions to this order.\n\n        Returns\n        -------\n        transposed : Variable\n            The returned object has transposed data and dimensions with the\n            same attributes as the original.\n\n        Notes\n        -----\n        This operation returns a view of this variable's data. It is\n        lazy for dask-backed Variables but not for numpy-backed Variables.\n\n        See Also\n        --------\n        numpy.transpose\n        \"\"\"\n        if len(dims) == 0:\n            dims = self.dims[::-1]\n        dims = tuple(infix_dims(dims, self.dims))\n        axes = self.get_axis_num(dims)\n        if len(dims) < 2 or dims == self.dims:\n            # no need to transpose if only one dimension\n            # or dims are in same order\n            return self.copy(deep=False)\n\n        data = as_indexable(self._data).transpose(axes)\n        return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n\n    @property\n    def T(self) -> \"Variable\":\n        return self.transpose()\n\n    def set_dims(self, dims, shape=None):\n        \"\"\"Return a new variable with given set of dimensions.\n        This method might be used to attach new dimension(s) to variable.\n\n        When possible, this operation does not copy this variable's data.\n\n        Parameters\n        ----------\n        dims : str or sequence of str or dict\n            Dimensions to include on the new variable. If a dict, values are\n            used to provide the sizes of new dimensions; otherwise, new\n            dimensions are inserted with length 1.\n\n        Returns\n        -------\n        Variable\n        \"\"\"\n        if isinstance(dims, str):\n            dims = [dims]\n\n        if shape is None and utils.is_dict_like(dims):\n            shape = dims.values()\n\n        missing_dims = set(self.dims) - set(dims)\n        if missing_dims:\n            raise ValueError(\n                \"new dimensions %r must be a superset of \"\n                \"existing dimensions %r\" % (dims, self.dims)\n            )\n\n        self_dims = set(self.dims)\n        expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims\n\n        if self.dims == expanded_dims:\n            # don't use broadcast_to unless necessary so the result remains\n            # writeable if possible\n            expanded_data = self.data\n        elif shape is not None:\n            dims_map = dict(zip(dims, shape))\n            tmp_shape = tuple(dims_map[d] for d in expanded_dims)\n            expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n        else:\n            expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n\n        expanded_var = Variable(\n            expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True\n        )\n        return expanded_var.transpose(*dims)\n\n    def _stack_once(self, dims: List[Hashable], new_dim: Hashable):\n        if not set(dims) <= set(self.dims):\n            raise ValueError(\"invalid existing dimensions: %s\" % dims)\n\n        if new_dim in self.dims:\n            raise ValueError(\n                \"cannot create a new dimension with the same \"\n                \"name as an existing dimension\"\n            )\n\n        if len(dims) == 0:\n            # don't stack\n            return self.copy(deep=False)\n\n        other_dims = [d for d in self.dims if d not in dims]\n        dim_order = other_dims + list(dims)\n        reordered = self.transpose(*dim_order)\n\n        new_shape = reordered.shape[: len(other_dims)] + (-1,)\n        new_data = reordered.data.reshape(new_shape)\n        new_dims = reordered.dims[: len(other_dims)] + (new_dim,)\n\n        return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n\n    def stack(self, dimensions=None, **dimensions_kwargs):\n        \"\"\"\n        Stack any number of existing dimensions into a single new dimension.\n\n        New dimensions will be added at the end, and the order of the data\n        along each new dimension will be in contiguous (C) order.\n\n        Parameters\n        ----------\n        dimensions : mapping of hashable to tuple of hashable\n            Mapping of form new_name=(dim1, dim2, ...) describing the\n            names of new dimensions, and the existing dimensions that\n            they replace.\n        **dimensions_kwargs\n            The keyword arguments form of ``dimensions``.\n            One of dimensions or dimensions_kwargs must be provided.\n\n        Returns\n        -------\n        stacked : Variable\n            Variable with the same attributes but stacked data.\n\n        See Also\n        --------\n        Variable.unstack\n        \"\"\"\n        dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n        result = self\n        for new_dim, dims in dimensions.items():\n            result = result._stack_once(dims, new_dim)\n        return result\n\n    def _unstack_once_full(\n        self, dims: Mapping[Hashable, int], old_dim: Hashable\n    ) -> \"Variable\":\n        \"\"\"\n        Unstacks the variable without needing an index.\n\n        Unlike `_unstack_once`, this function requires the existing dimension to\n        contain the full product of the new dimensions.\n        \"\"\"\n        new_dim_names = tuple(dims.keys())\n        new_dim_sizes = tuple(dims.values())\n\n        if old_dim not in self.dims:\n            raise ValueError(\"invalid existing dimension: %s\" % old_dim)\n\n        if set(new_dim_names).intersection(self.dims):\n            raise ValueError(\n                \"cannot create a new dimension with the same \"\n                \"name as an existing dimension\"\n            )\n\n        if np.prod(new_dim_sizes) != self.sizes[old_dim]:\n            raise ValueError(\n                \"the product of the new dimension sizes must \"\n                \"equal the size of the old dimension\"\n            )\n\n        other_dims = [d for d in self.dims if d != old_dim]\n        dim_order = other_dims + [old_dim]\n        reordered = self.transpose(*dim_order)\n\n        new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes\n        new_data = reordered.data.reshape(new_shape)\n        new_dims = reordered.dims[: len(other_dims)] + new_dim_names\n\n        return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n\n    def _unstack_once(\n        self,\n        index: pd.MultiIndex,\n        dim: Hashable,\n        fill_value=dtypes.NA,\n    ) -> \"Variable\":\n        \"\"\"\n        Unstacks this variable given an index to unstack and the name of the\n        dimension to which the index refers.\n        \"\"\"\n\n        reordered = self.transpose(..., dim)\n\n        new_dim_sizes = [lev.size for lev in index.levels]\n        new_dim_names = index.names\n        indexer = index.codes\n\n        # Potentially we could replace `len(other_dims)` with just `-1`\n        other_dims = [d for d in self.dims if d != dim]\n        new_shape = list(reordered.shape[: len(other_dims)]) + new_dim_sizes\n        new_dims = reordered.dims[: len(other_dims)] + new_dim_names\n\n        if fill_value is dtypes.NA:\n            is_missing_values = np.prod(new_shape) > np.prod(self.shape)\n            if is_missing_values:\n                dtype, fill_value = dtypes.maybe_promote(self.dtype)\n            else:\n                dtype = self.dtype\n                fill_value = dtypes.get_fill_value(dtype)\n        else:\n            dtype = self.dtype\n\n        # Currently fails on sparse due to https://github.com/pydata/sparse/issues/422\n        data = np.full_like(\n            self.data,\n            fill_value=fill_value,\n            shape=new_shape,\n            dtype=dtype,\n        )\n\n        # Indexer is a list of lists of locations. Each list is the locations\n        # on the new dimension. This is robust to the data being sparse; in that\n        # case the destinations will be NaN / zero.\n        data[(..., *indexer)] = reordered\n\n        return self._replace(dims=new_dims, data=data)\n\n    def unstack(self, dimensions=None, **dimensions_kwargs):\n        \"\"\"\n        Unstack an existing dimension into multiple new dimensions.\n\n        New dimensions will be added at the end, and the order of the data\n        along each new dimension will be in contiguous (C) order.\n\n        Note that unlike ``DataArray.unstack`` and ``Dataset.unstack``, this\n        method requires the existing dimension to contain the full product of\n        the new dimensions.\n\n        Parameters\n        ----------\n        dimensions : mapping of hashable to mapping of hashable to int\n            Mapping of the form old_dim={dim1: size1, ...} describing the\n            names of existing dimensions, and the new dimensions and sizes\n            that they map to.\n        **dimensions_kwargs\n            The keyword arguments form of ``dimensions``.\n            One of dimensions or dimensions_kwargs must be provided.\n\n        Returns\n        -------\n        unstacked : Variable\n            Variable with the same attributes but unstacked data.\n\n        See Also\n        --------\n        Variable.stack\n        DataArray.unstack\n        Dataset.unstack\n        \"\"\"\n        dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"unstack\")\n        result = self\n        for old_dim, dims in dimensions.items():\n            result = result._unstack_once_full(dims, old_dim)\n        return result\n\n    def fillna(self, value):\n        return ops.fillna(self, value)\n\n    def where(self, cond, other=dtypes.NA):\n        return ops.where_method(self, cond, other)\n\n    def reduce(\n        self,\n        func,\n        dim=None,\n        axis=None,\n        keep_attrs=None,\n        keepdims=False,\n        **kwargs,\n    ):\n        \"\"\"Reduce this array by applying `func` along some dimension(s).\n\n        Parameters\n        ----------\n        func : callable\n            Function which can be called in the form\n            `func(x, axis=axis, **kwargs)` to return the result of reducing an\n            np.ndarray over an integer valued axis.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply `func`.\n        axis : int or sequence of int, optional\n            Axis(es) over which to apply `func`. Only one of the 'dim'\n            and 'axis' arguments can be supplied. If neither are supplied, then\n            the reduction is calculated over the flattened array (by calling\n            `func(x)` without an axis argument).\n        keep_attrs : bool, optional\n            If True, the variable's attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n        keepdims : bool, default: False\n            If True, the dimensions which are reduced are left in the result\n            as dimensions of size one\n        **kwargs : dict\n            Additional keyword arguments passed on to `func`.\n\n        Returns\n        -------\n        reduced : Array\n            Array with summarized data and the indicated dimension(s)\n            removed.\n        \"\"\"\n        if dim == ...:\n            dim = None\n        if dim is not None and axis is not None:\n            raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n\n        if dim is not None:\n            axis = self.get_axis_num(dim)\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", r\"Mean of empty slice\", category=RuntimeWarning\n            )\n            if axis is not None:\n                data = func(self.data, axis=axis, **kwargs)\n            else:\n                data = func(self.data, **kwargs)\n\n        if getattr(data, \"shape\", ()) == self.shape:\n            dims = self.dims\n        else:\n            removed_axes = (\n                range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n            )\n            if keepdims:\n                # Insert np.newaxis for removed dims\n                slices = tuple(\n                    np.newaxis if i in removed_axes else slice(None, None)\n                    for i in range(self.ndim)\n                )\n                if getattr(data, \"shape\", None) is None:\n                    # Reduce has produced a scalar value, not an array-like\n                    data = np.asanyarray(data)[slices]\n                else:\n                    data = data[slices]\n                dims = self.dims\n            else:\n                dims = [\n                    adim for n, adim in enumerate(self.dims) if n not in removed_axes\n                ]\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        attrs = self._attrs if keep_attrs else None\n\n        return Variable(dims, data, attrs=attrs)\n\n    @classmethod\n    def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n        \"\"\"Concatenate variables along a new or existing dimension.\n\n        Parameters\n        ----------\n        variables : iterable of Variable\n            Arrays to stack together. Each variable is expected to have\n            matching dimensions and shape except for along the stacked\n            dimension.\n        dim : str or DataArray, optional\n            Name of the dimension to stack along. This can either be a new\n            dimension name, in which case it is added along axis=0, or an\n            existing dimension name, in which case the location of the\n            dimension is unchanged. Where to insert the new dimension is\n            determined by the first variable.\n        positions : None or list of array-like, optional\n            List of integer arrays which specifies the integer positions to\n            which to assign each dataset along the concatenated dimension.\n            If not supplied, objects are concatenated in the provided order.\n        shortcut : bool, optional\n            This option is used internally to speed-up groupby operations.\n            If `shortcut` is True, some checks of internal consistency between\n            arrays to concatenate are skipped.\n\n        Returns\n        -------\n        stacked : Variable\n            Concatenated Variable formed by stacking all the supplied variables\n            along the given dimension.\n        \"\"\"\n        if not isinstance(dim, str):\n            (dim,) = dim.dims\n\n        # can't do this lazily: we need to loop through variables at least\n        # twice\n        variables = list(variables)\n        first_var = variables[0]\n\n        arrays = [v.data for v in variables]\n\n        if dim in first_var.dims:\n            axis = first_var.get_axis_num(dim)\n            dims = first_var.dims\n            data = duck_array_ops.concatenate(arrays, axis=axis)\n            if positions is not None:\n                # TODO: deprecate this option -- we don't need it for groupby\n                # any more.\n                indices = nputils.inverse_permutation(np.concatenate(positions))\n                data = duck_array_ops.take(data, indices, axis=axis)\n        else:\n            axis = 0\n            dims = (dim,) + first_var.dims\n            data = duck_array_ops.stack(arrays, axis=axis)\n\n        attrs = dict(first_var.attrs)\n        encoding = dict(first_var.encoding)\n        if not shortcut:\n            for var in variables:\n                if var.dims != first_var.dims:\n                    raise ValueError(\n                        f\"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}\"\n                    )\n\n        return cls(dims, data, attrs, encoding)\n\n    def equals(self, other, equiv=duck_array_ops.array_equiv):\n        \"\"\"True if two Variables have the same dimensions and values;\n        otherwise False.\n\n        Variables can still be equal (like pandas objects) if they have NaN\n        values in the same locations.\n\n        This method is necessary because `v1 == v2` for Variables\n        does element-wise comparisons (like numpy.ndarrays).\n        \"\"\"\n        other = getattr(other, \"variable\", other)\n        try:\n            return self.dims == other.dims and (\n                self._data is other._data or equiv(self.data, other.data)\n            )\n        except (TypeError, AttributeError):\n            return False\n\n    def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):\n        \"\"\"True if two Variables have the values after being broadcast against\n        each other; otherwise False.\n\n        Variables can still be equal (like pandas objects) if they have NaN\n        values in the same locations.\n        \"\"\"\n        try:\n            self, other = broadcast_variables(self, other)\n        except (ValueError, AttributeError):\n            return False\n        return self.equals(other, equiv=equiv)\n\n    def identical(self, other, equiv=duck_array_ops.array_equiv):\n        \"\"\"Like equals, but also checks attributes.\"\"\"\n        try:\n            return utils.dict_equiv(self.attrs, other.attrs) and self.equals(\n                other, equiv=equiv\n            )\n        except (TypeError, AttributeError):\n            return False\n\n    def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):\n        \"\"\"True if the intersection of two Variable's non-null data is\n        equal; otherwise false.\n\n        Variables can thus still be equal if there are locations where either,\n        or both, contain NaN values.\n        \"\"\"\n        return self.broadcast_equals(other, equiv=equiv)\n\n    def quantile(\n        self, q, dim=None, interpolation=\"linear\", keep_attrs=None, skipna=True\n    ):\n        \"\"\"Compute the qth quantile of the data along the specified dimension.\n\n        Returns the qth quantiles(s) of the array elements.\n\n        Parameters\n        ----------\n        q : float or sequence of float\n            Quantile to compute, which must be between 0 and 1\n            inclusive.\n        dim : str or sequence of str, optional\n            Dimension(s) over which to apply quantile.\n        interpolation : {\"linear\", \"lower\", \"higher\", \"midpoint\", \"nearest\"}, default: \"linear\"\n            This optional parameter specifies the interpolation method to\n            use when the desired quantile lies between two data points\n            ``i < j``:\n\n                * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n                  the fractional part of the index surrounded by ``i`` and\n                  ``j``.\n                * lower: ``i``.\n                * higher: ``j``.\n                * nearest: ``i`` or ``j``, whichever is nearest.\n                * midpoint: ``(i + j) / 2``.\n        keep_attrs : bool, optional\n            If True, the variable's attributes (`attrs`) will be copied from\n            the original object to the new one.  If False (default), the new\n            object will be returned without attributes.\n\n        Returns\n        -------\n        quantiles : Variable\n            If `q` is a single quantile, then the result\n            is a scalar. If multiple percentiles are given, first axis of\n            the result corresponds to the quantile and a quantile dimension\n            is added to the return array. The other dimensions are the\n            dimensions that remain after the reduction of the array.\n\n        See Also\n        --------\n        numpy.nanquantile, pandas.Series.quantile, Dataset.quantile\n        DataArray.quantile\n        \"\"\"\n\n        from .computation import apply_ufunc\n\n        _quantile_func = np.nanquantile if skipna else np.quantile\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        scalar = utils.is_scalar(q)\n        q = np.atleast_1d(np.asarray(q, dtype=np.float64))\n\n        if dim is None:\n            dim = self.dims\n\n        if utils.is_scalar(dim):\n            dim = [dim]\n\n        def _wrapper(npa, **kwargs):\n            # move quantile axis to end. required for apply_ufunc\n            return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)\n\n        axis = np.arange(-1, -1 * len(dim) - 1, -1)\n        result = apply_ufunc(\n            _wrapper,\n            self,\n            input_core_dims=[dim],\n            exclude_dims=set(dim),\n            output_core_dims=[[\"quantile\"]],\n            output_dtypes=[np.float64],\n            dask_gufunc_kwargs=dict(output_sizes={\"quantile\": len(q)}),\n            dask=\"parallelized\",\n            kwargs={\"q\": q, \"axis\": axis, \"interpolation\": interpolation},\n        )\n\n        # for backward compatibility\n        result = result.transpose(\"quantile\", ...)\n        if scalar:\n            result = result.squeeze(\"quantile\")\n        if keep_attrs:\n            result.attrs = self._attrs\n        return result\n\n    def rank(self, dim, pct=False):\n        \"\"\"Ranks the data.\n\n        Equal values are assigned a rank that is the average of the ranks that\n        would have been otherwise assigned to all of the values within that\n        set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.\n\n        NaNs in the input array are returned as NaNs.\n\n        The `bottleneck` library is required.\n\n        Parameters\n        ----------\n        dim : str\n            Dimension over which to compute rank.\n        pct : bool, optional\n            If True, compute percentage ranks, otherwise compute integer ranks.\n\n        Returns\n        -------\n        ranked : Variable\n\n        See Also\n        --------\n        Dataset.rank, DataArray.rank\n        \"\"\"\n        import bottleneck as bn\n\n        data = self.data\n\n        if is_duck_dask_array(data):\n            raise TypeError(\n                \"rank does not work for arrays stored as dask \"\n                \"arrays. Load the data via .compute() or .load() \"\n                \"prior to calling this method.\"\n            )\n        elif not isinstance(data, np.ndarray):\n            raise TypeError(\n                \"rank is not implemented for {} objects.\".format(type(data))\n            )\n\n        axis = self.get_axis_num(dim)\n        func = bn.nanrankdata if self.dtype.kind == \"f\" else bn.rankdata\n        ranked = func(data, axis=axis)\n        if pct:\n            count = np.sum(~np.isnan(data), axis=axis, keepdims=True)\n            ranked /= count\n        return Variable(self.dims, ranked)\n\n    def rolling_window(\n        self, dim, window, window_dim, center=False, fill_value=dtypes.NA\n    ):\n        \"\"\"\n        Make a rolling_window along dim and add a new_dim to the last place.\n\n        Parameters\n        ----------\n        dim : str\n            Dimension over which to compute rolling_window.\n            For nd-rolling, should be list of dimensions.\n        window : int\n            Window size of the rolling\n            For nd-rolling, should be list of integers.\n        window_dim : str\n            New name of the window dimension.\n            For nd-rolling, should be list of integers.\n        center : bool, default: False\n            If True, pad fill_value for both ends. Otherwise, pad in the head\n            of the axis.\n        fill_value\n            value to be filled.\n\n        Returns\n        -------\n        Variable that is a view of the original array with a added dimension of\n        size w.\n        The return dim: self.dims + (window_dim, )\n        The return shape: self.shape + (window, )\n\n        Examples\n        --------\n        >>> v = Variable((\"a\", \"b\"), np.arange(8).reshape((2, 4)))\n        >>> v.rolling_window(\"b\", 3, \"window_dim\")\n        <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n        array([[[nan, nan,  0.],\n                [nan,  0.,  1.],\n                [ 0.,  1.,  2.],\n                [ 1.,  2.,  3.]],\n        <BLANKLINE>\n               [[nan, nan,  4.],\n                [nan,  4.,  5.],\n                [ 4.,  5.,  6.],\n                [ 5.,  6.,  7.]]])\n\n        >>> v.rolling_window(\"b\", 3, \"window_dim\", center=True)\n        <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n        array([[[nan,  0.,  1.],\n                [ 0.,  1.,  2.],\n                [ 1.,  2.,  3.],\n                [ 2.,  3., nan]],\n        <BLANKLINE>\n               [[nan,  4.,  5.],\n                [ 4.,  5.,  6.],\n                [ 5.,  6.,  7.],\n                [ 6.,  7., nan]]])\n        \"\"\"\n        if fill_value is dtypes.NA:  # np.nan is passed\n            dtype, fill_value = dtypes.maybe_promote(self.dtype)\n            array = self.astype(dtype, copy=False).data\n        else:\n            dtype = self.dtype\n            array = self.data\n\n        if isinstance(dim, list):\n            assert len(dim) == len(window)\n            assert len(dim) == len(window_dim)\n            assert len(dim) == len(center)\n        else:\n            dim = [dim]\n            window = [window]\n            window_dim = [window_dim]\n            center = [center]\n        axis = [self.get_axis_num(d) for d in dim]\n        new_dims = self.dims + tuple(window_dim)\n        return Variable(\n            new_dims,\n            duck_array_ops.rolling_window(\n                array, axis=axis, window=window, center=center, fill_value=fill_value\n            ),\n        )\n\n    def coarsen(\n        self, windows, func, boundary=\"exact\", side=\"left\", keep_attrs=None, **kwargs\n    ):\n        \"\"\"\n        Apply reduction function.\n        \"\"\"\n        windows = {k: v for k, v in windows.items() if k in self.dims}\n        if not windows:\n            return self.copy()\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        if keep_attrs:\n            _attrs = self.attrs\n        else:\n            _attrs = None\n\n        reshaped, axes = self._coarsen_reshape(windows, boundary, side)\n        if isinstance(func, str):\n            name = func\n            func = getattr(duck_array_ops, name, None)\n            if func is None:\n                raise NameError(f\"{name} is not a valid method.\")\n\n        return self._replace(data=func(reshaped, axis=axes, **kwargs), attrs=_attrs)\n\n    def _coarsen_reshape(self, windows, boundary, side):\n        \"\"\"\n        Construct a reshaped-array for coarsen\n        \"\"\"\n        if not utils.is_dict_like(boundary):\n            boundary = {d: boundary for d in windows.keys()}\n\n        if not utils.is_dict_like(side):\n            side = {d: side for d in windows.keys()}\n\n        # remove unrelated dimensions\n        boundary = {k: v for k, v in boundary.items() if k in windows}\n        side = {k: v for k, v in side.items() if k in windows}\n\n        for d, window in windows.items():\n            if window <= 0:\n                raise ValueError(f\"window must be > 0. Given {window}\")\n\n        variable = self\n        for d, window in windows.items():\n            # trim or pad the object\n            size = variable.shape[self._get_axis_num(d)]\n            n = int(size / window)\n            if boundary[d] == \"exact\":\n                if n * window != size:\n                    raise ValueError(\n                        \"Could not coarsen a dimension of size {} with \"\n                        \"window {}\".format(size, window)\n                    )\n            elif boundary[d] == \"trim\":\n                if side[d] == \"left\":\n                    variable = variable.isel({d: slice(0, window * n)})\n                else:\n                    excess = size - window * n\n                    variable = variable.isel({d: slice(excess, None)})\n            elif boundary[d] == \"pad\":  # pad\n                pad = window * n - size\n                if pad < 0:\n                    pad += window\n                if side[d] == \"left\":\n                    pad_width = {d: (0, pad)}\n                else:\n                    pad_width = {d: (pad, 0)}\n                variable = variable.pad(pad_width, mode=\"constant\")\n            else:\n                raise TypeError(\n                    \"{} is invalid for boundary. Valid option is 'exact', \"\n                    \"'trim' and 'pad'\".format(boundary[d])\n                )\n\n        shape = []\n        axes = []\n        axis_count = 0\n        for i, d in enumerate(variable.dims):\n            if d in windows:\n                size = variable.shape[i]\n                shape.append(int(size / windows[d]))\n                shape.append(windows[d])\n                axis_count += 1\n                axes.append(i + axis_count)\n            else:\n                shape.append(variable.shape[i])\n\n        return variable.data.reshape(shape), tuple(axes)\n\n    def isnull(self, keep_attrs: bool = None):\n        \"\"\"Test each value in the array for whether it is a missing value.\n\n        Returns\n        -------\n        isnull : Variable\n            Same type and shape as object, but the dtype of the data is bool.\n\n        See Also\n        --------\n        pandas.isnull\n\n        Examples\n        --------\n        >>> var = xr.Variable(\"x\", [1, np.nan, 3])\n        >>> var\n        <xarray.Variable (x: 3)>\n        array([ 1., nan,  3.])\n        >>> var.isnull()\n        <xarray.Variable (x: 3)>\n        array([False,  True, False])\n        \"\"\"\n        from .computation import apply_ufunc\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        return apply_ufunc(\n            duck_array_ops.isnull,\n            self,\n            dask=\"allowed\",\n            keep_attrs=keep_attrs,\n        )\n\n    def notnull(self, keep_attrs: bool = None):\n        \"\"\"Test each value in the array for whether it is not a missing value.\n\n        Returns\n        -------\n        notnull : Variable\n            Same type and shape as object, but the dtype of the data is bool.\n\n        See Also\n        --------\n        pandas.notnull\n\n        Examples\n        --------\n        >>> var = xr.Variable(\"x\", [1, np.nan, 3])\n        >>> var\n        <xarray.Variable (x: 3)>\n        array([ 1., nan,  3.])\n        >>> var.notnull()\n        <xarray.Variable (x: 3)>\n        array([ True, False,  True])\n        \"\"\"\n        from .computation import apply_ufunc\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n\n        return apply_ufunc(\n            duck_array_ops.notnull,\n            self,\n            dask=\"allowed\",\n            keep_attrs=keep_attrs,\n        )\n\n    @property\n    def real(self):\n        return type(self)(self.dims, self.data.real, self._attrs)\n\n    @property\n    def imag(self):\n        return type(self)(self.dims, self.data.imag, self._attrs)\n\n    def __array_wrap__(self, obj, context=None):\n        return Variable(self.dims, obj)\n\n    @staticmethod\n    def _unary_op(f):\n        @functools.wraps(f)\n        def func(self, *args, **kwargs):\n            keep_attrs = kwargs.pop(\"keep_attrs\", None)\n            if keep_attrs is None:\n                keep_attrs = _get_keep_attrs(default=True)\n            with np.errstate(all=\"ignore\"):\n                result = self.__array_wrap__(f(self.data, *args, **kwargs))\n                if keep_attrs:\n                    result.attrs = self.attrs\n                return result\n\n        return func\n\n    @staticmethod\n    def _binary_op(f, reflexive=False, **ignored_kwargs):\n        @functools.wraps(f)\n        def func(self, other):\n            if isinstance(other, (xr.DataArray, xr.Dataset)):\n                return NotImplemented\n            self_data, other_data, dims = _broadcast_compat_data(self, other)\n            keep_attrs = _get_keep_attrs(default=False)\n            attrs = self._attrs if keep_attrs else None\n            with np.errstate(all=\"ignore\"):\n                new_data = (\n                    f(self_data, other_data)\n                    if not reflexive\n                    else f(other_data, self_data)\n                )\n            result = Variable(dims, new_data, attrs=attrs)\n            return result\n\n        return func\n\n    @staticmethod\n    def _inplace_binary_op(f):\n        @functools.wraps(f)\n        def func(self, other):\n            if isinstance(other, xr.Dataset):\n                raise TypeError(\"cannot add a Dataset to a Variable in-place\")\n            self_data, other_data, dims = _broadcast_compat_data(self, other)\n            if dims != self.dims:\n                raise ValueError(\"dimensions cannot change for in-place operations\")\n            with np.errstate(all=\"ignore\"):\n                self.values = f(self_data, other_data)\n            return self\n\n        return func\n\n    def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n        \"\"\"A (private) method to convert datetime array to numeric dtype\n        See duck_array_ops.datetime_to_numeric\n        \"\"\"\n        numeric_array = duck_array_ops.datetime_to_numeric(\n            self.data, offset, datetime_unit, dtype\n        )\n        return type(self)(self.dims, numeric_array, self._attrs)\n\n    def _unravel_argminmax(\n        self,\n        argminmax: str,\n        dim: Union[Hashable, Sequence[Hashable], None],\n        axis: Union[int, None],\n        keep_attrs: Optional[bool],\n        skipna: Optional[bool],\n    ) -> Union[\"Variable\", Dict[Hashable, \"Variable\"]]:\n        \"\"\"Apply argmin or argmax over one or more dimensions, returning the result as a\n        dict of DataArray that can be passed directly to isel.\n        \"\"\"\n        if dim is None and axis is None:\n            warnings.warn(\n                \"Behaviour of argmin/argmax with neither dim nor axis argument will \"\n                \"change to return a dict of indices of each dimension. To get a \"\n                \"single, flat index, please use np.argmin(da.data) or \"\n                \"np.argmax(da.data) instead of da.argmin() or da.argmax().\",\n                DeprecationWarning,\n                stacklevel=3,\n            )\n\n        argminmax_func = getattr(duck_array_ops, argminmax)\n\n        if dim is ...:\n            # In future, should do this also when (dim is None and axis is None)\n            dim = self.dims\n        if (\n            dim is None\n            or axis is not None\n            or not isinstance(dim, Sequence)\n            or isinstance(dim, str)\n        ):\n            # Return int index if single dimension is passed, and is not part of a\n            # sequence\n            return self.reduce(\n                argminmax_func, dim=dim, axis=axis, keep_attrs=keep_attrs, skipna=skipna\n            )\n\n        # Get a name for the new dimension that does not conflict with any existing\n        # dimension\n        newdimname = \"_unravel_argminmax_dim_0\"\n        count = 1\n        while newdimname in self.dims:\n            newdimname = f\"_unravel_argminmax_dim_{count}\"\n            count += 1\n\n        stacked = self.stack({newdimname: dim})\n\n        result_dims = stacked.dims[:-1]\n        reduce_shape = tuple(self.sizes[d] for d in dim)\n\n        result_flat_indices = stacked.reduce(argminmax_func, axis=-1, skipna=skipna)\n\n        result_unravelled_indices = duck_array_ops.unravel_index(\n            result_flat_indices.data, reduce_shape\n        )\n\n        result = {\n            d: Variable(dims=result_dims, data=i)\n            for d, i in zip(dim, result_unravelled_indices)\n        }\n\n        if keep_attrs is None:\n            keep_attrs = _get_keep_attrs(default=False)\n        if keep_attrs:\n            for v in result.values():\n                v.attrs = self.attrs\n\n        return result\n\n    def argmin(\n        self,\n        dim: Union[Hashable, Sequence[Hashable]] = None,\n        axis: int = None,\n        keep_attrs: bool = None,\n        skipna: bool = None,\n    ) -> Union[\"Variable\", Dict[Hashable, \"Variable\"]]:\n        \"\"\"Index or indices of the minimum of the Variable over one or more dimensions.\n        If a sequence is passed to 'dim', then result returned as dict of Variables,\n        which can be passed directly to isel(). If a single str is passed to 'dim' then\n        returns a Variable with dtype int.\n\n        If there are multiple minima, the indices of the first one found will be\n        returned.\n\n        Parameters\n        ----------\n        dim : hashable, sequence of hashable or ..., optional\n            The dimensions over which to find the minimum. By default, finds minimum over\n            all dimensions - for now returning an int for backward compatibility, but\n            this is deprecated, in future will return a dict with indices for all\n            dimensions; to return a dict with all dimensions now, pass '...'.\n        axis : int, optional\n            Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments\n            can be supplied.\n        keep_attrs : bool, optional\n            If True, the attributes (`attrs`) will be copied from the original\n            object to the new one.  If False (default), the new object will be\n            returned without attributes.\n        skipna : bool, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or skipna=True has not been\n            implemented (object, datetime64 or timedelta64).\n\n        Returns\n        -------\n        result : Variable or dict of Variable\n\n        See Also\n        --------\n        DataArray.argmin, DataArray.idxmin\n        \"\"\"\n        return self._unravel_argminmax(\"argmin\", dim, axis, keep_attrs, skipna)\n\n    def argmax(\n        self,\n        dim: Union[Hashable, Sequence[Hashable]] = None,\n        axis: int = None,\n        keep_attrs: bool = None,\n        skipna: bool = None,\n    ) -> Union[\"Variable\", Dict[Hashable, \"Variable\"]]:\n        \"\"\"Index or indices of the maximum of the Variable over one or more dimensions.\n        If a sequence is passed to 'dim', then result returned as dict of Variables,\n        which can be passed directly to isel(). If a single str is passed to 'dim' then\n        returns a Variable with dtype int.\n\n        If there are multiple maxima, the indices of the first one found will be\n        returned.\n\n        Parameters\n        ----------\n        dim : hashable, sequence of hashable or ..., optional\n            The dimensions over which to find the maximum. By default, finds maximum over\n            all dimensions - for now returning an int for backward compatibility, but\n            this is deprecated, in future will return a dict with indices for all\n            dimensions; to return a dict with all dimensions now, pass '...'.\n        axis : int, optional\n            Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments\n            can be supplied.\n        keep_attrs : bool, optional\n            If True, the attributes (`attrs`) will be copied from the original\n            object to the new one.  If False (default), the new object will be\n            returned without attributes.\n        skipna : bool, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or skipna=True has not been\n            implemented (object, datetime64 or timedelta64).\n\n        Returns\n        -------\n        result : Variable or dict of Variable\n\n        See Also\n        --------\n        DataArray.argmax, DataArray.idxmax\n        \"\"\"\n        return self._unravel_argminmax(\"argmax\", dim, axis, keep_attrs, skipna)\n\n\nops.inject_all_ops_and_reduce_methods(Variable)\n\n\nclass IndexVariable(Variable):\n    \"\"\"Wrapper for accommodating a pandas.Index in an xarray.Variable.\n\n    IndexVariable preserve loaded values in the form of a pandas.Index instead\n    of a NumPy array. Hence, their values are immutable and must always be one-\n    dimensional.\n\n    They also have a name property, which is the name of their sole dimension\n    unless another name is given.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n        super().__init__(dims, data, attrs, encoding, fastpath)\n        if self.ndim != 1:\n            raise ValueError(\"%s objects must be 1-dimensional\" % type(self).__name__)\n\n        # Unlike in Variable, always eagerly load values into memory\n        if not isinstance(self._data, PandasIndexAdapter):\n            self._data = PandasIndexAdapter(self._data)\n\n    def __dask_tokenize__(self):\n        from dask.base import normalize_token\n\n        # Don't waste time converting pd.Index to np.ndarray\n        return normalize_token((type(self), self._dims, self._data.array, self._attrs))\n\n    def load(self):\n        # data is already loaded into memory for IndexVariable\n        return self\n\n    # https://github.com/python/mypy/issues/1465\n    @Variable.data.setter  # type: ignore\n    def data(self, data):\n        raise ValueError(\n            f\"Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\n            f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\n        )\n\n    @Variable.values.setter  # type: ignore\n    def values(self, values):\n        raise ValueError(\n            f\"Cannot assign to the .values attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\n            f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\n        )\n\n    def chunk(self, chunks={}, name=None, lock=False):\n        # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()\n        return self.copy(deep=False)\n\n    def _as_sparse(self, sparse_format=_default, fill_value=_default):\n        # Dummy\n        return self.copy(deep=False)\n\n    def _to_dense(self):\n        # Dummy\n        return self.copy(deep=False)\n\n    def _finalize_indexing_result(self, dims, data):\n        if getattr(data, \"ndim\", 0) != 1:\n            # returns Variable rather than IndexVariable if multi-dimensional\n            return Variable(dims, data, self._attrs, self._encoding)\n        else:\n            return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n\n    def __setitem__(self, key, value):\n        raise TypeError(\"%s values cannot be modified\" % type(self).__name__)\n\n    @classmethod\n    def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n        \"\"\"Specialized version of Variable.concat for IndexVariable objects.\n\n        This exists because we want to avoid converting Index objects to NumPy\n        arrays, if possible.\n        \"\"\"\n        if not isinstance(dim, str):\n            (dim,) = dim.dims\n\n        variables = list(variables)\n        first_var = variables[0]\n\n        if any(not isinstance(v, cls) for v in variables):\n            raise TypeError(\n                \"IndexVariable.concat requires that all input \"\n                \"variables be IndexVariable objects\"\n            )\n\n        indexes = [v._data.array for v in variables]\n\n        if not indexes:\n            data = []\n        else:\n            data = indexes[0].append(indexes[1:])\n\n            if positions is not None:\n                indices = nputils.inverse_permutation(np.concatenate(positions))\n                data = data.take(indices)\n\n        # keep as str if possible as pandas.Index uses object (converts to numpy array)\n        data = maybe_coerce_to_str(data, variables)\n\n        attrs = dict(first_var.attrs)\n        if not shortcut:\n            for var in variables:\n                if var.dims != first_var.dims:\n                    raise ValueError(\"inconsistent dimensions\")\n                utils.remove_incompatible_items(attrs, var.attrs)\n\n        return cls(first_var.dims, data, attrs)\n\n    def copy(self, deep=True, data=None):\n        \"\"\"Returns a copy of this object.\n\n        `deep` is ignored since data is stored in the form of\n        pandas.Index, which is already immutable. Dimensions, attributes\n        and encodings are always copied.\n\n        Use `data` to create a new object with the same structure as\n        original but entirely new data.\n\n        Parameters\n        ----------\n        deep : bool, optional\n            Deep is ignored when data is given. Whether the data array is\n            loaded into memory and copied onto the new object. Default is True.\n        data : array_like, optional\n            Data to use in the new object. Must have same shape as original.\n\n        Returns\n        -------\n        object : Variable\n            New object with dimensions, attributes, encodings, and optionally\n            data copied from original.\n        \"\"\"\n        if data is None:\n            data = self._data.copy(deep=deep)\n        else:\n            data = as_compatible_data(data)\n            if self.shape != data.shape:\n                raise ValueError(\n                    \"Data shape {} must match shape of object {}\".format(\n                        data.shape, self.shape\n                    )\n                )\n        return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n\n    def equals(self, other, equiv=None):\n        # if equiv is specified, super up\n        if equiv is not None:\n            return super().equals(other, equiv)\n\n        # otherwise use the native index equals, rather than looking at _data\n        other = getattr(other, \"variable\", other)\n        try:\n            return self.dims == other.dims and self._data_equals(other)\n        except (TypeError, AttributeError):\n            return False\n\n    def _data_equals(self, other):\n        return self.to_index().equals(other.to_index())\n\n    def to_index_variable(self):\n        \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n        return self\n\n    to_coord = utils.alias(to_index_variable, \"to_coord\")\n\n    def to_index(self):\n        \"\"\"Convert this variable to a pandas.Index\"\"\"\n        # n.b. creating a new pandas.Index from an old pandas.Index is\n        # basically free as pandas.Index objects are immutable\n        assert self.ndim == 1\n        index = self._data.array\n        if isinstance(index, pd.MultiIndex):\n            # set default names for multi-index unnamed levels so that\n            # we can safely rename dimension / coordinate later\n            valid_level_names = [\n                name or \"{}_level_{}\".format(self.dims[0], i)\n                for i, name in enumerate(index.names)\n            ]\n            index = index.set_names(valid_level_names)\n        else:\n            index = index.set_names(self.name)\n        return index\n\n    @property\n    def level_names(self):\n        \"\"\"Return MultiIndex level names or None if this IndexVariable has no\n        MultiIndex.\n        \"\"\"\n        index = self.to_index()\n        if isinstance(index, pd.MultiIndex):\n            return index.names\n        else:\n            return None\n\n    def get_level_variable(self, level):\n        \"\"\"Return a new IndexVariable from a given MultiIndex level.\"\"\"\n        if self.level_names is None:\n            raise ValueError(\"IndexVariable %r has no MultiIndex\" % self.name)\n        index = self.to_index()\n        return type(self)(self.dims, index.get_level_values(level))\n\n    @property\n    def name(self):\n        return self.dims[0]\n\n    @name.setter\n    def name(self, value):\n        raise AttributeError(\"cannot modify name of IndexVariable in-place\")\n\n\n# for backwards compatibility\nCoordinate = utils.alias(IndexVariable, \"Coordinate\")\n\n\ndef _unified_dims(variables):\n    # validate dimensions\n    all_dims = {}\n    for var in variables:\n        var_dims = var.dims\n        if len(set(var_dims)) < len(var_dims):\n            raise ValueError(\n                \"broadcasting cannot handle duplicate \"\n                \"dimensions: %r\" % list(var_dims)\n            )\n        for d, s in zip(var_dims, var.shape):\n            if d not in all_dims:\n                all_dims[d] = s\n            elif all_dims[d] != s:\n                raise ValueError(\n                    \"operands cannot be broadcast together \"\n                    \"with mismatched lengths for dimension %r: %s\"\n                    % (d, (all_dims[d], s))\n                )\n    return all_dims\n\n\ndef _broadcast_compat_variables(*variables):\n    \"\"\"Create broadcast compatible variables, with the same dimensions.\n\n    Unlike the result of broadcast_variables(), some variables may have\n    dimensions of size 1 instead of the the size of the broadcast dimension.\n    \"\"\"\n    dims = tuple(_unified_dims(variables))\n    return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)\n\n\ndef broadcast_variables(*variables):\n    \"\"\"Given any number of variables, return variables with matching dimensions\n    and broadcast data.\n\n    The data on the returned variables will be a view of the data on the\n    corresponding original arrays, but dimensions will be reordered and\n    inserted so that both broadcast arrays have the same dimensions. The new\n    dimensions are sorted in order of appearance in the first variable's\n    dimensions followed by the second variable's dimensions.\n    \"\"\"\n    dims_map = _unified_dims(variables)\n    dims_tuple = tuple(dims_map)\n    return tuple(\n        var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables\n    )\n\n\ndef _broadcast_compat_data(self, other):\n    if all(hasattr(other, attr) for attr in [\"dims\", \"data\", \"shape\", \"encoding\"]):\n        # `other` satisfies the necessary Variable API for broadcast_variables\n        new_self, new_other = _broadcast_compat_variables(self, other)\n        self_data = new_self.data\n        other_data = new_other.data\n        dims = new_self.dims\n    else:\n        # rely on numpy broadcasting rules\n        self_data = self.data\n        other_data = other\n        dims = self.dims\n    return self_data, other_data, dims\n\n\ndef concat(variables, dim=\"concat_dim\", positions=None, shortcut=False):\n    \"\"\"Concatenate variables along a new or existing dimension.\n\n    Parameters\n    ----------\n    variables : iterable of Variable\n        Arrays to stack together. Each variable is expected to have\n        matching dimensions and shape except for along the stacked\n        dimension.\n    dim : str or DataArray, optional\n        Name of the dimension to stack along. This can either be a new\n        dimension name, in which case it is added along axis=0, or an\n        existing dimension name, in which case the location of the\n        dimension is unchanged. Where to insert the new dimension is\n        determined by the first variable.\n    positions : None or list of array-like, optional\n        List of integer arrays which specifies the integer positions to which\n        to assign each dataset along the concatenated dimension. If not\n        supplied, objects are concatenated in the provided order.\n    shortcut : bool, optional\n        This option is used internally to speed-up groupby operations.\n        If `shortcut` is True, some checks of internal consistency between\n        arrays to concatenate are skipped.\n\n    Returns\n    -------\n    stacked : Variable\n        Concatenated Variable formed by stacking all the supplied variables\n        along the given dimension.\n    \"\"\"\n    variables = list(variables)\n    if all(isinstance(v, IndexVariable) for v in variables):\n        return IndexVariable.concat(variables, dim, positions, shortcut)\n    else:\n        return Variable.concat(variables, dim, positions, shortcut)\n\n\ndef assert_unique_multiindex_level_names(variables):\n    \"\"\"Check for uniqueness of MultiIndex level names in all given\n    variables.\n\n    Not public API. Used for checking consistency of DataArray and Dataset\n    objects.\n    \"\"\"\n    level_names = defaultdict(list)\n    all_level_names = set()\n    for var_name, var in variables.items():\n        if isinstance(var._data, PandasIndexAdapter):\n            idx_level_names = var.to_index_variable().level_names\n            if idx_level_names is not None:\n                for n in idx_level_names:\n                    level_names[n].append(f\"{n!r} ({var_name})\")\n            if idx_level_names:\n                all_level_names.update(idx_level_names)\n\n    for k, v in level_names.items():\n        if k in variables:\n            v.append(\"(%s)\" % k)\n\n    duplicate_names = [v for v in level_names.values() if len(v) > 1]\n    if duplicate_names:\n        conflict_str = \"\\n\".join(\", \".join(v) for v in duplicate_names)\n        raise ValueError(\"conflicting MultiIndex level name(s):\\n%s\" % conflict_str)\n    # Check confliction between level names and dimensions GH:2299\n    for k, v in variables.items():\n        for d in v.dims:\n            if d in all_level_names:\n                raise ValueError(\n                    \"conflicting level / dimension names. {} \"\n                    \"already exists as a level name.\".format(d)\n                )"}
{"model_name_or_path": "agentless", "instance_id": "pydata__xarray-7233", "model_patch": "", "raw_model_patch": "", "original_file_content": ""}
{"model_name_or_path": "agentless", "instance_id": "pytest-dev__pytest-7324", "model_patch": "diff --git a/src/_pytest/mark/expression.py b/src/_pytest/mark/expression.py\nindex 337bbf3..2e97d33 100644\n--- a/src/_pytest/mark/expression.py\n+++ b/src/_pytest/mark/expression.py\n@@ -198,6 +198,9 @@ class Expression:\n \n         :param input: The input expression - one line.\n         \"\"\"\n+        if input in {\"None\", \"True\", \"False\"}:\n+            raise ValueError(f\"Invalid match expression: {input} is a reserved keyword.\")\n+        \n         astexpr = expression(Scanner(input))\n         code = compile(\n             astexpr, filename=\"<pytest match expression>\", mode=\"eval\",\n", "raw_model_patch": "diff --git a/src/_pytest/mark/expression.py b/src/_pytest/mark/expression.py\nindex 337bbf3..2e97d33 100644\n--- a/src/_pytest/mark/expression.py\n+++ b/src/_pytest/mark/expression.py\n@@ -198,6 +198,9 @@ class Expression:\n \n         :param input: The input expression - one line.\n         \"\"\"\n+        if input in {\"None\", \"True\", \"False\"}:\n+            raise ValueError(f\"Invalid match expression: {input} is a reserved keyword.\")\n+        \n         astexpr = expression(Scanner(input))\n         code = compile(\n             astexpr, filename=\"<pytest match expression>\", mode=\"eval\",\n", "original_file_content": "r\"\"\"\nEvaluate match expressions, as used by `-k` and `-m`.\n\nThe grammar is:\n\nexpression: expr? EOF\nexpr:       and_expr ('or' and_expr)*\nand_expr:   not_expr ('and' not_expr)*\nnot_expr:   'not' not_expr | '(' expr ')' | ident\nident:      (\\w|:|\\+|-|\\.|\\[|\\])+\n\nThe semantics are:\n\n- Empty expression evaluates to False.\n- ident evaluates to True of False according to a provided matcher function.\n- or/and/not evaluate according to the usual boolean semantics.\n\"\"\"\nimport ast\nimport enum\nimport re\nimport types\nfrom typing import Callable\nfrom typing import Iterator\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import Sequence\n\nimport attr\n\nfrom _pytest.compat import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from typing import NoReturn\n\n\n__all__ = [\n    \"Expression\",\n    \"ParseError\",\n]\n\n\nclass TokenType(enum.Enum):\n    LPAREN = \"left parenthesis\"\n    RPAREN = \"right parenthesis\"\n    OR = \"or\"\n    AND = \"and\"\n    NOT = \"not\"\n    IDENT = \"identifier\"\n    EOF = \"end of input\"\n\n\n@attr.s(frozen=True, slots=True)\nclass Token:\n    type = attr.ib(type=TokenType)\n    value = attr.ib(type=str)\n    pos = attr.ib(type=int)\n\n\nclass ParseError(Exception):\n    \"\"\"The expression contains invalid syntax.\n\n    :param column: The column in the line where the error occurred (1-based).\n    :param message: A description of the error.\n    \"\"\"\n\n    def __init__(self, column: int, message: str) -> None:\n        self.column = column\n        self.message = message\n\n    def __str__(self) -> str:\n        return \"at column {}: {}\".format(self.column, self.message)\n\n\nclass Scanner:\n    __slots__ = (\"tokens\", \"current\")\n\n    def __init__(self, input: str) -> None:\n        self.tokens = self.lex(input)\n        self.current = next(self.tokens)\n\n    def lex(self, input: str) -> Iterator[Token]:\n        pos = 0\n        while pos < len(input):\n            if input[pos] in (\" \", \"\\t\"):\n                pos += 1\n            elif input[pos] == \"(\":\n                yield Token(TokenType.LPAREN, \"(\", pos)\n                pos += 1\n            elif input[pos] == \")\":\n                yield Token(TokenType.RPAREN, \")\", pos)\n                pos += 1\n            else:\n                match = re.match(r\"(:?\\w|:|\\+|-|\\.|\\[|\\])+\", input[pos:])\n                if match:\n                    value = match.group(0)\n                    if value == \"or\":\n                        yield Token(TokenType.OR, value, pos)\n                    elif value == \"and\":\n                        yield Token(TokenType.AND, value, pos)\n                    elif value == \"not\":\n                        yield Token(TokenType.NOT, value, pos)\n                    else:\n                        yield Token(TokenType.IDENT, value, pos)\n                    pos += len(value)\n                else:\n                    raise ParseError(\n                        pos + 1, 'unexpected character \"{}\"'.format(input[pos]),\n                    )\n        yield Token(TokenType.EOF, \"\", pos)\n\n    def accept(self, type: TokenType, *, reject: bool = False) -> Optional[Token]:\n        if self.current.type is type:\n            token = self.current\n            if token.type is not TokenType.EOF:\n                self.current = next(self.tokens)\n            return token\n        if reject:\n            self.reject((type,))\n        return None\n\n    def reject(self, expected: Sequence[TokenType]) -> \"NoReturn\":\n        raise ParseError(\n            self.current.pos + 1,\n            \"expected {}; got {}\".format(\n                \" OR \".join(type.value for type in expected), self.current.type.value,\n            ),\n        )\n\n\ndef expression(s: Scanner) -> ast.Expression:\n    if s.accept(TokenType.EOF):\n        ret = ast.NameConstant(False)  # type: ast.expr\n    else:\n        ret = expr(s)\n        s.accept(TokenType.EOF, reject=True)\n    return ast.fix_missing_locations(ast.Expression(ret))\n\n\ndef expr(s: Scanner) -> ast.expr:\n    ret = and_expr(s)\n    while s.accept(TokenType.OR):\n        rhs = and_expr(s)\n        ret = ast.BoolOp(ast.Or(), [ret, rhs])\n    return ret\n\n\ndef and_expr(s: Scanner) -> ast.expr:\n    ret = not_expr(s)\n    while s.accept(TokenType.AND):\n        rhs = not_expr(s)\n        ret = ast.BoolOp(ast.And(), [ret, rhs])\n    return ret\n\n\ndef not_expr(s: Scanner) -> ast.expr:\n    if s.accept(TokenType.NOT):\n        return ast.UnaryOp(ast.Not(), not_expr(s))\n    if s.accept(TokenType.LPAREN):\n        ret = expr(s)\n        s.accept(TokenType.RPAREN, reject=True)\n        return ret\n    ident = s.accept(TokenType.IDENT)\n    if ident:\n        return ast.Name(ident.value, ast.Load())\n    s.reject((TokenType.NOT, TokenType.LPAREN, TokenType.IDENT))\n\n\nclass MatcherAdapter(Mapping[str, bool]):\n    \"\"\"Adapts a matcher function to a locals mapping as required by eval().\"\"\"\n\n    def __init__(self, matcher: Callable[[str], bool]) -> None:\n        self.matcher = matcher\n\n    def __getitem__(self, key: str) -> bool:\n        return self.matcher(key)\n\n    def __iter__(self) -> Iterator[str]:\n        raise NotImplementedError()\n\n    def __len__(self) -> int:\n        raise NotImplementedError()\n\n\nclass Expression:\n    \"\"\"A compiled match expression as used by -k and -m.\n\n    The expression can be evaulated against different matchers.\n    \"\"\"\n\n    __slots__ = (\"code\",)\n\n    def __init__(self, code: types.CodeType) -> None:\n        self.code = code\n\n    @classmethod\n    def compile(self, input: str) -> \"Expression\":\n        \"\"\"Compile a match expression.\n\n        :param input: The input expression - one line.\n        \"\"\"\n        astexpr = expression(Scanner(input))\n        code = compile(\n            astexpr, filename=\"<pytest match expression>\", mode=\"eval\",\n        )  # type: types.CodeType\n        return Expression(code)\n\n    def evaluate(self, matcher: Callable[[str], bool]) -> bool:\n        \"\"\"Evaluate the match expression.\n\n        :param matcher: Given an identifier, should return whether it matches or not.\n                        Should be prepared to handle arbitrary strings as input.\n\n        Returns whether the expression matches or not.\n        \"\"\"\n        ret = eval(\n            self.code, {\"__builtins__\": {}}, MatcherAdapter(matcher)\n        )  # type: bool\n        return ret"}
{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-12973", "model_patch": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex b1d16f1..c7f4639 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,7 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+    def fit(self, X, y, copy_X=None):\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n", "raw_model_patch": "diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py\nindex b1d16f1..c7f4639 100644\n--- a/sklearn/linear_model/least_angle.py\n+++ b/sklearn/linear_model/least_angle.py\n@@ -1479,7 +1479,7 @@ class LassoLarsIC(LassoLars):\n         self.eps = eps\n         self.fit_path = True\n \n-    def fit(self, X, y, copy_X=True):\n+    def fit(self, X, y, copy_X=None):\n         \"\"\"Fit the model using X, y as training data.\n \n         Parameters\n", "original_file_content": "\"\"\"\nLeast Angle Regression algorithm. See the documentation on the\nGeneralized Linear Model for a complete discussion.\n\"\"\"\nfrom __future__ import print_function\n\n# Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#         Gael Varoquaux\n#\n# License: BSD 3 clause\n\nfrom math import log\nimport sys\nimport warnings\n\nimport numpy as np\nfrom scipy import linalg, interpolate\nfrom scipy.linalg.lapack import get_lapack_funcs\n\nfrom .base import LinearModel\nfrom ..base import RegressorMixin\nfrom ..utils import arrayfuncs, as_float_array, check_X_y\nfrom ..model_selection import check_cv\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils._joblib import Parallel, delayed\n\nsolve_triangular_args = {'check_finite': False}\n\n\ndef lars_path(X, y, Xy=None, Gram=None, max_iter=500,\n              alpha_min=0, method='lar', copy_X=True,\n              eps=np.finfo(np.float).eps,\n              copy_Gram=True, verbose=0, return_path=True,\n              return_n_iter=False, positive=False):\n    \"\"\"Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n\n    The optimization objective for the case method='lasso' is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    in the case of method='lars', the objective function is only known in\n    the form of an implicit equation (see discussion in [1])\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    -----------\n    X : array, shape: (n_samples, n_features)\n        Input data.\n\n    y : array, shape: (n_samples)\n        Input targets.\n\n    Xy : array-like, shape (n_samples,) or (n_samples, n_targets), \\\n            optional\n        Xy = np.dot(X.T, y) that can be precomputed. It is useful\n        only when the Gram matrix is precomputed.\n\n    Gram : None, 'auto', array, shape: (n_features, n_features), optional\n        Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram\n        matrix is precomputed from the given X, if there are more samples\n        than features.\n\n    max_iter : integer, optional (default=500)\n        Maximum number of iterations to perform, set to infinity for no limit.\n\n    alpha_min : float, optional (default=0)\n        Minimum correlation along the path. It corresponds to the\n        regularization parameter alpha parameter in the Lasso.\n\n    method : {'lar', 'lasso'}, optional (default='lar')\n        Specifies the returned model. Select ``'lar'`` for Least Angle\n        Regression, ``'lasso'`` for the Lasso.\n\n    copy_X : bool, optional (default=True)\n        If ``False``, ``X`` is overwritten.\n\n    eps : float, optional (default=``np.finfo(np.float).eps``)\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems.\n\n    copy_Gram : bool, optional (default=True)\n        If ``False``, ``Gram`` is overwritten.\n\n    verbose : int (default=0)\n        Controls output verbosity.\n\n    return_path : bool, optional (default=True)\n        If ``return_path==True`` returns the entire path, else returns only the\n        last point of the path.\n\n    return_n_iter : bool, optional (default=False)\n        Whether to return the number of iterations.\n\n    positive : boolean (default=False)\n        Restrict coefficients to be >= 0.\n        This option is only allowed with method 'lasso'. Note that the model\n        coefficients will not converge to the ordinary-least-squares solution\n        for small values of alpha. Only coefficients up to the smallest alpha\n        value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by\n        the stepwise Lars-Lasso algorithm are typically in congruence with the\n        solution of the coordinate descent lasso_path function.\n\n    Returns\n    --------\n    alphas : array, shape: [n_alphas + 1]\n        Maximum of covariances (in absolute value) at each iteration.\n        ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n        number of nodes in the path with ``alpha >= alpha_min``, whichever\n        is smaller.\n\n    active : array, shape [n_alphas]\n        Indices of active variables at the end of the path.\n\n    coefs : array, shape (n_features, n_alphas + 1)\n        Coefficients along the path\n\n    n_iter : int\n        Number of iterations run. Returned only if return_n_iter is set\n        to True.\n\n    See also\n    --------\n    lasso_path\n    LassoLars\n    Lars\n    LassoLarsCV\n    LarsCV\n    sklearn.decomposition.sparse_encode\n\n    References\n    ----------\n    .. [1] \"Least Angle Regression\", Effron et al.\n           https://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n    .. [2] `Wikipedia entry on the Least-angle regression\n           <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n    .. [3] `Wikipedia entry on the Lasso\n           <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n\n    \"\"\"\n    if method == 'lar' and positive:\n        warnings.warn('positive option is broken for Least'\n                      ' Angle Regression (LAR). Use method=\"lasso\".'\n                      ' This option will be removed in version 0.22.',\n                      DeprecationWarning)\n\n    n_features = X.shape[1]\n    n_samples = y.size\n    max_features = min(max_iter, n_features)\n\n    if return_path:\n        coefs = np.zeros((max_features + 1, n_features))\n        alphas = np.zeros(max_features + 1)\n    else:\n        coef, prev_coef = np.zeros(n_features), np.zeros(n_features)\n        alpha, prev_alpha = np.array([0.]), np.array([0.])  # better ideas?\n\n    n_iter, n_active = 0, 0\n    active, indices = list(), np.arange(n_features)\n    # holds the sign of covariance\n    sign_active = np.empty(max_features, dtype=np.int8)\n    drop = False\n\n    # will hold the cholesky factorization. Only lower part is\n    # referenced.\n    L = np.empty((max_features, max_features), dtype=X.dtype)\n    swap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (X,))\n    solve_cholesky, = get_lapack_funcs(('potrs',), (X,))\n\n    if Gram is None or Gram is False:\n        Gram = None\n        if copy_X:\n            # force copy. setting the array to be fortran-ordered\n            # speeds up the calculation of the (partial) Gram matrix\n            # and allows to easily swap columns\n            X = X.copy('F')\n\n    elif isinstance(Gram, str) and Gram == 'auto' or Gram is True:\n        if Gram is True or X.shape[0] > X.shape[1]:\n            Gram = np.dot(X.T, X)\n        else:\n            Gram = None\n    elif copy_Gram:\n        Gram = Gram.copy()\n\n    if Xy is None:\n        Cov = np.dot(X.T, y)\n    else:\n        Cov = Xy.copy()\n\n    if verbose:\n        if verbose > 1:\n            print(\"Step\\t\\tAdded\\t\\tDropped\\t\\tActive set size\\t\\tC\")\n        else:\n            sys.stdout.write('.')\n            sys.stdout.flush()\n\n    tiny32 = np.finfo(np.float32).tiny  # to avoid division by 0 warning\n    equality_tolerance = np.finfo(np.float32).eps\n\n    while True:\n        if Cov.size:\n            if positive:\n                C_idx = np.argmax(Cov)\n            else:\n                C_idx = np.argmax(np.abs(Cov))\n\n            C_ = Cov[C_idx]\n\n            if positive:\n                C = C_\n            else:\n                C = np.fabs(C_)\n        else:\n            C = 0.\n\n        if return_path:\n            alpha = alphas[n_iter, np.newaxis]\n            coef = coefs[n_iter]\n            prev_alpha = alphas[n_iter - 1, np.newaxis]\n            prev_coef = coefs[n_iter - 1]\n\n        alpha[0] = C / n_samples\n        if alpha[0] <= alpha_min + equality_tolerance:  # early stopping\n            if abs(alpha[0] - alpha_min) > equality_tolerance:\n                # interpolation factor 0 <= ss < 1\n                if n_iter > 0:\n                    # In the first iteration, all alphas are zero, the formula\n                    # below would make ss a NaN\n                    ss = ((prev_alpha[0] - alpha_min) /\n                          (prev_alpha[0] - alpha[0]))\n                    coef[:] = prev_coef + ss * (coef - prev_coef)\n                alpha[0] = alpha_min\n            if return_path:\n                coefs[n_iter] = coef\n            break\n\n        if n_iter >= max_iter or n_active >= n_features:\n            break\n\n        if not drop:\n\n            ##########################################################\n            # Append x_j to the Cholesky factorization of (Xa * Xa') #\n            #                                                        #\n            #            ( L   0 )                                   #\n            #     L  ->  (       )  , where L * w = Xa' x_j          #\n            #            ( w   z )    and z = ||x_j||                #\n            #                                                        #\n            ##########################################################\n\n            if positive:\n                sign_active[n_active] = np.ones_like(C_)\n            else:\n                sign_active[n_active] = np.sign(C_)\n            m, n = n_active, C_idx + n_active\n\n            Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\n            indices[n], indices[m] = indices[m], indices[n]\n            Cov_not_shortened = Cov\n            Cov = Cov[1:]  # remove Cov[0]\n\n            if Gram is None:\n                X.T[n], X.T[m] = swap(X.T[n], X.T[m])\n                c = nrm2(X.T[n_active]) ** 2\n                L[n_active, :n_active] = \\\n                    np.dot(X.T[n_active], X.T[:n_active].T)\n            else:\n                # swap does only work inplace if matrix is fortran\n                # contiguous ...\n                Gram[m], Gram[n] = swap(Gram[m], Gram[n])\n                Gram[:, m], Gram[:, n] = swap(Gram[:, m], Gram[:, n])\n                c = Gram[n_active, n_active]\n                L[n_active, :n_active] = Gram[n_active, :n_active]\n\n            # Update the cholesky decomposition for the Gram matrix\n            if n_active:\n                linalg.solve_triangular(L[:n_active, :n_active],\n                                        L[n_active, :n_active],\n                                        trans=0, lower=1,\n                                        overwrite_b=True,\n                                        **solve_triangular_args)\n\n            v = np.dot(L[n_active, :n_active], L[n_active, :n_active])\n            diag = max(np.sqrt(np.abs(c - v)), eps)\n            L[n_active, n_active] = diag\n\n            if diag < 1e-7:\n                # The system is becoming too ill-conditioned.\n                # We have degenerate vectors in our active set.\n                # We'll 'drop for good' the last regressor added.\n\n                # Note: this case is very rare. It is no longer triggered by\n                # the test suite. The `equality_tolerance` margin added in 0.16\n                # to get early stopping to work consistently on all versions of\n                # Python including 32 bit Python under Windows seems to make it\n                # very difficult to trigger the 'drop for good' strategy.\n                warnings.warn('Regressors in active set degenerate. '\n                              'Dropping a regressor, after %i iterations, '\n                              'i.e. alpha=%.3e, '\n                              'with an active set of %i regressors, and '\n                              'the smallest cholesky pivot element being %.3e.'\n                              ' Reduce max_iter or increase eps parameters.'\n                              % (n_iter, alpha, n_active, diag),\n                              ConvergenceWarning)\n\n                # XXX: need to figure a 'drop for good' way\n                Cov = Cov_not_shortened\n                Cov[0] = 0\n                Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])\n                continue\n\n            active.append(indices[n_active])\n            n_active += 1\n\n            if verbose > 1:\n                print(\"%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (n_iter, active[-1], '',\n                                                      n_active, C))\n\n        if method == 'lasso' and n_iter > 0 and prev_alpha[0] < alpha[0]:\n            # alpha is increasing. This is because the updates of Cov are\n            # bringing in too much numerical error that is greater than\n            # than the remaining correlation with the\n            # regressors. Time to bail out\n            warnings.warn('Early stopping the lars path, as the residues '\n                          'are small and the current value of alpha is no '\n                          'longer well controlled. %i iterations, alpha=%.3e, '\n                          'previous alpha=%.3e, with an active set of %i '\n                          'regressors.'\n                          % (n_iter, alpha, prev_alpha, n_active),\n                          ConvergenceWarning)\n            break\n\n        # least squares solution\n        least_squares, info = solve_cholesky(L[:n_active, :n_active],\n                                             sign_active[:n_active],\n                                             lower=True)\n\n        if least_squares.size == 1 and least_squares == 0:\n            # This happens because sign_active[:n_active] = 0\n            least_squares[...] = 1\n            AA = 1.\n        else:\n            # is this really needed ?\n            AA = 1. / np.sqrt(np.sum(least_squares * sign_active[:n_active]))\n\n            if not np.isfinite(AA):\n                # L is too ill-conditioned\n                i = 0\n                L_ = L[:n_active, :n_active].copy()\n                while not np.isfinite(AA):\n                    L_.flat[::n_active + 1] += (2 ** i) * eps\n                    least_squares, info = solve_cholesky(\n                        L_, sign_active[:n_active], lower=True)\n                    tmp = max(np.sum(least_squares * sign_active[:n_active]),\n                              eps)\n                    AA = 1. / np.sqrt(tmp)\n                    i += 1\n            least_squares *= AA\n\n        if Gram is None:\n            # equiangular direction of variables in the active set\n            eq_dir = np.dot(X.T[:n_active].T, least_squares)\n            # correlation between each unactive variables and\n            # eqiangular vector\n            corr_eq_dir = np.dot(X.T[n_active:], eq_dir)\n        else:\n            # if huge number of features, this takes 50% of time, I\n            # think could be avoided if we just update it using an\n            # orthogonal (QR) decomposition of X\n            corr_eq_dir = np.dot(Gram[:n_active, n_active:].T,\n                                 least_squares)\n\n        g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))\n        if positive:\n            gamma_ = min(g1, C / AA)\n        else:\n            g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))\n            gamma_ = min(g1, g2, C / AA)\n\n        # TODO: better names for these variables: z\n        drop = False\n        z = -coef[active] / (least_squares + tiny32)\n        z_pos = arrayfuncs.min_pos(z)\n        if z_pos < gamma_:\n            # some coefficients have changed sign\n            idx = np.where(z == z_pos)[0][::-1]\n\n            # update the sign, important for LAR\n            sign_active[idx] = -sign_active[idx]\n\n            if method == 'lasso':\n                gamma_ = z_pos\n            drop = True\n\n        n_iter += 1\n\n        if return_path:\n            if n_iter >= coefs.shape[0]:\n                del coef, alpha, prev_alpha, prev_coef\n                # resize the coefs and alphas array\n                add_features = 2 * max(1, (max_features - n_active))\n                coefs = np.resize(coefs, (n_iter + add_features, n_features))\n                coefs[-add_features:] = 0\n                alphas = np.resize(alphas, n_iter + add_features)\n                alphas[-add_features:] = 0\n            coef = coefs[n_iter]\n            prev_coef = coefs[n_iter - 1]\n        else:\n            # mimic the effect of incrementing n_iter on the array references\n            prev_coef = coef\n            prev_alpha[0] = alpha[0]\n            coef = np.zeros_like(coef)\n\n        coef[active] = prev_coef[active] + gamma_ * least_squares\n\n        # update correlations\n        Cov -= gamma_ * corr_eq_dir\n\n        # See if any coefficient has changed sign\n        if drop and method == 'lasso':\n\n            # handle the case when idx is not length of 1\n            [arrayfuncs.cholesky_delete(L[:n_active, :n_active], ii) for ii in\n                idx]\n\n            n_active -= 1\n            # handle the case when idx is not length of 1\n            drop_idx = [active.pop(ii) for ii in idx]\n\n            if Gram is None:\n                # propagate dropped variable\n                for ii in idx:\n                    for i in range(ii, n_active):\n                        X.T[i], X.T[i + 1] = swap(X.T[i], X.T[i + 1])\n                        # yeah this is stupid\n                        indices[i], indices[i + 1] = indices[i + 1], indices[i]\n\n                # TODO: this could be updated\n                residual = y - np.dot(X[:, :n_active], coef[active])\n                temp = np.dot(X.T[n_active], residual)\n\n                Cov = np.r_[temp, Cov]\n            else:\n                for ii in idx:\n                    for i in range(ii, n_active):\n                        indices[i], indices[i + 1] = indices[i + 1], indices[i]\n                        Gram[i], Gram[i + 1] = swap(Gram[i], Gram[i + 1])\n                        Gram[:, i], Gram[:, i + 1] = swap(Gram[:, i],\n                                                          Gram[:, i + 1])\n\n                # Cov_n = Cov_j + x_j * X + increment(betas) TODO:\n                # will this still work with multiple drops ?\n\n                # recompute covariance. Probably could be done better\n                # wrong as Xy is not swapped with the rest of variables\n\n                # TODO: this could be updated\n                residual = y - np.dot(X, coef)\n                temp = np.dot(X.T[drop_idx], residual)\n                Cov = np.r_[temp, Cov]\n\n            sign_active = np.delete(sign_active, idx)\n            sign_active = np.append(sign_active, 0.)  # just to maintain size\n            if verbose > 1:\n                print(\"%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (n_iter, '', drop_idx,\n                                                      n_active, abs(temp)))\n\n    if return_path:\n        # resize coefs in case of early stop\n        alphas = alphas[:n_iter + 1]\n        coefs = coefs[:n_iter + 1]\n\n        if return_n_iter:\n            return alphas, active, coefs.T, n_iter\n        else:\n            return alphas, active, coefs.T\n    else:\n        if return_n_iter:\n            return alpha, active, coef, n_iter\n        else:\n            return alpha, active, coef\n\n\n###############################################################################\n# Estimator classes\n\nclass Lars(LinearModel, RegressorMixin):\n    \"\"\"Least Angle Regression model a.k.a. LAR\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    fit_intercept : boolean\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    verbose : boolean or integer, optional\n        Sets the verbosity amount\n\n    normalize : boolean, optional, default True\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | 'auto' | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    n_nonzero_coefs : int, optional\n        Target number of non-zero coefficients. Use ``np.inf`` for no limit.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    fit_path : boolean\n        If True the full path is stored in the ``coef_path_`` attribute.\n        If you compute the solution for a large problem or many targets,\n        setting ``fit_path`` to ``False`` will lead to a speedup, especially\n        with a small alpha.\n\n    positive : boolean (default=False)\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n\n        .. deprecated:: 0.20\n\n            The option is broken and deprecated. It will be removed in v0.22.\n\n    Attributes\n    ----------\n    alphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays\n        Maximum of covariances (in absolute value) at each iteration. \\\n        ``n_alphas`` is either ``n_nonzero_coefs`` or ``n_features``, \\\n        whichever is smaller.\n\n    active_ : list, length = n_alphas | list of n_targets such lists\n        Indices of active variables at the end of the path.\n\n    coef_path_ : array, shape (n_features, n_alphas + 1) \\\n        | list of n_targets such arrays\n        The varying values of the coefficients along the path. It is not\n        present if the ``fit_path`` parameter is ``False``.\n\n    coef_ : array, shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the formulation formula).\n\n    intercept_ : float | array, shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : array-like or int\n        The number of iterations taken by lars_path to find the\n        grid of alphas for each target.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.Lars(n_nonzero_coefs=1)\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n    ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    Lars(copy_X=True, eps=..., fit_intercept=True, fit_path=True,\n       n_nonzero_coefs=1, normalize=True, positive=False, precompute='auto',\n       verbose=False)\n    >>> print(reg.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    [ 0. -1.11...]\n\n    See also\n    --------\n    lars_path, LarsCV\n    sklearn.decomposition.sparse_encode\n\n    \"\"\"\n    method = 'lar'\n\n    def __init__(self, fit_intercept=True, verbose=False, normalize=True,\n                 precompute='auto', n_nonzero_coefs=500,\n                 eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n                 positive=False):\n        self.fit_intercept = fit_intercept\n        self.verbose = verbose\n        self.normalize = normalize\n        self.precompute = precompute\n        self.n_nonzero_coefs = n_nonzero_coefs\n        self.positive = positive\n        self.eps = eps\n        self.copy_X = copy_X\n        self.fit_path = fit_path\n\n    @staticmethod\n    def _get_gram(precompute, X, y):\n        if (not hasattr(precompute, '__array__')) and (\n                (precompute is True) or\n                (precompute == 'auto' and X.shape[0] > X.shape[1]) or\n                (precompute == 'auto' and y.shape[1] > 1)):\n            precompute = np.dot(X.T, X)\n\n        return precompute\n\n    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):\n        \"\"\"Auxiliary method to fit the model using X, y as training data\"\"\"\n        n_features = X.shape[1]\n\n        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, self.copy_X)\n\n        if y.ndim == 1:\n            y = y[:, np.newaxis]\n\n        n_targets = y.shape[1]\n\n        Gram = self._get_gram(self.precompute, X, y)\n\n        self.alphas_ = []\n        self.n_iter_ = []\n        self.coef_ = np.empty((n_targets, n_features))\n\n        if fit_path:\n            self.active_ = []\n            self.coef_path_ = []\n            for k in range(n_targets):\n                this_Xy = None if Xy is None else Xy[:, k]\n                alphas, active, coef_path, n_iter_ = lars_path(\n                    X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X,\n                    copy_Gram=True, alpha_min=alpha, method=self.method,\n                    verbose=max(0, self.verbose - 1), max_iter=max_iter,\n                    eps=self.eps, return_path=True,\n                    return_n_iter=True, positive=self.positive)\n                self.alphas_.append(alphas)\n                self.active_.append(active)\n                self.n_iter_.append(n_iter_)\n                self.coef_path_.append(coef_path)\n                self.coef_[k] = coef_path[:, -1]\n\n            if n_targets == 1:\n                self.alphas_, self.active_, self.coef_path_, self.coef_ = [\n                    a[0] for a in (self.alphas_, self.active_, self.coef_path_,\n                                   self.coef_)]\n                self.n_iter_ = self.n_iter_[0]\n        else:\n            for k in range(n_targets):\n                this_Xy = None if Xy is None else Xy[:, k]\n                alphas, _, self.coef_[k], n_iter_ = lars_path(\n                    X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X,\n                    copy_Gram=True, alpha_min=alpha, method=self.method,\n                    verbose=max(0, self.verbose - 1), max_iter=max_iter,\n                    eps=self.eps, return_path=False, return_n_iter=True,\n                    positive=self.positive)\n                self.alphas_.append(alphas)\n                self.n_iter_.append(n_iter_)\n            if n_targets == 1:\n                self.alphas_ = self.alphas_[0]\n                self.n_iter_ = self.n_iter_[0]\n\n        self._set_intercept(X_offset, y_offset, X_scale)\n        return self\n\n    def fit(self, X, y, Xy=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        Xy : array-like, shape (n_samples,) or (n_samples, n_targets), \\\n                optional\n            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n            only when the Gram matrix is precomputed.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True, multi_output=True)\n\n        alpha = getattr(self, 'alpha', 0.)\n        if hasattr(self, 'n_nonzero_coefs'):\n            alpha = 0.  # n_nonzero_coefs parametrization takes priority\n            max_iter = self.n_nonzero_coefs\n        else:\n            max_iter = self.max_iter\n\n        self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path,\n                  Xy=Xy)\n\n        return self\n\n\nclass LassoLars(Lars):\n    \"\"\"Lasso model fit with Least Angle Regression a.k.a. Lars\n\n    It is a Linear Model trained with an L1 prior as regularizer.\n\n    The optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    alpha : float\n        Constant that multiplies the penalty term. Defaults to 1.0.\n        ``alpha = 0`` is equivalent to an ordinary least square, solved\n        by :class:`LinearRegression`. For numerical reasons, using\n        ``alpha = 0`` with the LassoLars object is not advised and you\n        should prefer the LinearRegression object.\n\n    fit_intercept : boolean\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    verbose : boolean or integer, optional\n        Sets the verbosity amount\n\n    normalize : boolean, optional, default True\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | 'auto' | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : integer, optional\n        Maximum number of iterations to perform.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_X : boolean, optional, default True\n        If True, X will be copied; else, it may be overwritten.\n\n    fit_path : boolean\n        If ``True`` the full path is stored in the ``coef_path_`` attribute.\n        If you compute the solution for a large problem or many targets,\n        setting ``fit_path`` to ``False`` will lead to a speedup, especially\n        with a small alpha.\n\n    positive : boolean (default=False)\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        Under the positive restriction the model coefficients will not converge\n        to the ordinary-least-squares solution for small values of alpha.\n        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent Lasso estimator.\n\n    Attributes\n    ----------\n    alphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays\n        Maximum of covariances (in absolute value) at each iteration. \\\n        ``n_alphas`` is either ``max_iter``, ``n_features``, or the number of \\\n        nodes in the path with correlation greater than ``alpha``, whichever \\\n        is smaller.\n\n    active_ : list, length = n_alphas | list of n_targets such lists\n        Indices of active variables at the end of the path.\n\n    coef_path_ : array, shape (n_features, n_alphas + 1) or list\n        If a list is passed it's expected to be one of n_targets such arrays.\n        The varying values of the coefficients along the path. It is not\n        present if the ``fit_path`` parameter is ``False``.\n\n    coef_ : array, shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the formulation formula).\n\n    intercept_ : float | array, shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : array-like or int.\n        The number of iterations taken by lars_path to find the\n        grid of alphas for each target.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LassoLars(alpha=0.01)\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n    ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    LassoLars(alpha=0.01, copy_X=True, eps=..., fit_intercept=True,\n         fit_path=True, max_iter=500, normalize=True, positive=False,\n         precompute='auto', verbose=False)\n    >>> print(reg.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    [ 0.         -0.963257...]\n\n    See also\n    --------\n    lars_path\n    lasso_path\n    Lasso\n    LassoCV\n    LassoLarsCV\n    LassoLarsIC\n    sklearn.decomposition.sparse_encode\n\n    \"\"\"\n    method = 'lasso'\n\n    def __init__(self, alpha=1.0, fit_intercept=True, verbose=False,\n                 normalize=True, precompute='auto', max_iter=500,\n                 eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n                 positive=False):\n        self.alpha = alpha\n        self.fit_intercept = fit_intercept\n        self.max_iter = max_iter\n        self.verbose = verbose\n        self.normalize = normalize\n        self.positive = positive\n        self.precompute = precompute\n        self.copy_X = copy_X\n        self.eps = eps\n        self.fit_path = fit_path\n\n\n###############################################################################\n# Cross-validated estimator classes\n\ndef _check_copy_and_writeable(array, copy=False):\n    if copy or not array.flags.writeable:\n        return array.copy()\n    return array\n\n\ndef _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None,\n                        copy=True, method='lars', verbose=False,\n                        fit_intercept=True, normalize=True, max_iter=500,\n                        eps=np.finfo(np.float).eps, positive=False):\n    \"\"\"Compute the residues on left-out data for a full LARS path\n\n    Parameters\n    -----------\n    X_train : array, shape (n_samples, n_features)\n        The data to fit the LARS on\n\n    y_train : array, shape (n_samples)\n        The target variable to fit LARS on\n\n    X_test : array, shape (n_samples, n_features)\n        The data to compute the residues on\n\n    y_test : array, shape (n_samples)\n        The target variable to compute the residues on\n\n    Gram : None, 'auto', array, shape: (n_features, n_features), optional\n        Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram\n        matrix is precomputed from the given X, if there are more samples\n        than features\n\n    copy : boolean, optional\n        Whether X_train, X_test, y_train and y_test should be copied;\n        if False, they may be overwritten.\n\n    method : 'lar' | 'lasso'\n        Specifies the returned model. Select ``'lar'`` for Least Angle\n        Regression, ``'lasso'`` for the Lasso.\n\n    verbose : integer, optional\n        Sets the amount of verbosity\n\n    fit_intercept : boolean\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    positive : boolean (default=False)\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        See reservations for using this option in combination with method\n        'lasso' for expected small values of alpha in the doc of LassoLarsCV\n        and LassoLarsIC.\n\n    normalize : boolean, optional, default True\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    max_iter : integer, optional\n        Maximum number of iterations to perform.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n\n    Returns\n    --------\n    alphas : array, shape (n_alphas,)\n        Maximum of covariances (in absolute value) at each iteration.\n        ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever\n        is smaller.\n\n    active : list\n        Indices of active variables at the end of the path.\n\n    coefs : array, shape (n_features, n_alphas)\n        Coefficients along the path\n\n    residues : array, shape (n_alphas, n_samples)\n        Residues of the prediction on the test data\n    \"\"\"\n    X_train = _check_copy_and_writeable(X_train, copy)\n    y_train = _check_copy_and_writeable(y_train, copy)\n    X_test = _check_copy_and_writeable(X_test, copy)\n    y_test = _check_copy_and_writeable(y_test, copy)\n\n    if fit_intercept:\n        X_mean = X_train.mean(axis=0)\n        X_train -= X_mean\n        X_test -= X_mean\n        y_mean = y_train.mean(axis=0)\n        y_train = as_float_array(y_train, copy=False)\n        y_train -= y_mean\n        y_test = as_float_array(y_test, copy=False)\n        y_test -= y_mean\n\n    if normalize:\n        norms = np.sqrt(np.sum(X_train ** 2, axis=0))\n        nonzeros = np.flatnonzero(norms)\n        X_train[:, nonzeros] /= norms[nonzeros]\n\n    alphas, active, coefs = lars_path(\n        X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False,\n        method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps,\n        positive=positive)\n    if normalize:\n        coefs[nonzeros] /= norms[nonzeros][:, np.newaxis]\n    residues = np.dot(X_test, coefs) - y_test[:, np.newaxis]\n    return alphas, active, coefs, residues.T\n\n\nclass LarsCV(Lars):\n    \"\"\"Cross-validated Least Angle Regression model.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    fit_intercept : boolean\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    verbose : boolean or integer, optional\n        Sets the verbosity amount\n\n    max_iter : integer, optional\n        Maximum number of iterations to perform.\n\n    normalize : boolean, optional, default True\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | 'auto' | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram matrix\n        cannot be passed as argument since we will use only subsets of X.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    max_n_alphas : integer, optional\n        The maximum number of points on the path used to compute the\n        residuals in the cross-validation\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems.\n\n    copy_X : boolean, optional, default True\n        If ``True``, X will be copied; else, it may be overwritten.\n\n    positive : boolean (default=False)\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n\n        .. deprecated:: 0.20\n            The option is broken and deprecated. It will be removed in v0.22.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,)\n        parameter vector (w in the formulation formula)\n\n    intercept_ : float\n        independent term in decision function\n\n    coef_path_ : array, shape (n_features, n_alphas)\n        the varying values of the coefficients along the path\n\n    alpha_ : float\n        the estimated regularization parameter alpha\n\n    alphas_ : array, shape (n_alphas,)\n        the different values of alpha along the path\n\n    cv_alphas_ : array, shape (n_cv_alphas,)\n        all the values of alpha along the path for the different folds\n\n    mse_path_ : array, shape (n_folds, n_cv_alphas)\n        the mean square error on left-out for each fold along the path\n        (alpha values given by ``cv_alphas``)\n\n    n_iter_ : array-like or int\n        the number of iterations run by Lars with the optimal alpha.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LarsCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n    >>> reg = LarsCV(cv=5).fit(X, y)\n    >>> reg.score(X, y) # doctest: +ELLIPSIS\n    0.9996...\n    >>> reg.alpha_\n    0.0254...\n    >>> reg.predict(X[:1,])\n    array([154.0842...])\n\n    See also\n    --------\n    lars_path, LassoLars, LassoLarsCV\n    \"\"\"\n\n    method = 'lar'\n\n    def __init__(self, fit_intercept=True, verbose=False, max_iter=500,\n                 normalize=True, precompute='auto', cv='warn',\n                 max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n                 copy_X=True, positive=False):\n        self.max_iter = max_iter\n        self.cv = cv\n        self.max_n_alphas = max_n_alphas\n        self.n_jobs = n_jobs\n        super().__init__(fit_intercept=fit_intercept,\n                         verbose=verbose, normalize=normalize,\n                         precompute=precompute,\n                         n_nonzero_coefs=500,\n                         eps=eps, copy_X=copy_X, fit_path=True,\n                         positive=positive)\n\n    def fit(self, X, y):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n        X = as_float_array(X, copy=self.copy_X)\n        y = as_float_array(y, copy=self.copy_X)\n\n        # init cross-validation generator\n        cv = check_cv(self.cv, classifier=False)\n\n        # As we use cross-validation, the Gram matrix is not precomputed here\n        Gram = self.precompute\n        if hasattr(Gram, '__array__'):\n            warnings.warn(\"Parameter 'precompute' cannot be an array in \"\n                          \"%s. Automatically switch to 'auto' instead.\"\n                          % self.__class__.__name__)\n            Gram = 'auto'\n\n        cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n            delayed(_lars_path_residues)(\n                X[train], y[train], X[test], y[test], Gram=Gram, copy=False,\n                method=self.method, verbose=max(0, self.verbose - 1),\n                normalize=self.normalize, fit_intercept=self.fit_intercept,\n                max_iter=self.max_iter, eps=self.eps, positive=self.positive)\n            for train, test in cv.split(X, y))\n        all_alphas = np.concatenate(list(zip(*cv_paths))[0])\n        # Unique also sorts\n        all_alphas = np.unique(all_alphas)\n        # Take at most max_n_alphas values\n        stride = int(max(1, int(len(all_alphas) / float(self.max_n_alphas))))\n        all_alphas = all_alphas[::stride]\n\n        mse_path = np.empty((len(all_alphas), len(cv_paths)))\n        for index, (alphas, active, coefs, residues) in enumerate(cv_paths):\n            alphas = alphas[::-1]\n            residues = residues[::-1]\n            if alphas[0] != 0:\n                alphas = np.r_[0, alphas]\n                residues = np.r_[residues[0, np.newaxis], residues]\n            if alphas[-1] != all_alphas[-1]:\n                alphas = np.r_[alphas, all_alphas[-1]]\n                residues = np.r_[residues, residues[-1, np.newaxis]]\n            this_residues = interpolate.interp1d(alphas,\n                                                 residues,\n                                                 axis=0)(all_alphas)\n            this_residues **= 2\n            mse_path[:, index] = np.mean(this_residues, axis=-1)\n\n        mask = np.all(np.isfinite(mse_path), axis=-1)\n        all_alphas = all_alphas[mask]\n        mse_path = mse_path[mask]\n        # Select the alpha that minimizes left-out error\n        i_best_alpha = np.argmin(mse_path.mean(axis=-1))\n        best_alpha = all_alphas[i_best_alpha]\n\n        # Store our parameters\n        self.alpha_ = best_alpha\n        self.cv_alphas_ = all_alphas\n        self.mse_path_ = mse_path\n\n        # Now compute the full model\n        # it will call a lasso internally when self if LassoLarsCV\n        # as self.method == 'lasso'\n        self._fit(X, y, max_iter=self.max_iter, alpha=best_alpha,\n                  Xy=None, fit_path=True)\n        return self\n\n\nclass LassoLarsCV(LarsCV):\n    \"\"\"Cross-validated Lasso, using the LARS algorithm.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    The optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    fit_intercept : boolean\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    verbose : boolean or integer, optional\n        Sets the verbosity amount\n\n    max_iter : integer, optional\n        Maximum number of iterations to perform.\n\n    normalize : boolean, optional, default True\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | 'auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram matrix\n        cannot be passed as argument since we will use only subsets of X.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.20\n            ``cv`` default value if None will change from 3-fold to 5-fold\n            in v0.22.\n\n    max_n_alphas : integer, optional\n        The maximum number of points on the path used to compute the\n        residuals in the cross-validation\n\n    n_jobs : int or None, optional (default=None)\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems.\n\n    copy_X : boolean, optional, default True\n        If True, X will be copied; else, it may be overwritten.\n\n    positive : boolean (default=False)\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        Under the positive restriction the model coefficients do not converge\n        to the ordinary-least-squares solution for small values of alpha.\n        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent Lasso estimator.\n        As a consequence using LassoLarsCV only makes sense for problems where\n        a sparse solution is expected and/or reached.\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,)\n        parameter vector (w in the formulation formula)\n\n    intercept_ : float\n        independent term in decision function.\n\n    coef_path_ : array, shape (n_features, n_alphas)\n        the varying values of the coefficients along the path\n\n    alpha_ : float\n        the estimated regularization parameter alpha\n\n    alphas_ : array, shape (n_alphas,)\n        the different values of alpha along the path\n\n    cv_alphas_ : array, shape (n_cv_alphas,)\n        all the values of alpha along the path for the different folds\n\n    mse_path_ : array, shape (n_folds, n_cv_alphas)\n        the mean square error on left-out for each fold along the path\n        (alpha values given by ``cv_alphas``)\n\n    n_iter_ : array-like or int\n        the number of iterations run by Lars with the optimal alpha.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoLarsCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4.0, random_state=0)\n    >>> reg = LassoLarsCV(cv=5).fit(X, y)\n    >>> reg.score(X, y) # doctest: +ELLIPSIS\n    0.9992...\n    >>> reg.alpha_\n    0.0484...\n    >>> reg.predict(X[:1,])\n    array([-77.8723...])\n\n    Notes\n    -----\n\n    The object solves the same problem as the LassoCV object. However,\n    unlike the LassoCV, it find the relevant alphas values by itself.\n    In general, because of this property, it will be more stable.\n    However, it is more fragile to heavily multicollinear datasets.\n\n    It is more efficient than the LassoCV if only a small number of\n    features are selected compared to the total number, for instance if\n    there are very few samples compared to the number of features.\n\n    See also\n    --------\n    lars_path, LassoLars, LarsCV, LassoCV\n    \"\"\"\n\n    method = 'lasso'\n\n    def __init__(self, fit_intercept=True, verbose=False, max_iter=500,\n                 normalize=True, precompute='auto', cv='warn',\n                 max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n                 copy_X=True, positive=False):\n        self.fit_intercept = fit_intercept\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.normalize = normalize\n        self.precompute = precompute\n        self.cv = cv\n        self.max_n_alphas = max_n_alphas\n        self.n_jobs = n_jobs\n        self.eps = eps\n        self.copy_X = copy_X\n        self.positive = positive\n        # XXX : we don't use super().__init__\n        # to avoid setting n_nonzero_coefs\n\n\nclass LassoLarsIC(LassoLars):\n    \"\"\"Lasso model fit with Lars using BIC or AIC for model selection\n\n    The optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    AIC is the Akaike information criterion and BIC is the Bayes\n    Information criterion. Such criteria are useful to select the value\n    of the regularization parameter by making a trade-off between the\n    goodness of fit and the complexity of the model. A good model should\n    explain well the data while being simple.\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    criterion : 'bic' | 'aic'\n        The type of criterion to use.\n\n    fit_intercept : boolean\n        whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (e.g. data is expected to be already centered).\n\n    verbose : boolean or integer, optional\n        Sets the verbosity amount\n\n    normalize : boolean, optional, default True\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n    precompute : True | False | 'auto' | array-like\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram\n        matrix can also be passed as argument.\n\n    max_iter : integer, optional\n        Maximum number of iterations to perform. Can be used for\n        early stopping.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_X : boolean, optional, default True\n        If True, X will be copied; else, it may be overwritten.\n\n    positive : boolean (default=False)\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        Under the positive restriction the model coefficients do not converge\n        to the ordinary-least-squares solution for small values of alpha.\n        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent Lasso estimator.\n        As a consequence using LassoLarsIC only makes sense for problems where\n        a sparse solution is expected and/or reached.\n\n\n    Attributes\n    ----------\n    coef_ : array, shape (n_features,)\n        parameter vector (w in the formulation formula)\n\n    intercept_ : float\n        independent term in decision function.\n\n    alpha_ : float\n        the alpha parameter chosen by the information criterion\n\n    n_iter_ : int\n        number of iterations run by lars_path to find the grid of\n        alphas.\n\n    criterion_ : array, shape (n_alphas,)\n        The value of the information criteria ('aic', 'bic') across all\n        alphas. The alpha which has the smallest information criterion is\n        chosen. This value is larger by a factor of ``n_samples`` compared to\n        Eqns. 2.15 and 2.16 in (Zou et al, 2007).\n\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LassoLarsIC(criterion='bic')\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n    ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    LassoLarsIC(copy_X=True, criterion='bic', eps=..., fit_intercept=True,\n          max_iter=500, normalize=True, positive=False, precompute='auto',\n          verbose=False)\n    >>> print(reg.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n    [ 0.  -1.11...]\n\n    Notes\n    -----\n    The estimation of the number of degrees of freedom is given by:\n\n    \"On the degrees of freedom of the lasso\"\n    Hui Zou, Trevor Hastie, and Robert Tibshirani\n    Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.\n\n    https://en.wikipedia.org/wiki/Akaike_information_criterion\n    https://en.wikipedia.org/wiki/Bayesian_information_criterion\n\n    See also\n    --------\n    lars_path, LassoLars, LassoLarsCV\n    \"\"\"\n    def __init__(self, criterion='aic', fit_intercept=True, verbose=False,\n                 normalize=True, precompute='auto', max_iter=500,\n                 eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n        self.criterion = criterion\n        self.fit_intercept = fit_intercept\n        self.positive = positive\n        self.max_iter = max_iter\n        self.verbose = verbose\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.precompute = precompute\n        self.eps = eps\n        self.fit_path = True\n\n    def fit(self, X, y, copy_X=True):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values. Will be cast to X's dtype if necessary\n\n        copy_X : boolean, optional, default True\n            If ``True``, X will be copied; else, it may be overwritten.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        \"\"\"\n        X, y = check_X_y(X, y, y_numeric=True)\n\n        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(\n            X, y, self.fit_intercept, self.normalize, self.copy_X)\n        max_iter = self.max_iter\n\n        Gram = self.precompute\n\n        alphas_, active_, coef_path_, self.n_iter_ = lars_path(\n            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,\n            method='lasso', verbose=self.verbose, max_iter=max_iter,\n            eps=self.eps, return_n_iter=True, positive=self.positive)\n\n        n_samples = X.shape[0]\n\n        if self.criterion == 'aic':\n            K = 2  # AIC\n        elif self.criterion == 'bic':\n            K = log(n_samples)  # BIC\n        else:\n            raise ValueError('criterion should be either bic or aic')\n\n        R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals\n        mean_squared_error = np.mean(R ** 2, axis=0)\n        sigma2 = np.var(y)\n\n        df = np.zeros(coef_path_.shape[1], dtype=np.int)  # Degrees of freedom\n        for k, coef in enumerate(coef_path_.T):\n            mask = np.abs(coef) > np.finfo(coef.dtype).eps\n            if not np.any(mask):\n                continue\n            # get the number of degrees of freedom equal to:\n            # Xc = X[:, mask]\n            # Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs\n            df[k] = np.sum(mask)\n\n        self.alphas_ = alphas_\n        eps64 = np.finfo('float64').eps\n        self.criterion_ = (n_samples * mean_squared_error / (sigma2 + eps64) +\n                           K * df)  # Eqns. 2.15--16 in (Zou et al, 2007)\n        n_best = np.argmin(self.criterion_)\n\n        self.alpha_ = alphas_[n_best]\n        self.coef_ = coef_path_[:, n_best]\n        self._set_intercept(Xmean, ymean, Xstd)\n        return self"}
{"model_name_or_path": "agentless", "instance_id": "sphinx-doc__sphinx-7910", "model_patch": "", "raw_model_patch": "", "original_file_content": "\"\"\"\n    sphinx.domains.python\n    ~~~~~~~~~~~~~~~~~~~~~\n\n    The Python domain.\n\n    :copyright: Copyright 2007-2020 by the Sphinx team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport builtins\nimport inspect\nimport re\nimport typing\nimport warnings\nfrom inspect import Parameter\nfrom typing import Any, Dict, Iterable, Iterator, List, NamedTuple, Tuple\nfrom typing import cast\n\nfrom docutils import nodes\nfrom docutils.nodes import Element, Node\nfrom docutils.parsers.rst import directives\n\nfrom sphinx import addnodes\nfrom sphinx.addnodes import pending_xref, desc_signature\nfrom sphinx.application import Sphinx\nfrom sphinx.builders import Builder\nfrom sphinx.deprecation import RemovedInSphinx40Warning, RemovedInSphinx50Warning\nfrom sphinx.directives import ObjectDescription\nfrom sphinx.domains import Domain, ObjType, Index, IndexEntry\nfrom sphinx.environment import BuildEnvironment\nfrom sphinx.locale import _, __\nfrom sphinx.pycode.ast import ast, parse as ast_parse\nfrom sphinx.roles import XRefRole\nfrom sphinx.util import logging\nfrom sphinx.util.docfields import Field, GroupedField, TypedField\nfrom sphinx.util.docutils import SphinxDirective\nfrom sphinx.util.inspect import signature_from_str\nfrom sphinx.util.nodes import make_id, make_refnode\nfrom sphinx.util.typing import TextlikeNode\n\nif False:\n    # For type annotation\n    from typing import Type  # for python3.5.1\n\n\nlogger = logging.getLogger(__name__)\n\n\n# REs for Python signatures\npy_sig_re = re.compile(\n    r'''^ ([\\w.]*\\.)?            # class name(s)\n          (\\w+)  \\s*             # thing name\n          (?: \\(\\s*(.*)\\s*\\)     # optional: arguments\n           (?:\\s* -> \\s* (.*))?  #           return annotation\n          )? $                   # and nothing more\n          ''', re.VERBOSE)\n\n\npairindextypes = {\n    'module':    _('module'),\n    'keyword':   _('keyword'),\n    'operator':  _('operator'),\n    'object':    _('object'),\n    'exception': _('exception'),\n    'statement': _('statement'),\n    'builtin':   _('built-in function'),\n}\n\nObjectEntry = NamedTuple('ObjectEntry', [('docname', str),\n                                         ('node_id', str),\n                                         ('objtype', str)])\nModuleEntry = NamedTuple('ModuleEntry', [('docname', str),\n                                         ('node_id', str),\n                                         ('synopsis', str),\n                                         ('platform', str),\n                                         ('deprecated', bool)])\n\n\ndef type_to_xref(text: str) -> addnodes.pending_xref:\n    \"\"\"Convert a type string to a cross reference node.\"\"\"\n    if text == 'None':\n        reftype = 'obj'\n    else:\n        reftype = 'class'\n\n    return pending_xref('', nodes.Text(text),\n                        refdomain='py', reftype=reftype, reftarget=text)\n\n\ndef _parse_annotation(annotation: str) -> List[Node]:\n    \"\"\"Parse type annotation.\"\"\"\n    def unparse(node: ast.AST) -> List[Node]:\n        if isinstance(node, ast.Attribute):\n            return [nodes.Text(\"%s.%s\" % (unparse(node.value)[0], node.attr))]\n        elif isinstance(node, ast.Expr):\n            return unparse(node.value)\n        elif isinstance(node, ast.Index):\n            return unparse(node.value)\n        elif isinstance(node, ast.List):\n            result = [addnodes.desc_sig_punctuation('', '[')]  # type: List[Node]\n            for elem in node.elts:\n                result.extend(unparse(elem))\n                result.append(addnodes.desc_sig_punctuation('', ', '))\n            result.pop()\n            result.append(addnodes.desc_sig_punctuation('', ']'))\n            return result\n        elif isinstance(node, ast.Module):\n            return sum((unparse(e) for e in node.body), [])\n        elif isinstance(node, ast.Name):\n            return [nodes.Text(node.id)]\n        elif isinstance(node, ast.Subscript):\n            result = unparse(node.value)\n            result.append(addnodes.desc_sig_punctuation('', '['))\n            result.extend(unparse(node.slice))\n            result.append(addnodes.desc_sig_punctuation('', ']'))\n            return result\n        elif isinstance(node, ast.Tuple):\n            if node.elts:\n                result = []\n                for elem in node.elts:\n                    result.extend(unparse(elem))\n                    result.append(addnodes.desc_sig_punctuation('', ', '))\n                result.pop()\n            else:\n                result = [addnodes.desc_sig_punctuation('', '('),\n                          addnodes.desc_sig_punctuation('', ')')]\n\n            return result\n        else:\n            raise SyntaxError  # unsupported syntax\n\n    try:\n        tree = ast_parse(annotation)\n        result = unparse(tree)\n        for i, node in enumerate(result):\n            if isinstance(node, nodes.Text):\n                result[i] = type_to_xref(str(node))\n        return result\n    except SyntaxError:\n        return [type_to_xref(annotation)]\n\n\ndef _parse_arglist(arglist: str) -> addnodes.desc_parameterlist:\n    \"\"\"Parse a list of arguments using AST parser\"\"\"\n    params = addnodes.desc_parameterlist(arglist)\n    sig = signature_from_str('(%s)' % arglist)\n    last_kind = None\n    for param in sig.parameters.values():\n        if param.kind != param.POSITIONAL_ONLY and last_kind == param.POSITIONAL_ONLY:\n            # PEP-570: Separator for Positional Only Parameter: /\n            params += addnodes.desc_parameter('', '', addnodes.desc_sig_operator('', '/'))\n        if param.kind == param.KEYWORD_ONLY and last_kind in (param.POSITIONAL_OR_KEYWORD,\n                                                              param.POSITIONAL_ONLY,\n                                                              None):\n            # PEP-3102: Separator for Keyword Only Parameter: *\n            params += addnodes.desc_parameter('', '', addnodes.desc_sig_operator('', '*'))\n\n        node = addnodes.desc_parameter()\n        if param.kind == param.VAR_POSITIONAL:\n            node += addnodes.desc_sig_operator('', '*')\n            node += addnodes.desc_sig_name('', param.name)\n        elif param.kind == param.VAR_KEYWORD:\n            node += addnodes.desc_sig_operator('', '**')\n            node += addnodes.desc_sig_name('', param.name)\n        else:\n            node += addnodes.desc_sig_name('', param.name)\n\n        if param.annotation is not param.empty:\n            children = _parse_annotation(param.annotation)\n            node += addnodes.desc_sig_punctuation('', ':')\n            node += nodes.Text(' ')\n            node += addnodes.desc_sig_name('', '', *children)  # type: ignore\n        if param.default is not param.empty:\n            if param.annotation is not param.empty:\n                node += nodes.Text(' ')\n                node += addnodes.desc_sig_operator('', '=')\n                node += nodes.Text(' ')\n            else:\n                node += addnodes.desc_sig_operator('', '=')\n            node += nodes.inline('', param.default, classes=['default_value'],\n                                 support_smartquotes=False)\n\n        params += node\n        last_kind = param.kind\n\n    if last_kind == Parameter.POSITIONAL_ONLY:\n        # PEP-570: Separator for Positional Only Parameter: /\n        params += addnodes.desc_parameter('', '', addnodes.desc_sig_operator('', '/'))\n\n    return params\n\n\ndef _pseudo_parse_arglist(signode: desc_signature, arglist: str) -> None:\n    \"\"\"\"Parse\" a list of arguments separated by commas.\n\n    Arguments can have \"optional\" annotations given by enclosing them in\n    brackets.  Currently, this will split at any comma, even if it's inside a\n    string literal (e.g. default argument value).\n    \"\"\"\n    paramlist = addnodes.desc_parameterlist()\n    stack = [paramlist]  # type: List[Element]\n    try:\n        for argument in arglist.split(','):\n            argument = argument.strip()\n            ends_open = ends_close = 0\n            while argument.startswith('['):\n                stack.append(addnodes.desc_optional())\n                stack[-2] += stack[-1]\n                argument = argument[1:].strip()\n            while argument.startswith(']'):\n                stack.pop()\n                argument = argument[1:].strip()\n            while argument.endswith(']') and not argument.endswith('[]'):\n                ends_close += 1\n                argument = argument[:-1].strip()\n            while argument.endswith('['):\n                ends_open += 1\n                argument = argument[:-1].strip()\n            if argument:\n                stack[-1] += addnodes.desc_parameter(argument, argument)\n            while ends_open:\n                stack.append(addnodes.desc_optional())\n                stack[-2] += stack[-1]\n                ends_open -= 1\n            while ends_close:\n                stack.pop()\n                ends_close -= 1\n        if len(stack) != 1:\n            raise IndexError\n    except IndexError:\n        # if there are too few or too many elements on the stack, just give up\n        # and treat the whole argument list as one argument, discarding the\n        # already partially populated paramlist node\n        paramlist = addnodes.desc_parameterlist()\n        paramlist += addnodes.desc_parameter(arglist, arglist)\n        signode += paramlist\n    else:\n        signode += paramlist\n\n\n# This override allows our inline type specifiers to behave like :class: link\n# when it comes to handling \".\" and \"~\" prefixes.\nclass PyXrefMixin:\n    def make_xref(self, rolename: str, domain: str, target: str,\n                  innernode: \"Type[TextlikeNode]\" = nodes.emphasis,\n                  contnode: Node = None, env: BuildEnvironment = None) -> Node:\n        result = super().make_xref(rolename, domain, target,  # type: ignore\n                                   innernode, contnode, env)\n        result['refspecific'] = True\n        if target.startswith(('.', '~')):\n            prefix, result['reftarget'] = target[0], target[1:]\n            if prefix == '.':\n                text = target[1:]\n            elif prefix == '~':\n                text = target.split('.')[-1]\n            for node in result.traverse(nodes.Text):\n                node.parent[node.parent.index(node)] = nodes.Text(text)\n                break\n        return result\n\n    def make_xrefs(self, rolename: str, domain: str, target: str,\n                   innernode: \"Type[TextlikeNode]\" = nodes.emphasis,\n                   contnode: Node = None, env: BuildEnvironment = None) -> List[Node]:\n        delims = r'(\\s*[\\[\\]\\(\\),](?:\\s*or\\s)?\\s*|\\s+or\\s+)'\n        delims_re = re.compile(delims)\n        sub_targets = re.split(delims, target)\n\n        split_contnode = bool(contnode and contnode.astext() == target)\n\n        results = []\n        for sub_target in filter(None, sub_targets):\n            if split_contnode:\n                contnode = nodes.Text(sub_target)\n\n            if delims_re.match(sub_target):\n                results.append(contnode or innernode(sub_target, sub_target))\n            else:\n                results.append(self.make_xref(rolename, domain, sub_target,\n                                              innernode, contnode, env))\n\n        return results\n\n\nclass PyField(PyXrefMixin, Field):\n    def make_xref(self, rolename: str, domain: str, target: str,\n                  innernode: \"Type[TextlikeNode]\" = nodes.emphasis,\n                  contnode: Node = None, env: BuildEnvironment = None) -> Node:\n        if rolename == 'class' and target == 'None':\n            # None is not a type, so use obj role instead.\n            rolename = 'obj'\n\n        return super().make_xref(rolename, domain, target, innernode, contnode, env)\n\n\nclass PyGroupedField(PyXrefMixin, GroupedField):\n    pass\n\n\nclass PyTypedField(PyXrefMixin, TypedField):\n    def make_xref(self, rolename: str, domain: str, target: str,\n                  innernode: \"Type[TextlikeNode]\" = nodes.emphasis,\n                  contnode: Node = None, env: BuildEnvironment = None) -> Node:\n        if rolename == 'class' and target == 'None':\n            # None is not a type, so use obj role instead.\n            rolename = 'obj'\n\n        return super().make_xref(rolename, domain, target, innernode, contnode, env)\n\n\nclass PyObject(ObjectDescription):\n    \"\"\"\n    Description of a general Python object.\n\n    :cvar allow_nesting: Class is an object that allows for nested namespaces\n    :vartype allow_nesting: bool\n    \"\"\"\n    option_spec = {\n        'noindex': directives.flag,\n        'noindexentry': directives.flag,\n        'module': directives.unchanged,\n        'annotation': directives.unchanged,\n    }\n\n    doc_field_types = [\n        PyTypedField('parameter', label=_('Parameters'),\n                     names=('param', 'parameter', 'arg', 'argument',\n                            'keyword', 'kwarg', 'kwparam'),\n                     typerolename='class', typenames=('paramtype', 'type'),\n                     can_collapse=True),\n        PyTypedField('variable', label=_('Variables'), rolename='obj',\n                     names=('var', 'ivar', 'cvar'),\n                     typerolename='class', typenames=('vartype',),\n                     can_collapse=True),\n        PyGroupedField('exceptions', label=_('Raises'), rolename='exc',\n                       names=('raises', 'raise', 'exception', 'except'),\n                       can_collapse=True),\n        Field('returnvalue', label=_('Returns'), has_arg=False,\n              names=('returns', 'return')),\n        PyField('returntype', label=_('Return type'), has_arg=False,\n                names=('rtype',), bodyrolename='class'),\n    ]\n\n    allow_nesting = False\n\n    def get_signature_prefix(self, sig: str) -> str:\n        \"\"\"May return a prefix to put before the object name in the\n        signature.\n        \"\"\"\n        return ''\n\n    def needs_arglist(self) -> bool:\n        \"\"\"May return true if an empty argument list is to be generated even if\n        the document contains none.\n        \"\"\"\n        return False\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        \"\"\"Transform a Python signature into RST nodes.\n\n        Return (fully qualified name of the thing, classname if any).\n\n        If inside a class, the current class name is handled intelligently:\n        * it is stripped from the displayed name if present\n        * it is added to the full name (return value) if not present\n        \"\"\"\n        m = py_sig_re.match(sig)\n        if m is None:\n            raise ValueError\n        prefix, name, arglist, retann = m.groups()\n\n        # determine module and class name (if applicable), as well as full name\n        modname = self.options.get('module', self.env.ref_context.get('py:module'))\n        classname = self.env.ref_context.get('py:class')\n        if classname:\n            add_module = False\n            if prefix and (prefix == classname or\n                           prefix.startswith(classname + \".\")):\n                fullname = prefix + name\n                # class name is given again in the signature\n                prefix = prefix[len(classname):].lstrip('.')\n            elif prefix:\n                # class name is given in the signature, but different\n                # (shouldn't happen)\n                fullname = classname + '.' + prefix + name\n            else:\n                # class name is not given in the signature\n                fullname = classname + '.' + name\n        else:\n            add_module = True\n            if prefix:\n                classname = prefix.rstrip('.')\n                fullname = prefix + name\n            else:\n                classname = ''\n                fullname = name\n\n        signode['module'] = modname\n        signode['class'] = classname\n        signode['fullname'] = fullname\n\n        sig_prefix = self.get_signature_prefix(sig)\n        if sig_prefix:\n            signode += addnodes.desc_annotation(sig_prefix, sig_prefix)\n\n        if prefix:\n            signode += addnodes.desc_addname(prefix, prefix)\n        elif add_module and self.env.config.add_module_names:\n            if modname and modname != 'exceptions':\n                # exceptions are a special case, since they are documented in the\n                # 'exceptions' module.\n                nodetext = modname + '.'\n                signode += addnodes.desc_addname(nodetext, nodetext)\n\n        signode += addnodes.desc_name(name, name)\n        if arglist:\n            try:\n                signode += _parse_arglist(arglist)\n            except SyntaxError:\n                # fallback to parse arglist original parser.\n                # it supports to represent optional arguments (ex. \"func(foo [, bar])\")\n                _pseudo_parse_arglist(signode, arglist)\n            except NotImplementedError as exc:\n                logger.warning(\"could not parse arglist (%r): %s\", arglist, exc,\n                               location=signode)\n                _pseudo_parse_arglist(signode, arglist)\n        else:\n            if self.needs_arglist():\n                # for callables, add an empty parameter list\n                signode += addnodes.desc_parameterlist()\n\n        if retann:\n            children = _parse_annotation(retann)\n            signode += addnodes.desc_returns(retann, '', *children)\n\n        anno = self.options.get('annotation')\n        if anno:\n            signode += addnodes.desc_annotation(' ' + anno, ' ' + anno)\n\n        return fullname, prefix\n\n    def get_index_text(self, modname: str, name: Tuple[str, str]) -> str:\n        \"\"\"Return the text for the index entry of the object.\"\"\"\n        raise NotImplementedError('must be implemented in subclasses')\n\n    def add_target_and_index(self, name_cls: Tuple[str, str], sig: str,\n                             signode: desc_signature) -> None:\n        modname = self.options.get('module', self.env.ref_context.get('py:module'))\n        fullname = (modname + '.' if modname else '') + name_cls[0]\n        node_id = make_id(self.env, self.state.document, '', fullname)\n        signode['ids'].append(node_id)\n\n        # Assign old styled node_id(fullname) not to break old hyperlinks (if possible)\n        # Note: Will removed in Sphinx-5.0  (RemovedInSphinx50Warning)\n        if node_id != fullname and fullname not in self.state.document.ids:\n            signode['ids'].append(fullname)\n\n        self.state.document.note_explicit_target(signode)\n\n        domain = cast(PythonDomain, self.env.get_domain('py'))\n        domain.note_object(fullname, self.objtype, node_id, location=signode)\n\n        if 'noindexentry' not in self.options:\n            indextext = self.get_index_text(modname, name_cls)\n            if indextext:\n                self.indexnode['entries'].append(('single', indextext, node_id, '', None))\n\n    def before_content(self) -> None:\n        \"\"\"Handle object nesting before content\n\n        :py:class:`PyObject` represents Python language constructs. For\n        constructs that are nestable, such as a Python classes, this method will\n        build up a stack of the nesting heirarchy so that it can be later\n        de-nested correctly, in :py:meth:`after_content`.\n\n        For constructs that aren't nestable, the stack is bypassed, and instead\n        only the most recent object is tracked. This object prefix name will be\n        removed with :py:meth:`after_content`.\n        \"\"\"\n        prefix = None\n        if self.names:\n            # fullname and name_prefix come from the `handle_signature` method.\n            # fullname represents the full object name that is constructed using\n            # object nesting and explicit prefixes. `name_prefix` is the\n            # explicit prefix given in a signature\n            (fullname, name_prefix) = self.names[-1]\n            if self.allow_nesting:\n                prefix = fullname\n            elif name_prefix:\n                prefix = name_prefix.strip('.')\n        if prefix:\n            self.env.ref_context['py:class'] = prefix\n            if self.allow_nesting:\n                classes = self.env.ref_context.setdefault('py:classes', [])\n                classes.append(prefix)\n        if 'module' in self.options:\n            modules = self.env.ref_context.setdefault('py:modules', [])\n            modules.append(self.env.ref_context.get('py:module'))\n            self.env.ref_context['py:module'] = self.options['module']\n\n    def after_content(self) -> None:\n        \"\"\"Handle object de-nesting after content\n\n        If this class is a nestable object, removing the last nested class prefix\n        ends further nesting in the object.\n\n        If this class is not a nestable object, the list of classes should not\n        be altered as we didn't affect the nesting levels in\n        :py:meth:`before_content`.\n        \"\"\"\n        classes = self.env.ref_context.setdefault('py:classes', [])\n        if self.allow_nesting:\n            try:\n                classes.pop()\n            except IndexError:\n                pass\n        self.env.ref_context['py:class'] = (classes[-1] if len(classes) > 0\n                                            else None)\n        if 'module' in self.options:\n            modules = self.env.ref_context.setdefault('py:modules', [])\n            if modules:\n                self.env.ref_context['py:module'] = modules.pop()\n            else:\n                self.env.ref_context.pop('py:module')\n\n\nclass PyModulelevel(PyObject):\n    \"\"\"\n    Description of an object on module level (functions, data).\n    \"\"\"\n\n    def run(self) -> List[Node]:\n        for cls in self.__class__.__mro__:\n            if cls.__name__ != 'DirectiveAdapter':\n                warnings.warn('PyModulelevel is deprecated. '\n                              'Please check the implementation of %s' % cls,\n                              RemovedInSphinx40Warning, stacklevel=2)\n                break\n        else:\n            warnings.warn('PyModulelevel is deprecated',\n                          RemovedInSphinx40Warning, stacklevel=2)\n\n        return super().run()\n\n    def needs_arglist(self) -> bool:\n        return self.objtype == 'function'\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        if self.objtype == 'function':\n            if not modname:\n                return _('%s() (built-in function)') % name_cls[0]\n            return _('%s() (in module %s)') % (name_cls[0], modname)\n        elif self.objtype == 'data':\n            if not modname:\n                return _('%s (built-in variable)') % name_cls[0]\n            return _('%s (in module %s)') % (name_cls[0], modname)\n        else:\n            return ''\n\n\nclass PyFunction(PyObject):\n    \"\"\"Description of a function.\"\"\"\n\n    option_spec = PyObject.option_spec.copy()\n    option_spec.update({\n        'async': directives.flag,\n    })\n\n    def get_signature_prefix(self, sig: str) -> str:\n        if 'async' in self.options:\n            return 'async '\n        else:\n            return ''\n\n    def needs_arglist(self) -> bool:\n        return True\n\n    def add_target_and_index(self, name_cls: Tuple[str, str], sig: str,\n                             signode: desc_signature) -> None:\n        super().add_target_and_index(name_cls, sig, signode)\n        if 'noindexentry' not in self.options:\n            modname = self.options.get('module', self.env.ref_context.get('py:module'))\n            node_id = signode['ids'][0]\n\n            name, cls = name_cls\n            if modname:\n                text = _('%s() (in module %s)') % (name, modname)\n                self.indexnode['entries'].append(('single', text, node_id, '', None))\n            else:\n                text = '%s; %s()' % (pairindextypes['builtin'], name)\n                self.indexnode['entries'].append(('pair', text, node_id, '', None))\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        # add index in own add_target_and_index() instead.\n        return None\n\n\nclass PyDecoratorFunction(PyFunction):\n    \"\"\"Description of a decorator.\"\"\"\n\n    def run(self) -> List[Node]:\n        # a decorator function is a function after all\n        self.name = 'py:function'\n        return super().run()\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        ret = super().handle_signature(sig, signode)\n        signode.insert(0, addnodes.desc_addname('@', '@'))\n        return ret\n\n    def needs_arglist(self) -> bool:\n        return False\n\n\nclass PyVariable(PyObject):\n    \"\"\"Description of a variable.\"\"\"\n\n    option_spec = PyObject.option_spec.copy()\n    option_spec.update({\n        'type': directives.unchanged,\n        'value': directives.unchanged,\n    })\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        fullname, prefix = super().handle_signature(sig, signode)\n\n        typ = self.options.get('type')\n        if typ:\n            annotations = _parse_annotation(typ)\n            signode += addnodes.desc_annotation(typ, '', nodes.Text(': '), *annotations)\n\n        value = self.options.get('value')\n        if value:\n            signode += addnodes.desc_annotation(value, ' = ' + value)\n\n        return fullname, prefix\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        name, cls = name_cls\n        if modname:\n            return _('%s (in module %s)') % (name, modname)\n        else:\n            return _('%s (built-in variable)') % name\n\n\nclass PyClasslike(PyObject):\n    \"\"\"\n    Description of a class-like object (classes, interfaces, exceptions).\n    \"\"\"\n\n    option_spec = PyObject.option_spec.copy()\n    option_spec.update({\n        'final': directives.flag,\n    })\n\n    allow_nesting = True\n\n    def get_signature_prefix(self, sig: str) -> str:\n        if 'final' in self.options:\n            return 'final %s ' % self.objtype\n        else:\n            return '%s ' % self.objtype\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        if self.objtype == 'class':\n            if not modname:\n                return _('%s (built-in class)') % name_cls[0]\n            return _('%s (class in %s)') % (name_cls[0], modname)\n        elif self.objtype == 'exception':\n            return name_cls[0]\n        else:\n            return ''\n\n\nclass PyClassmember(PyObject):\n    \"\"\"\n    Description of a class member (methods, attributes).\n    \"\"\"\n\n    def run(self) -> List[Node]:\n        for cls in self.__class__.__mro__:\n            if cls.__name__ != 'DirectiveAdapter':\n                warnings.warn('PyClassmember is deprecated. '\n                              'Please check the implementation of %s' % cls,\n                              RemovedInSphinx40Warning, stacklevel=2)\n                break\n        else:\n            warnings.warn('PyClassmember is deprecated',\n                          RemovedInSphinx40Warning, stacklevel=2)\n\n        return super().run()\n\n    def needs_arglist(self) -> bool:\n        return self.objtype.endswith('method')\n\n    def get_signature_prefix(self, sig: str) -> str:\n        if self.objtype == 'staticmethod':\n            return 'static '\n        elif self.objtype == 'classmethod':\n            return 'classmethod '\n        return ''\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        name, cls = name_cls\n        add_modules = self.env.config.add_module_names\n        if self.objtype == 'method':\n            try:\n                clsname, methname = name.rsplit('.', 1)\n            except ValueError:\n                if modname:\n                    return _('%s() (in module %s)') % (name, modname)\n                else:\n                    return '%s()' % name\n            if modname and add_modules:\n                return _('%s() (%s.%s method)') % (methname, modname, clsname)\n            else:\n                return _('%s() (%s method)') % (methname, clsname)\n        elif self.objtype == 'staticmethod':\n            try:\n                clsname, methname = name.rsplit('.', 1)\n            except ValueError:\n                if modname:\n                    return _('%s() (in module %s)') % (name, modname)\n                else:\n                    return '%s()' % name\n            if modname and add_modules:\n                return _('%s() (%s.%s static method)') % (methname, modname,\n                                                          clsname)\n            else:\n                return _('%s() (%s static method)') % (methname, clsname)\n        elif self.objtype == 'classmethod':\n            try:\n                clsname, methname = name.rsplit('.', 1)\n            except ValueError:\n                if modname:\n                    return _('%s() (in module %s)') % (name, modname)\n                else:\n                    return '%s()' % name\n            if modname:\n                return _('%s() (%s.%s class method)') % (methname, modname,\n                                                         clsname)\n            else:\n                return _('%s() (%s class method)') % (methname, clsname)\n        elif self.objtype == 'attribute':\n            try:\n                clsname, attrname = name.rsplit('.', 1)\n            except ValueError:\n                if modname:\n                    return _('%s (in module %s)') % (name, modname)\n                else:\n                    return name\n            if modname and add_modules:\n                return _('%s (%s.%s attribute)') % (attrname, modname, clsname)\n            else:\n                return _('%s (%s attribute)') % (attrname, clsname)\n        else:\n            return ''\n\n\nclass PyMethod(PyObject):\n    \"\"\"Description of a method.\"\"\"\n\n    option_spec = PyObject.option_spec.copy()\n    option_spec.update({\n        'abstractmethod': directives.flag,\n        'async': directives.flag,\n        'classmethod': directives.flag,\n        'final': directives.flag,\n        'property': directives.flag,\n        'staticmethod': directives.flag,\n    })\n\n    def needs_arglist(self) -> bool:\n        if 'property' in self.options:\n            return False\n        else:\n            return True\n\n    def get_signature_prefix(self, sig: str) -> str:\n        prefix = []\n        if 'final' in self.options:\n            prefix.append('final')\n        if 'abstractmethod' in self.options:\n            prefix.append('abstract')\n        if 'async' in self.options:\n            prefix.append('async')\n        if 'classmethod' in self.options:\n            prefix.append('classmethod')\n        if 'property' in self.options:\n            prefix.append('property')\n        if 'staticmethod' in self.options:\n            prefix.append('static')\n\n        if prefix:\n            return ' '.join(prefix) + ' '\n        else:\n            return ''\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        name, cls = name_cls\n        try:\n            clsname, methname = name.rsplit('.', 1)\n            if modname and self.env.config.add_module_names:\n                clsname = '.'.join([modname, clsname])\n        except ValueError:\n            if modname:\n                return _('%s() (in module %s)') % (name, modname)\n            else:\n                return '%s()' % name\n\n        if 'classmethod' in self.options:\n            return _('%s() (%s class method)') % (methname, clsname)\n        elif 'property' in self.options:\n            return _('%s() (%s property)') % (methname, clsname)\n        elif 'staticmethod' in self.options:\n            return _('%s() (%s static method)') % (methname, clsname)\n        else:\n            return _('%s() (%s method)') % (methname, clsname)\n\n\nclass PyClassMethod(PyMethod):\n    \"\"\"Description of a classmethod.\"\"\"\n\n    option_spec = PyObject.option_spec.copy()\n\n    def run(self) -> List[Node]:\n        self.name = 'py:method'\n        self.options['classmethod'] = True\n\n        return super().run()\n\n\nclass PyStaticMethod(PyMethod):\n    \"\"\"Description of a staticmethod.\"\"\"\n\n    option_spec = PyObject.option_spec.copy()\n\n    def run(self) -> List[Node]:\n        self.name = 'py:method'\n        self.options['staticmethod'] = True\n\n        return super().run()\n\n\nclass PyDecoratorMethod(PyMethod):\n    \"\"\"Description of a decoratormethod.\"\"\"\n\n    def run(self) -> List[Node]:\n        self.name = 'py:method'\n        return super().run()\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        ret = super().handle_signature(sig, signode)\n        signode.insert(0, addnodes.desc_addname('@', '@'))\n        return ret\n\n    def needs_arglist(self) -> bool:\n        return False\n\n\nclass PyAttribute(PyObject):\n    \"\"\"Description of an attribute.\"\"\"\n\n    option_spec = PyObject.option_spec.copy()\n    option_spec.update({\n        'type': directives.unchanged,\n        'value': directives.unchanged,\n    })\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        fullname, prefix = super().handle_signature(sig, signode)\n\n        typ = self.options.get('type')\n        if typ:\n            annotations = _parse_annotation(typ)\n            signode += addnodes.desc_annotation(typ, '', nodes.Text(': '), *annotations)\n\n        value = self.options.get('value')\n        if value:\n            signode += addnodes.desc_annotation(value, ' = ' + value)\n\n        return fullname, prefix\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        name, cls = name_cls\n        try:\n            clsname, attrname = name.rsplit('.', 1)\n            if modname and self.env.config.add_module_names:\n                clsname = '.'.join([modname, clsname])\n        except ValueError:\n            if modname:\n                return _('%s (in module %s)') % (name, modname)\n            else:\n                return name\n\n        return _('%s (%s attribute)') % (attrname, clsname)\n\n\nclass PyDecoratorMixin:\n    \"\"\"\n    Mixin for decorator directives.\n    \"\"\"\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        for cls in self.__class__.__mro__:\n            if cls.__name__ != 'DirectiveAdapter':\n                warnings.warn('PyDecoratorMixin is deprecated. '\n                              'Please check the implementation of %s' % cls,\n                              RemovedInSphinx50Warning, stacklevel=2)\n                break\n        else:\n            warnings.warn('PyDecoratorMixin is deprecated',\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        ret = super().handle_signature(sig, signode)  # type: ignore\n        signode.insert(0, addnodes.desc_addname('@', '@'))\n        return ret\n\n    def needs_arglist(self) -> bool:\n        return False\n\n\nclass PyModule(SphinxDirective):\n    \"\"\"\n    Directive to mark description of a new module.\n    \"\"\"\n\n    has_content = False\n    required_arguments = 1\n    optional_arguments = 0\n    final_argument_whitespace = False\n    option_spec = {\n        'platform': lambda x: x,\n        'synopsis': lambda x: x,\n        'noindex': directives.flag,\n        'deprecated': directives.flag,\n    }\n\n    def run(self) -> List[Node]:\n        domain = cast(PythonDomain, self.env.get_domain('py'))\n\n        modname = self.arguments[0].strip()\n        noindex = 'noindex' in self.options\n        self.env.ref_context['py:module'] = modname\n        ret = []  # type: List[Node]\n        if not noindex:\n            # note module to the domain\n            node_id = make_id(self.env, self.state.document, 'module', modname)\n            target = nodes.target('', '', ids=[node_id], ismod=True)\n            self.set_source_info(target)\n\n            # Assign old styled node_id not to break old hyperlinks (if possible)\n            # Note: Will removed in Sphinx-5.0  (RemovedInSphinx50Warning)\n            old_node_id = self.make_old_id(modname)\n            if node_id != old_node_id and old_node_id not in self.state.document.ids:\n                target['ids'].append(old_node_id)\n\n            self.state.document.note_explicit_target(target)\n\n            domain.note_module(modname,\n                               node_id,\n                               self.options.get('synopsis', ''),\n                               self.options.get('platform', ''),\n                               'deprecated' in self.options)\n            domain.note_object(modname, 'module', node_id, location=target)\n\n            # the platform and synopsis aren't printed; in fact, they are only\n            # used in the modindex currently\n            ret.append(target)\n            indextext = '%s; %s' % (pairindextypes['module'], modname)\n            inode = addnodes.index(entries=[('pair', indextext, node_id, '', None)])\n            ret.append(inode)\n        return ret\n\n    def make_old_id(self, name: str) -> str:\n        \"\"\"Generate old styled node_id.\n\n        Old styled node_id is incompatible with docutils' node_id.\n        It can contain dots and hyphens.\n\n        .. note:: Old styled node_id was mainly used until Sphinx-3.0.\n        \"\"\"\n        return 'module-%s' % name\n\n\nclass PyCurrentModule(SphinxDirective):\n    \"\"\"\n    This directive is just to tell Sphinx that we're documenting\n    stuff in module foo, but links to module foo won't lead here.\n    \"\"\"\n\n    has_content = False\n    required_arguments = 1\n    optional_arguments = 0\n    final_argument_whitespace = False\n    option_spec = {}  # type: Dict\n\n    def run(self) -> List[Node]:\n        modname = self.arguments[0].strip()\n        if modname == 'None':\n            self.env.ref_context.pop('py:module', None)\n        else:\n            self.env.ref_context['py:module'] = modname\n        return []\n\n\nclass PyXRefRole(XRefRole):\n    def process_link(self, env: BuildEnvironment, refnode: Element,\n                     has_explicit_title: bool, title: str, target: str) -> Tuple[str, str]:\n        refnode['py:module'] = env.ref_context.get('py:module')\n        refnode['py:class'] = env.ref_context.get('py:class')\n        if not has_explicit_title:\n            title = title.lstrip('.')    # only has a meaning for the target\n            target = target.lstrip('~')  # only has a meaning for the title\n            # if the first character is a tilde, don't display the module/class\n            # parts of the contents\n            if title[0:1] == '~':\n                title = title[1:]\n                dot = title.rfind('.')\n                if dot != -1:\n                    title = title[dot + 1:]\n        # if the first character is a dot, search more specific namespaces first\n        # else search builtins first\n        if target[0:1] == '.':\n            target = target[1:]\n            refnode['refspecific'] = True\n        return title, target\n\n\ndef filter_meta_fields(app: Sphinx, domain: str, objtype: str, content: Element) -> None:\n    \"\"\"Filter ``:meta:`` field from its docstring.\"\"\"\n    if domain != 'py':\n        return\n\n    for node in content:\n        if isinstance(node, nodes.field_list):\n            fields = cast(List[nodes.field], node)\n            for field in fields:\n                field_name = cast(nodes.field_body, field[0]).astext().strip()\n                if field_name == 'meta' or field_name.startswith('meta '):\n                    node.remove(field)\n                    break\n\n\nclass PythonModuleIndex(Index):\n    \"\"\"\n    Index subclass to provide the Python module index.\n    \"\"\"\n\n    name = 'modindex'\n    localname = _('Python Module Index')\n    shortname = _('modules')\n\n    def generate(self, docnames: Iterable[str] = None\n                 ) -> Tuple[List[Tuple[str, List[IndexEntry]]], bool]:\n        content = {}  # type: Dict[str, List[IndexEntry]]\n        # list of prefixes to ignore\n        ignores = None  # type: List[str]\n        ignores = self.domain.env.config['modindex_common_prefix']  # type: ignore\n        ignores = sorted(ignores, key=len, reverse=True)\n        # list of all modules, sorted by module name\n        modules = sorted(self.domain.data['modules'].items(),\n                         key=lambda x: x[0].lower())\n        # sort out collapsable modules\n        prev_modname = ''\n        num_toplevels = 0\n        for modname, (docname, node_id, synopsis, platforms, deprecated) in modules:\n            if docnames and docname not in docnames:\n                continue\n\n            for ignore in ignores:\n                if modname.startswith(ignore):\n                    modname = modname[len(ignore):]\n                    stripped = ignore\n                    break\n            else:\n                stripped = ''\n\n            # we stripped the whole module name?\n            if not modname:\n                modname, stripped = stripped, ''\n\n            entries = content.setdefault(modname[0].lower(), [])\n\n            package = modname.split('.')[0]\n            if package != modname:\n                # it's a submodule\n                if prev_modname == package:\n                    # first submodule - make parent a group head\n                    if entries:\n                        last = entries[-1]\n                        entries[-1] = IndexEntry(last[0], 1, last[2], last[3],\n                                                 last[4], last[5], last[6])\n                elif not prev_modname.startswith(package):\n                    # submodule without parent in list, add dummy entry\n                    entries.append(IndexEntry(stripped + package, 1, '', '', '', '', ''))\n                subtype = 2\n            else:\n                num_toplevels += 1\n                subtype = 0\n\n            qualifier = _('Deprecated') if deprecated else ''\n            entries.append(IndexEntry(stripped + modname, subtype, docname,\n                                      node_id, platforms, qualifier, synopsis))\n            prev_modname = modname\n\n        # apply heuristics when to collapse modindex at page load:\n        # only collapse if number of toplevel modules is larger than\n        # number of submodules\n        collapse = len(modules) - num_toplevels < num_toplevels\n\n        # sort by first letter\n        sorted_content = sorted(content.items())\n\n        return sorted_content, collapse\n\n\nclass PythonDomain(Domain):\n    \"\"\"Python language domain.\"\"\"\n    name = 'py'\n    label = 'Python'\n    object_types = {\n        'function':     ObjType(_('function'),      'func', 'obj'),\n        'data':         ObjType(_('data'),          'data', 'obj'),\n        'class':        ObjType(_('class'),         'class', 'exc', 'obj'),\n        'exception':    ObjType(_('exception'),     'exc', 'class', 'obj'),\n        'method':       ObjType(_('method'),        'meth', 'obj'),\n        'classmethod':  ObjType(_('class method'),  'meth', 'obj'),\n        'staticmethod': ObjType(_('static method'), 'meth', 'obj'),\n        'attribute':    ObjType(_('attribute'),     'attr', 'obj'),\n        'module':       ObjType(_('module'),        'mod', 'obj'),\n    }  # type: Dict[str, ObjType]\n\n    directives = {\n        'function':        PyFunction,\n        'data':            PyVariable,\n        'class':           PyClasslike,\n        'exception':       PyClasslike,\n        'method':          PyMethod,\n        'classmethod':     PyClassMethod,\n        'staticmethod':    PyStaticMethod,\n        'attribute':       PyAttribute,\n        'module':          PyModule,\n        'currentmodule':   PyCurrentModule,\n        'decorator':       PyDecoratorFunction,\n        'decoratormethod': PyDecoratorMethod,\n    }\n    roles = {\n        'data':  PyXRefRole(),\n        'exc':   PyXRefRole(),\n        'func':  PyXRefRole(fix_parens=True),\n        'class': PyXRefRole(),\n        'const': PyXRefRole(),\n        'attr':  PyXRefRole(),\n        'meth':  PyXRefRole(fix_parens=True),\n        'mod':   PyXRefRole(),\n        'obj':   PyXRefRole(),\n    }\n    initial_data = {\n        'objects': {},  # fullname -> docname, objtype\n        'modules': {},  # modname -> docname, synopsis, platform, deprecated\n    }  # type: Dict[str, Dict[str, Tuple[Any]]]\n    indices = [\n        PythonModuleIndex,\n    ]\n\n    @property\n    def objects(self) -> Dict[str, ObjectEntry]:\n        return self.data.setdefault('objects', {})  # fullname -> ObjectEntry\n\n    def note_object(self, name: str, objtype: str, node_id: str, location: Any = None) -> None:\n        \"\"\"Note a python object for cross reference.\n\n        .. versionadded:: 2.1\n        \"\"\"\n        if name in self.objects:\n            other = self.objects[name]\n            logger.warning(__('duplicate object description of %s, '\n                              'other instance in %s, use :noindex: for one of them'),\n                           name, other.docname, location=location)\n        self.objects[name] = ObjectEntry(self.env.docname, node_id, objtype)\n\n    @property\n    def modules(self) -> Dict[str, ModuleEntry]:\n        return self.data.setdefault('modules', {})  # modname -> ModuleEntry\n\n    def note_module(self, name: str, node_id: str, synopsis: str,\n                    platform: str, deprecated: bool) -> None:\n        \"\"\"Note a python module for cross reference.\n\n        .. versionadded:: 2.1\n        \"\"\"\n        self.modules[name] = ModuleEntry(self.env.docname, node_id,\n                                         synopsis, platform, deprecated)\n\n    def clear_doc(self, docname: str) -> None:\n        for fullname, obj in list(self.objects.items()):\n            if obj.docname == docname:\n                del self.objects[fullname]\n        for modname, mod in list(self.modules.items()):\n            if mod.docname == docname:\n                del self.modules[modname]\n\n    def merge_domaindata(self, docnames: List[str], otherdata: Dict) -> None:\n        # XXX check duplicates?\n        for fullname, obj in otherdata['objects'].items():\n            if obj.docname in docnames:\n                self.objects[fullname] = obj\n        for modname, mod in otherdata['modules'].items():\n            if mod.docname in docnames:\n                self.modules[modname] = mod\n\n    def find_obj(self, env: BuildEnvironment, modname: str, classname: str,\n                 name: str, type: str, searchmode: int = 0\n                 ) -> List[Tuple[str, ObjectEntry]]:\n        \"\"\"Find a Python object for \"name\", perhaps using the given module\n        and/or classname.  Returns a list of (name, object entry) tuples.\n        \"\"\"\n        # skip parens\n        if name[-2:] == '()':\n            name = name[:-2]\n\n        if not name:\n            return []\n\n        matches = []  # type: List[Tuple[str, ObjectEntry]]\n\n        newname = None\n        if searchmode == 1:\n            if type is None:\n                objtypes = list(self.object_types)\n            else:\n                objtypes = self.objtypes_for_role(type)\n            if objtypes is not None:\n                if modname and classname:\n                    fullname = modname + '.' + classname + '.' + name\n                    if fullname in self.objects and self.objects[fullname].objtype in objtypes:\n                        newname = fullname\n                if not newname:\n                    if modname and modname + '.' + name in self.objects and \\\n                       self.objects[modname + '.' + name].objtype in objtypes:\n                        newname = modname + '.' + name\n                    elif name in self.objects and self.objects[name].objtype in objtypes:\n                        newname = name\n                    else:\n                        # \"fuzzy\" searching mode\n                        searchname = '.' + name\n                        matches = [(oname, self.objects[oname]) for oname in self.objects\n                                   if oname.endswith(searchname) and\n                                   self.objects[oname].objtype in objtypes]\n        else:\n            # NOTE: searching for exact match, object type is not considered\n            if name in self.objects:\n                newname = name\n            elif type == 'mod':\n                # only exact matches allowed for modules\n                return []\n            elif classname and classname + '.' + name in self.objects:\n                newname = classname + '.' + name\n            elif modname and modname + '.' + name in self.objects:\n                newname = modname + '.' + name\n            elif modname and classname and \\\n                    modname + '.' + classname + '.' + name in self.objects:\n                newname = modname + '.' + classname + '.' + name\n        if newname is not None:\n            matches.append((newname, self.objects[newname]))\n        return matches\n\n    def resolve_xref(self, env: BuildEnvironment, fromdocname: str, builder: Builder,\n                     type: str, target: str, node: pending_xref, contnode: Element\n                     ) -> Element:\n        modname = node.get('py:module')\n        clsname = node.get('py:class')\n        searchmode = 1 if node.hasattr('refspecific') else 0\n        matches = self.find_obj(env, modname, clsname, target,\n                                type, searchmode)\n\n        if not matches and type == 'attr':\n            # fallback to meth (for property)\n            matches = self.find_obj(env, modname, clsname, target, 'meth', searchmode)\n\n        if not matches:\n            return None\n        elif len(matches) > 1:\n            logger.warning(__('more than one target found for cross-reference %r: %s'),\n                           target, ', '.join(match[0] for match in matches),\n                           type='ref', subtype='python', location=node)\n        name, obj = matches[0]\n\n        if obj[2] == 'module':\n            return self._make_module_refnode(builder, fromdocname, name, contnode)\n        else:\n            return make_refnode(builder, fromdocname, obj[0], obj[1], contnode, name)\n\n    def resolve_any_xref(self, env: BuildEnvironment, fromdocname: str, builder: Builder,\n                         target: str, node: pending_xref, contnode: Element\n                         ) -> List[Tuple[str, Element]]:\n        modname = node.get('py:module')\n        clsname = node.get('py:class')\n        results = []  # type: List[Tuple[str, Element]]\n\n        # always search in \"refspecific\" mode with the :any: role\n        matches = self.find_obj(env, modname, clsname, target, None, 1)\n        for name, obj in matches:\n            if obj[2] == 'module':\n                results.append(('py:mod',\n                                self._make_module_refnode(builder, fromdocname,\n                                                          name, contnode)))\n            else:\n                results.append(('py:' + self.role_for_objtype(obj[2]),\n                                make_refnode(builder, fromdocname, obj[0], obj[1],\n                                             contnode, name)))\n        return results\n\n    def _make_module_refnode(self, builder: Builder, fromdocname: str, name: str,\n                             contnode: Node) -> Element:\n        # get additional info for modules\n        module = self.modules[name]\n        title = name\n        if module.synopsis:\n            title += ': ' + module.synopsis\n        if module.deprecated:\n            title += _(' (deprecated)')\n        if module.platform:\n            title += ' (' + module.platform + ')'\n        return make_refnode(builder, fromdocname, module.docname, module.node_id,\n                            contnode, title)\n\n    def get_objects(self) -> Iterator[Tuple[str, str, str, str, str, int]]:\n        for modname, mod in self.modules.items():\n            yield (modname, modname, 'module', mod.docname, mod.node_id, 0)\n        for refname, obj in self.objects.items():\n            if obj.objtype != 'module':  # modules are already handled\n                yield (refname, refname, obj.objtype, obj.docname, obj.node_id, 1)\n\n    def get_full_qualified_name(self, node: Element) -> str:\n        modname = node.get('py:module')\n        clsname = node.get('py:class')\n        target = node.get('reftarget')\n        if target is None:\n            return None\n        else:\n            return '.'.join(filter(None, [modname, clsname, target]))\n\n\ndef builtin_resolver(app: Sphinx, env: BuildEnvironment,\n                     node: pending_xref, contnode: Element) -> Element:\n    \"\"\"Do not emit nitpicky warnings for built-in types.\"\"\"\n    def istyping(s: str) -> bool:\n        if s.startswith('typing.'):\n            s = s.split('.', 1)[1]\n\n        return s in typing.__all__  # type: ignore\n\n    if node.get('refdomain') != 'py':\n        return None\n    elif node.get('reftype') in ('class', 'obj') and node.get('reftarget') == 'None':\n        return contnode\n    elif node.get('reftype') in ('class', 'exc'):\n        reftarget = node.get('reftarget')\n        if inspect.isclass(getattr(builtins, reftarget, None)):\n            # built-in class\n            return contnode\n        elif istyping(reftarget):\n            # typing class\n            return contnode\n\n    return None\n\n\ndef setup(app: Sphinx) -> Dict[str, Any]:\n    app.setup_extension('sphinx.directives')\n\n    app.add_domain(PythonDomain)\n    app.connect('object-description-transform', filter_meta_fields)\n    app.connect('missing-reference', builtin_resolver, priority=900)\n\n    return {\n        'version': 'builtin',\n        'env_version': 2,\n        'parallel_read_safe': True,\n        'parallel_write_safe': True,\n    }"}
{"model_name_or_path": "agentless", "instance_id": "sphinx-doc__sphinx-8035", "model_patch": "diff --git a/sphinx/ext/autodoc/__init__.py b/sphinx/ext/autodoc/__init__.py\nindex d3cef15..7aa90b7 100644\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -859,7 +859,8 @@ class ModuleDocumenter(Documenter):\n         'show-inheritance': bool_option, 'synopsis': identity,\n         'platform': identity, 'deprecated': bool_option,\n         'member-order': member_order_option, 'exclude-members': members_set_option,\n-        'private-members': bool_option, 'special-members': members_option,\n+        'private-members': members_option,  # Change here to accept specific private members\n+        'special-members': members_option,\n         'imported-members': bool_option, 'ignore-module-all': bool_option\n     }  # type: Dict[str, Callable]\n \n", "raw_model_patch": "diff --git a/sphinx/ext/autodoc/__init__.py b/sphinx/ext/autodoc/__init__.py\nindex d3cef15..7aa90b7 100644\n--- a/sphinx/ext/autodoc/__init__.py\n+++ b/sphinx/ext/autodoc/__init__.py\n@@ -859,7 +859,8 @@ class ModuleDocumenter(Documenter):\n         'show-inheritance': bool_option, 'synopsis': identity,\n         'platform': identity, 'deprecated': bool_option,\n         'member-order': member_order_option, 'exclude-members': members_set_option,\n-        'private-members': bool_option, 'special-members': members_option,\n+        'private-members': members_option,  # Change here to accept specific private members\n+        'special-members': members_option,\n         'imported-members': bool_option, 'ignore-module-all': bool_option\n     }  # type: Dict[str, Callable]\n \n", "original_file_content": "\"\"\"\n    sphinx.ext.autodoc\n    ~~~~~~~~~~~~~~~~~~\n\n    Automatically insert docstrings for functions, classes or whole modules into\n    the doctree, thus avoiding duplication between docstrings and documentation\n    for those who like elaborate docstrings.\n\n    :copyright: Copyright 2007-2020 by the Sphinx team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport importlib\nimport re\nimport warnings\nfrom inspect import Parameter, Signature\nfrom types import ModuleType\nfrom typing import (\n    Any, Callable, Dict, Iterator, List, Optional, Sequence, Set, Tuple, Type, TypeVar, Union\n)\n\nfrom docutils.statemachine import StringList\n\nimport sphinx\nfrom sphinx.application import Sphinx\nfrom sphinx.config import Config, ENUM\nfrom sphinx.deprecation import RemovedInSphinx40Warning, RemovedInSphinx50Warning\nfrom sphinx.environment import BuildEnvironment\nfrom sphinx.ext.autodoc.importer import import_object, get_module_members, get_object_members\nfrom sphinx.ext.autodoc.mock import mock\nfrom sphinx.locale import _, __\nfrom sphinx.pycode import ModuleAnalyzer, PycodeError\nfrom sphinx.util import inspect\nfrom sphinx.util import logging\nfrom sphinx.util.docstrings import extract_metadata, prepare_docstring\nfrom sphinx.util.inspect import (\n    evaluate_signature, getdoc, object_description, safe_getattr, stringify_signature\n)\nfrom sphinx.util.typing import stringify as stringify_typehint\n\nif False:\n    # For type annotation\n    from typing import Type  # NOQA # for python3.5.1\n    from sphinx.ext.autodoc.directive import DocumenterBridge\n\n\nlogger = logging.getLogger(__name__)\n\n\n# This type isn't exposed directly in any modules, but can be found\n# here in most Python versions\nMethodDescriptorType = type(type.__subclasses__)\n\n\n#: extended signature RE: with explicit module name separated by ::\npy_ext_sig_re = re.compile(\n    r'''^ ([\\w.]+::)?            # explicit module name\n          ([\\w.]+\\.)?            # module and/or class name(s)\n          (\\w+)  \\s*             # thing name\n          (?: \\((.*)\\)           # optional: arguments\n           (?:\\s* -> \\s* (.*))?  #           return annotation\n          )? $                   # and nothing more\n          ''', re.VERBOSE)\n\n\ndef identity(x: Any) -> Any:\n    return x\n\n\nALL = object()\nUNINITIALIZED_ATTR = object()\nINSTANCEATTR = object()\nSLOTSATTR = object()\n\n\ndef members_option(arg: Any) -> Union[object, List[str]]:\n    \"\"\"Used to convert the :members: option to auto directives.\"\"\"\n    if arg is None or arg is True:\n        return ALL\n    return [x.strip() for x in arg.split(',') if x.strip()]\n\n\ndef members_set_option(arg: Any) -> Union[object, Set[str]]:\n    \"\"\"Used to convert the :members: option to auto directives.\"\"\"\n    if arg is None:\n        return ALL\n    return {x.strip() for x in arg.split(',') if x.strip()}\n\n\ndef inherited_members_option(arg: Any) -> Union[object, Set[str]]:\n    \"\"\"Used to convert the :members: option to auto directives.\"\"\"\n    if arg is None:\n        return 'object'\n    else:\n        return arg\n\n\ndef member_order_option(arg: Any) -> Optional[str]:\n    \"\"\"Used to convert the :members: option to auto directives.\"\"\"\n    if arg is None:\n        return None\n    elif arg in ('alphabetical', 'bysource', 'groupwise'):\n        return arg\n    else:\n        raise ValueError(__('invalid value for member-order option: %s') % arg)\n\n\nSUPPRESS = object()\n\n\ndef annotation_option(arg: Any) -> Any:\n    if arg is None:\n        # suppress showing the representation of the object\n        return SUPPRESS\n    else:\n        return arg\n\n\ndef bool_option(arg: Any) -> bool:\n    \"\"\"Used to convert flag options to auto directives.  (Instead of\n    directives.flag(), which returns None).\n    \"\"\"\n    return True\n\n\ndef merge_special_members_option(options: Dict) -> None:\n    \"\"\"Merge :special-members: option to :members: option.\"\"\"\n    if 'special-members' in options and options['special-members'] is not ALL:\n        if options.get('members') is ALL:\n            pass\n        elif options.get('members'):\n            for member in options['special-members']:\n                if member not in options['members']:\n                    options['members'].append(member)\n        else:\n            options['members'] = options['special-members']\n\n\n# Some useful event listener factories for autodoc-process-docstring.\n\ndef cut_lines(pre: int, post: int = 0, what: str = None) -> Callable:\n    \"\"\"Return a listener that removes the first *pre* and last *post*\n    lines of every docstring.  If *what* is a sequence of strings,\n    only docstrings of a type in *what* will be processed.\n\n    Use like this (e.g. in the ``setup()`` function of :file:`conf.py`)::\n\n       from sphinx.ext.autodoc import cut_lines\n       app.connect('autodoc-process-docstring', cut_lines(4, what=['module']))\n\n    This can (and should) be used in place of :confval:`automodule_skip_lines`.\n    \"\"\"\n    def process(app: Sphinx, what_: str, name: str, obj: Any, options: Any, lines: List[str]\n                ) -> None:\n        if what and what_ not in what:\n            return\n        del lines[:pre]\n        if post:\n            # remove one trailing blank line.\n            if lines and not lines[-1]:\n                lines.pop(-1)\n            del lines[-post:]\n        # make sure there is a blank line at the end\n        if lines and lines[-1]:\n            lines.append('')\n    return process\n\n\ndef between(marker: str, what: Sequence[str] = None, keepempty: bool = False,\n            exclude: bool = False) -> Callable:\n    \"\"\"Return a listener that either keeps, or if *exclude* is True excludes,\n    lines between lines that match the *marker* regular expression.  If no line\n    matches, the resulting docstring would be empty, so no change will be made\n    unless *keepempty* is true.\n\n    If *what* is a sequence of strings, only docstrings of a type in *what* will\n    be processed.\n    \"\"\"\n    marker_re = re.compile(marker)\n\n    def process(app: Sphinx, what_: str, name: str, obj: Any, options: Any, lines: List[str]\n                ) -> None:\n        if what and what_ not in what:\n            return\n        deleted = 0\n        delete = not exclude\n        orig_lines = lines[:]\n        for i, line in enumerate(orig_lines):\n            if delete:\n                lines.pop(i - deleted)\n                deleted += 1\n            if marker_re.match(line):\n                delete = not delete\n                if delete:\n                    lines.pop(i - deleted)\n                    deleted += 1\n        if not lines and not keepempty:\n            lines[:] = orig_lines\n        # make sure there is a blank line at the end\n        if lines and lines[-1]:\n            lines.append('')\n    return process\n\n\n# This class is used only in ``sphinx.ext.autodoc.directive``,\n# But we define this class here to keep compatibility (see #4538)\nclass Options(dict):\n    \"\"\"A dict/attribute hybrid that returns None on nonexisting keys.\"\"\"\n    def __getattr__(self, name: str) -> Any:\n        try:\n            return self[name.replace('_', '-')]\n        except KeyError:\n            return None\n\n\nclass Documenter:\n    \"\"\"\n    A Documenter knows how to autodocument a single object type.  When\n    registered with the AutoDirective, it will be used to document objects\n    of that type when needed by autodoc.\n\n    Its *objtype* attribute selects what auto directive it is assigned to\n    (the directive name is 'auto' + objtype), and what directive it generates\n    by default, though that can be overridden by an attribute called\n    *directivetype*.\n\n    A Documenter has an *option_spec* that works like a docutils directive's;\n    in fact, it will be used to parse an auto directive's options that matches\n    the documenter.\n    \"\"\"\n    #: name by which the directive is called (auto...) and the default\n    #: generated directive name\n    objtype = 'object'\n    #: indentation by which to indent the directive content\n    content_indent = '   '\n    #: priority if multiple documenters return True from can_document_member\n    priority = 0\n    #: order if autodoc_member_order is set to 'groupwise'\n    member_order = 0\n    #: true if the generated content may contain titles\n    titles_allowed = False\n\n    option_spec = {'noindex': bool_option}  # type: Dict[str, Callable]\n\n    def get_attr(self, obj: Any, name: str, *defargs: Any) -> Any:\n        \"\"\"getattr() override for types such as Zope interfaces.\"\"\"\n        return autodoc_attrgetter(self.env.app, obj, name, *defargs)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"Called to see if a member can be documented by this documenter.\"\"\"\n        raise NotImplementedError('must be implemented in subclasses')\n\n    def __init__(self, directive: \"DocumenterBridge\", name: str, indent: str = '') -> None:\n        self.directive = directive\n        self.env = directive.env    # type: BuildEnvironment\n        self.options = directive.genopt\n        self.name = name\n        self.indent = indent\n        # the module and object path within the module, and the fully\n        # qualified name (all set after resolve_name succeeds)\n        self.modname = None         # type: str\n        self.module = None          # type: ModuleType\n        self.objpath = None         # type: List[str]\n        self.fullname = None        # type: str\n        # extra signature items (arguments and return annotation,\n        # also set after resolve_name succeeds)\n        self.args = None            # type: str\n        self.retann = None          # type: str\n        # the object to document (set after import_object succeeds)\n        self.object = None          # type: Any\n        self.object_name = None     # type: str\n        # the parent/owner of the object to document\n        self.parent = None          # type: Any\n        # the module analyzer to get at attribute docs, or None\n        self.analyzer = None        # type: ModuleAnalyzer\n\n    @property\n    def documenters(self) -> Dict[str, \"Type[Documenter]\"]:\n        \"\"\"Returns registered Documenter classes\"\"\"\n        return self.env.app.registry.documenters\n\n    def add_line(self, line: str, source: str, *lineno: int) -> None:\n        \"\"\"Append one line of generated reST to the output.\"\"\"\n        if line.strip():  # not a blank line\n            self.directive.result.append(self.indent + line, source, *lineno)\n        else:\n            self.directive.result.append('', source, *lineno)\n\n    def resolve_name(self, modname: str, parents: Any, path: str, base: Any\n                     ) -> Tuple[str, List[str]]:\n        \"\"\"Resolve the module and name of the object to document given by the\n        arguments and the current module/class.\n\n        Must return a pair of the module name and a chain of attributes; for\n        example, it would return ``('zipfile', ['ZipFile', 'open'])`` for the\n        ``zipfile.ZipFile.open`` method.\n        \"\"\"\n        raise NotImplementedError('must be implemented in subclasses')\n\n    def parse_name(self) -> bool:\n        \"\"\"Determine what module to import and what attribute to document.\n\n        Returns True and sets *self.modname*, *self.objpath*, *self.fullname*,\n        *self.args* and *self.retann* if parsing and resolving was successful.\n        \"\"\"\n        # first, parse the definition -- auto directives for classes and\n        # functions can contain a signature which is then used instead of\n        # an autogenerated one\n        try:\n            explicit_modname, path, base, args, retann = \\\n                py_ext_sig_re.match(self.name).groups()\n        except AttributeError:\n            logger.warning(__('invalid signature for auto%s (%r)') % (self.objtype, self.name),\n                           type='autodoc')\n            return False\n\n        # support explicit module and class name separation via ::\n        if explicit_modname is not None:\n            modname = explicit_modname[:-2]\n            parents = path.rstrip('.').split('.') if path else []\n        else:\n            modname = None\n            parents = []\n\n        with mock(self.env.config.autodoc_mock_imports):\n            self.modname, self.objpath = self.resolve_name(modname, parents, path, base)\n\n        if not self.modname:\n            return False\n\n        self.args = args\n        self.retann = retann\n        self.fullname = (self.modname or '') + \\\n                        ('.' + '.'.join(self.objpath) if self.objpath else '')\n        return True\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Import the object given by *self.modname* and *self.objpath* and set\n        it as *self.object*.\n\n        Returns True if successful, False if an error occurred.\n        \"\"\"\n        with mock(self.env.config.autodoc_mock_imports):\n            try:\n                ret = import_object(self.modname, self.objpath, self.objtype,\n                                    attrgetter=self.get_attr,\n                                    warningiserror=self.env.config.autodoc_warningiserror)\n                self.module, self.parent, self.object_name, self.object = ret\n                return True\n            except ImportError as exc:\n                if raiseerror:\n                    raise\n                else:\n                    logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                    self.env.note_reread()\n                    return False\n\n    def get_real_modname(self) -> str:\n        \"\"\"Get the real module name of an object to document.\n\n        It can differ from the name of the module through which the object was\n        imported.\n        \"\"\"\n        return self.get_attr(self.object, '__module__', None) or self.modname\n\n    def check_module(self) -> bool:\n        \"\"\"Check if *self.object* is really defined in the module given by\n        *self.modname*.\n        \"\"\"\n        if self.options.imported_members:\n            return True\n\n        subject = inspect.unpartial(self.object)\n        modname = self.get_attr(subject, '__module__', None)\n        if modname and modname != self.modname:\n            return False\n        return True\n\n    def format_args(self, **kwargs: Any) -> str:\n        \"\"\"Format the argument signature of *self.object*.\n\n        Should return None if the object does not have a signature.\n        \"\"\"\n        return None\n\n    def format_name(self) -> str:\n        \"\"\"Format the name of *self.object*.\n\n        This normally should be something that can be parsed by the generated\n        directive, but doesn't need to be (Sphinx will display it unparsed\n        then).\n        \"\"\"\n        # normally the name doesn't contain the module (except for module\n        # directives of course)\n        return '.'.join(self.objpath) or self.modname\n\n    def _call_format_args(self, **kwargs: Any) -> str:\n        if kwargs:\n            try:\n                return self.format_args(**kwargs)\n            except TypeError:\n                # avoid chaining exceptions, by putting nothing here\n                pass\n\n        # retry without arguments for old documenters\n        return self.format_args()\n\n    def format_signature(self, **kwargs: Any) -> str:\n        \"\"\"Format the signature (arguments and return annotation) of the object.\n\n        Let the user process it via the ``autodoc-process-signature`` event.\n        \"\"\"\n        if self.args is not None:\n            # signature given explicitly\n            args = \"(%s)\" % self.args\n            retann = self.retann\n        else:\n            # try to introspect the signature\n            try:\n                retann = None\n                args = self._call_format_args(**kwargs)\n                if args:\n                    matched = re.match(r'^(\\(.*\\))\\s+->\\s+(.*)$', args)\n                    if matched:\n                        args = matched.group(1)\n                        retann = matched.group(2)\n            except Exception as exc:\n                logger.warning(__('error while formatting arguments for %s: %s'),\n                               self.fullname, exc, type='autodoc')\n                args = None\n\n        result = self.env.events.emit_firstresult('autodoc-process-signature',\n                                                  self.objtype, self.fullname,\n                                                  self.object, self.options, args, retann)\n        if result:\n            args, retann = result\n\n        if args is not None:\n            return args + ((' -> %s' % retann) if retann else '')\n        else:\n            return ''\n\n    def add_directive_header(self, sig: str) -> None:\n        \"\"\"Add the directive header and options to the generated content.\"\"\"\n        domain = getattr(self, 'domain', 'py')\n        directive = getattr(self, 'directivetype', self.objtype)\n        name = self.format_name()\n        sourcename = self.get_sourcename()\n\n        # one signature per line, indented by column\n        prefix = '.. %s:%s:: ' % (domain, directive)\n        for i, sig_line in enumerate(sig.split(\"\\n\")):\n            self.add_line('%s%s%s' % (prefix, name, sig_line),\n                          sourcename)\n            if i == 0:\n                prefix = \" \" * len(prefix)\n\n        if self.options.noindex:\n            self.add_line('   :noindex:', sourcename)\n        if self.objpath:\n            # Be explicit about the module, this is necessary since .. class::\n            # etc. don't support a prepended module name\n            self.add_line('   :module: %s' % self.modname, sourcename)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        \"\"\"Decode and return lines of the docstring(s) for the object.\"\"\"\n        if encoding is not None:\n            warnings.warn(\"The 'encoding' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx40Warning, stacklevel=2)\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n        docstring = getdoc(self.object, self.get_attr,\n                           self.env.config.autodoc_inherit_docstrings,\n                           self.parent, self.object_name)\n        if docstring:\n            tab_width = self.directive.state.document.settings.tab_width\n            return [prepare_docstring(docstring, ignore, tab_width)]\n        return []\n\n    def process_doc(self, docstrings: List[List[str]]) -> Iterator[str]:\n        \"\"\"Let the user process the docstrings before adding them.\"\"\"\n        for docstringlines in docstrings:\n            if self.env.app:\n                # let extensions preprocess docstrings\n                self.env.app.emit('autodoc-process-docstring',\n                                  self.objtype, self.fullname, self.object,\n                                  self.options, docstringlines)\n            yield from docstringlines\n\n    def get_sourcename(self) -> str:\n        if self.analyzer:\n            return '%s:docstring of %s' % (self.analyzer.srcname, self.fullname)\n        return 'docstring of %s' % self.fullname\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Add content from docstrings, attribute documentation and user.\"\"\"\n        # set sourcename and add content from attribute documentation\n        sourcename = self.get_sourcename()\n        if self.analyzer:\n            attr_docs = self.analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    no_docstring = True\n                    # make a copy of docstring for attributes to avoid cache\n                    # the change of autodoc-process-docstring event.\n                    docstrings = [list(attr_docs[key])]\n\n                    for i, line in enumerate(self.process_doc(docstrings)):\n                        self.add_line(line, sourcename, i)\n\n        # add content from docstrings\n        if not no_docstring:\n            docstrings = self.get_doc()\n            if not docstrings:\n                # append at least a dummy docstring, so that the event\n                # autodoc-process-docstring is fired and can add some\n                # content if desired\n                docstrings.append([])\n            for i, line in enumerate(self.process_doc(docstrings)):\n                self.add_line(line, sourcename, i)\n\n        # add additional content (e.g. from document), if present\n        if more_content:\n            for line, src in zip(more_content.data, more_content.items):\n                self.add_line(line, src[0], src[1])\n\n    def get_object_members(self, want_all: bool) -> Tuple[bool, List[Tuple[str, Any]]]:\n        \"\"\"Return `(members_check_module, members)` where `members` is a\n        list of `(membername, member)` pairs of the members of *self.object*.\n\n        If *want_all* is True, return all members.  Else, only return those\n        members given by *self.options.members* (which may also be none).\n        \"\"\"\n        members = get_object_members(self.object, self.objpath, self.get_attr, self.analyzer)\n        if not want_all:\n            if not self.options.members:\n                return False, []\n            # specific members given\n            selected = []\n            for name in self.options.members:\n                if name in members:\n                    selected.append((name, members[name].value))\n                else:\n                    logger.warning(__('missing attribute %s in object %s') %\n                                   (name, self.fullname), type='autodoc')\n            return False, selected\n        elif self.options.inherited_members:\n            return False, [(m.name, m.value) for m in members.values()]\n        else:\n            return False, [(m.name, m.value) for m in members.values()\n                           if m.directly_defined]\n\n    def filter_members(self, members: List[Tuple[str, Any]], want_all: bool\n                       ) -> List[Tuple[str, Any, bool]]:\n        \"\"\"Filter the given member list.\n\n        Members are skipped if\n\n        - they are private (except if given explicitly or the private-members\n          option is set)\n        - they are special methods (except if given explicitly or the\n          special-members option is set)\n        - they are undocumented (except if the undoc-members option is set)\n\n        The user can override the skipping decision by connecting to the\n        ``autodoc-skip-member`` event.\n        \"\"\"\n        def is_filtered_inherited_member(name: str) -> bool:\n            if inspect.isclass(self.object):\n                for cls in self.object.__mro__:\n                    if cls.__name__ == self.options.inherited_members and cls != self.object:\n                        # given member is a member of specified *super class*\n                        return True\n                    elif name in cls.__dict__:\n                        return False\n\n            return False\n\n        ret = []\n\n        # search for members in source code too\n        namespace = '.'.join(self.objpath)  # will be empty for modules\n\n        if self.analyzer:\n            attr_docs = self.analyzer.find_attr_docs()\n        else:\n            attr_docs = {}\n\n        # process members and determine which to skip\n        for (membername, member) in members:\n            # if isattr is True, the member is documented as an attribute\n            if member is INSTANCEATTR:\n                isattr = True\n            else:\n                isattr = False\n\n            doc = getdoc(member, self.get_attr, self.env.config.autodoc_inherit_docstrings,\n                         self.parent, self.object_name)\n            if not isinstance(doc, str):\n                # Ignore non-string __doc__\n                doc = None\n\n            # if the member __doc__ is the same as self's __doc__, it's just\n            # inherited and therefore not the member's doc\n            cls = self.get_attr(member, '__class__', None)\n            if cls:\n                cls_doc = self.get_attr(cls, '__doc__', None)\n                if cls_doc == doc:\n                    doc = None\n            has_doc = bool(doc)\n\n            metadata = extract_metadata(doc)\n            if 'private' in metadata:\n                # consider a member private if docstring has \"private\" metadata\n                isprivate = True\n            elif 'public' in metadata:\n                # consider a member public if docstring has \"public\" metadata\n                isprivate = False\n            else:\n                isprivate = membername.startswith('_')\n\n            keep = False\n            if safe_getattr(member, '__sphinx_mock__', False):\n                # mocked module or object\n                pass\n            elif (self.options.exclude_members not in (None, ALL) and\n                  membername in self.options.exclude_members):\n                # remove members given by exclude-members\n                keep = False\n            elif want_all and membername.startswith('__') and \\\n                    membername.endswith('__') and len(membername) > 4:\n                # special __methods__\n                if self.options.special_members is ALL:\n                    if membername == '__doc__':\n                        keep = False\n                    elif is_filtered_inherited_member(membername):\n                        keep = False\n                    else:\n                        keep = has_doc or self.options.undoc_members\n                elif self.options.special_members:\n                    if membername in self.options.special_members:\n                        keep = has_doc or self.options.undoc_members\n            elif (namespace, membername) in attr_docs:\n                if want_all and isprivate:\n                    # ignore members whose name starts with _ by default\n                    keep = self.options.private_members\n                else:\n                    # keep documented attributes\n                    keep = True\n                isattr = True\n            elif want_all and isprivate:\n                # ignore members whose name starts with _ by default\n                keep = self.options.private_members and \\\n                    (has_doc or self.options.undoc_members)\n            else:\n                if self.options.members is ALL and is_filtered_inherited_member(membername):\n                    keep = False\n                else:\n                    # ignore undocumented members if :undoc-members: is not given\n                    keep = has_doc or self.options.undoc_members\n\n            # give the user a chance to decide whether this member\n            # should be skipped\n            if self.env.app:\n                # let extensions preprocess docstrings\n                try:\n                    skip_user = self.env.app.emit_firstresult(\n                        'autodoc-skip-member', self.objtype, membername, member,\n                        not keep, self.options)\n                    if skip_user is not None:\n                        keep = not skip_user\n                except Exception as exc:\n                    logger.warning(__('autodoc: failed to determine %r to be documented, '\n                                      'the following exception was raised:\\n%s'),\n                                   member, exc, type='autodoc')\n                    keep = False\n\n            if keep:\n                ret.append((membername, member, isattr))\n\n        return ret\n\n    def document_members(self, all_members: bool = False) -> None:\n        \"\"\"Generate reST for member documentation.\n\n        If *all_members* is True, do all members, else those given by\n        *self.options.members*.\n        \"\"\"\n        # set current namespace for finding members\n        self.env.temp_data['autodoc:module'] = self.modname\n        if self.objpath:\n            self.env.temp_data['autodoc:class'] = self.objpath[0]\n\n        want_all = all_members or self.options.inherited_members or \\\n            self.options.members is ALL\n        # find out which members are documentable\n        members_check_module, members = self.get_object_members(want_all)\n\n        # document non-skipped members\n        memberdocumenters = []  # type: List[Tuple[Documenter, bool]]\n        for (mname, member, isattr) in self.filter_members(members, want_all):\n            classes = [cls for cls in self.documenters.values()\n                       if cls.can_document_member(member, mname, isattr, self)]\n            if not classes:\n                # don't know how to document this member\n                continue\n            # prefer the documenter with the highest priority\n            classes.sort(key=lambda cls: cls.priority)\n            # give explicitly separated module name, so that members\n            # of inner classes can be documented\n            full_mname = self.modname + '::' + \\\n                '.'.join(self.objpath + [mname])\n            documenter = classes[-1](self.directive, full_mname, self.indent)\n            memberdocumenters.append((documenter, isattr))\n\n        member_order = self.options.member_order or self.env.config.autodoc_member_order\n        memberdocumenters = self.sort_members(memberdocumenters, member_order)\n\n        for documenter, isattr in memberdocumenters:\n            documenter.generate(\n                all_members=True, real_modname=self.real_modname,\n                check_module=members_check_module and not isattr)\n\n        # reset current objects\n        self.env.temp_data['autodoc:module'] = None\n        self.env.temp_data['autodoc:class'] = None\n\n    def sort_members(self, documenters: List[Tuple[\"Documenter\", bool]],\n                     order: str) -> List[Tuple[\"Documenter\", bool]]:\n        \"\"\"Sort the given member list.\"\"\"\n        if order == 'groupwise':\n            # sort by group; alphabetically within groups\n            documenters.sort(key=lambda e: (e[0].member_order, e[0].name))\n        elif order == 'bysource':\n            if self.analyzer:\n                # sort by source order, by virtue of the module analyzer\n                tagorder = self.analyzer.tagorder\n\n                def keyfunc(entry: Tuple[Documenter, bool]) -> int:\n                    fullname = entry[0].name.split('::')[1]\n                    return tagorder.get(fullname, len(tagorder))\n                documenters.sort(key=keyfunc)\n            else:\n                # Assume that member discovery order matches source order.\n                # This is a reasonable assumption in Python 3.6 and up, where\n                # module.__dict__ is insertion-ordered.\n                pass\n        else:  # alphabetical\n            documenters.sort(key=lambda e: e[0].name)\n\n        return documenters\n\n    def generate(self, more_content: Any = None, real_modname: str = None,\n                 check_module: bool = False, all_members: bool = False) -> None:\n        \"\"\"Generate reST for the object given by *self.name*, and possibly for\n        its members.\n\n        If *more_content* is given, include that content. If *real_modname* is\n        given, use that module name to find attribute docs. If *check_module* is\n        True, only generate if the object is defined in the module name it is\n        imported from. If *all_members* is True, document all members.\n        \"\"\"\n        if not self.parse_name():\n            # need a module to import\n            logger.warning(\n                __('don\\'t know which module to import for autodocumenting '\n                   '%r (try placing a \"module\" or \"currentmodule\" directive '\n                   'in the document, or giving an explicit module name)') %\n                self.name, type='autodoc')\n            return\n\n        # now, import the module and get object to document\n        if not self.import_object():\n            return\n\n        # If there is no real module defined, figure out which to use.\n        # The real module is used in the module analyzer to look up the module\n        # where the attribute documentation would actually be found in.\n        # This is used for situations where you have a module that collects the\n        # functions and classes of internal submodules.\n        guess_modname = self.get_real_modname()\n        self.real_modname = real_modname or guess_modname\n\n        # try to also get a source code analyzer for attribute docs\n        try:\n            self.analyzer = ModuleAnalyzer.for_module(self.real_modname)\n            # parse right now, to get PycodeErrors on parsing (results will\n            # be cached anyway)\n            self.analyzer.find_attr_docs()\n        except PycodeError as exc:\n            logger.debug('[autodoc] module analyzer failed: %s', exc)\n            # no source file -- e.g. for builtin and C modules\n            self.analyzer = None\n            # at least add the module.__file__ as a dependency\n            if hasattr(self.module, '__file__') and self.module.__file__:\n                self.directive.filename_set.add(self.module.__file__)\n        else:\n            self.directive.filename_set.add(self.analyzer.srcname)\n\n        if self.real_modname != guess_modname:\n            # Add module to dependency list if target object is defined in other module.\n            try:\n                analyzer = ModuleAnalyzer.for_module(guess_modname)\n                self.directive.filename_set.add(analyzer.srcname)\n            except PycodeError:\n                pass\n\n        # check __module__ of object (for members not given explicitly)\n        if check_module:\n            if not self.check_module():\n                return\n\n        sourcename = self.get_sourcename()\n\n        # make sure that the result starts with an empty line.  This is\n        # necessary for some situations where another directive preprocesses\n        # reST and no starting newline is present\n        self.add_line('', sourcename)\n\n        # format the object's signature, if any\n        try:\n            sig = self.format_signature()\n        except Exception as exc:\n            logger.warning(__('error while formatting signature for %s: %s'),\n                           self.fullname, exc, type='autodoc')\n            return\n\n        # generate the directive header and options, if applicable\n        self.add_directive_header(sig)\n        self.add_line('', sourcename)\n\n        # e.g. the module directive doesn't have content\n        self.indent += self.content_indent\n\n        # add all content (from docstrings, attribute docs etc.)\n        self.add_content(more_content)\n\n        # document members, if possible\n        self.document_members(all_members)\n\n\nclass ModuleDocumenter(Documenter):\n    \"\"\"\n    Specialized Documenter subclass for modules.\n    \"\"\"\n    objtype = 'module'\n    content_indent = ''\n    titles_allowed = True\n\n    option_spec = {\n        'members': members_option, 'undoc-members': bool_option,\n        'noindex': bool_option, 'inherited-members': inherited_members_option,\n        'show-inheritance': bool_option, 'synopsis': identity,\n        'platform': identity, 'deprecated': bool_option,\n        'member-order': member_order_option, 'exclude-members': members_set_option,\n        'private-members': bool_option, 'special-members': members_option,\n        'imported-members': bool_option, 'ignore-module-all': bool_option\n    }  # type: Dict[str, Callable]\n\n    def __init__(self, *args: Any) -> None:\n        super().__init__(*args)\n        merge_special_members_option(self.options)\n        self.__all__ = None\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        # don't document submodules automatically\n        return False\n\n    def resolve_name(self, modname: str, parents: Any, path: str, base: Any\n                     ) -> Tuple[str, List[str]]:\n        if modname is not None:\n            logger.warning(__('\"::\" in automodule name doesn\\'t make sense'),\n                           type='autodoc')\n        return (path or '') + base, []\n\n    def parse_name(self) -> bool:\n        ret = super().parse_name()\n        if self.args or self.retann:\n            logger.warning(__('signature arguments or return annotation '\n                              'given for automodule %s') % self.fullname,\n                           type='autodoc')\n        return ret\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        def is_valid_module_all(__all__: Any) -> bool:\n            \"\"\"Check the given *__all__* is valid for a module.\"\"\"\n            if (isinstance(__all__, (list, tuple)) and\n                    all(isinstance(e, str) for e in __all__)):\n                return True\n            else:\n                return False\n\n        ret = super().import_object(raiseerror)\n\n        if not self.options.ignore_module_all:\n            __all__ = getattr(self.object, '__all__', None)\n            if is_valid_module_all(__all__):\n                # valid __all__ found. copy it to self.__all__\n                self.__all__ = __all__\n            elif __all__:\n                # invalid __all__ found.\n                logger.warning(__('__all__ should be a list of strings, not %r '\n                                  '(in module %s) -- ignoring __all__') %\n                               (__all__, self.fullname), type='autodoc')\n\n        return ret\n\n    def add_directive_header(self, sig: str) -> None:\n        Documenter.add_directive_header(self, sig)\n\n        sourcename = self.get_sourcename()\n\n        # add some module-specific options\n        if self.options.synopsis:\n            self.add_line('   :synopsis: ' + self.options.synopsis, sourcename)\n        if self.options.platform:\n            self.add_line('   :platform: ' + self.options.platform, sourcename)\n        if self.options.deprecated:\n            self.add_line('   :deprecated:', sourcename)\n\n    def get_object_members(self, want_all: bool) -> Tuple[bool, List[Tuple[str, Any]]]:\n        if want_all:\n            if self.__all__:\n                memberlist = self.__all__\n            else:\n                # for implicit module members, check __module__ to avoid\n                # documenting imported objects\n                return True, get_module_members(self.object)\n        else:\n            memberlist = self.options.members or []\n        ret = []\n        for mname in memberlist:\n            try:\n                ret.append((mname, safe_getattr(self.object, mname)))\n            except AttributeError:\n                logger.warning(\n                    __('missing attribute mentioned in :members: or __all__: '\n                       'module %s, attribute %s') %\n                    (safe_getattr(self.object, '__name__', '???'), mname),\n                    type='autodoc'\n                )\n        return False, ret\n\n    def sort_members(self, documenters: List[Tuple[\"Documenter\", bool]],\n                     order: str) -> List[Tuple[\"Documenter\", bool]]:\n        if order == 'bysource' and self.__all__:\n            # Sort alphabetically first (for members not listed on the __all__)\n            documenters.sort(key=lambda e: e[0].name)\n\n            # Sort by __all__\n            def keyfunc(entry: Tuple[Documenter, bool]) -> int:\n                name = entry[0].name.split('::')[1]\n                if name in self.__all__:\n                    return self.__all__.index(name)\n                else:\n                    return len(self.__all__)\n            documenters.sort(key=keyfunc)\n\n            return documenters\n        else:\n            return super().sort_members(documenters, order)\n\n\nclass ModuleLevelDocumenter(Documenter):\n    \"\"\"\n    Specialized Documenter subclass for objects on module level (functions,\n    classes, data/constants).\n    \"\"\"\n    def resolve_name(self, modname: str, parents: Any, path: str, base: Any\n                     ) -> Tuple[str, List[str]]:\n        if modname is None:\n            if path:\n                modname = path.rstrip('.')\n            else:\n                # if documenting a toplevel object without explicit module,\n                # it can be contained in another auto directive ...\n                modname = self.env.temp_data.get('autodoc:module')\n                # ... or in the scope of a module directive\n                if not modname:\n                    modname = self.env.ref_context.get('py:module')\n                # ... else, it stays None, which means invalid\n        return modname, parents + [base]\n\n\nclass ClassLevelDocumenter(Documenter):\n    \"\"\"\n    Specialized Documenter subclass for objects on class level (methods,\n    attributes).\n    \"\"\"\n    def resolve_name(self, modname: str, parents: Any, path: str, base: Any\n                     ) -> Tuple[str, List[str]]:\n        if modname is None:\n            if path:\n                mod_cls = path.rstrip('.')\n            else:\n                mod_cls = None\n                # if documenting a class-level object without path,\n                # there must be a current class, either from a parent\n                # auto directive ...\n                mod_cls = self.env.temp_data.get('autodoc:class')\n                # ... or from a class directive\n                if mod_cls is None:\n                    mod_cls = self.env.ref_context.get('py:class')\n                # ... if still None, there's no way to know\n                if mod_cls is None:\n                    return None, []\n            modname, sep, cls = mod_cls.rpartition('.')\n            parents = [cls]\n            # if the module name is still missing, get it like above\n            if not modname:\n                modname = self.env.temp_data.get('autodoc:module')\n            if not modname:\n                modname = self.env.ref_context.get('py:module')\n            # ... else, it stays None, which means invalid\n        return modname, parents + [base]\n\n\nclass DocstringSignatureMixin:\n    \"\"\"\n    Mixin for FunctionDocumenter and MethodDocumenter to provide the\n    feature of reading the signature from the docstring.\n    \"\"\"\n    _new_docstrings = None  # type: List[List[str]]\n    _signatures = None      # type: List[str]\n\n    def _find_signature(self, encoding: str = None) -> Tuple[str, str]:\n        if encoding is not None:\n            warnings.warn(\"The 'encoding' argument to autodoc.%s._find_signature() is \"\n                          \"deprecated.\" % self.__class__.__name__,\n                          RemovedInSphinx40Warning, stacklevel=2)\n\n        # candidates of the object name\n        valid_names = [self.objpath[-1]]  # type: ignore\n        if isinstance(self, ClassDocumenter):\n            valid_names.append('__init__')\n            if hasattr(self.object, '__mro__'):\n                valid_names.extend(cls.__name__ for cls in self.object.__mro__)\n\n        docstrings = self.get_doc()\n        self._new_docstrings = docstrings[:]\n        self._signatures = []\n        result = None\n        for i, doclines in enumerate(docstrings):\n            for j, line in enumerate(doclines):\n                if not line:\n                    # no lines in docstring, no match\n                    break\n\n                if line.endswith('\\\\'):\n                    multiline = True\n                    line = line.rstrip('\\\\').rstrip()\n                else:\n                    multiline = False\n\n                # match first line of docstring against signature RE\n                match = py_ext_sig_re.match(line)\n                if not match:\n                    continue\n                exmod, path, base, args, retann = match.groups()\n\n                # the base name must match ours\n                if base not in valid_names:\n                    continue\n\n                # re-prepare docstring to ignore more leading indentation\n                tab_width = self.directive.state.document.settings.tab_width  # type: ignore\n                self._new_docstrings[i] = prepare_docstring('\\n'.join(doclines[j + 1:]),\n                                                            tabsize=tab_width)\n\n                if result is None:\n                    # first signature\n                    result = args, retann\n                else:\n                    # subsequent signatures\n                    self._signatures.append(\"(%s) -> %s\" % (args, retann))\n\n                if multiline:\n                    # the signature have multiple signatures on docstring\n                    continue\n                else:\n                    # don't look any further\n                    break\n\n            if result:\n                # finish the loop when signature found\n                break\n\n        return result\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if encoding is not None:\n            warnings.warn(\"The 'encoding' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx40Warning, stacklevel=2)\n        if self._new_docstrings is not None:\n            return self._new_docstrings\n        return super().get_doc(None, ignore)  # type: ignore\n\n    def format_signature(self, **kwargs: Any) -> str:\n        if self.args is None and self.env.config.autodoc_docstring_signature:  # type: ignore\n            # only act if a signature is not explicitly given already, and if\n            # the feature is enabled\n            result = self._find_signature()\n            if result is not None:\n                self.args, self.retann = result\n        sig = super().format_signature(**kwargs)  # type: ignore\n        if self._signatures:\n            return \"\\n\".join([sig] + self._signatures)\n        else:\n            return sig\n\n\nclass DocstringStripSignatureMixin(DocstringSignatureMixin):\n    \"\"\"\n    Mixin for AttributeDocumenter to provide the\n    feature of stripping any function signature from the docstring.\n    \"\"\"\n    def format_signature(self, **kwargs: Any) -> str:\n        if self.args is None and self.env.config.autodoc_docstring_signature:  # type: ignore\n            # only act if a signature is not explicitly given already, and if\n            # the feature is enabled\n            result = self._find_signature()\n            if result is not None:\n                # Discarding _args is a only difference with\n                # DocstringSignatureMixin.format_signature.\n                # Documenter.format_signature use self.args value to format.\n                _args, self.retann = result\n        return super().format_signature(**kwargs)\n\n\nclass FunctionDocumenter(DocstringSignatureMixin, ModuleLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for functions.\n    \"\"\"\n    objtype = 'function'\n    member_order = 30\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        # supports functions, builtins and bound methods exported at the module level\n        return (inspect.isfunction(member) or inspect.isbuiltin(member) or\n                (inspect.isroutine(member) and isinstance(parent, ModuleDocumenter)))\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.env.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            self.env.app.emit('autodoc-before-process-signature', self.object, False)\n            sig = inspect.signature(self.object, follow_wrapped=True)\n            args = stringify_signature(sig, **kwargs)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a function signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n        except ValueError:\n            args = ''\n\n        if self.env.config.strip_signature_backslash:\n            # escape backslashes for reST\n            args = args.replace('\\\\', '\\\\\\\\')\n        return args\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def add_directive_header(self, sig: str) -> None:\n        sourcename = self.get_sourcename()\n        super().add_directive_header(sig)\n\n        if inspect.iscoroutinefunction(self.object):\n            self.add_line('   :async:', sourcename)\n\n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.overloads:\n            # Use signatures for overloaded functions instead of the implementation function.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n\n        if inspect.is_singledispatch_function(self.object):\n            # append signature of singledispatch'ed functions\n            for typ, func in self.object.registry.items():\n                if typ is object:\n                    pass  # default implementation. skipped.\n                else:\n                    self.annotate_to_first_argument(func, typ)\n\n                    documenter = FunctionDocumenter(self.directive, '')\n                    documenter.object = func\n                    documenter.objpath = [None]\n                    sigs.append(documenter.format_signature())\n        if overloaded:\n            __globals__ = safe_getattr(self.object, '__globals__', {})\n            for overload in self.analyzer.overloads.get('.'.join(self.objpath)):\n                overload = evaluate_signature(overload, __globals__)\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def annotate_to_first_argument(self, func: Callable, typ: Type) -> None:\n        \"\"\"Annotate type hint to the first argument of function if needed.\"\"\"\n        try:\n            sig = inspect.signature(func)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a function signature for %s: %s\"),\n                           self.fullname, exc)\n            return\n        except ValueError:\n            return\n\n        if len(sig.parameters) == 0:\n            return\n\n        params = list(sig.parameters.values())\n        if params[0].annotation is Parameter.empty:\n            params[0] = params[0].replace(annotation=typ)\n            try:\n                func.__signature__ = sig.replace(parameters=params)  # type: ignore\n            except TypeError:\n                # failed to update signature (ex. built-in or extension types)\n                return\n\n\nclass SingledispatchFunctionDocumenter(FunctionDocumenter):\n    \"\"\"\n    Used to be a specialized Documenter subclass for singledispatch'ed functions.\n\n    Retained for backwards compatibility, now does the same as the FunctionDocumenter\n    \"\"\"\n\n\nclass DecoratorDocumenter(FunctionDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for decorator functions.\n    \"\"\"\n    objtype = 'decorator'\n\n    # must be lower than FunctionDocumenter\n    priority = -1\n\n    def format_args(self, **kwargs: Any) -> Any:\n        args = super().format_args(**kwargs)\n        if ',' in args:\n            return args\n        else:\n            return None\n\n\n# Types which have confusing metaclass signatures it would be best not to show.\n# These are listed by name, rather than storing the objects themselves, to avoid\n# needing to import the modules.\n_METACLASS_CALL_BLACKLIST = [\n    'enum.EnumMeta.__call__',\n]\n\n\nclass ClassDocumenter(DocstringSignatureMixin, ModuleLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for classes.\n    \"\"\"\n    objtype = 'class'\n    member_order = 20\n    option_spec = {\n        'members': members_option, 'undoc-members': bool_option,\n        'noindex': bool_option, 'inherited-members': inherited_members_option,\n        'show-inheritance': bool_option, 'member-order': member_order_option,\n        'exclude-members': members_set_option,\n        'private-members': bool_option, 'special-members': members_option,\n    }  # type: Dict[str, Callable]\n\n    _signature_class = None  # type: Any\n    _signature_method_name = None  # type: str\n\n    def __init__(self, *args: Any) -> None:\n        super().__init__(*args)\n        merge_special_members_option(self.options)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, type)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n        # if the class is documented under another name, document it\n        # as data/attribute\n        if ret:\n            if hasattr(self.object, '__name__'):\n                self.doc_as_attr = (self.objpath[-1] != self.object.__name__)\n            else:\n                self.doc_as_attr = True\n        return ret\n\n    def _get_signature(self) -> Tuple[Optional[Any], Optional[str], Optional[Signature]]:\n        def get_user_defined_function_or_method(obj: Any, attr: str) -> Any:\n            \"\"\" Get the `attr` function or method from `obj`, if it is user-defined. \"\"\"\n            if inspect.is_builtin_class_method(obj, attr):\n                return None\n            attr = self.get_attr(obj, attr, None)\n            if not (inspect.ismethod(attr) or inspect.isfunction(attr)):\n                return None\n            return attr\n\n        # This sequence is copied from inspect._signature_from_callable.\n        # ValueError means that no signature could be found, so we keep going.\n\n        # First, let's see if it has an overloaded __call__ defined\n        # in its metaclass\n        call = get_user_defined_function_or_method(type(self.object), '__call__')\n\n        if call is not None:\n            if \"{0.__module__}.{0.__qualname__}\".format(call) in _METACLASS_CALL_BLACKLIST:\n                call = None\n\n        if call is not None:\n            self.env.app.emit('autodoc-before-process-signature', call, True)\n            try:\n                sig = inspect.signature(call, bound_method=True)\n                return type(self.object), '__call__', sig\n            except ValueError:\n                pass\n\n        # Now we check if the 'obj' class has a '__new__' method\n        new = get_user_defined_function_or_method(self.object, '__new__')\n        if new is not None:\n            self.env.app.emit('autodoc-before-process-signature', new, True)\n            try:\n                sig = inspect.signature(new, bound_method=True)\n                return self.object, '__new__', sig\n            except ValueError:\n                pass\n\n        # Finally, we should have at least __init__ implemented\n        init = get_user_defined_function_or_method(self.object, '__init__')\n        if init is not None:\n            self.env.app.emit('autodoc-before-process-signature', init, True)\n            try:\n                sig = inspect.signature(init, bound_method=True)\n                return self.object, '__init__', sig\n            except ValueError:\n                pass\n\n        # None of the attributes are user-defined, so fall back to let inspect\n        # handle it.\n        # We don't know the exact method that inspect.signature will read\n        # the signature from, so just pass the object itself to our hook.\n        self.env.app.emit('autodoc-before-process-signature', self.object, False)\n        try:\n            sig = inspect.signature(self.object, bound_method=False)\n            return None, None, sig\n        except ValueError:\n            pass\n\n        # Still no signature: happens e.g. for old-style classes\n        # with __init__ in C and no `__text_signature__`.\n        return None, None, None\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.env.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            self._signature_class, self._signature_method_name, sig = self._get_signature()\n        except TypeError as exc:\n            # __signature__ attribute contained junk\n            logger.warning(__(\"Failed to get a constructor signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n\n        if sig is None:\n            return None\n\n        return stringify_signature(sig, show_return_annotation=False, **kwargs)\n\n    def format_signature(self, **kwargs: Any) -> str:\n        if self.doc_as_attr:\n            return ''\n\n        sig = super().format_signature()\n\n        overloaded = False\n        qualname = None\n        # TODO: recreate analyzer for the module of class (To be clear, owner of the method)\n        if self._signature_class and self._signature_method_name and self.analyzer:\n            qualname = '.'.join([self._signature_class.__qualname__,\n                                 self._signature_method_name])\n            if qualname in self.analyzer.overloads:\n                overloaded = True\n\n        sigs = []\n        if overloaded:\n            # Use signatures for overloaded methods instead of the implementation method.\n            method = safe_getattr(self._signature_class, self._signature_method_name, None)\n            __globals__ = safe_getattr(method, '__globals__', {})\n            for overload in self.analyzer.overloads.get(qualname):\n                overload = evaluate_signature(overload, __globals__)\n\n                parameters = list(overload.parameters.values())\n                overload = overload.replace(parameters=parameters[1:],\n                                            return_annotation=Parameter.empty)\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n        else:\n            sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def add_directive_header(self, sig: str) -> None:\n        sourcename = self.get_sourcename()\n\n        if self.doc_as_attr:\n            self.directivetype = 'attribute'\n        super().add_directive_header(sig)\n\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n        # add inheritance info, if wanted\n        if not self.doc_as_attr and self.options.show_inheritance:\n            sourcename = self.get_sourcename()\n            self.add_line('', sourcename)\n            if hasattr(self.object, '__bases__') and len(self.object.__bases__):\n                bases = [':class:`%s`' % b.__name__\n                         if b.__module__ in ('__builtin__', 'builtins')\n                         else ':class:`%s.%s`' % (b.__module__, b.__qualname__)\n                         for b in self.object.__bases__]\n                self.add_line('   ' + _('Bases: %s') % ', '.join(bases),\n                              sourcename)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if encoding is not None:\n            warnings.warn(\"The 'encoding' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx40Warning, stacklevel=2)\n        lines = getattr(self, '_new_docstrings', None)\n        if lines is not None:\n            return lines\n\n        content = self.env.config.autoclass_content\n\n        docstrings = []\n        attrdocstring = self.get_attr(self.object, '__doc__', None)\n        if attrdocstring:\n            docstrings.append(attrdocstring)\n\n        # for classes, what the \"docstring\" is can be controlled via a\n        # config value; the default is only the class docstring\n        if content in ('both', 'init'):\n            __init__ = self.get_attr(self.object, '__init__', None)\n            initdocstring = getdoc(__init__, self.get_attr,\n                                   self.env.config.autodoc_inherit_docstrings,\n                                   self.parent, self.object_name)\n            # for new-style classes, no __init__ means default __init__\n            if (initdocstring is not None and\n                (initdocstring == object.__init__.__doc__ or  # for pypy\n                 initdocstring.strip() == object.__init__.__doc__)):  # for !pypy\n                initdocstring = None\n            if not initdocstring:\n                # try __new__\n                __new__ = self.get_attr(self.object, '__new__', None)\n                initdocstring = getdoc(__new__, self.get_attr,\n                                       self.env.config.autodoc_inherit_docstrings,\n                                       self.parent, self.object_name)\n                # for new-style classes, no __new__ means default __new__\n                if (initdocstring is not None and\n                    (initdocstring == object.__new__.__doc__ or  # for pypy\n                     initdocstring.strip() == object.__new__.__doc__)):  # for !pypy\n                    initdocstring = None\n            if initdocstring:\n                if content == 'init':\n                    docstrings = [initdocstring]\n                else:\n                    docstrings.append(initdocstring)\n\n        tab_width = self.directive.state.document.settings.tab_width\n        return [prepare_docstring(docstring, ignore, tab_width) for docstring in docstrings]\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        if self.doc_as_attr:\n            classname = safe_getattr(self.object, '__qualname__', None)\n            if not classname:\n                classname = safe_getattr(self.object, '__name__', None)\n            if classname:\n                module = safe_getattr(self.object, '__module__', None)\n                parentmodule = safe_getattr(self.parent, '__module__', None)\n                if module and module != parentmodule:\n                    classname = str(module) + '.' + str(classname)\n                content = StringList([_('alias of :class:`%s`') % classname], source='')\n                super().add_content(content, no_docstring=True)\n        else:\n            super().add_content(more_content)\n\n    def document_members(self, all_members: bool = False) -> None:\n        if self.doc_as_attr:\n            return\n        super().document_members(all_members)\n\n    def generate(self, more_content: Any = None, real_modname: str = None,\n                 check_module: bool = False, all_members: bool = False) -> None:\n        # Do not pass real_modname and use the name from the __module__\n        # attribute of the class.\n        # If a class gets imported into the module real_modname\n        # the analyzer won't find the source of the class, if\n        # it looks in real_modname.\n        return super().generate(more_content=more_content,\n                                check_module=check_module,\n                                all_members=all_members)\n\n\nclass ExceptionDocumenter(ClassDocumenter):\n    \"\"\"\n    Specialized ClassDocumenter subclass for exceptions.\n    \"\"\"\n    objtype = 'exception'\n    member_order = 10\n\n    # needs a higher priority than ClassDocumenter\n    priority = 10\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, type) and issubclass(member, BaseException)\n\n\nclass DataDocumenter(ModuleLevelDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data items.\n    \"\"\"\n    objtype = 'data'\n    member_order = 40\n    priority = -10\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(parent, ModuleDocumenter) and isattr\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain annotation for this data\n            annotations = getattr(self.parent, '__annotations__', {})\n            if annotations and self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            try:\n                if self.object is UNINITIALIZED_ATTR:\n                    pass\n                else:\n                    objrepr = object_description(self.object)\n                    self.add_line('   :value: ' + objrepr, sourcename)\n            except ValueError:\n                pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation,\n                          sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n\nclass DataDeclarationDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for data that cannot be imported\n    because they are declared without initial value (refs: PEP-526).\n    \"\"\"\n    objtype = 'datadecl'\n    directivetype = 'data'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as a data\n        self.objtype = 'data'\n        self.object = UNINITIALIZED_ATTR\n        try:\n            # import module to obtain type annotation\n            self.parent = importlib.import_module(self.modname)\n        except ImportError:\n            pass\n\n        return True\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Never try to get a docstring from the object.\"\"\"\n        super().add_content(more_content, no_docstring=True)\n\n\nclass GenericAliasDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for GenericAliases.\n    \"\"\"\n\n    objtype = 'genericalias'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isgenericalias(member)\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options.annotation = SUPPRESS  # type: ignore\n        super().add_directive_header(sig)\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        name = stringify_typehint(self.object)\n        content = StringList([_('alias of %s') % name], source='')\n        super().add_content(content)\n\n\nclass TypeVarDocumenter(DataDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for TypeVars.\n    \"\"\"\n\n    objtype = 'typevar'\n    directivetype = 'data'\n    priority = DataDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return isinstance(member, TypeVar) and isattr  # type: ignore\n\n    def add_directive_header(self, sig: str) -> None:\n        self.options.annotation = SUPPRESS  # type: ignore\n        super().add_directive_header(sig)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        if self.object.__doc__ != TypeVar.__doc__:\n            return super().get_doc()\n        else:\n            return []\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        attrs = [repr(self.object.__name__)]\n        for constraint in self.object.__constraints__:\n            attrs.append(stringify_typehint(constraint))\n        if self.object.__covariant__:\n            attrs.append(\"covariant=True\")\n        if self.object.__contravariant__:\n            attrs.append(\"contravariant=True\")\n\n        content = StringList([_('alias of TypeVar(%s)') % \", \".join(attrs)], source='')\n        super().add_content(content)\n\n\nclass MethodDocumenter(DocstringSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for methods (normal, static and class).\n    \"\"\"\n    objtype = 'method'\n    directivetype = 'method'\n    member_order = 50\n    priority = 1  # must be more than FunctionDocumenter\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isroutine(member) and \\\n            not isinstance(parent, ModuleDocumenter)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        ret = super().import_object(raiseerror)\n        if not ret:\n            return ret\n\n        # to distinguish classmethod/staticmethod\n        obj = self.parent.__dict__.get(self.object_name)\n        if obj is None:\n            obj = self.object\n\n        if (inspect.isclassmethod(obj) or\n                inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name)):\n            # document class and static members before ordinary ones\n            self.member_order = self.member_order - 1\n\n        return ret\n\n    def format_args(self, **kwargs: Any) -> str:\n        if self.env.config.autodoc_typehints in ('none', 'description'):\n            kwargs.setdefault('show_annotation', False)\n\n        try:\n            if self.object == object.__init__ and self.parent != object:\n                # Classes not having own __init__() method are shown as no arguments.\n                #\n                # Note: The signature of object.__init__() is (self, /, *args, **kwargs).\n                #       But it makes users confused.\n                args = '()'\n            else:\n                if inspect.isstaticmethod(self.object, cls=self.parent, name=self.object_name):\n                    self.env.app.emit('autodoc-before-process-signature', self.object, False)\n                    sig = inspect.signature(self.object, bound_method=False)\n                else:\n                    self.env.app.emit('autodoc-before-process-signature', self.object, True)\n                    sig = inspect.signature(self.object, bound_method=True,\n                                            follow_wrapped=True)\n                args = stringify_signature(sig, **kwargs)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return None\n        except ValueError:\n            args = ''\n\n        if self.env.config.strip_signature_backslash:\n            # escape backslashes for reST\n            args = args.replace('\\\\', '\\\\\\\\')\n        return args\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n\n        sourcename = self.get_sourcename()\n        obj = self.parent.__dict__.get(self.object_name, self.object)\n        if inspect.isabstractmethod(obj):\n            self.add_line('   :abstractmethod:', sourcename)\n        if inspect.iscoroutinefunction(obj):\n            self.add_line('   :async:', sourcename)\n        if inspect.isclassmethod(obj):\n            self.add_line('   :classmethod:', sourcename)\n        if inspect.isstaticmethod(obj, cls=self.parent, name=self.object_name):\n            self.add_line('   :staticmethod:', sourcename)\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.finals:\n            self.add_line('   :final:', sourcename)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def format_signature(self, **kwargs: Any) -> str:\n        sigs = []\n        if self.analyzer and '.'.join(self.objpath) in self.analyzer.overloads:\n            # Use signatures for overloaded methods instead of the implementation method.\n            overloaded = True\n        else:\n            overloaded = False\n            sig = super().format_signature(**kwargs)\n            sigs.append(sig)\n\n        meth = self.parent.__dict__.get(self.objpath[-1])\n        if inspect.is_singledispatch_method(meth):\n            # append signature of singledispatch'ed functions\n            for typ, func in meth.dispatcher.registry.items():\n                if typ is object:\n                    pass  # default implementation. skipped.\n                else:\n                    self.annotate_to_first_argument(func, typ)\n\n                    documenter = MethodDocumenter(self.directive, '')\n                    documenter.parent = self.parent\n                    documenter.object = func\n                    documenter.objpath = [None]\n                    sigs.append(documenter.format_signature())\n        if overloaded:\n            __globals__ = safe_getattr(self.object, '__globals__', {})\n            for overload in self.analyzer.overloads.get('.'.join(self.objpath)):\n                overload = evaluate_signature(overload, __globals__)\n                if not inspect.isstaticmethod(self.object, cls=self.parent,\n                                              name=self.object_name):\n                    parameters = list(overload.parameters.values())\n                    overload = overload.replace(parameters=parameters[1:])\n                sig = stringify_signature(overload, **kwargs)\n                sigs.append(sig)\n\n        return \"\\n\".join(sigs)\n\n    def annotate_to_first_argument(self, func: Callable, typ: Type) -> None:\n        \"\"\"Annotate type hint to the first argument of function if needed.\"\"\"\n        try:\n            sig = inspect.signature(func)\n        except TypeError as exc:\n            logger.warning(__(\"Failed to get a method signature for %s: %s\"),\n                           self.fullname, exc)\n            return\n        except ValueError:\n            return\n        if len(sig.parameters) == 1:\n            return\n\n        params = list(sig.parameters.values())\n        if params[1].annotation is Parameter.empty:\n            params[1] = params[1].replace(annotation=typ)\n            try:\n                func.__signature__ = sig.replace(parameters=params)  # type: ignore\n            except TypeError:\n                # failed to update signature (ex. built-in or extension types)\n                return\n\n\nclass SingledispatchMethodDocumenter(MethodDocumenter):\n    \"\"\"\n    Used to be a specialized Documenter subclass for singledispatch'ed methods.\n\n    Retained for backwards compatibility, now does the same as the MethodDocumenter\n    \"\"\"\n\n\nclass AttributeDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for attributes.\n    \"\"\"\n    objtype = 'attribute'\n    member_order = 60\n    option_spec = dict(ModuleLevelDocumenter.option_spec)\n    option_spec[\"annotation\"] = annotation_option\n\n    # must be higher than the MethodDocumenter, else it will recognize\n    # some non-data descriptors as methods\n    priority = 10\n\n    @staticmethod\n    def is_function_or_method(obj: Any) -> bool:\n        return inspect.isfunction(obj) or inspect.isbuiltin(obj) or inspect.ismethod(obj)\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        if inspect.isattributedescriptor(member):\n            return True\n        elif (not isinstance(parent, ModuleDocumenter) and\n              not inspect.isroutine(member) and\n              not isinstance(member, type)):\n            return True\n        else:\n            return False\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def isinstanceattribute(self) -> bool:\n        \"\"\"Check the subject is an instance attribute.\"\"\"\n        try:\n            analyzer = ModuleAnalyzer.for_module(self.modname)\n            attr_docs = analyzer.find_attr_docs()\n            if self.objpath:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if key in attr_docs:\n                    return True\n\n            return False\n        except PycodeError:\n            return False\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        try:\n            ret = super().import_object(raiseerror=True)\n            if inspect.isenumattribute(self.object):\n                self.object = self.object.value\n            if inspect.isattributedescriptor(self.object):\n                self._datadescriptor = True\n            else:\n                # if it's not a data descriptor\n                self._datadescriptor = False\n        except ImportError as exc:\n            if self.isinstanceattribute():\n                self.object = INSTANCEATTR\n                self._datadescriptor = False\n                ret = True\n            elif raiseerror:\n                raise\n            else:\n                logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                self.env.note_reread()\n                ret = False\n\n        return ret\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if not self.options.annotation:\n            # obtain type annotation for this attribute\n            annotations = getattr(self.parent, '__annotations__', {})\n            if annotations and self.objpath[-1] in annotations:\n                objrepr = stringify_typehint(annotations.get(self.objpath[-1]))\n                self.add_line('   :type: ' + objrepr, sourcename)\n            else:\n                key = ('.'.join(self.objpath[:-1]), self.objpath[-1])\n                if self.analyzer and key in self.analyzer.annotations:\n                    self.add_line('   :type: ' + self.analyzer.annotations[key],\n                                  sourcename)\n\n            # data descriptors do not have useful values\n            if not self._datadescriptor:\n                try:\n                    if self.object is INSTANCEATTR:\n                        pass\n                    else:\n                        objrepr = object_description(self.object)\n                        self.add_line('   :value: ' + objrepr, sourcename)\n                except ValueError:\n                    pass\n        elif self.options.annotation is SUPPRESS:\n            pass\n        else:\n            self.add_line('   :annotation: %s' % self.options.annotation, sourcename)\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        try:\n            # Disable `autodoc_inherit_docstring` temporarily to avoid to obtain\n            # a docstring from the value which descriptor returns unexpectedly.\n            # ref: https://github.com/sphinx-doc/sphinx/issues/7805\n            orig = self.env.config.autodoc_inherit_docstrings\n            self.env.config.autodoc_inherit_docstrings = False  # type: ignore\n            return super().get_doc(encoding, ignore)\n        finally:\n            self.env.config.autodoc_inherit_docstrings = orig  # type: ignore\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        if not self._datadescriptor:\n            # if it's not a data descriptor, its docstring is very probably the\n            # wrong thing to display\n            no_docstring = True\n        super().add_content(more_content, no_docstring)\n\n\nclass PropertyDocumenter(DocstringStripSignatureMixin, ClassLevelDocumenter):  # type: ignore\n    \"\"\"\n    Specialized Documenter subclass for properties.\n    \"\"\"\n    objtype = 'property'\n    directivetype = 'method'\n    member_order = 60\n\n    # before AttributeDocumenter\n    priority = AttributeDocumenter.priority + 1\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        return inspect.isproperty(member) and isinstance(parent, ClassDocumenter)\n\n    def document_members(self, all_members: bool = False) -> None:\n        pass\n\n    def get_real_modname(self) -> str:\n        return self.get_attr(self.parent or self.object, '__module__', None) \\\n            or self.modname\n\n    def add_directive_header(self, sig: str) -> None:\n        super().add_directive_header(sig)\n        sourcename = self.get_sourcename()\n        if inspect.isabstractmethod(self.object):\n            self.add_line('   :abstractmethod:', sourcename)\n        self.add_line('   :property:', sourcename)\n\n\nclass InstanceAttributeDocumenter(AttributeDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for attributes that cannot be imported\n    because they are instance attributes (e.g. assigned in __init__).\n    \"\"\"\n    objtype = 'instanceattribute'\n    directivetype = 'attribute'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only INSTANCEATTR members.\"\"\"\n        return (not isinstance(parent, ModuleDocumenter) and\n                isattr and\n                member is INSTANCEATTR)\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as an attribute\n        self.objtype = 'attribute'\n        self.object = INSTANCEATTR\n        self._datadescriptor = False\n        return True\n\n    def add_content(self, more_content: Any, no_docstring: bool = False) -> None:\n        \"\"\"Never try to get a docstring from the object.\"\"\"\n        super().add_content(more_content, no_docstring=True)\n\n\nclass SlotsAttributeDocumenter(AttributeDocumenter):\n    \"\"\"\n    Specialized Documenter subclass for attributes that cannot be imported\n    because they are attributes in __slots__.\n    \"\"\"\n    objtype = 'slotsattribute'\n    directivetype = 'attribute'\n    member_order = 60\n\n    # must be higher than AttributeDocumenter\n    priority = 11\n\n    @classmethod\n    def can_document_member(cls, member: Any, membername: str, isattr: bool, parent: Any\n                            ) -> bool:\n        \"\"\"This documents only SLOTSATTR members.\"\"\"\n        return member is SLOTSATTR\n\n    def import_object(self, raiseerror: bool = False) -> bool:\n        \"\"\"Never import anything.\"\"\"\n        # disguise as an attribute\n        self.objtype = 'attribute'\n        self._datadescriptor = True\n\n        with mock(self.env.config.autodoc_mock_imports):\n            try:\n                ret = import_object(self.modname, self.objpath[:-1], 'class',\n                                    attrgetter=self.get_attr,\n                                    warningiserror=self.env.config.autodoc_warningiserror)\n                self.module, _, _, self.parent = ret\n                return True\n            except ImportError as exc:\n                if raiseerror:\n                    raise\n                else:\n                    logger.warning(exc.args[0], type='autodoc', subtype='import_object')\n                    self.env.note_reread()\n                    return False\n\n    def get_doc(self, encoding: str = None, ignore: int = None) -> List[List[str]]:\n        \"\"\"Decode and return lines of the docstring(s) for the object.\"\"\"\n        if ignore is not None:\n            warnings.warn(\"The 'ignore' argument to autodoc.%s.get_doc() is deprecated.\"\n                          % self.__class__.__name__,\n                          RemovedInSphinx50Warning, stacklevel=2)\n        name = self.objpath[-1]\n        __slots__ = safe_getattr(self.parent, '__slots__', [])\n        if isinstance(__slots__, dict) and isinstance(__slots__.get(name), str):\n            docstring = prepare_docstring(__slots__[name])\n            return [docstring]\n        else:\n            return []\n\n\ndef get_documenters(app: Sphinx) -> Dict[str, \"Type[Documenter]\"]:\n    \"\"\"Returns registered Documenter classes\"\"\"\n    warnings.warn(\"get_documenters() is deprecated.\", RemovedInSphinx50Warning, stacklevel=2)\n    return app.registry.documenters\n\n\ndef autodoc_attrgetter(app: Sphinx, obj: Any, name: str, *defargs: Any) -> Any:\n    \"\"\"Alternative getattr() for types\"\"\"\n    for typ, func in app.registry.autodoc_attrgettrs.items():\n        if isinstance(obj, typ):\n            return func(obj, name, *defargs)\n\n    return safe_getattr(obj, name, *defargs)\n\n\ndef migrate_autodoc_member_order(app: Sphinx, config: Config) -> None:\n    if config.autodoc_member_order == 'alphabetic':\n        # RemovedInSphinx50Warning\n        logger.warning(__('autodoc_member_order now accepts \"alphabetical\" '\n                          'instead of \"alphabetic\". Please update your setting.'))\n        config.autodoc_member_order = 'alphabetical'  # type: ignore\n\n\ndef setup(app: Sphinx) -> Dict[str, Any]:\n    app.add_autodocumenter(ModuleDocumenter)\n    app.add_autodocumenter(ClassDocumenter)\n    app.add_autodocumenter(ExceptionDocumenter)\n    app.add_autodocumenter(DataDocumenter)\n    app.add_autodocumenter(DataDeclarationDocumenter)\n    app.add_autodocumenter(GenericAliasDocumenter)\n    app.add_autodocumenter(TypeVarDocumenter)\n    app.add_autodocumenter(FunctionDocumenter)\n    app.add_autodocumenter(DecoratorDocumenter)\n    app.add_autodocumenter(MethodDocumenter)\n    app.add_autodocumenter(AttributeDocumenter)\n    app.add_autodocumenter(PropertyDocumenter)\n    app.add_autodocumenter(InstanceAttributeDocumenter)\n    app.add_autodocumenter(SlotsAttributeDocumenter)\n\n    app.add_config_value('autoclass_content', 'class', True, ENUM('both', 'class', 'init'))\n    app.add_config_value('autodoc_member_order', 'alphabetical', True,\n                         ENUM('alphabetic', 'alphabetical', 'bysource', 'groupwise'))\n    app.add_config_value('autodoc_default_options', {}, True)\n    app.add_config_value('autodoc_docstring_signature', True, True)\n    app.add_config_value('autodoc_mock_imports', [], True)\n    app.add_config_value('autodoc_typehints', \"signature\", True,\n                         ENUM(\"signature\", \"description\", \"none\"))\n    app.add_config_value('autodoc_warningiserror', True, True)\n    app.add_config_value('autodoc_inherit_docstrings', True, True)\n    app.add_event('autodoc-before-process-signature')\n    app.add_event('autodoc-process-docstring')\n    app.add_event('autodoc-process-signature')\n    app.add_event('autodoc-skip-member')\n\n    app.connect('config-inited', migrate_autodoc_member_order, priority=800)\n\n    app.setup_extension('sphinx.ext.autodoc.type_comment')\n    app.setup_extension('sphinx.ext.autodoc.typehints')\n\n    return {'version': sphinx.__display_version__, 'parallel_read_safe': True}"}
{"model_name_or_path": "agentless", "instance_id": "sphinx-doc__sphinx-8721", "model_patch": "", "raw_model_patch": "", "original_file_content": "\"\"\"\n    sphinx.builders._epub_base\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Base class of epub2/epub3 builders.\n\n    :copyright: Copyright 2007-2021 by the Sphinx team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport html\nimport os\nimport re\nimport warnings\nfrom collections import namedtuple\nfrom os import path\nfrom typing import Any, Dict, List, Set, Tuple\nfrom zipfile import ZIP_DEFLATED, ZIP_STORED, ZipFile\n\nfrom docutils import nodes\nfrom docutils.nodes import Element, Node\nfrom docutils.utils import smartquotes\n\nfrom sphinx import addnodes\nfrom sphinx.builders.html import BuildInfo, StandaloneHTMLBuilder\nfrom sphinx.deprecation import RemovedInSphinx40Warning\nfrom sphinx.locale import __\nfrom sphinx.util import logging, status_iterator\nfrom sphinx.util.fileutil import copy_asset_file\nfrom sphinx.util.i18n import format_date\nfrom sphinx.util.osutil import copyfile, ensuredir\n\ntry:\n    from PIL import Image\nexcept ImportError:\n    Image = None\n\n\nlogger = logging.getLogger(__name__)\n\n\n# (Fragment) templates from which the metainfo files content.opf and\n# toc.ncx are created.\n# This template section also defines strings that are embedded in the html\n# output but that may be customized by (re-)setting module attributes,\n# e.g. from conf.py.\n\nCOVERPAGE_NAME = 'epub-cover.xhtml'\n\nTOCTREE_TEMPLATE = 'toctree-l%d'\n\nLINK_TARGET_TEMPLATE = ' [%(uri)s]'\n\nFOOTNOTE_LABEL_TEMPLATE = '#%d'\n\nFOOTNOTES_RUBRIC_NAME = 'Footnotes'\n\nCSS_LINK_TARGET_CLASS = 'link-target'\n\n# XXX These strings should be localized according to epub_language\nGUIDE_TITLES = {\n    'toc': 'Table of Contents',\n    'cover': 'Cover'\n}\n\nMEDIA_TYPES = {\n    '.xhtml': 'application/xhtml+xml',\n    '.css': 'text/css',\n    '.png': 'image/png',\n    '.gif': 'image/gif',\n    '.svg': 'image/svg+xml',\n    '.jpg': 'image/jpeg',\n    '.jpeg': 'image/jpeg',\n    '.otf': 'application/x-font-otf',\n    '.ttf': 'application/x-font-ttf',\n    '.woff': 'application/font-woff',\n}\n\nVECTOR_GRAPHICS_EXTENSIONS = ('.svg',)\n\n# Regular expression to match colons only in local fragment identifiers.\n# If the URI contains a colon before the #,\n# it is an external link that should not change.\nREFURI_RE = re.compile(\"([^#:]*#)(.*)\")\n\n\nManifestItem = namedtuple('ManifestItem', ['href', 'id', 'media_type'])\nSpine = namedtuple('Spine', ['idref', 'linear'])\nGuide = namedtuple('Guide', ['type', 'title', 'uri'])\nNavPoint = namedtuple('NavPoint', ['navpoint', 'playorder', 'text', 'refuri', 'children'])\n\n\ndef sphinx_smarty_pants(t: str, language: str = 'en') -> str:\n    t = t.replace('&quot;', '\"')\n    t = smartquotes.educateDashesOldSchool(t)\n    t = smartquotes.educateQuotes(t, language)\n    t = t.replace('\"', '&quot;')\n    return t\n\n\nssp = sphinx_smarty_pants\n\n\n# The epub publisher\n\nclass EpubBuilder(StandaloneHTMLBuilder):\n    \"\"\"\n    Builder that outputs epub files.\n\n    It creates the metainfo files container.opf, toc.ncx, mimetype, and\n    META-INF/container.xml.  Afterwards, all necessary files are zipped to an\n    epub file.\n    \"\"\"\n\n    # don't copy the reST source\n    copysource = False\n    supported_image_types = ['image/svg+xml', 'image/png', 'image/gif',\n                             'image/jpeg']\n    supported_remote_images = False\n\n    # don't add links\n    add_permalinks = False\n    # don't use # as current path. ePub check reject it.\n    allow_sharp_as_current_path = False\n    # don't add sidebar etc.\n    embedded = True\n    # disable download role\n    download_support = False\n    # dont' create links to original images from images\n    html_scaled_image_link = False\n    # don't generate search index or include search page\n    search = False\n\n    coverpage_name = COVERPAGE_NAME\n    toctree_template = TOCTREE_TEMPLATE\n    link_target_template = LINK_TARGET_TEMPLATE\n    css_link_target_class = CSS_LINK_TARGET_CLASS\n    guide_titles = GUIDE_TITLES\n    media_types = MEDIA_TYPES\n    refuri_re = REFURI_RE\n    template_dir = \"\"\n    doctype = \"\"\n\n    def init(self) -> None:\n        super().init()\n        # the output files for epub must be .html only\n        self.out_suffix = '.xhtml'\n        self.link_suffix = '.xhtml'\n        self.playorder = 0\n        self.tocid = 0\n        self.id_cache = {}  # type: Dict[str, str]\n        self.use_index = self.get_builder_config('use_index', 'epub')\n        self.refnodes = []  # type: List[Dict[str, Any]]\n\n    def create_build_info(self) -> BuildInfo:\n        return BuildInfo(self.config, self.tags, ['html', 'epub'])\n\n    def get_theme_config(self) -> Tuple[str, Dict]:\n        return self.config.epub_theme, self.config.epub_theme_options\n\n    # generic support functions\n    def make_id(self, name: str) -> str:\n        # id_cache is intentionally mutable\n        \"\"\"Return a unique id for name.\"\"\"\n        id = self.id_cache.get(name)\n        if not id:\n            id = 'epub-%d' % self.env.new_serialno('epub')\n            self.id_cache[name] = id\n        return id\n\n    def esc(self, name: str) -> str:\n        \"\"\"Replace all characters not allowed in text an attribute values.\"\"\"\n        warnings.warn(\n            '%s.esc() is deprecated. Use html.escape() instead.' % self.__class__.__name__,\n            RemovedInSphinx40Warning, stacklevel=2)\n        name = name.replace('&', '&amp;')\n        name = name.replace('<', '&lt;')\n        name = name.replace('>', '&gt;')\n        name = name.replace('\"', '&quot;')\n        name = name.replace('\\'', '&#39;')\n        return name\n\n    def get_refnodes(self, doctree: Node, result: List[Dict[str, Any]]) -> List[Dict[str, Any]]:  # NOQA\n        \"\"\"Collect section titles, their depth in the toc and the refuri.\"\"\"\n        # XXX: is there a better way than checking the attribute\n        # toctree-l[1-8] on the parent node?\n        if isinstance(doctree, nodes.reference) and doctree.get('refuri'):\n            refuri = doctree['refuri']\n            if refuri.startswith('http://') or refuri.startswith('https://') \\\n               or refuri.startswith('irc:') or refuri.startswith('mailto:'):\n                return result\n            classes = doctree.parent.attributes['classes']\n            for level in range(8, 0, -1):  # or range(1, 8)?\n                if (self.toctree_template % level) in classes:\n                    result.append({\n                        'level': level,\n                        'refuri': html.escape(refuri),\n                        'text': ssp(html.escape(doctree.astext()))\n                    })\n                    break\n        elif isinstance(doctree, nodes.Element):\n            for elem in doctree:\n                result = self.get_refnodes(elem, result)\n        return result\n\n    def check_refnodes(self, nodes: List[Dict[str, Any]]) -> None:\n        appeared = set()  # type: Set[str]\n        for node in nodes:\n            if node['refuri'] in appeared:\n                logger.warning(\n                    __('duplicated ToC entry found: %s'),\n                    node['refuri'],\n                    type=\"epub\",\n                    subtype=\"duplicated_toc_entry\",\n                )\n            else:\n                appeared.add(node['refuri'])\n\n    def get_toc(self) -> None:\n        \"\"\"Get the total table of contents, containing the master_doc\n        and pre and post files not managed by sphinx.\n        \"\"\"\n        doctree = self.env.get_and_resolve_doctree(self.config.master_doc,\n                                                   self, prune_toctrees=False,\n                                                   includehidden=True)\n        self.refnodes = self.get_refnodes(doctree, [])\n        master_dir = path.dirname(self.config.master_doc)\n        if master_dir:\n            master_dir += '/'  # XXX or os.sep?\n            for item in self.refnodes:\n                item['refuri'] = master_dir + item['refuri']\n        self.toc_add_files(self.refnodes)\n\n    def toc_add_files(self, refnodes: List[Dict[str, Any]]) -> None:\n        \"\"\"Add the master_doc, pre and post files to a list of refnodes.\n        \"\"\"\n        refnodes.insert(0, {\n            'level': 1,\n            'refuri': html.escape(self.config.master_doc + self.out_suffix),\n            'text': ssp(html.escape(\n                self.env.titles[self.config.master_doc].astext()))\n        })\n        for file, text in reversed(self.config.epub_pre_files):\n            refnodes.insert(0, {\n                'level': 1,\n                'refuri': html.escape(file),\n                'text': ssp(html.escape(text))\n            })\n        for file, text in self.config.epub_post_files:\n            refnodes.append({\n                'level': 1,\n                'refuri': html.escape(file),\n                'text': ssp(html.escape(text))\n            })\n\n    def fix_fragment(self, prefix: str, fragment: str) -> str:\n        \"\"\"Return a href/id attribute with colons replaced by hyphens.\"\"\"\n        return prefix + fragment.replace(':', '-')\n\n    def fix_ids(self, tree: nodes.document) -> None:\n        \"\"\"Replace colons with hyphens in href and id attributes.\n\n        Some readers crash because they interpret the part as a\n        transport protocol specification.\n        \"\"\"\n        def update_node_id(node: Element) -> None:\n            \"\"\"Update IDs of given *node*.\"\"\"\n            new_ids = []\n            for node_id in node['ids']:\n                new_id = self.fix_fragment('', node_id)\n                if new_id not in new_ids:\n                    new_ids.append(new_id)\n            node['ids'] = new_ids\n\n        for reference in tree.traverse(nodes.reference):\n            if 'refuri' in reference:\n                m = self.refuri_re.match(reference['refuri'])\n                if m:\n                    reference['refuri'] = self.fix_fragment(m.group(1), m.group(2))\n            if 'refid' in reference:\n                reference['refid'] = self.fix_fragment('', reference['refid'])\n\n        for target in tree.traverse(nodes.target):\n            update_node_id(target)\n\n            next_node = target.next_node(ascend=True)  # type: Node\n            if isinstance(next_node, nodes.Element):\n                update_node_id(next_node)\n\n        for desc_signature in tree.traverse(addnodes.desc_signature):\n            update_node_id(desc_signature)\n\n    def add_visible_links(self, tree: nodes.document, show_urls: str = 'inline') -> None:\n        \"\"\"Add visible link targets for external links\"\"\"\n\n        def make_footnote_ref(doc: nodes.document, label: str) -> nodes.footnote_reference:\n            \"\"\"Create a footnote_reference node with children\"\"\"\n            footnote_ref = nodes.footnote_reference('[#]_')\n            footnote_ref.append(nodes.Text(label))\n            doc.note_autofootnote_ref(footnote_ref)\n            return footnote_ref\n\n        def make_footnote(doc: nodes.document, label: str, uri: str) -> nodes.footnote:\n            \"\"\"Create a footnote node with children\"\"\"\n            footnote = nodes.footnote(uri)\n            para = nodes.paragraph()\n            para.append(nodes.Text(uri))\n            footnote.append(para)\n            footnote.insert(0, nodes.label('', label))\n            doc.note_autofootnote(footnote)\n            return footnote\n\n        def footnote_spot(tree: nodes.document) -> Tuple[Element, int]:\n            \"\"\"Find or create a spot to place footnotes.\n\n            The function returns the tuple (parent, index).\"\"\"\n            # The code uses the following heuristic:\n            # a) place them after the last existing footnote\n            # b) place them after an (empty) Footnotes rubric\n            # c) create an empty Footnotes rubric at the end of the document\n            fns = tree.traverse(nodes.footnote)\n            if fns:\n                fn = fns[-1]\n                return fn.parent, fn.parent.index(fn) + 1\n            for node in tree.traverse(nodes.rubric):\n                if len(node) == 1 and node.astext() == FOOTNOTES_RUBRIC_NAME:\n                    return node.parent, node.parent.index(node) + 1\n            doc = tree.traverse(nodes.document)[0]\n            rub = nodes.rubric()\n            rub.append(nodes.Text(FOOTNOTES_RUBRIC_NAME))\n            doc.append(rub)\n            return doc, doc.index(rub) + 1\n\n        if show_urls == 'no':\n            return\n        if show_urls == 'footnote':\n            doc = tree.traverse(nodes.document)[0]\n            fn_spot, fn_idx = footnote_spot(tree)\n            nr = 1\n        for node in tree.traverse(nodes.reference):\n            uri = node.get('refuri', '')\n            if (uri.startswith('http:') or uri.startswith('https:') or\n                    uri.startswith('ftp:')) and uri not in node.astext():\n                idx = node.parent.index(node) + 1\n                if show_urls == 'inline':\n                    uri = self.link_target_template % {'uri': uri}\n                    link = nodes.inline(uri, uri)\n                    link['classes'].append(self.css_link_target_class)\n                    node.parent.insert(idx, link)\n                elif show_urls == 'footnote':\n                    label = FOOTNOTE_LABEL_TEMPLATE % nr\n                    nr += 1\n                    footnote_ref = make_footnote_ref(doc, label)\n                    node.parent.insert(idx, footnote_ref)\n                    footnote = make_footnote(doc, label, uri)\n                    fn_spot.insert(fn_idx, footnote)\n                    footnote_ref['refid'] = footnote['ids'][0]\n                    footnote.add_backref(footnote_ref['ids'][0])\n                    fn_idx += 1\n\n    def write_doc(self, docname: str, doctree: nodes.document) -> None:\n        \"\"\"Write one document file.\n\n        This method is overwritten in order to fix fragment identifiers\n        and to add visible external links.\n        \"\"\"\n        self.fix_ids(doctree)\n        self.add_visible_links(doctree, self.config.epub_show_urls)\n        super().write_doc(docname, doctree)\n\n    def fix_genindex(self, tree: List[Tuple[str, List[Tuple[str, Any]]]]) -> None:\n        \"\"\"Fix href attributes for genindex pages.\"\"\"\n        # XXX: modifies tree inline\n        # Logic modeled from themes/basic/genindex.html\n        for key, columns in tree:\n            for entryname, (links, subitems, key_) in columns:\n                for (i, (ismain, link)) in enumerate(links):\n                    m = self.refuri_re.match(link)\n                    if m:\n                        links[i] = (ismain,\n                                    self.fix_fragment(m.group(1), m.group(2)))\n                for subentryname, subentrylinks in subitems:\n                    for (i, (ismain, link)) in enumerate(subentrylinks):\n                        m = self.refuri_re.match(link)\n                        if m:\n                            subentrylinks[i] = (ismain,\n                                                self.fix_fragment(m.group(1), m.group(2)))\n\n    def is_vector_graphics(self, filename: str) -> bool:\n        \"\"\"Does the filename extension indicate a vector graphic format?\"\"\"\n        ext = path.splitext(filename)[-1]\n        return ext in VECTOR_GRAPHICS_EXTENSIONS\n\n    def copy_image_files_pil(self) -> None:\n        \"\"\"Copy images using Pillow, the Python Imaging Library.\n        The method tries to read and write the files with Pillow, converting\n        the format and resizing the image if necessary/possible.\n        \"\"\"\n        ensuredir(path.join(self.outdir, self.imagedir))\n        for src in status_iterator(self.images, __('copying images... '), \"brown\",\n                                   len(self.images), self.app.verbosity):\n            dest = self.images[src]\n            try:\n                img = Image.open(path.join(self.srcdir, src))\n            except OSError:\n                if not self.is_vector_graphics(src):\n                    logger.warning(__('cannot read image file %r: copying it instead'),\n                                   path.join(self.srcdir, src))\n                try:\n                    copyfile(path.join(self.srcdir, src),\n                             path.join(self.outdir, self.imagedir, dest))\n                except OSError as err:\n                    logger.warning(__('cannot copy image file %r: %s'),\n                                   path.join(self.srcdir, src), err)\n                continue\n            if self.config.epub_fix_images:\n                if img.mode in ('P',):\n                    # See the Pillow documentation for Image.convert()\n                    # https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.convert\n                    img = img.convert()\n            if self.config.epub_max_image_width > 0:\n                (width, height) = img.size\n                nw = self.config.epub_max_image_width\n                if width > nw:\n                    nh = (height * nw) / width\n                    img = img.resize((nw, nh), Image.BICUBIC)\n            try:\n                img.save(path.join(self.outdir, self.imagedir, dest))\n            except OSError as err:\n                logger.warning(__('cannot write image file %r: %s'),\n                               path.join(self.srcdir, src), err)\n\n    def copy_image_files(self) -> None:\n        \"\"\"Copy image files to destination directory.\n        This overwritten method can use Pillow to convert image files.\n        \"\"\"\n        if self.images:\n            if self.config.epub_fix_images or self.config.epub_max_image_width:\n                if not Image:\n                    logger.warning(__('Pillow not found - copying image files'))\n                    super().copy_image_files()\n                else:\n                    self.copy_image_files_pil()\n            else:\n                super().copy_image_files()\n\n    def copy_download_files(self) -> None:\n        pass\n\n    def handle_page(self, pagename: str, addctx: Dict, templatename: str = 'page.html',\n                    outfilename: str = None, event_arg: Any = None) -> None:\n        \"\"\"Create a rendered page.\n\n        This method is overwritten for genindex pages in order to fix href link\n        attributes.\n        \"\"\"\n        if pagename.startswith('genindex') and 'genindexentries' in addctx:\n            if not self.use_index:\n                return\n            self.fix_genindex(addctx['genindexentries'])\n        addctx['doctype'] = self.doctype\n        super().handle_page(pagename, addctx, templatename, outfilename, event_arg)\n\n    def build_mimetype(self, outdir: str = None, outname: str = 'mimetype') -> None:\n        \"\"\"Write the metainfo file mimetype.\"\"\"\n        if outdir:\n            warnings.warn('The arguments of EpubBuilder.build_mimetype() is deprecated.',\n                          RemovedInSphinx40Warning, stacklevel=2)\n        else:\n            outdir = self.outdir\n\n        logger.info(__('writing %s file...'), outname)\n        copy_asset_file(path.join(self.template_dir, 'mimetype'),\n                        path.join(outdir, outname))\n\n    def build_container(self, outdir: str = None, outname: str = 'META-INF/container.xml') -> None:  # NOQA\n        \"\"\"Write the metainfo file META-INF/container.xml.\"\"\"\n        if outdir:\n            warnings.warn('The arguments of EpubBuilder.build_container() is deprecated.',\n                          RemovedInSphinx40Warning, stacklevel=2)\n        else:\n            outdir = self.outdir\n\n        logger.info(__('writing %s file...'), outname)\n        filename = path.join(outdir, outname)\n        ensuredir(path.dirname(filename))\n        copy_asset_file(path.join(self.template_dir, 'container.xml'), filename)\n\n    def content_metadata(self) -> Dict[str, Any]:\n        \"\"\"Create a dictionary with all metadata for the content.opf\n        file properly escaped.\n        \"\"\"\n        metadata = {}  # type: Dict[str, Any]\n        metadata['title'] = html.escape(self.config.epub_title)\n        metadata['author'] = html.escape(self.config.epub_author)\n        metadata['uid'] = html.escape(self.config.epub_uid)\n        metadata['lang'] = html.escape(self.config.epub_language)\n        metadata['publisher'] = html.escape(self.config.epub_publisher)\n        metadata['copyright'] = html.escape(self.config.epub_copyright)\n        metadata['scheme'] = html.escape(self.config.epub_scheme)\n        metadata['id'] = html.escape(self.config.epub_identifier)\n        metadata['date'] = html.escape(format_date(\"%Y-%m-%d\"))\n        metadata['manifest_items'] = []\n        metadata['spines'] = []\n        metadata['guides'] = []\n        return metadata\n\n    def build_content(self, outdir: str = None, outname: str = 'content.opf') -> None:\n        \"\"\"Write the metainfo file content.opf It contains bibliographic data,\n        a file list and the spine (the reading order).\n        \"\"\"\n        if outdir:\n            warnings.warn('The arguments of EpubBuilder.build_content() is deprecated.',\n                          RemovedInSphinx40Warning, stacklevel=2)\n        else:\n            outdir = self.outdir\n\n        logger.info(__('writing %s file...'), outname)\n        metadata = self.content_metadata()\n\n        # files\n        if not outdir.endswith(os.sep):\n            outdir += os.sep\n        olen = len(outdir)\n        self.files = []  # type: List[str]\n        self.ignored_files = ['.buildinfo', 'mimetype', 'content.opf',\n                              'toc.ncx', 'META-INF/container.xml',\n                              'Thumbs.db', 'ehthumbs.db', '.DS_Store',\n                              'nav.xhtml', self.config.epub_basename + '.epub'] + \\\n            self.config.epub_exclude_files\n        if not self.use_index:\n            self.ignored_files.append('genindex' + self.out_suffix)\n        for root, dirs, files in os.walk(outdir):\n            dirs.sort()\n            for fn in sorted(files):\n                filename = path.join(root, fn)[olen:]\n                if filename in self.ignored_files:\n                    continue\n                ext = path.splitext(filename)[-1]\n                if ext not in self.media_types:\n                    # we always have JS and potentially OpenSearch files, don't\n                    # always warn about them\n                    if ext not in ('.js', '.xml'):\n                        logger.warning(__('unknown mimetype for %s, ignoring'), filename,\n                                       type='epub', subtype='unknown_project_files')\n                    continue\n                filename = filename.replace(os.sep, '/')\n                item = ManifestItem(html.escape(filename),\n                                    html.escape(self.make_id(filename)),\n                                    html.escape(self.media_types[ext]))\n                metadata['manifest_items'].append(item)\n                self.files.append(filename)\n\n        # spine\n        spinefiles = set()\n        for refnode in self.refnodes:\n            if '#' in refnode['refuri']:\n                continue\n            if refnode['refuri'] in self.ignored_files:\n                continue\n            spine = Spine(html.escape(self.make_id(refnode['refuri'])), True)\n            metadata['spines'].append(spine)\n            spinefiles.add(refnode['refuri'])\n        for info in self.domain_indices:\n            spine = Spine(html.escape(self.make_id(info[0] + self.out_suffix)), True)\n            metadata['spines'].append(spine)\n            spinefiles.add(info[0] + self.out_suffix)\n        if self.use_index:\n            spine = Spine(html.escape(self.make_id('genindex' + self.out_suffix)), True)\n            metadata['spines'].append(spine)\n            spinefiles.add('genindex' + self.out_suffix)\n        # add auto generated files\n        for name in self.files:\n            if name not in spinefiles and name.endswith(self.out_suffix):\n                spine = Spine(html.escape(self.make_id(name)), False)\n                metadata['spines'].append(spine)\n\n        # add the optional cover\n        html_tmpl = None\n        if self.config.epub_cover:\n            image, html_tmpl = self.config.epub_cover\n            image = image.replace(os.sep, '/')\n            metadata['cover'] = html.escape(self.make_id(image))\n            if html_tmpl:\n                spine = Spine(html.escape(self.make_id(self.coverpage_name)), True)\n                metadata['spines'].insert(0, spine)\n                if self.coverpage_name not in self.files:\n                    ext = path.splitext(self.coverpage_name)[-1]\n                    self.files.append(self.coverpage_name)\n                    item = ManifestItem(html.escape(self.coverpage_name),\n                                        html.escape(self.make_id(self.coverpage_name)),\n                                        html.escape(self.media_types[ext]))\n                    metadata['manifest_items'].append(item)\n                ctx = {'image': html.escape(image), 'title': self.config.project}\n                self.handle_page(\n                    path.splitext(self.coverpage_name)[0], ctx, html_tmpl)\n                spinefiles.add(self.coverpage_name)\n\n        auto_add_cover = True\n        auto_add_toc = True\n        if self.config.epub_guide:\n            for type, uri, title in self.config.epub_guide:\n                file = uri.split('#')[0]\n                if file not in self.files:\n                    self.files.append(file)\n                if type == 'cover':\n                    auto_add_cover = False\n                if type == 'toc':\n                    auto_add_toc = False\n                metadata['guides'].append(Guide(html.escape(type),\n                                                html.escape(title),\n                                                html.escape(uri)))\n        if auto_add_cover and html_tmpl:\n            metadata['guides'].append(Guide('cover',\n                                            self.guide_titles['cover'],\n                                            html.escape(self.coverpage_name)))\n        if auto_add_toc and self.refnodes:\n            metadata['guides'].append(Guide('toc',\n                                            self.guide_titles['toc'],\n                                            html.escape(self.refnodes[0]['refuri'])))\n\n        # write the project file\n        copy_asset_file(path.join(self.template_dir, 'content.opf_t'),\n                        path.join(outdir, outname),\n                        metadata)\n\n    def new_navpoint(self, node: Dict[str, Any], level: int, incr: bool = True) -> NavPoint:\n        \"\"\"Create a new entry in the toc from the node at given level.\"\"\"\n        # XXX Modifies the node\n        if incr:\n            self.playorder += 1\n        self.tocid += 1\n        return NavPoint('navPoint%d' % self.tocid, self.playorder,\n                        node['text'], node['refuri'], [])\n\n    def build_navpoints(self, nodes: List[Dict[str, Any]]) -> List[NavPoint]:\n        \"\"\"Create the toc navigation structure.\n\n        Subelements of a node are nested inside the navpoint.  For nested nodes\n        the parent node is reinserted in the subnav.\n        \"\"\"\n        navstack = []  # type: List[NavPoint]\n        navstack.append(NavPoint('dummy', '', '', '', []))\n        level = 0\n        lastnode = None\n        for node in nodes:\n            if not node['text']:\n                continue\n            file = node['refuri'].split('#')[0]\n            if file in self.ignored_files:\n                continue\n            if node['level'] > self.config.epub_tocdepth:\n                continue\n            if node['level'] == level:\n                navpoint = self.new_navpoint(node, level)\n                navstack.pop()\n                navstack[-1].children.append(navpoint)\n                navstack.append(navpoint)\n            elif node['level'] == level + 1:\n                level += 1\n                if lastnode and self.config.epub_tocdup:\n                    # Insert starting point in subtoc with same playOrder\n                    navstack[-1].children.append(self.new_navpoint(lastnode, level, False))\n                navpoint = self.new_navpoint(node, level)\n                navstack[-1].children.append(navpoint)\n                navstack.append(navpoint)\n            elif node['level'] < level:\n                while node['level'] < len(navstack):\n                    navstack.pop()\n                level = node['level']\n                navpoint = self.new_navpoint(node, level)\n                navstack[-1].children.append(navpoint)\n                navstack.append(navpoint)\n            else:\n                raise\n            lastnode = node\n\n        return navstack[0].children\n\n    def toc_metadata(self, level: int, navpoints: List[NavPoint]) -> Dict[str, Any]:\n        \"\"\"Create a dictionary with all metadata for the toc.ncx file\n        properly escaped.\n        \"\"\"\n        metadata = {}  # type: Dict[str, Any]\n        metadata['uid'] = self.config.epub_uid\n        metadata['title'] = html.escape(self.config.epub_title)\n        metadata['level'] = level\n        metadata['navpoints'] = navpoints\n        return metadata\n\n    def build_toc(self, outdir: str = None, outname: str = 'toc.ncx') -> None:\n        \"\"\"Write the metainfo file toc.ncx.\"\"\"\n        if outdir:\n            warnings.warn('The arguments of EpubBuilder.build_toc() is deprecated.',\n                          RemovedInSphinx40Warning, stacklevel=2)\n        else:\n            outdir = self.outdir\n\n        logger.info(__('writing %s file...'), outname)\n\n        if self.config.epub_tocscope == 'default':\n            doctree = self.env.get_and_resolve_doctree(self.config.master_doc,\n                                                       self, prune_toctrees=False,\n                                                       includehidden=False)\n            refnodes = self.get_refnodes(doctree, [])\n            self.toc_add_files(refnodes)\n        else:\n            # 'includehidden'\n            refnodes = self.refnodes\n        self.check_refnodes(refnodes)\n        navpoints = self.build_navpoints(refnodes)\n        level = max(item['level'] for item in self.refnodes)\n        level = min(level, self.config.epub_tocdepth)\n        copy_asset_file(path.join(self.template_dir, 'toc.ncx_t'),\n                        path.join(outdir, outname),\n                        self.toc_metadata(level, navpoints))\n\n    def build_epub(self, outdir: str = None, outname: str = None) -> None:\n        \"\"\"Write the epub file.\n\n        It is a zip file with the mimetype file stored uncompressed as the first\n        entry.\n        \"\"\"\n        if outdir:\n            warnings.warn('The arguments of EpubBuilder.build_epub() is deprecated.',\n                          RemovedInSphinx40Warning, stacklevel=2)\n        else:\n            outdir = self.outdir\n            outname = self.config.epub_basename + '.epub'\n\n        logger.info(__('writing %s file...'), outname)\n        epub_filename = path.join(outdir, outname)\n        with ZipFile(epub_filename, 'w', ZIP_DEFLATED) as epub:\n            epub.write(path.join(outdir, 'mimetype'), 'mimetype', ZIP_STORED)\n            for filename in ['META-INF/container.xml', 'content.opf', 'toc.ncx']:\n                epub.write(path.join(outdir, filename), filename, ZIP_DEFLATED)\n            for filename in self.files:\n                epub.write(path.join(outdir, filename), filename, ZIP_DEFLATED)"}
{"model_name_or_path": "agentless", "instance_id": "sphinx-doc__sphinx-9230", "model_patch": "diff --git a/sphinx/domains/python.py b/sphinx/domains/python.py\nindex b8c505d..34d9ee7 100644\n--- a/sphinx/domains/python.py\n+++ b/sphinx/domains/python.py\n@@ -207,7 +207,13 @@ def _parse_arglist(arglist: str, env: BuildEnvironment = None) -> addnodes.desc_\n             node += addnodes.desc_sig_name('', param.name)\n \n         if param.annotation is not param.empty:\n-            children = _parse_annotation(param.annotation, env)\n+            # Handle dictionary types explicitly to ensure correct formatting\n+            if isinstance(param.annotation, str) and param.annotation.startswith('dict'):\n+                # Handle the dict annotation separately if needed\n+                dict_annotation = param.annotation.replace('dict', 'dict(') + ')'\n+                children = _parse_annotation(dict_annotation, env)\n+            else:\n+                children = _parse_annotation(param.annotation, env)\n             node += addnodes.desc_sig_punctuation('', ':')\n             node += nodes.Text(' ')\n             node += addnodes.desc_sig_name('', '', *children)  # type: ignore\n", "raw_model_patch": "diff --git a/sphinx/domains/python.py b/sphinx/domains/python.py\nindex b8c505d..34d9ee7 100644\n--- a/sphinx/domains/python.py\n+++ b/sphinx/domains/python.py\n@@ -207,7 +207,13 @@ def _parse_arglist(arglist: str, env: BuildEnvironment = None) -> addnodes.desc_\n             node += addnodes.desc_sig_name('', param.name)\n \n         if param.annotation is not param.empty:\n-            children = _parse_annotation(param.annotation, env)\n+            # Handle dictionary types explicitly to ensure correct formatting\n+            if isinstance(param.annotation, str) and param.annotation.startswith('dict'):\n+                # Handle the dict annotation separately if needed\n+                dict_annotation = param.annotation.replace('dict', 'dict(') + ')'\n+                children = _parse_annotation(dict_annotation, env)\n+            else:\n+                children = _parse_annotation(param.annotation, env)\n             node += addnodes.desc_sig_punctuation('', ':')\n             node += nodes.Text(' ')\n             node += addnodes.desc_sig_name('', '', *children)  # type: ignore\n", "original_file_content": "\"\"\"\n    sphinx.domains.python\n    ~~~~~~~~~~~~~~~~~~~~~\n\n    The Python domain.\n\n    :copyright: Copyright 2007-2021 by the Sphinx team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport builtins\nimport inspect\nimport re\nimport sys\nimport typing\nimport warnings\nfrom inspect import Parameter\nfrom typing import Any, Dict, Iterable, Iterator, List, NamedTuple, Optional, Tuple, Type, cast\n\nfrom docutils import nodes\nfrom docutils.nodes import Element, Node\nfrom docutils.parsers.rst import directives\n\nfrom sphinx import addnodes\nfrom sphinx.addnodes import desc_signature, pending_xref, pending_xref_condition\nfrom sphinx.application import Sphinx\nfrom sphinx.builders import Builder\nfrom sphinx.deprecation import RemovedInSphinx50Warning\nfrom sphinx.directives import ObjectDescription\nfrom sphinx.domains import Domain, Index, IndexEntry, ObjType\nfrom sphinx.environment import BuildEnvironment\nfrom sphinx.locale import _, __\nfrom sphinx.pycode.ast import ast\nfrom sphinx.pycode.ast import parse as ast_parse\nfrom sphinx.roles import XRefRole\nfrom sphinx.util import logging\nfrom sphinx.util.docfields import Field, GroupedField, TypedField\nfrom sphinx.util.docutils import SphinxDirective\nfrom sphinx.util.inspect import signature_from_str\nfrom sphinx.util.nodes import find_pending_xref_condition, make_id, make_refnode\nfrom sphinx.util.typing import OptionSpec, TextlikeNode\n\nlogger = logging.getLogger(__name__)\n\n\n# REs for Python signatures\npy_sig_re = re.compile(\n    r'''^ ([\\w.]*\\.)?            # class name(s)\n          (\\w+)  \\s*             # thing name\n          (?: \\(\\s*(.*)\\s*\\)     # optional: arguments\n           (?:\\s* -> \\s* (.*))?  #           return annotation\n          )? $                   # and nothing more\n          ''', re.VERBOSE)\n\n\npairindextypes = {\n    'module':    _('module'),\n    'keyword':   _('keyword'),\n    'operator':  _('operator'),\n    'object':    _('object'),\n    'exception': _('exception'),\n    'statement': _('statement'),\n    'builtin':   _('built-in function'),\n}\n\n\nclass ObjectEntry(NamedTuple):\n    docname: str\n    node_id: str\n    objtype: str\n    aliased: bool\n\n\nclass ModuleEntry(NamedTuple):\n    docname: str\n    node_id: str\n    synopsis: str\n    platform: str\n    deprecated: bool\n\n\ndef type_to_xref(text: str, env: BuildEnvironment = None) -> addnodes.pending_xref:\n    \"\"\"Convert a type string to a cross reference node.\"\"\"\n    if text == 'None':\n        reftype = 'obj'\n    else:\n        reftype = 'class'\n\n    if env:\n        kwargs = {'py:module': env.ref_context.get('py:module'),\n                  'py:class': env.ref_context.get('py:class')}\n    else:\n        kwargs = {}\n\n    if env.config.python_use_unqualified_type_names:\n        # Note: It would be better to use qualname to describe the object to support support\n        # nested classes.  But python domain can't access the real python object because this\n        # module should work not-dynamically.\n        shortname = text.split('.')[-1]\n        contnodes: List[Node] = [pending_xref_condition('', shortname, condition='resolved'),\n                                 pending_xref_condition('', text, condition='*')]\n    else:\n        contnodes = [nodes.Text(text)]\n\n    return pending_xref('', *contnodes,\n                        refdomain='py', reftype=reftype, reftarget=text, **kwargs)\n\n\ndef _parse_annotation(annotation: str, env: BuildEnvironment = None) -> List[Node]:\n    \"\"\"Parse type annotation.\"\"\"\n    def unparse(node: ast.AST) -> List[Node]:\n        if isinstance(node, ast.Attribute):\n            return [nodes.Text(\"%s.%s\" % (unparse(node.value)[0], node.attr))]\n        elif isinstance(node, ast.BinOp):\n            result: List[Node] = unparse(node.left)\n            result.extend(unparse(node.op))\n            result.extend(unparse(node.right))\n            return result\n        elif isinstance(node, ast.BitOr):\n            return [nodes.Text(' '), addnodes.desc_sig_punctuation('', '|'), nodes.Text(' ')]\n        elif isinstance(node, ast.Constant):  # type: ignore\n            if node.value is Ellipsis:\n                return [addnodes.desc_sig_punctuation('', \"...\")]\n            else:\n                return [nodes.Text(node.value)]\n        elif isinstance(node, ast.Expr):\n            return unparse(node.value)\n        elif isinstance(node, ast.Index):\n            return unparse(node.value)\n        elif isinstance(node, ast.List):\n            result = [addnodes.desc_sig_punctuation('', '[')]\n            for elem in node.elts:\n                result.extend(unparse(elem))\n                result.append(addnodes.desc_sig_punctuation('', ', '))\n            result.pop()\n            result.append(addnodes.desc_sig_punctuation('', ']'))\n            return result\n        elif isinstance(node, ast.Module):\n            return sum((unparse(e) for e in node.body), [])\n        elif isinstance(node, ast.Name):\n            return [nodes.Text(node.id)]\n        elif isinstance(node, ast.Subscript):\n            result = unparse(node.value)\n            result.append(addnodes.desc_sig_punctuation('', '['))\n            result.extend(unparse(node.slice))\n            result.append(addnodes.desc_sig_punctuation('', ']'))\n            return result\n        elif isinstance(node, ast.Tuple):\n            if node.elts:\n                result = []\n                for elem in node.elts:\n                    result.extend(unparse(elem))\n                    result.append(addnodes.desc_sig_punctuation('', ', '))\n                result.pop()\n            else:\n                result = [addnodes.desc_sig_punctuation('', '('),\n                          addnodes.desc_sig_punctuation('', ')')]\n\n            return result\n        else:\n            if sys.version_info < (3, 8):\n                if isinstance(node, ast.Ellipsis):\n                    return [addnodes.desc_sig_punctuation('', \"...\")]\n                elif isinstance(node, ast.NameConstant):\n                    return [nodes.Text(node.value)]\n\n            raise SyntaxError  # unsupported syntax\n\n    if env is None:\n        warnings.warn(\"The env parameter for _parse_annotation becomes required now.\",\n                      RemovedInSphinx50Warning, stacklevel=2)\n\n    try:\n        tree = ast_parse(annotation)\n        result = unparse(tree)\n        for i, node in enumerate(result):\n            if isinstance(node, nodes.Text) and node.strip():\n                result[i] = type_to_xref(str(node), env)\n        return result\n    except SyntaxError:\n        return [type_to_xref(annotation, env)]\n\n\ndef _parse_arglist(arglist: str, env: BuildEnvironment = None) -> addnodes.desc_parameterlist:\n    \"\"\"Parse a list of arguments using AST parser\"\"\"\n    params = addnodes.desc_parameterlist(arglist)\n    sig = signature_from_str('(%s)' % arglist)\n    last_kind = None\n    for param in sig.parameters.values():\n        if param.kind != param.POSITIONAL_ONLY and last_kind == param.POSITIONAL_ONLY:\n            # PEP-570: Separator for Positional Only Parameter: /\n            params += addnodes.desc_parameter('', '', addnodes.desc_sig_operator('', '/'))\n        if param.kind == param.KEYWORD_ONLY and last_kind in (param.POSITIONAL_OR_KEYWORD,\n                                                              param.POSITIONAL_ONLY,\n                                                              None):\n            # PEP-3102: Separator for Keyword Only Parameter: *\n            params += addnodes.desc_parameter('', '', addnodes.desc_sig_operator('', '*'))\n\n        node = addnodes.desc_parameter()\n        if param.kind == param.VAR_POSITIONAL:\n            node += addnodes.desc_sig_operator('', '*')\n            node += addnodes.desc_sig_name('', param.name)\n        elif param.kind == param.VAR_KEYWORD:\n            node += addnodes.desc_sig_operator('', '**')\n            node += addnodes.desc_sig_name('', param.name)\n        else:\n            node += addnodes.desc_sig_name('', param.name)\n\n        if param.annotation is not param.empty:\n            children = _parse_annotation(param.annotation, env)\n            node += addnodes.desc_sig_punctuation('', ':')\n            node += nodes.Text(' ')\n            node += addnodes.desc_sig_name('', '', *children)  # type: ignore\n        if param.default is not param.empty:\n            if param.annotation is not param.empty:\n                node += nodes.Text(' ')\n                node += addnodes.desc_sig_operator('', '=')\n                node += nodes.Text(' ')\n            else:\n                node += addnodes.desc_sig_operator('', '=')\n            node += nodes.inline('', param.default, classes=['default_value'],\n                                 support_smartquotes=False)\n\n        params += node\n        last_kind = param.kind\n\n    if last_kind == Parameter.POSITIONAL_ONLY:\n        # PEP-570: Separator for Positional Only Parameter: /\n        params += addnodes.desc_parameter('', '', addnodes.desc_sig_operator('', '/'))\n\n    return params\n\n\ndef _pseudo_parse_arglist(signode: desc_signature, arglist: str) -> None:\n    \"\"\"\"Parse\" a list of arguments separated by commas.\n\n    Arguments can have \"optional\" annotations given by enclosing them in\n    brackets.  Currently, this will split at any comma, even if it's inside a\n    string literal (e.g. default argument value).\n    \"\"\"\n    paramlist = addnodes.desc_parameterlist()\n    stack: List[Element] = [paramlist]\n    try:\n        for argument in arglist.split(','):\n            argument = argument.strip()\n            ends_open = ends_close = 0\n            while argument.startswith('['):\n                stack.append(addnodes.desc_optional())\n                stack[-2] += stack[-1]\n                argument = argument[1:].strip()\n            while argument.startswith(']'):\n                stack.pop()\n                argument = argument[1:].strip()\n            while argument.endswith(']') and not argument.endswith('[]'):\n                ends_close += 1\n                argument = argument[:-1].strip()\n            while argument.endswith('['):\n                ends_open += 1\n                argument = argument[:-1].strip()\n            if argument:\n                stack[-1] += addnodes.desc_parameter(argument, argument)\n            while ends_open:\n                stack.append(addnodes.desc_optional())\n                stack[-2] += stack[-1]\n                ends_open -= 1\n            while ends_close:\n                stack.pop()\n                ends_close -= 1\n        if len(stack) != 1:\n            raise IndexError\n    except IndexError:\n        # if there are too few or too many elements on the stack, just give up\n        # and treat the whole argument list as one argument, discarding the\n        # already partially populated paramlist node\n        paramlist = addnodes.desc_parameterlist()\n        paramlist += addnodes.desc_parameter(arglist, arglist)\n        signode += paramlist\n    else:\n        signode += paramlist\n\n\n# This override allows our inline type specifiers to behave like :class: link\n# when it comes to handling \".\" and \"~\" prefixes.\nclass PyXrefMixin:\n    def make_xref(self, rolename: str, domain: str, target: str,\n                  innernode: Type[TextlikeNode] = nodes.emphasis,\n                  contnode: Node = None, env: BuildEnvironment = None) -> Node:\n        result = super().make_xref(rolename, domain, target,  # type: ignore\n                                   innernode, contnode, env)\n        result['refspecific'] = True\n        result['py:module'] = env.ref_context.get('py:module')\n        result['py:class'] = env.ref_context.get('py:class')\n        if target.startswith(('.', '~')):\n            prefix, result['reftarget'] = target[0], target[1:]\n            if prefix == '.':\n                text = target[1:]\n            elif prefix == '~':\n                text = target.split('.')[-1]\n            for node in result.traverse(nodes.Text):\n                node.parent[node.parent.index(node)] = nodes.Text(text)\n                break\n        return result\n\n    def make_xrefs(self, rolename: str, domain: str, target: str,\n                   innernode: Type[TextlikeNode] = nodes.emphasis,\n                   contnode: Node = None, env: BuildEnvironment = None) -> List[Node]:\n        delims = r'(\\s*[\\[\\]\\(\\),](?:\\s*or\\s)?\\s*|\\s+or\\s+|\\.\\.\\.)'\n        delims_re = re.compile(delims)\n        sub_targets = re.split(delims, target)\n\n        split_contnode = bool(contnode and contnode.astext() == target)\n\n        results = []\n        for sub_target in filter(None, sub_targets):\n            if split_contnode:\n                contnode = nodes.Text(sub_target)\n\n            if delims_re.match(sub_target):\n                results.append(contnode or innernode(sub_target, sub_target))\n            else:\n                results.append(self.make_xref(rolename, domain, sub_target,\n                                              innernode, contnode, env))\n\n        return results\n\n\nclass PyField(PyXrefMixin, Field):\n    def make_xref(self, rolename: str, domain: str, target: str,\n                  innernode: Type[TextlikeNode] = nodes.emphasis,\n                  contnode: Node = None, env: BuildEnvironment = None) -> Node:\n        if rolename == 'class' and target == 'None':\n            # None is not a type, so use obj role instead.\n            rolename = 'obj'\n\n        return super().make_xref(rolename, domain, target, innernode, contnode, env)\n\n\nclass PyGroupedField(PyXrefMixin, GroupedField):\n    pass\n\n\nclass PyTypedField(PyXrefMixin, TypedField):\n    def make_xref(self, rolename: str, domain: str, target: str,\n                  innernode: Type[TextlikeNode] = nodes.emphasis,\n                  contnode: Node = None, env: BuildEnvironment = None) -> Node:\n        if rolename == 'class' and target == 'None':\n            # None is not a type, so use obj role instead.\n            rolename = 'obj'\n\n        return super().make_xref(rolename, domain, target, innernode, contnode, env)\n\n\nclass PyObject(ObjectDescription[Tuple[str, str]]):\n    \"\"\"\n    Description of a general Python object.\n\n    :cvar allow_nesting: Class is an object that allows for nested namespaces\n    :vartype allow_nesting: bool\n    \"\"\"\n    option_spec: OptionSpec = {\n        'noindex': directives.flag,\n        'noindexentry': directives.flag,\n        'module': directives.unchanged,\n        'canonical': directives.unchanged,\n        'annotation': directives.unchanged,\n    }\n\n    doc_field_types = [\n        PyTypedField('parameter', label=_('Parameters'),\n                     names=('param', 'parameter', 'arg', 'argument',\n                            'keyword', 'kwarg', 'kwparam'),\n                     typerolename='class', typenames=('paramtype', 'type'),\n                     can_collapse=True),\n        PyTypedField('variable', label=_('Variables'),\n                     names=('var', 'ivar', 'cvar'),\n                     typerolename='class', typenames=('vartype',),\n                     can_collapse=True),\n        PyGroupedField('exceptions', label=_('Raises'), rolename='exc',\n                       names=('raises', 'raise', 'exception', 'except'),\n                       can_collapse=True),\n        Field('returnvalue', label=_('Returns'), has_arg=False,\n              names=('returns', 'return')),\n        PyField('returntype', label=_('Return type'), has_arg=False,\n                names=('rtype',), bodyrolename='class'),\n    ]\n\n    allow_nesting = False\n\n    def get_signature_prefix(self, sig: str) -> str:\n        \"\"\"May return a prefix to put before the object name in the\n        signature.\n        \"\"\"\n        return ''\n\n    def needs_arglist(self) -> bool:\n        \"\"\"May return true if an empty argument list is to be generated even if\n        the document contains none.\n        \"\"\"\n        return False\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        \"\"\"Transform a Python signature into RST nodes.\n\n        Return (fully qualified name of the thing, classname if any).\n\n        If inside a class, the current class name is handled intelligently:\n        * it is stripped from the displayed name if present\n        * it is added to the full name (return value) if not present\n        \"\"\"\n        m = py_sig_re.match(sig)\n        if m is None:\n            raise ValueError\n        prefix, name, arglist, retann = m.groups()\n\n        # determine module and class name (if applicable), as well as full name\n        modname = self.options.get('module', self.env.ref_context.get('py:module'))\n        classname = self.env.ref_context.get('py:class')\n        if classname:\n            add_module = False\n            if prefix and (prefix == classname or\n                           prefix.startswith(classname + \".\")):\n                fullname = prefix + name\n                # class name is given again in the signature\n                prefix = prefix[len(classname):].lstrip('.')\n            elif prefix:\n                # class name is given in the signature, but different\n                # (shouldn't happen)\n                fullname = classname + '.' + prefix + name\n            else:\n                # class name is not given in the signature\n                fullname = classname + '.' + name\n        else:\n            add_module = True\n            if prefix:\n                classname = prefix.rstrip('.')\n                fullname = prefix + name\n            else:\n                classname = ''\n                fullname = name\n\n        signode['module'] = modname\n        signode['class'] = classname\n        signode['fullname'] = fullname\n\n        sig_prefix = self.get_signature_prefix(sig)\n        if sig_prefix:\n            signode += addnodes.desc_annotation(sig_prefix, sig_prefix)\n\n        if prefix:\n            signode += addnodes.desc_addname(prefix, prefix)\n        elif add_module and self.env.config.add_module_names:\n            if modname and modname != 'exceptions':\n                # exceptions are a special case, since they are documented in the\n                # 'exceptions' module.\n                nodetext = modname + '.'\n                signode += addnodes.desc_addname(nodetext, nodetext)\n\n        signode += addnodes.desc_name(name, name)\n        if arglist:\n            try:\n                signode += _parse_arglist(arglist, self.env)\n            except SyntaxError:\n                # fallback to parse arglist original parser.\n                # it supports to represent optional arguments (ex. \"func(foo [, bar])\")\n                _pseudo_parse_arglist(signode, arglist)\n            except NotImplementedError as exc:\n                logger.warning(\"could not parse arglist (%r): %s\", arglist, exc,\n                               location=signode)\n                _pseudo_parse_arglist(signode, arglist)\n        else:\n            if self.needs_arglist():\n                # for callables, add an empty parameter list\n                signode += addnodes.desc_parameterlist()\n\n        if retann:\n            children = _parse_annotation(retann, self.env)\n            signode += addnodes.desc_returns(retann, '', *children)\n\n        anno = self.options.get('annotation')\n        if anno:\n            signode += addnodes.desc_annotation(' ' + anno, ' ' + anno)\n\n        return fullname, prefix\n\n    def get_index_text(self, modname: str, name: Tuple[str, str]) -> str:\n        \"\"\"Return the text for the index entry of the object.\"\"\"\n        raise NotImplementedError('must be implemented in subclasses')\n\n    def add_target_and_index(self, name_cls: Tuple[str, str], sig: str,\n                             signode: desc_signature) -> None:\n        modname = self.options.get('module', self.env.ref_context.get('py:module'))\n        fullname = (modname + '.' if modname else '') + name_cls[0]\n        node_id = make_id(self.env, self.state.document, '', fullname)\n        signode['ids'].append(node_id)\n\n        # Assign old styled node_id(fullname) not to break old hyperlinks (if possible)\n        # Note: Will removed in Sphinx-5.0  (RemovedInSphinx50Warning)\n        if node_id != fullname and fullname not in self.state.document.ids:\n            signode['ids'].append(fullname)\n\n        self.state.document.note_explicit_target(signode)\n\n        domain = cast(PythonDomain, self.env.get_domain('py'))\n        domain.note_object(fullname, self.objtype, node_id, location=signode)\n\n        canonical_name = self.options.get('canonical')\n        if canonical_name:\n            domain.note_object(canonical_name, self.objtype, node_id, aliased=True,\n                               location=signode)\n\n        if 'noindexentry' not in self.options:\n            indextext = self.get_index_text(modname, name_cls)\n            if indextext:\n                self.indexnode['entries'].append(('single', indextext, node_id, '', None))\n\n    def before_content(self) -> None:\n        \"\"\"Handle object nesting before content\n\n        :py:class:`PyObject` represents Python language constructs. For\n        constructs that are nestable, such as a Python classes, this method will\n        build up a stack of the nesting hierarchy so that it can be later\n        de-nested correctly, in :py:meth:`after_content`.\n\n        For constructs that aren't nestable, the stack is bypassed, and instead\n        only the most recent object is tracked. This object prefix name will be\n        removed with :py:meth:`after_content`.\n        \"\"\"\n        prefix = None\n        if self.names:\n            # fullname and name_prefix come from the `handle_signature` method.\n            # fullname represents the full object name that is constructed using\n            # object nesting and explicit prefixes. `name_prefix` is the\n            # explicit prefix given in a signature\n            (fullname, name_prefix) = self.names[-1]\n            if self.allow_nesting:\n                prefix = fullname\n            elif name_prefix:\n                prefix = name_prefix.strip('.')\n        if prefix:\n            self.env.ref_context['py:class'] = prefix\n            if self.allow_nesting:\n                classes = self.env.ref_context.setdefault('py:classes', [])\n                classes.append(prefix)\n        if 'module' in self.options:\n            modules = self.env.ref_context.setdefault('py:modules', [])\n            modules.append(self.env.ref_context.get('py:module'))\n            self.env.ref_context['py:module'] = self.options['module']\n\n    def after_content(self) -> None:\n        \"\"\"Handle object de-nesting after content\n\n        If this class is a nestable object, removing the last nested class prefix\n        ends further nesting in the object.\n\n        If this class is not a nestable object, the list of classes should not\n        be altered as we didn't affect the nesting levels in\n        :py:meth:`before_content`.\n        \"\"\"\n        classes = self.env.ref_context.setdefault('py:classes', [])\n        if self.allow_nesting:\n            try:\n                classes.pop()\n            except IndexError:\n                pass\n        self.env.ref_context['py:class'] = (classes[-1] if len(classes) > 0\n                                            else None)\n        if 'module' in self.options:\n            modules = self.env.ref_context.setdefault('py:modules', [])\n            if modules:\n                self.env.ref_context['py:module'] = modules.pop()\n            else:\n                self.env.ref_context.pop('py:module')\n\n\nclass PyFunction(PyObject):\n    \"\"\"Description of a function.\"\"\"\n\n    option_spec: OptionSpec = PyObject.option_spec.copy()\n    option_spec.update({\n        'async': directives.flag,\n    })\n\n    def get_signature_prefix(self, sig: str) -> str:\n        if 'async' in self.options:\n            return 'async '\n        else:\n            return ''\n\n    def needs_arglist(self) -> bool:\n        return True\n\n    def add_target_and_index(self, name_cls: Tuple[str, str], sig: str,\n                             signode: desc_signature) -> None:\n        super().add_target_and_index(name_cls, sig, signode)\n        if 'noindexentry' not in self.options:\n            modname = self.options.get('module', self.env.ref_context.get('py:module'))\n            node_id = signode['ids'][0]\n\n            name, cls = name_cls\n            if modname:\n                text = _('%s() (in module %s)') % (name, modname)\n                self.indexnode['entries'].append(('single', text, node_id, '', None))\n            else:\n                text = '%s; %s()' % (pairindextypes['builtin'], name)\n                self.indexnode['entries'].append(('pair', text, node_id, '', None))\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        # add index in own add_target_and_index() instead.\n        return None\n\n\nclass PyDecoratorFunction(PyFunction):\n    \"\"\"Description of a decorator.\"\"\"\n\n    def run(self) -> List[Node]:\n        # a decorator function is a function after all\n        self.name = 'py:function'\n        return super().run()\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        ret = super().handle_signature(sig, signode)\n        signode.insert(0, addnodes.desc_addname('@', '@'))\n        return ret\n\n    def needs_arglist(self) -> bool:\n        return False\n\n\nclass PyVariable(PyObject):\n    \"\"\"Description of a variable.\"\"\"\n\n    option_spec: OptionSpec = PyObject.option_spec.copy()\n    option_spec.update({\n        'type': directives.unchanged,\n        'value': directives.unchanged,\n    })\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        fullname, prefix = super().handle_signature(sig, signode)\n\n        typ = self.options.get('type')\n        if typ:\n            annotations = _parse_annotation(typ, self.env)\n            signode += addnodes.desc_annotation(typ, '', nodes.Text(': '), *annotations)\n\n        value = self.options.get('value')\n        if value:\n            signode += addnodes.desc_annotation(value, ' = ' + value)\n\n        return fullname, prefix\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        name, cls = name_cls\n        if modname:\n            return _('%s (in module %s)') % (name, modname)\n        else:\n            return _('%s (built-in variable)') % name\n\n\nclass PyClasslike(PyObject):\n    \"\"\"\n    Description of a class-like object (classes, interfaces, exceptions).\n    \"\"\"\n\n    option_spec: OptionSpec = PyObject.option_spec.copy()\n    option_spec.update({\n        'final': directives.flag,\n    })\n\n    allow_nesting = True\n\n    def get_signature_prefix(self, sig: str) -> str:\n        if 'final' in self.options:\n            return 'final %s ' % self.objtype\n        else:\n            return '%s ' % self.objtype\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        if self.objtype == 'class':\n            if not modname:\n                return _('%s (built-in class)') % name_cls[0]\n            return _('%s (class in %s)') % (name_cls[0], modname)\n        elif self.objtype == 'exception':\n            return name_cls[0]\n        else:\n            return ''\n\n\nclass PyMethod(PyObject):\n    \"\"\"Description of a method.\"\"\"\n\n    option_spec: OptionSpec = PyObject.option_spec.copy()\n    option_spec.update({\n        'abstractmethod': directives.flag,\n        'async': directives.flag,\n        'classmethod': directives.flag,\n        'final': directives.flag,\n        'property': directives.flag,\n        'staticmethod': directives.flag,\n    })\n\n    def needs_arglist(self) -> bool:\n        if 'property' in self.options:\n            return False\n        else:\n            return True\n\n    def get_signature_prefix(self, sig: str) -> str:\n        prefix = []\n        if 'final' in self.options:\n            prefix.append('final')\n        if 'abstractmethod' in self.options:\n            prefix.append('abstract')\n        if 'async' in self.options:\n            prefix.append('async')\n        if 'classmethod' in self.options:\n            prefix.append('classmethod')\n        if 'property' in self.options:\n            prefix.append('property')\n        if 'staticmethod' in self.options:\n            prefix.append('static')\n\n        if prefix:\n            return ' '.join(prefix) + ' '\n        else:\n            return ''\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        name, cls = name_cls\n        try:\n            clsname, methname = name.rsplit('.', 1)\n            if modname and self.env.config.add_module_names:\n                clsname = '.'.join([modname, clsname])\n        except ValueError:\n            if modname:\n                return _('%s() (in module %s)') % (name, modname)\n            else:\n                return '%s()' % name\n\n        if 'classmethod' in self.options:\n            return _('%s() (%s class method)') % (methname, clsname)\n        elif 'property' in self.options:\n            return _('%s() (%s property)') % (methname, clsname)\n        elif 'staticmethod' in self.options:\n            return _('%s() (%s static method)') % (methname, clsname)\n        else:\n            return _('%s() (%s method)') % (methname, clsname)\n\n\nclass PyClassMethod(PyMethod):\n    \"\"\"Description of a classmethod.\"\"\"\n\n    option_spec: OptionSpec = PyObject.option_spec.copy()\n\n    def run(self) -> List[Node]:\n        self.name = 'py:method'\n        self.options['classmethod'] = True\n\n        return super().run()\n\n\nclass PyStaticMethod(PyMethod):\n    \"\"\"Description of a staticmethod.\"\"\"\n\n    option_spec: OptionSpec = PyObject.option_spec.copy()\n\n    def run(self) -> List[Node]:\n        self.name = 'py:method'\n        self.options['staticmethod'] = True\n\n        return super().run()\n\n\nclass PyDecoratorMethod(PyMethod):\n    \"\"\"Description of a decoratormethod.\"\"\"\n\n    def run(self) -> List[Node]:\n        self.name = 'py:method'\n        return super().run()\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        ret = super().handle_signature(sig, signode)\n        signode.insert(0, addnodes.desc_addname('@', '@'))\n        return ret\n\n    def needs_arglist(self) -> bool:\n        return False\n\n\nclass PyAttribute(PyObject):\n    \"\"\"Description of an attribute.\"\"\"\n\n    option_spec: OptionSpec = PyObject.option_spec.copy()\n    option_spec.update({\n        'type': directives.unchanged,\n        'value': directives.unchanged,\n    })\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        fullname, prefix = super().handle_signature(sig, signode)\n\n        typ = self.options.get('type')\n        if typ:\n            annotations = _parse_annotation(typ, self.env)\n            signode += addnodes.desc_annotation(typ, '', nodes.Text(': '), *annotations)\n\n        value = self.options.get('value')\n        if value:\n            signode += addnodes.desc_annotation(value, ' = ' + value)\n\n        return fullname, prefix\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        name, cls = name_cls\n        try:\n            clsname, attrname = name.rsplit('.', 1)\n            if modname and self.env.config.add_module_names:\n                clsname = '.'.join([modname, clsname])\n        except ValueError:\n            if modname:\n                return _('%s (in module %s)') % (name, modname)\n            else:\n                return name\n\n        return _('%s (%s attribute)') % (attrname, clsname)\n\n\nclass PyProperty(PyObject):\n    \"\"\"Description of an attribute.\"\"\"\n\n    option_spec = PyObject.option_spec.copy()\n    option_spec.update({\n        'abstractmethod': directives.flag,\n        'type': directives.unchanged,\n    })\n\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        fullname, prefix = super().handle_signature(sig, signode)\n\n        typ = self.options.get('type')\n        if typ:\n            signode += addnodes.desc_annotation(typ, ': ' + typ)\n\n        return fullname, prefix\n\n    def get_signature_prefix(self, sig: str) -> str:\n        prefix = ['property']\n        if 'abstractmethod' in self.options:\n            prefix.insert(0, 'abstract')\n\n        return ' '.join(prefix) + ' '\n\n    def get_index_text(self, modname: str, name_cls: Tuple[str, str]) -> str:\n        name, cls = name_cls\n        try:\n            clsname, attrname = name.rsplit('.', 1)\n            if modname and self.env.config.add_module_names:\n                clsname = '.'.join([modname, clsname])\n        except ValueError:\n            if modname:\n                return _('%s (in module %s)') % (name, modname)\n            else:\n                return name\n\n        return _('%s (%s property)') % (attrname, clsname)\n\n\nclass PyDecoratorMixin:\n    \"\"\"\n    Mixin for decorator directives.\n    \"\"\"\n    def handle_signature(self, sig: str, signode: desc_signature) -> Tuple[str, str]:\n        for cls in self.__class__.__mro__:\n            if cls.__name__ != 'DirectiveAdapter':\n                warnings.warn('PyDecoratorMixin is deprecated. '\n                              'Please check the implementation of %s' % cls,\n                              RemovedInSphinx50Warning, stacklevel=2)\n                break\n        else:\n            warnings.warn('PyDecoratorMixin is deprecated',\n                          RemovedInSphinx50Warning, stacklevel=2)\n\n        ret = super().handle_signature(sig, signode)  # type: ignore\n        signode.insert(0, addnodes.desc_addname('@', '@'))\n        return ret\n\n    def needs_arglist(self) -> bool:\n        return False\n\n\nclass PyModule(SphinxDirective):\n    \"\"\"\n    Directive to mark description of a new module.\n    \"\"\"\n\n    has_content = False\n    required_arguments = 1\n    optional_arguments = 0\n    final_argument_whitespace = False\n    option_spec: OptionSpec = {\n        'platform': lambda x: x,\n        'synopsis': lambda x: x,\n        'noindex': directives.flag,\n        'deprecated': directives.flag,\n    }\n\n    def run(self) -> List[Node]:\n        domain = cast(PythonDomain, self.env.get_domain('py'))\n\n        modname = self.arguments[0].strip()\n        noindex = 'noindex' in self.options\n        self.env.ref_context['py:module'] = modname\n        ret: List[Node] = []\n        if not noindex:\n            # note module to the domain\n            node_id = make_id(self.env, self.state.document, 'module', modname)\n            target = nodes.target('', '', ids=[node_id], ismod=True)\n            self.set_source_info(target)\n\n            # Assign old styled node_id not to break old hyperlinks (if possible)\n            # Note: Will removed in Sphinx-5.0  (RemovedInSphinx50Warning)\n            old_node_id = self.make_old_id(modname)\n            if node_id != old_node_id and old_node_id not in self.state.document.ids:\n                target['ids'].append(old_node_id)\n\n            self.state.document.note_explicit_target(target)\n\n            domain.note_module(modname,\n                               node_id,\n                               self.options.get('synopsis', ''),\n                               self.options.get('platform', ''),\n                               'deprecated' in self.options)\n            domain.note_object(modname, 'module', node_id, location=target)\n\n            # the platform and synopsis aren't printed; in fact, they are only\n            # used in the modindex currently\n            ret.append(target)\n            indextext = '%s; %s' % (pairindextypes['module'], modname)\n            inode = addnodes.index(entries=[('pair', indextext, node_id, '', None)])\n            ret.append(inode)\n        return ret\n\n    def make_old_id(self, name: str) -> str:\n        \"\"\"Generate old styled node_id.\n\n        Old styled node_id is incompatible with docutils' node_id.\n        It can contain dots and hyphens.\n\n        .. note:: Old styled node_id was mainly used until Sphinx-3.0.\n        \"\"\"\n        return 'module-%s' % name\n\n\nclass PyCurrentModule(SphinxDirective):\n    \"\"\"\n    This directive is just to tell Sphinx that we're documenting\n    stuff in module foo, but links to module foo won't lead here.\n    \"\"\"\n\n    has_content = False\n    required_arguments = 1\n    optional_arguments = 0\n    final_argument_whitespace = False\n    option_spec: OptionSpec = {}\n\n    def run(self) -> List[Node]:\n        modname = self.arguments[0].strip()\n        if modname == 'None':\n            self.env.ref_context.pop('py:module', None)\n        else:\n            self.env.ref_context['py:module'] = modname\n        return []\n\n\nclass PyXRefRole(XRefRole):\n    def process_link(self, env: BuildEnvironment, refnode: Element,\n                     has_explicit_title: bool, title: str, target: str) -> Tuple[str, str]:\n        refnode['py:module'] = env.ref_context.get('py:module')\n        refnode['py:class'] = env.ref_context.get('py:class')\n        if not has_explicit_title:\n            title = title.lstrip('.')    # only has a meaning for the target\n            target = target.lstrip('~')  # only has a meaning for the title\n            # if the first character is a tilde, don't display the module/class\n            # parts of the contents\n            if title[0:1] == '~':\n                title = title[1:]\n                dot = title.rfind('.')\n                if dot != -1:\n                    title = title[dot + 1:]\n        # if the first character is a dot, search more specific namespaces first\n        # else search builtins first\n        if target[0:1] == '.':\n            target = target[1:]\n            refnode['refspecific'] = True\n        return title, target\n\n\ndef filter_meta_fields(app: Sphinx, domain: str, objtype: str, content: Element) -> None:\n    \"\"\"Filter ``:meta:`` field from its docstring.\"\"\"\n    if domain != 'py':\n        return\n\n    for node in content:\n        if isinstance(node, nodes.field_list):\n            fields = cast(List[nodes.field], node)\n            for field in fields:\n                field_name = cast(nodes.field_body, field[0]).astext().strip()\n                if field_name == 'meta' or field_name.startswith('meta '):\n                    node.remove(field)\n                    break\n\n\nclass PythonModuleIndex(Index):\n    \"\"\"\n    Index subclass to provide the Python module index.\n    \"\"\"\n\n    name = 'modindex'\n    localname = _('Python Module Index')\n    shortname = _('modules')\n\n    def generate(self, docnames: Iterable[str] = None\n                 ) -> Tuple[List[Tuple[str, List[IndexEntry]]], bool]:\n        content: Dict[str, List[IndexEntry]] = {}\n        # list of prefixes to ignore\n        ignores: List[str] = self.domain.env.config['modindex_common_prefix']\n        ignores = sorted(ignores, key=len, reverse=True)\n        # list of all modules, sorted by module name\n        modules = sorted(self.domain.data['modules'].items(),\n                         key=lambda x: x[0].lower())\n        # sort out collapsable modules\n        prev_modname = ''\n        num_toplevels = 0\n        for modname, (docname, node_id, synopsis, platforms, deprecated) in modules:\n            if docnames and docname not in docnames:\n                continue\n\n            for ignore in ignores:\n                if modname.startswith(ignore):\n                    modname = modname[len(ignore):]\n                    stripped = ignore\n                    break\n            else:\n                stripped = ''\n\n            # we stripped the whole module name?\n            if not modname:\n                modname, stripped = stripped, ''\n\n            entries = content.setdefault(modname[0].lower(), [])\n\n            package = modname.split('.')[0]\n            if package != modname:\n                # it's a submodule\n                if prev_modname == package:\n                    # first submodule - make parent a group head\n                    if entries:\n                        last = entries[-1]\n                        entries[-1] = IndexEntry(last[0], 1, last[2], last[3],\n                                                 last[4], last[5], last[6])\n                elif not prev_modname.startswith(package):\n                    # submodule without parent in list, add dummy entry\n                    entries.append(IndexEntry(stripped + package, 1, '', '', '', '', ''))\n                subtype = 2\n            else:\n                num_toplevels += 1\n                subtype = 0\n\n            qualifier = _('Deprecated') if deprecated else ''\n            entries.append(IndexEntry(stripped + modname, subtype, docname,\n                                      node_id, platforms, qualifier, synopsis))\n            prev_modname = modname\n\n        # apply heuristics when to collapse modindex at page load:\n        # only collapse if number of toplevel modules is larger than\n        # number of submodules\n        collapse = len(modules) - num_toplevels < num_toplevels\n\n        # sort by first letter\n        sorted_content = sorted(content.items())\n\n        return sorted_content, collapse\n\n\nclass PythonDomain(Domain):\n    \"\"\"Python language domain.\"\"\"\n    name = 'py'\n    label = 'Python'\n    object_types: Dict[str, ObjType] = {\n        'function':     ObjType(_('function'),      'func', 'obj'),\n        'data':         ObjType(_('data'),          'data', 'obj'),\n        'class':        ObjType(_('class'),         'class', 'exc', 'obj'),\n        'exception':    ObjType(_('exception'),     'exc', 'class', 'obj'),\n        'method':       ObjType(_('method'),        'meth', 'obj'),\n        'classmethod':  ObjType(_('class method'),  'meth', 'obj'),\n        'staticmethod': ObjType(_('static method'), 'meth', 'obj'),\n        'attribute':    ObjType(_('attribute'),     'attr', 'obj'),\n        'property':     ObjType(_('property'),      'attr', '_prop', 'obj'),\n        'module':       ObjType(_('module'),        'mod', 'obj'),\n    }\n\n    directives = {\n        'function':        PyFunction,\n        'data':            PyVariable,\n        'class':           PyClasslike,\n        'exception':       PyClasslike,\n        'method':          PyMethod,\n        'classmethod':     PyClassMethod,\n        'staticmethod':    PyStaticMethod,\n        'attribute':       PyAttribute,\n        'property':        PyProperty,\n        'module':          PyModule,\n        'currentmodule':   PyCurrentModule,\n        'decorator':       PyDecoratorFunction,\n        'decoratormethod': PyDecoratorMethod,\n    }\n    roles = {\n        'data':  PyXRefRole(),\n        'exc':   PyXRefRole(),\n        'func':  PyXRefRole(fix_parens=True),\n        'class': PyXRefRole(),\n        'const': PyXRefRole(),\n        'attr':  PyXRefRole(),\n        'meth':  PyXRefRole(fix_parens=True),\n        'mod':   PyXRefRole(),\n        'obj':   PyXRefRole(),\n    }\n    initial_data: Dict[str, Dict[str, Tuple[Any]]] = {\n        'objects': {},  # fullname -> docname, objtype\n        'modules': {},  # modname -> docname, synopsis, platform, deprecated\n    }\n    indices = [\n        PythonModuleIndex,\n    ]\n\n    @property\n    def objects(self) -> Dict[str, ObjectEntry]:\n        return self.data.setdefault('objects', {})  # fullname -> ObjectEntry\n\n    def note_object(self, name: str, objtype: str, node_id: str,\n                    aliased: bool = False, location: Any = None) -> None:\n        \"\"\"Note a python object for cross reference.\n\n        .. versionadded:: 2.1\n        \"\"\"\n        if name in self.objects:\n            other = self.objects[name]\n            if other.aliased and aliased is False:\n                # The original definition found. Override it!\n                pass\n            elif other.aliased is False and aliased:\n                # The original definition is already registered.\n                return\n            else:\n                # duplicated\n                logger.warning(__('duplicate object description of %s, '\n                                  'other instance in %s, use :noindex: for one of them'),\n                               name, other.docname, location=location)\n        self.objects[name] = ObjectEntry(self.env.docname, node_id, objtype, aliased)\n\n    @property\n    def modules(self) -> Dict[str, ModuleEntry]:\n        return self.data.setdefault('modules', {})  # modname -> ModuleEntry\n\n    def note_module(self, name: str, node_id: str, synopsis: str,\n                    platform: str, deprecated: bool) -> None:\n        \"\"\"Note a python module for cross reference.\n\n        .. versionadded:: 2.1\n        \"\"\"\n        self.modules[name] = ModuleEntry(self.env.docname, node_id,\n                                         synopsis, platform, deprecated)\n\n    def clear_doc(self, docname: str) -> None:\n        for fullname, obj in list(self.objects.items()):\n            if obj.docname == docname:\n                del self.objects[fullname]\n        for modname, mod in list(self.modules.items()):\n            if mod.docname == docname:\n                del self.modules[modname]\n\n    def merge_domaindata(self, docnames: List[str], otherdata: Dict) -> None:\n        # XXX check duplicates?\n        for fullname, obj in otherdata['objects'].items():\n            if obj.docname in docnames:\n                self.objects[fullname] = obj\n        for modname, mod in otherdata['modules'].items():\n            if mod.docname in docnames:\n                self.modules[modname] = mod\n\n    def find_obj(self, env: BuildEnvironment, modname: str, classname: str,\n                 name: str, type: str, searchmode: int = 0\n                 ) -> List[Tuple[str, ObjectEntry]]:\n        \"\"\"Find a Python object for \"name\", perhaps using the given module\n        and/or classname.  Returns a list of (name, object entry) tuples.\n        \"\"\"\n        # skip parens\n        if name[-2:] == '()':\n            name = name[:-2]\n\n        if not name:\n            return []\n\n        matches: List[Tuple[str, ObjectEntry]] = []\n\n        newname = None\n        if searchmode == 1:\n            if type is None:\n                objtypes = list(self.object_types)\n            else:\n                objtypes = self.objtypes_for_role(type)\n            if objtypes is not None:\n                if modname and classname:\n                    fullname = modname + '.' + classname + '.' + name\n                    if fullname in self.objects and self.objects[fullname].objtype in objtypes:\n                        newname = fullname\n                if not newname:\n                    if modname and modname + '.' + name in self.objects and \\\n                       self.objects[modname + '.' + name].objtype in objtypes:\n                        newname = modname + '.' + name\n                    elif name in self.objects and self.objects[name].objtype in objtypes:\n                        newname = name\n                    else:\n                        # \"fuzzy\" searching mode\n                        searchname = '.' + name\n                        matches = [(oname, self.objects[oname]) for oname in self.objects\n                                   if oname.endswith(searchname) and\n                                   self.objects[oname].objtype in objtypes]\n        else:\n            # NOTE: searching for exact match, object type is not considered\n            if name in self.objects:\n                newname = name\n            elif type == 'mod':\n                # only exact matches allowed for modules\n                return []\n            elif classname and classname + '.' + name in self.objects:\n                newname = classname + '.' + name\n            elif modname and modname + '.' + name in self.objects:\n                newname = modname + '.' + name\n            elif modname and classname and \\\n                    modname + '.' + classname + '.' + name in self.objects:\n                newname = modname + '.' + classname + '.' + name\n        if newname is not None:\n            matches.append((newname, self.objects[newname]))\n        return matches\n\n    def resolve_xref(self, env: BuildEnvironment, fromdocname: str, builder: Builder,\n                     type: str, target: str, node: pending_xref, contnode: Element\n                     ) -> Optional[Element]:\n        modname = node.get('py:module')\n        clsname = node.get('py:class')\n        searchmode = 1 if node.hasattr('refspecific') else 0\n        matches = self.find_obj(env, modname, clsname, target,\n                                type, searchmode)\n\n        if not matches and type == 'attr':\n            # fallback to meth (for property; Sphinx-2.4.x)\n            # this ensures that `:attr:` role continues to refer to the old property entry\n            # that defined by ``method`` directive in old reST files.\n            matches = self.find_obj(env, modname, clsname, target, 'meth', searchmode)\n        if not matches and type == 'meth':\n            # fallback to attr (for property)\n            # this ensures that `:meth:` in the old reST files can refer to the property\n            # entry that defined by ``property`` directive.\n            #\n            # Note: _prop is a secret role only for internal look-up.\n            matches = self.find_obj(env, modname, clsname, target, '_prop', searchmode)\n\n        if not matches:\n            return None\n        elif len(matches) > 1:\n            canonicals = [m for m in matches if not m[1].aliased]\n            if len(canonicals) == 1:\n                matches = canonicals\n            else:\n                logger.warning(__('more than one target found for cross-reference %r: %s'),\n                               target, ', '.join(match[0] for match in matches),\n                               type='ref', subtype='python', location=node)\n        name, obj = matches[0]\n\n        if obj[2] == 'module':\n            return self._make_module_refnode(builder, fromdocname, name, contnode)\n        else:\n            # determine the content of the reference by conditions\n            content = find_pending_xref_condition(node, 'resolved')\n            if content:\n                children = content.children\n            else:\n                # if not found, use contnode\n                children = [contnode]\n\n            return make_refnode(builder, fromdocname, obj[0], obj[1], children, name)\n\n    def resolve_any_xref(self, env: BuildEnvironment, fromdocname: str, builder: Builder,\n                         target: str, node: pending_xref, contnode: Element\n                         ) -> List[Tuple[str, Element]]:\n        modname = node.get('py:module')\n        clsname = node.get('py:class')\n        results: List[Tuple[str, Element]] = []\n\n        # always search in \"refspecific\" mode with the :any: role\n        matches = self.find_obj(env, modname, clsname, target, None, 1)\n        for name, obj in matches:\n            if obj[2] == 'module':\n                results.append(('py:mod',\n                                self._make_module_refnode(builder, fromdocname,\n                                                          name, contnode)))\n            else:\n                # determine the content of the reference by conditions\n                content = find_pending_xref_condition(node, 'resolved')\n                if content:\n                    children = content.children\n                else:\n                    # if not found, use contnode\n                    children = [contnode]\n\n                results.append(('py:' + self.role_for_objtype(obj[2]),\n                                make_refnode(builder, fromdocname, obj[0], obj[1],\n                                             children, name)))\n        return results\n\n    def _make_module_refnode(self, builder: Builder, fromdocname: str, name: str,\n                             contnode: Node) -> Element:\n        # get additional info for modules\n        module = self.modules[name]\n        title = name\n        if module.synopsis:\n            title += ': ' + module.synopsis\n        if module.deprecated:\n            title += _(' (deprecated)')\n        if module.platform:\n            title += ' (' + module.platform + ')'\n        return make_refnode(builder, fromdocname, module.docname, module.node_id,\n                            contnode, title)\n\n    def get_objects(self) -> Iterator[Tuple[str, str, str, str, str, int]]:\n        for modname, mod in self.modules.items():\n            yield (modname, modname, 'module', mod.docname, mod.node_id, 0)\n        for refname, obj in self.objects.items():\n            if obj.objtype != 'module':  # modules are already handled\n                if obj.aliased:\n                    # aliased names are not full-text searchable.\n                    yield (refname, refname, obj.objtype, obj.docname, obj.node_id, -1)\n                else:\n                    yield (refname, refname, obj.objtype, obj.docname, obj.node_id, 1)\n\n    def get_full_qualified_name(self, node: Element) -> Optional[str]:\n        modname = node.get('py:module')\n        clsname = node.get('py:class')\n        target = node.get('reftarget')\n        if target is None:\n            return None\n        else:\n            return '.'.join(filter(None, [modname, clsname, target]))\n\n\ndef builtin_resolver(app: Sphinx, env: BuildEnvironment,\n                     node: pending_xref, contnode: Element) -> Element:\n    \"\"\"Do not emit nitpicky warnings for built-in types.\"\"\"\n    def istyping(s: str) -> bool:\n        if s.startswith('typing.'):\n            s = s.split('.', 1)[1]\n\n        return s in typing.__all__  # type: ignore\n\n    content = find_pending_xref_condition(node, 'resolved')\n    if content:\n        contnode = content.children[0]  # type: ignore\n\n    if node.get('refdomain') != 'py':\n        return None\n    elif node.get('reftype') in ('class', 'obj') and node.get('reftarget') == 'None':\n        return contnode\n    elif node.get('reftype') in ('class', 'exc'):\n        reftarget = node.get('reftarget')\n        if inspect.isclass(getattr(builtins, reftarget, None)):\n            # built-in class\n            return contnode\n        elif istyping(reftarget):\n            # typing class\n            return contnode\n\n    return None\n\n\ndef setup(app: Sphinx) -> Dict[str, Any]:\n    app.setup_extension('sphinx.directives')\n\n    app.add_domain(PythonDomain)\n    app.add_config_value('python_use_unqualified_type_names', False, 'env')\n    app.connect('object-description-transform', filter_meta_fields)\n    app.connect('missing-reference', builtin_resolver, priority=900)\n\n    return {\n        'version': 'builtin',\n        'env_version': 3,\n        'parallel_read_safe': True,\n        'parallel_write_safe': True,\n    }"}
{"model_name_or_path": "agentless", "instance_id": "sphinx-doc__sphinx-9320", "model_patch": "diff --git a/sphinx/cmd/quickstart.py b/sphinx/cmd/quickstart.py\nindex 9978773..c262a8c 100644\n--- a/sphinx/cmd/quickstart.py\n+++ b/sphinx/cmd/quickstart.py\n@@ -224,7 +224,8 @@ def ask_user(d: Dict) -> None:\n         print()\n         d['path'] = do_prompt(__('Please enter a new root path (or just Enter to exit)'),\n                               '', is_path)\n-        if not d['path']:\n+        if not d['path'] or not path.isdir(d['path']):\n+            print(__('Exiting...'))\n             sys.exit(1)\n \n     if 'sep' not in d:\n", "raw_model_patch": "diff --git a/sphinx/cmd/quickstart.py b/sphinx/cmd/quickstart.py\nindex 9978773..c262a8c 100644\n--- a/sphinx/cmd/quickstart.py\n+++ b/sphinx/cmd/quickstart.py\n@@ -224,7 +224,8 @@ def ask_user(d: Dict) -> None:\n         print()\n         d['path'] = do_prompt(__('Please enter a new root path (or just Enter to exit)'),\n                               '', is_path)\n-        if not d['path']:\n+        if not d['path'] or not path.isdir(d['path']):\n+            print(__('Exiting...'))\n             sys.exit(1)\n \n     if 'sep' not in d:\n", "original_file_content": "\"\"\"\n    sphinx.cmd.quickstart\n    ~~~~~~~~~~~~~~~~~~~~~\n\n    Quickly setup documentation source to work with Sphinx.\n\n    :copyright: Copyright 2007-2021 by the Sphinx team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport argparse\nimport locale\nimport os\nimport sys\nimport time\nfrom collections import OrderedDict\nfrom os import path\nfrom typing import Any, Callable, Dict, List, Union\n\n# try to import readline, unix specific enhancement\ntry:\n    import readline\n    if readline.__doc__ and 'libedit' in readline.__doc__:\n        readline.parse_and_bind(\"bind ^I rl_complete\")\n        USE_LIBEDIT = True\n    else:\n        readline.parse_and_bind(\"tab: complete\")\n        USE_LIBEDIT = False\nexcept ImportError:\n    readline = None\n    USE_LIBEDIT = False\n\nfrom docutils.utils import column_width\n\nimport sphinx.locale\nfrom sphinx import __display_version__, package_dir\nfrom sphinx.locale import __\nfrom sphinx.util.console import bold, color_terminal, colorize, nocolor, red  # type: ignore\nfrom sphinx.util.osutil import ensuredir\nfrom sphinx.util.template import SphinxRenderer\n\nEXTENSIONS = OrderedDict([\n    ('autodoc', __('automatically insert docstrings from modules')),\n    ('doctest', __('automatically test code snippets in doctest blocks')),\n    ('intersphinx', __('link between Sphinx documentation of different projects')),\n    ('todo', __('write \"todo\" entries that can be shown or hidden on build')),\n    ('coverage', __('checks for documentation coverage')),\n    ('imgmath', __('include math, rendered as PNG or SVG images')),\n    ('mathjax', __('include math, rendered in the browser by MathJax')),\n    ('ifconfig', __('conditional inclusion of content based on config values')),\n    ('viewcode', __('include links to the source code of documented Python objects')),\n    ('githubpages', __('create .nojekyll file to publish the document on GitHub pages')),\n])\n\nDEFAULTS = {\n    'path': '.',\n    'sep': False,\n    'dot': '_',\n    'language': None,\n    'suffix': '.rst',\n    'master': 'index',\n    'makefile': True,\n    'batchfile': True,\n}\n\nPROMPT_PREFIX = '> '\n\nif sys.platform == 'win32':\n    # On Windows, show questions as bold because of color scheme of PowerShell (refs: #5294).\n    COLOR_QUESTION = 'bold'\nelse:\n    COLOR_QUESTION = 'purple'\n\n\n# function to get input from terminal -- overridden by the test suite\ndef term_input(prompt: str) -> str:\n    if sys.platform == 'win32':\n        # Important: On windows, readline is not enabled by default.  In these\n        #            environment, escape sequences have been broken.  To avoid the\n        #            problem, quickstart uses ``print()`` to show prompt.\n        print(prompt, end='')\n        return input('')\n    else:\n        return input(prompt)\n\n\nclass ValidationError(Exception):\n    \"\"\"Raised for validation errors.\"\"\"\n\n\ndef is_path(x: str) -> str:\n    x = path.expanduser(x)\n    if not path.isdir(x):\n        raise ValidationError(__(\"Please enter a valid path name.\"))\n    return x\n\n\ndef allow_empty(x: str) -> str:\n    return x\n\n\ndef nonempty(x: str) -> str:\n    if not x:\n        raise ValidationError(__(\"Please enter some text.\"))\n    return x\n\n\ndef choice(*l: str) -> Callable[[str], str]:\n    def val(x: str) -> str:\n        if x not in l:\n            raise ValidationError(__('Please enter one of %s.') % ', '.join(l))\n        return x\n    return val\n\n\ndef boolean(x: str) -> bool:\n    if x.upper() not in ('Y', 'YES', 'N', 'NO'):\n        raise ValidationError(__(\"Please enter either 'y' or 'n'.\"))\n    return x.upper() in ('Y', 'YES')\n\n\ndef suffix(x: str) -> str:\n    if not (x[0:1] == '.' and len(x) > 1):\n        raise ValidationError(__(\"Please enter a file suffix, e.g. '.rst' or '.txt'.\"))\n    return x\n\n\ndef ok(x: str) -> str:\n    return x\n\n\ndef do_prompt(text: str, default: str = None, validator: Callable[[str], Any] = nonempty) -> Union[str, bool]:  # NOQA\n    while True:\n        if default is not None:\n            prompt = PROMPT_PREFIX + '%s [%s]: ' % (text, default)\n        else:\n            prompt = PROMPT_PREFIX + text + ': '\n        if USE_LIBEDIT:\n            # Note: libedit has a problem for combination of ``input()`` and escape\n            # sequence (see #5335).  To avoid the problem, all prompts are not colored\n            # on libedit.\n            pass\n        elif readline:\n            # pass input_mode=True if readline available\n            prompt = colorize(COLOR_QUESTION, prompt, input_mode=True)\n        else:\n            prompt = colorize(COLOR_QUESTION, prompt, input_mode=False)\n        x = term_input(prompt).strip()\n        if default and not x:\n            x = default\n        try:\n            x = validator(x)\n        except ValidationError as err:\n            print(red('* ' + str(err)))\n            continue\n        break\n    return x\n\n\nclass QuickstartRenderer(SphinxRenderer):\n    def __init__(self, templatedir: str) -> None:\n        self.templatedir = templatedir or ''\n        super().__init__()\n\n    def _has_custom_template(self, template_name: str) -> bool:\n        \"\"\"Check if custom template file exists.\n\n        Note: Please don't use this function from extensions.\n              It will be removed in the future without deprecation period.\n        \"\"\"\n        template = path.join(self.templatedir, path.basename(template_name))\n        if self.templatedir and path.exists(template):\n            return True\n        else:\n            return False\n\n    def render(self, template_name: str, context: Dict) -> str:\n        if self._has_custom_template(template_name):\n            custom_template = path.join(self.templatedir, path.basename(template_name))\n            return self.render_from_file(custom_template, context)\n        else:\n            return super().render(template_name, context)\n\n\ndef ask_user(d: Dict) -> None:\n    \"\"\"Ask the user for quickstart values missing from *d*.\n\n    Values are:\n\n    * path:      root path\n    * sep:       separate source and build dirs (bool)\n    * dot:       replacement for dot in _templates etc.\n    * project:   project name\n    * author:    author names\n    * version:   version of project\n    * release:   release of project\n    * language:  document language\n    * suffix:    source file suffix\n    * master:    master document name\n    * extensions:  extensions to use (list)\n    * makefile:  make Makefile\n    * batchfile: make command file\n    \"\"\"\n\n    print(bold(__('Welcome to the Sphinx %s quickstart utility.')) % __display_version__)\n    print()\n    print(__('Please enter values for the following settings (just press Enter to\\n'\n             'accept a default value, if one is given in brackets).'))\n\n    if 'path' in d:\n        print()\n        print(bold(__('Selected root path: %s')) % d['path'])\n    else:\n        print()\n        print(__('Enter the root path for documentation.'))\n        d['path'] = do_prompt(__('Root path for the documentation'), '.', is_path)\n\n    while path.isfile(path.join(d['path'], 'conf.py')) or \\\n            path.isfile(path.join(d['path'], 'source', 'conf.py')):\n        print()\n        print(bold(__('Error: an existing conf.py has been found in the '\n                      'selected root path.')))\n        print(__('sphinx-quickstart will not overwrite existing Sphinx projects.'))\n        print()\n        d['path'] = do_prompt(__('Please enter a new root path (or just Enter to exit)'),\n                              '', is_path)\n        if not d['path']:\n            sys.exit(1)\n\n    if 'sep' not in d:\n        print()\n        print(__('You have two options for placing the build directory for Sphinx output.\\n'\n                 'Either, you use a directory \"_build\" within the root path, or you separate\\n'\n                 '\"source\" and \"build\" directories within the root path.'))\n        d['sep'] = do_prompt(__('Separate source and build directories (y/n)'), 'n', boolean)\n\n    if 'dot' not in d:\n        print()\n        print(__('Inside the root directory, two more directories will be created; \"_templates\"\\n'      # NOQA\n                 'for custom HTML templates and \"_static\" for custom stylesheets and other static\\n'    # NOQA\n                 'files. You can enter another prefix (such as \".\") to replace the underscore.'))       # NOQA\n        d['dot'] = do_prompt(__('Name prefix for templates and static dir'), '_', ok)\n\n    if 'project' not in d:\n        print()\n        print(__('The project name will occur in several places in the built documentation.'))\n        d['project'] = do_prompt(__('Project name'))\n    if 'author' not in d:\n        d['author'] = do_prompt(__('Author name(s)'))\n\n    if 'version' not in d:\n        print()\n        print(__('Sphinx has the notion of a \"version\" and a \"release\" for the\\n'\n                 'software. Each version can have multiple releases. For example, for\\n'\n                 'Python the version is something like 2.5 or 3.0, while the release is\\n'\n                 'something like 2.5.1 or 3.0a1. If you don\\'t need this dual structure,\\n'\n                 'just set both to the same value.'))\n        d['version'] = do_prompt(__('Project version'), '', allow_empty)\n    if 'release' not in d:\n        d['release'] = do_prompt(__('Project release'), d['version'], allow_empty)\n\n    if 'language' not in d:\n        print()\n        print(__('If the documents are to be written in a language other than English,\\n'\n                 'you can select a language here by its language code. Sphinx will then\\n'\n                 'translate text that it generates into that language.\\n'\n                 '\\n'\n                 'For a list of supported codes, see\\n'\n                 'https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-language.'))  # NOQA\n        d['language'] = do_prompt(__('Project language'), 'en')\n        if d['language'] == 'en':\n            d['language'] = None\n\n    if 'suffix' not in d:\n        print()\n        print(__('The file name suffix for source files. Commonly, this is either \".txt\"\\n'\n                 'or \".rst\". Only files with this suffix are considered documents.'))\n        d['suffix'] = do_prompt(__('Source file suffix'), '.rst', suffix)\n\n    if 'master' not in d:\n        print()\n        print(__('One document is special in that it is considered the top node of the\\n'\n                 '\"contents tree\", that is, it is the root of the hierarchical structure\\n'\n                 'of the documents. Normally, this is \"index\", but if your \"index\"\\n'\n                 'document is a custom template, you can also set this to another filename.'))\n        d['master'] = do_prompt(__('Name of your master document (without suffix)'), 'index')\n\n    while path.isfile(path.join(d['path'], d['master'] + d['suffix'])) or \\\n            path.isfile(path.join(d['path'], 'source', d['master'] + d['suffix'])):\n        print()\n        print(bold(__('Error: the master file %s has already been found in the '\n                      'selected root path.') % (d['master'] + d['suffix'])))\n        print(__('sphinx-quickstart will not overwrite the existing file.'))\n        print()\n        d['master'] = do_prompt(__('Please enter a new file name, or rename the '\n                                   'existing file and press Enter'), d['master'])\n\n    if 'extensions' not in d:\n        print(__('Indicate which of the following Sphinx extensions should be enabled:'))\n        d['extensions'] = []\n        for name, description in EXTENSIONS.items():\n            if do_prompt('%s: %s (y/n)' % (name, description), 'n', boolean):\n                d['extensions'].append('sphinx.ext.%s' % name)\n\n        # Handle conflicting options\n        if {'sphinx.ext.imgmath', 'sphinx.ext.mathjax'}.issubset(d['extensions']):\n            print(__('Note: imgmath and mathjax cannot be enabled at the same time. '\n                     'imgmath has been deselected.'))\n            d['extensions'].remove('sphinx.ext.imgmath')\n\n    if 'makefile' not in d:\n        print()\n        print(__('A Makefile and a Windows command file can be generated for you so that you\\n'\n                 'only have to run e.g. `make html\\' instead of invoking sphinx-build\\n'\n                 'directly.'))\n        d['makefile'] = do_prompt(__('Create Makefile? (y/n)'), 'y', boolean)\n\n    if 'batchfile' not in d:\n        d['batchfile'] = do_prompt(__('Create Windows command file? (y/n)'), 'y', boolean)\n    print()\n\n\ndef generate(d: Dict, overwrite: bool = True, silent: bool = False, templatedir: str = None\n             ) -> None:\n    \"\"\"Generate project based on values in *d*.\"\"\"\n    template = QuickstartRenderer(templatedir=templatedir)\n\n    if 'mastertoctree' not in d:\n        d['mastertoctree'] = ''\n    if 'mastertocmaxdepth' not in d:\n        d['mastertocmaxdepth'] = 2\n\n    d['root_doc'] = d['master']\n    d['now'] = time.asctime()\n    d['project_underline'] = column_width(d['project']) * '='\n    d.setdefault('extensions', [])\n    d['copyright'] = time.strftime('%Y') + ', ' + d['author']\n\n    d[\"path\"] = os.path.abspath(d['path'])\n    ensuredir(d['path'])\n\n    srcdir = path.join(d['path'], 'source') if d['sep'] else d['path']\n\n    ensuredir(srcdir)\n    if d['sep']:\n        builddir = path.join(d['path'], 'build')\n        d['exclude_patterns'] = ''\n    else:\n        builddir = path.join(srcdir, d['dot'] + 'build')\n        exclude_patterns = map(repr, [\n            d['dot'] + 'build',\n            'Thumbs.db', '.DS_Store',\n        ])\n        d['exclude_patterns'] = ', '.join(exclude_patterns)\n    ensuredir(builddir)\n    ensuredir(path.join(srcdir, d['dot'] + 'templates'))\n    ensuredir(path.join(srcdir, d['dot'] + 'static'))\n\n    def write_file(fpath: str, content: str, newline: str = None) -> None:\n        if overwrite or not path.isfile(fpath):\n            if 'quiet' not in d:\n                print(__('Creating file %s.') % fpath)\n            with open(fpath, 'wt', encoding='utf-8', newline=newline) as f:\n                f.write(content)\n        else:\n            if 'quiet' not in d:\n                print(__('File %s already exists, skipping.') % fpath)\n\n    conf_path = os.path.join(templatedir, 'conf.py_t') if templatedir else None\n    if not conf_path or not path.isfile(conf_path):\n        conf_path = os.path.join(package_dir, 'templates', 'quickstart', 'conf.py_t')\n    with open(conf_path) as f:\n        conf_text = f.read()\n\n    write_file(path.join(srcdir, 'conf.py'), template.render_string(conf_text, d))\n\n    masterfile = path.join(srcdir, d['master'] + d['suffix'])\n    if template._has_custom_template('quickstart/master_doc.rst_t'):\n        msg = ('A custom template `master_doc.rst_t` found. It has been renamed to '\n               '`root_doc.rst_t`.  Please rename it on your project too.')\n        print(colorize('red', msg))  # RemovedInSphinx60Warning\n        write_file(masterfile, template.render('quickstart/master_doc.rst_t', d))\n    else:\n        write_file(masterfile, template.render('quickstart/root_doc.rst_t', d))\n\n    if d.get('make_mode') is True:\n        makefile_template = 'quickstart/Makefile.new_t'\n        batchfile_template = 'quickstart/make.bat.new_t'\n    else:\n        makefile_template = 'quickstart/Makefile_t'\n        batchfile_template = 'quickstart/make.bat_t'\n\n    if d['makefile'] is True:\n        d['rsrcdir'] = 'source' if d['sep'] else '.'\n        d['rbuilddir'] = 'build' if d['sep'] else d['dot'] + 'build'\n        # use binary mode, to avoid writing \\r\\n on Windows\n        write_file(path.join(d['path'], 'Makefile'),\n                   template.render(makefile_template, d), '\\n')\n\n    if d['batchfile'] is True:\n        d['rsrcdir'] = 'source' if d['sep'] else '.'\n        d['rbuilddir'] = 'build' if d['sep'] else d['dot'] + 'build'\n        write_file(path.join(d['path'], 'make.bat'),\n                   template.render(batchfile_template, d), '\\r\\n')\n\n    if silent:\n        return\n    print()\n    print(bold(__('Finished: An initial directory structure has been created.')))\n    print()\n    print(__('You should now populate your master file %s and create other documentation\\n'\n             'source files. ') % masterfile, end='')\n    if d['makefile'] or d['batchfile']:\n        print(__('Use the Makefile to build the docs, like so:\\n'\n                 '   make builder'))\n    else:\n        print(__('Use the sphinx-build command to build the docs, like so:\\n'\n                 '   sphinx-build -b builder %s %s') % (srcdir, builddir))\n    print(__('where \"builder\" is one of the supported builders, '\n             'e.g. html, latex or linkcheck.'))\n    print()\n\n\ndef valid_dir(d: Dict) -> bool:\n    dir = d['path']\n    if not path.exists(dir):\n        return True\n    if not path.isdir(dir):\n        return False\n\n    if {'Makefile', 'make.bat'} & set(os.listdir(dir)):\n        return False\n\n    if d['sep']:\n        dir = os.path.join('source', dir)\n        if not path.exists(dir):\n            return True\n        if not path.isdir(dir):\n            return False\n\n    reserved_names = [\n        'conf.py',\n        d['dot'] + 'static',\n        d['dot'] + 'templates',\n        d['master'] + d['suffix'],\n    ]\n    if set(reserved_names) & set(os.listdir(dir)):\n        return False\n\n    return True\n\n\ndef get_parser() -> argparse.ArgumentParser:\n    description = __(\n        \"\\n\"\n        \"Generate required files for a Sphinx project.\\n\"\n        \"\\n\"\n        \"sphinx-quickstart is an interactive tool that asks some questions about your\\n\"\n        \"project and then generates a complete documentation directory and sample\\n\"\n        \"Makefile to be used with sphinx-build.\\n\"\n    )\n    parser = argparse.ArgumentParser(\n        usage='%(prog)s [OPTIONS] <PROJECT_DIR>',\n        epilog=__(\"For more information, visit <http://sphinx-doc.org/>.\"),\n        description=description)\n\n    parser.add_argument('-q', '--quiet', action='store_true', dest='quiet',\n                        default=None,\n                        help=__('quiet mode'))\n    parser.add_argument('--version', action='version', dest='show_version',\n                        version='%%(prog)s %s' % __display_version__)\n\n    parser.add_argument('path', metavar='PROJECT_DIR', default='.', nargs='?',\n                        help=__('project root'))\n\n    group = parser.add_argument_group(__('Structure options'))\n    group.add_argument('--sep', action='store_true', dest='sep', default=None,\n                       help=__('if specified, separate source and build dirs'))\n    group.add_argument('--no-sep', action='store_false', dest='sep',\n                       help=__('if specified, create build dir under source dir'))\n    group.add_argument('--dot', metavar='DOT', default='_',\n                       help=__('replacement for dot in _templates etc.'))\n\n    group = parser.add_argument_group(__('Project basic options'))\n    group.add_argument('-p', '--project', metavar='PROJECT', dest='project',\n                       help=__('project name'))\n    group.add_argument('-a', '--author', metavar='AUTHOR', dest='author',\n                       help=__('author names'))\n    group.add_argument('-v', metavar='VERSION', dest='version', default='',\n                       help=__('version of project'))\n    group.add_argument('-r', '--release', metavar='RELEASE', dest='release',\n                       help=__('release of project'))\n    group.add_argument('-l', '--language', metavar='LANGUAGE', dest='language',\n                       help=__('document language'))\n    group.add_argument('--suffix', metavar='SUFFIX', default='.rst',\n                       help=__('source file suffix'))\n    group.add_argument('--master', metavar='MASTER', default='index',\n                       help=__('master document name'))\n    group.add_argument('--epub', action='store_true', default=False,\n                       help=__('use epub'))\n\n    group = parser.add_argument_group(__('Extension options'))\n    for ext in EXTENSIONS:\n        group.add_argument('--ext-%s' % ext, action='append_const',\n                           const='sphinx.ext.%s' % ext, dest='extensions',\n                           help=__('enable %s extension') % ext)\n    group.add_argument('--extensions', metavar='EXTENSIONS', dest='extensions',\n                       action='append', help=__('enable arbitrary extensions'))\n\n    group = parser.add_argument_group(__('Makefile and Batchfile creation'))\n    group.add_argument('--makefile', action='store_true', dest='makefile', default=True,\n                       help=__('create makefile'))\n    group.add_argument('--no-makefile', action='store_false', dest='makefile',\n                       help=__('do not create makefile'))\n    group.add_argument('--batchfile', action='store_true', dest='batchfile', default=True,\n                       help=__('create batchfile'))\n    group.add_argument('--no-batchfile', action='store_false',\n                       dest='batchfile',\n                       help=__('do not create batchfile'))\n    group.add_argument('-m', '--use-make-mode', action='store_true',\n                       dest='make_mode', default=True,\n                       help=__('use make-mode for Makefile/make.bat'))\n    group.add_argument('-M', '--no-use-make-mode', action='store_false',\n                       dest='make_mode',\n                       help=__('do not use make-mode for Makefile/make.bat'))\n\n    group = parser.add_argument_group(__('Project templating'))\n    group.add_argument('-t', '--templatedir', metavar='TEMPLATEDIR',\n                       dest='templatedir',\n                       help=__('template directory for template files'))\n    group.add_argument('-d', metavar='NAME=VALUE', action='append',\n                       dest='variables',\n                       help=__('define a template variable'))\n\n    return parser\n\n\ndef main(argv: List[str] = sys.argv[1:]) -> int:\n    sphinx.locale.setlocale(locale.LC_ALL, '')\n    sphinx.locale.init_console(os.path.join(package_dir, 'locale'), 'sphinx')\n\n    if not color_terminal():\n        nocolor()\n\n    # parse options\n    parser = get_parser()\n    try:\n        args = parser.parse_args(argv)\n    except SystemExit as err:\n        return err.code\n\n    d = vars(args)\n    # delete None or False value\n    d = {k: v for k, v in d.items() if v is not None}\n\n    # handle use of CSV-style extension values\n    d.setdefault('extensions', [])\n    for ext in d['extensions'][:]:\n        if ',' in ext:\n            d['extensions'].remove(ext)\n            d['extensions'].extend(ext.split(','))\n\n    try:\n        if 'quiet' in d:\n            if not {'project', 'author'}.issubset(d):\n                print(__('\"quiet\" is specified, but any of \"project\" or '\n                         '\"author\" is not specified.'))\n                return 1\n\n        if {'quiet', 'project', 'author'}.issubset(d):\n            # quiet mode with all required params satisfied, use default\n            d.setdefault('version', '')\n            d.setdefault('release', d['version'])\n            d2 = DEFAULTS.copy()\n            d2.update(d)\n            d = d2\n\n            if not valid_dir(d):\n                print()\n                print(bold(__('Error: specified path is not a directory, or sphinx'\n                              ' files already exist.')))\n                print(__('sphinx-quickstart only generate into a empty directory.'\n                         ' Please specify a new root path.'))\n                return 1\n        else:\n            ask_user(d)\n    except (KeyboardInterrupt, EOFError):\n        print()\n        print('[Interrupted.]')\n        return 130  # 128 + SIGINT\n\n    for variable in d.get('variables', []):\n        try:\n            name, value = variable.split('=')\n            d[name] = value\n        except ValueError:\n            print(__('Invalid template variable: %s') % variable)\n\n    generate(d, overwrite=False, templatedir=args.templatedir)\n    return 0\n\n\nif __name__ == '__main__':\n    sys.exit(main(sys.argv[1:]))"}
{"model_name_or_path": "agentless", "instance_id": "sympy__sympy-12489", "model_patch": "diff --git a/sympy/combinatorics/permutations.py b/sympy/combinatorics/permutations.py\nindex 007fa70..56c92a4 100644\n--- a/sympy/combinatorics/permutations.py\n+++ b/sympy/combinatorics/permutations.py\n@@ -944,7 +944,7 @@ class Permutation(Basic):\n         Permutation([2, 1, 3, 0])\n \n         \"\"\"\n-        p = Basic.__new__(Perm, perm)\n+        p = Basic.__new__(cls, perm)\n         p._array_form = perm\n         p._size = len(perm)\n         return p\n", "raw_model_patch": "diff --git a/sympy/combinatorics/permutations.py b/sympy/combinatorics/permutations.py\nindex 007fa70..56c92a4 100644\n--- a/sympy/combinatorics/permutations.py\n+++ b/sympy/combinatorics/permutations.py\n@@ -944,7 +944,7 @@ class Permutation(Basic):\n         Permutation([2, 1, 3, 0])\n \n         \"\"\"\n-        p = Basic.__new__(Perm, perm)\n+        p = Basic.__new__(cls, perm)\n         p._array_form = perm\n         p._size = len(perm)\n         return p\n", "original_file_content": "from __future__ import print_function, division\n\nimport random\nfrom collections import defaultdict\n\nfrom sympy.core import Basic\nfrom sympy.core.compatibility import is_sequence, reduce, range, as_int\nfrom sympy.utilities.iterables import (flatten, has_variety, minlex,\n    has_dups, runs)\nfrom sympy.polys.polytools import lcm\nfrom sympy.matrices import zeros\nfrom mpmath.libmp.libintmath import ifac\n\n\ndef _af_rmul(a, b):\n    \"\"\"\n    Return the product b*a; input and output are array forms. The ith value\n    is a[b[i]].\n\n    Examples\n    ========\n\n    >>> from sympy.combinatorics.permutations import _af_rmul, Permutation\n    >>> Permutation.print_cyclic = False\n\n    >>> a, b = [1, 0, 2], [0, 2, 1]\n    >>> _af_rmul(a, b)\n    [1, 2, 0]\n    >>> [a[b[i]] for i in range(3)]\n    [1, 2, 0]\n\n    This handles the operands in reverse order compared to the ``*`` operator:\n\n    >>> a = Permutation(a)\n    >>> b = Permutation(b)\n    >>> list(a*b)\n    [2, 0, 1]\n    >>> [b(a(i)) for i in range(3)]\n    [2, 0, 1]\n\n    See Also\n    ========\n    rmul, _af_rmuln\n    \"\"\"\n    return [a[i] for i in b]\n\n\ndef _af_rmuln(*abc):\n    \"\"\"\n    Given [a, b, c, ...] return the product of ...*c*b*a using array forms.\n    The ith value is a[b[c[i]]].\n\n    Examples\n    ========\n\n    >>> from sympy.combinatorics.permutations import _af_rmul, Permutation\n    >>> Permutation.print_cyclic = False\n\n    >>> a, b = [1, 0, 2], [0, 2, 1]\n    >>> _af_rmul(a, b)\n    [1, 2, 0]\n    >>> [a[b[i]] for i in range(3)]\n    [1, 2, 0]\n\n    This handles the operands in reverse order compared to the ``*`` operator:\n\n    >>> a = Permutation(a); b = Permutation(b)\n    >>> list(a*b)\n    [2, 0, 1]\n    >>> [b(a(i)) for i in range(3)]\n    [2, 0, 1]\n\n    See Also\n    ========\n    rmul, _af_rmul\n    \"\"\"\n    a = abc\n    m = len(a)\n    if m == 3:\n        p0, p1, p2 = a\n        return [p0[p1[i]] for i in p2]\n    if m == 4:\n        p0, p1, p2, p3 = a\n        return [p0[p1[p2[i]]] for i in p3]\n    if m == 5:\n        p0, p1, p2, p3, p4 = a\n        return [p0[p1[p2[p3[i]]]] for i in p4]\n    if m == 6:\n        p0, p1, p2, p3, p4, p5 = a\n        return [p0[p1[p2[p3[p4[i]]]]] for i in p5]\n    if m == 7:\n        p0, p1, p2, p3, p4, p5, p6 = a\n        return [p0[p1[p2[p3[p4[p5[i]]]]]] for i in p6]\n    if m == 8:\n        p0, p1, p2, p3, p4, p5, p6, p7 = a\n        return [p0[p1[p2[p3[p4[p5[p6[i]]]]]]] for i in p7]\n    if m == 1:\n        return a[0][:]\n    if m == 2:\n        a, b = a\n        return [a[i] for i in b]\n    if m == 0:\n        raise ValueError(\"String must not be empty\")\n    p0 = _af_rmuln(*a[:m//2])\n    p1 = _af_rmuln(*a[m//2:])\n    return [p0[i] for i in p1]\n\n\ndef _af_parity(pi):\n    \"\"\"\n    Computes the parity of a permutation in array form.\n\n    The parity of a permutation reflects the parity of the\n    number of inversions in the permutation, i.e., the\n    number of pairs of x and y such that x > y but p[x] < p[y].\n\n    Examples\n    ========\n\n    >>> from sympy.combinatorics.permutations import _af_parity\n    >>> _af_parity([0, 1, 2, 3])\n    0\n    >>> _af_parity([3, 2, 0, 1])\n    1\n\n    See Also\n    ========\n\n    Permutation\n    \"\"\"\n    n = len(pi)\n    a = [0] * n\n    c = 0\n    for j in range(n):\n        if a[j] == 0:\n            c += 1\n            a[j] = 1\n            i = j\n            while pi[i] != j:\n                i = pi[i]\n                a[i] = 1\n    return (n - c) % 2\n\n\ndef _af_invert(a):\n    \"\"\"\n    Finds the inverse, ~A, of a permutation, A, given in array form.\n\n    Examples\n    ========\n\n    >>> from sympy.combinatorics.permutations import _af_invert, _af_rmul\n    >>> A = [1, 2, 0, 3]\n    >>> _af_invert(A)\n    [2, 0, 1, 3]\n    >>> _af_rmul(_, A)\n    [0, 1, 2, 3]\n\n    See Also\n    ========\n\n    Permutation, __invert__\n    \"\"\"\n    inv_form = [0] * len(a)\n    for i, ai in enumerate(a):\n        inv_form[ai] = i\n    return inv_form\n\ndef _af_pow(a, n):\n    \"\"\"\n    Routine for finding powers of a permutation.\n\n    Examples\n    ========\n\n    >>> from sympy.combinatorics.permutations import Permutation, _af_pow\n    >>> Permutation.print_cyclic = False\n    >>> p = Permutation([2, 0, 3, 1])\n    >>> p.order()\n    4\n    >>> _af_pow(p._array_form, 4)\n    [0, 1, 2, 3]\n    \"\"\"\n    if n == 0:\n        return list(range(len(a)))\n    if n < 0:\n        return _af_pow(_af_invert(a), -n)\n    if n == 1:\n        return a[:]\n    elif n == 2:\n        b = [a[i] for i in a]\n    elif n == 3:\n        b = [a[a[i]] for i in a]\n    elif n == 4:\n        b = [a[a[a[i]]] for i in a]\n    else:\n        # use binary multiplication\n        b = list(range(len(a)))\n        while 1:\n            if n & 1:\n                b = [b[i] for i in a]\n                n -= 1\n                if not n:\n                    break\n            if n % 4 == 0:\n                a = [a[a[a[i]]] for i in a]\n                n = n // 4\n            elif n % 2 == 0:\n                a = [a[i] for i in a]\n                n = n // 2\n    return b\n\ndef _af_commutes_with(a, b):\n    \"\"\"\n    Checks if the two permutations with array forms\n    given by ``a`` and ``b`` commute.\n\n    Examples\n    ========\n\n    >>> from sympy.combinatorics.permutations import _af_commutes_with\n    >>> _af_commutes_with([1, 2, 0], [0, 2, 1])\n    False\n\n    See Also\n    ========\n\n    Permutation, commutes_with\n    \"\"\"\n    return not any(a[b[i]] != b[a[i]] for i in range(len(a) - 1))\n\n\nclass Cycle(dict):\n    \"\"\"\n    Wrapper around dict which provides the functionality of a disjoint cycle.\n\n    A cycle shows the rule to use to move subsets of elements to obtain\n    a permutation. The Cycle class is more flexible than Permutation in\n    that 1) all elements need not be present in order to investigate how\n    multiple cycles act in sequence and 2) it can contain singletons:\n\n    >>> from sympy.combinatorics.permutations import Perm, Cycle\n\n    A Cycle will automatically parse a cycle given as a tuple on the rhs:\n\n    >>> Cycle(1, 2)(2, 3)\n    (1 3 2)\n\n    The identity cycle, Cycle(), can be used to start a product:\n\n    >>> Cycle()(1, 2)(2, 3)\n    (1 3 2)\n\n    The array form of a Cycle can be obtained by calling the list\n    method (or passing it to the list function) and all elements from\n    0 will be shown:\n\n    >>> a = Cycle(1, 2)\n    >>> a.list()\n    [0, 2, 1]\n    >>> list(a)\n    [0, 2, 1]\n\n    If a larger (or smaller) range is desired use the list method and\n    provide the desired size -- but the Cycle cannot be truncated to\n    a size smaller than the largest element that is out of place:\n\n    >>> b = Cycle(2, 4)(1, 2)(3, 1, 4)(1, 3)\n    >>> b.list()\n    [0, 2, 1, 3, 4]\n    >>> b.list(b.size + 1)\n    [0, 2, 1, 3, 4, 5]\n    >>> b.list(-1)\n    [0, 2, 1]\n\n    Singletons are not shown when printing with one exception: the largest\n    element is always shown -- as a singleton if necessary:\n\n    >>> Cycle(1, 4, 10)(4, 5)\n    (1 5 4 10)\n    >>> Cycle(1, 2)(4)(5)(10)\n    (1 2)(10)\n\n    The array form can be used to instantiate a Permutation so other\n    properties of the permutation can be investigated:\n\n    >>> Perm(Cycle(1, 2)(3, 4).list()).transpositions()\n    [(1, 2), (3, 4)]\n\n    Notes\n    =====\n\n    The underlying structure of the Cycle is a dictionary and although\n    the __iter__ method has been redefined to give the array form of the\n    cycle, the underlying dictionary items are still available with the\n    such methods as items():\n\n    >>> list(Cycle(1, 2).items())\n    [(1, 2), (2, 1)]\n\n    See Also\n    ========\n\n    Permutation\n    \"\"\"\n    def __missing__(self, arg):\n        \"\"\"Enter arg into dictionary and return arg.\"\"\"\n        arg = as_int(arg)\n        self[arg] = arg\n        return arg\n\n    def __iter__(self):\n        for i in self.list():\n            yield i\n\n    def __call__(self, *other):\n        \"\"\"Return product of cycles processed from R to L.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Cycle as C\n        >>> from sympy.combinatorics.permutations import Permutation as Perm\n        >>> C(1, 2)(2, 3)\n        (1 3 2)\n\n        An instance of a Cycle will automatically parse list-like\n        objects and Permutations that are on the right. It is more\n        flexible than the Permutation in that all elements need not\n        be present:\n\n        >>> a = C(1, 2)\n        >>> a(2, 3)\n        (1 3 2)\n        >>> a(2, 3)(4, 5)\n        (1 3 2)(4 5)\n\n        \"\"\"\n        rv = Cycle(*other)\n        for k, v in zip(list(self.keys()), [rv[self[k]] for k in self.keys()]):\n            rv[k] = v\n        return rv\n\n    def list(self, size=None):\n        \"\"\"Return the cycles as an explicit list starting from 0 up\n        to the greater of the largest value in the cycles and size.\n\n        Truncation of trailing unmoved items will occur when size\n        is less than the maximum element in the cycle; if this is\n        desired, setting ``size=-1`` will guarantee such trimming.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Cycle\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> p = Cycle(2, 3)(4, 5)\n        >>> p.list()\n        [0, 1, 3, 2, 5, 4]\n        >>> p.list(10)\n        [0, 1, 3, 2, 5, 4, 6, 7, 8, 9]\n\n        Passing a length too small will trim trailing, unchanged elements\n        in the permutation:\n\n        >>> Cycle(2, 4)(1, 2, 4).list(-1)\n        [0, 2, 1]\n        \"\"\"\n        if not self and size is None:\n            raise ValueError('must give size for empty Cycle')\n        if size is not None:\n            big = max([i for i in self.keys() if self[i] != i] + [0])\n            size = max(size, big + 1)\n        else:\n            size = self.size\n        return [self[i] for i in range(size)]\n\n    def __repr__(self):\n        \"\"\"We want it to print as a Cycle, not as a dict.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Cycle\n        >>> Cycle(1, 2)\n        (1 2)\n        >>> print(_)\n        (1 2)\n        >>> list(Cycle(1, 2).items())\n        [(1, 2), (2, 1)]\n        \"\"\"\n        if not self:\n            return 'Cycle()'\n        cycles = Permutation(self).cyclic_form\n        s = ''.join(str(tuple(c)) for c in cycles)\n        big = self.size - 1\n        if not any(i == big for c in cycles for i in c):\n            s += '(%s)' % big\n        return 'Cycle%s' % s\n\n    def __str__(self):\n        \"\"\"We want it to be printed in a Cycle notation with no\n        comma in-between.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Cycle\n        >>> Cycle(1, 2)\n        (1 2)\n        >>> Cycle(1, 2, 4)(5, 6)\n        (1 2 4)(5 6)\n        \"\"\"\n        if not self:\n            return '()'\n        cycles = Permutation(self).cyclic_form\n        s = ''.join(str(tuple(c)) for c in cycles)\n        big = self.size - 1\n        if not any(i == big for c in cycles for i in c):\n            s += '(%s)' % big\n        s = s.replace(',', '')\n        return s\n\n    def __init__(self, *args):\n        \"\"\"Load up a Cycle instance with the values for the cycle.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Cycle\n        >>> Cycle(1, 2, 6)\n        (1 2 6)\n        \"\"\"\n\n        if not args:\n            return\n        if len(args) == 1:\n            if isinstance(args[0], Permutation):\n                for c in args[0].cyclic_form:\n                    self.update(self(*c))\n                return\n            elif isinstance(args[0], Cycle):\n                for k, v in args[0].items():\n                    self[k] = v\n                return\n        args = [as_int(a) for a in args]\n        if any(i < 0 for i in args):\n            raise ValueError('negative integers are not allowed in a cycle.')\n        if has_dups(args):\n            raise ValueError('All elements must be unique in a cycle.')\n        for i in range(-len(args), 0):\n            self[args[i]] = args[i + 1]\n\n    @property\n    def size(self):\n        if not self:\n            return 0\n        return max(self.keys()) + 1\n\n    def copy(self):\n        return Cycle(self)\n\nclass Permutation(Basic):\n    \"\"\"\n    A permutation, alternatively known as an 'arrangement number' or 'ordering'\n    is an arrangement of the elements of an ordered list into a one-to-one\n    mapping with itself. The permutation of a given arrangement is given by\n    indicating the positions of the elements after re-arrangement [2]_. For\n    example, if one started with elements [x, y, a, b] (in that order) and\n    they were reordered as [x, y, b, a] then the permutation would be\n    [0, 1, 3, 2]. Notice that (in SymPy) the first element is always referred\n    to as 0 and the permutation uses the indices of the elements in the\n    original ordering, not the elements (a, b, etc...) themselves.\n\n    >>> from sympy.combinatorics import Permutation\n    >>> Permutation.print_cyclic = False\n\n    Permutations Notation\n    =====================\n\n    Permutations are commonly represented in disjoint cycle or array forms.\n\n    Array Notation and 2-line Form\n    ------------------------------------\n\n    In the 2-line form, the elements and their final positions are shown\n    as a matrix with 2 rows:\n\n    [0    1    2     ... n-1]\n    [p(0) p(1) p(2)  ... p(n-1)]\n\n    Since the first line is always range(n), where n is the size of p,\n    it is sufficient to represent the permutation by the second line,\n    referred to as the \"array form\" of the permutation. This is entered\n    in brackets as the argument to the Permutation class:\n\n    >>> p = Permutation([0, 2, 1]); p\n    Permutation([0, 2, 1])\n\n    Given i in range(p.size), the permutation maps i to i^p\n\n    >>> [i^p for i in range(p.size)]\n    [0, 2, 1]\n\n    The composite of two permutations p*q means first apply p, then q, so\n    i^(p*q) = (i^p)^q which is i^p^q according to Python precedence rules:\n\n    >>> q = Permutation([2, 1, 0])\n    >>> [i^p^q for i in range(3)]\n    [2, 0, 1]\n    >>> [i^(p*q) for i in range(3)]\n    [2, 0, 1]\n\n    One can use also the notation p(i) = i^p, but then the composition\n    rule is (p*q)(i) = q(p(i)), not p(q(i)):\n\n    >>> [(p*q)(i) for i in range(p.size)]\n    [2, 0, 1]\n    >>> [q(p(i)) for i in range(p.size)]\n    [2, 0, 1]\n    >>> [p(q(i)) for i in range(p.size)]\n    [1, 2, 0]\n\n    Disjoint Cycle Notation\n    -----------------------\n\n    In disjoint cycle notation, only the elements that have shifted are\n    indicated. In the above case, the 2 and 1 switched places. This can\n    be entered in two ways:\n\n    >>> Permutation(1, 2) == Permutation([[1, 2]]) == p\n    True\n\n    Only the relative ordering of elements in a cycle matter:\n\n    >>> Permutation(1,2,3) == Permutation(2,3,1) == Permutation(3,1,2)\n    True\n\n    The disjoint cycle notation is convenient when representing permutations\n    that have several cycles in them:\n\n    >>> Permutation(1, 2)(3, 5) == Permutation([[1, 2], [3, 5]])\n    True\n\n    It also provides some economy in entry when computing products of\n    permutations that are written in disjoint cycle notation:\n\n    >>> Permutation(1, 2)(1, 3)(2, 3)\n    Permutation([0, 3, 2, 1])\n    >>> _ == Permutation([[1, 2]])*Permutation([[1, 3]])*Permutation([[2, 3]])\n    True\n\n    Entering a singleton in a permutation is a way to indicate the size of the\n    permutation. The ``size`` keyword can also be used.\n\n    Array-form entry:\n\n    >>> Permutation([[1, 2], [9]])\n    Permutation([0, 2, 1], size=10)\n    >>> Permutation([[1, 2]], size=10)\n    Permutation([0, 2, 1], size=10)\n\n    Cyclic-form entry:\n\n    >>> Permutation(1, 2, size=10)\n    Permutation([0, 2, 1], size=10)\n    >>> Permutation(9)(1, 2)\n    Permutation([0, 2, 1], size=10)\n\n    Caution: no singleton containing an element larger than the largest\n    in any previous cycle can be entered. This is an important difference\n    in how Permutation and Cycle handle the __call__ syntax. A singleton\n    argument at the start of a Permutation performs instantiation of the\n    Permutation and is permitted:\n\n    >>> Permutation(5)\n    Permutation([], size=6)\n\n    A singleton entered after instantiation is a call to the permutation\n    -- a function call -- and if the argument is out of range it will\n    trigger an error. For this reason, it is better to start the cycle\n    with the singleton:\n\n    The following fails because there is is no element 3:\n\n    >>> Permutation(1, 2)(3)\n    Traceback (most recent call last):\n    ...\n    IndexError: list index out of range\n\n    This is ok: only the call to an out of range singleton is prohibited;\n    otherwise the permutation autosizes:\n\n    >>> Permutation(3)(1, 2)\n    Permutation([0, 2, 1, 3])\n    >>> Permutation(1, 2)(3, 4) == Permutation(3, 4)(1, 2)\n    True\n\n\n    Equality testing\n    ----------------\n\n    The array forms must be the same in order for permutations to be equal:\n\n    >>> Permutation([1, 0, 2, 3]) == Permutation([1, 0])\n    False\n\n\n    Identity Permutation\n    --------------------\n\n    The identity permutation is a permutation in which no element is out of\n    place. It can be entered in a variety of ways. All the following create\n    an identity permutation of size 4:\n\n    >>> I = Permutation([0, 1, 2, 3])\n    >>> all(p == I for p in [\n    ... Permutation(3),\n    ... Permutation(range(4)),\n    ... Permutation([], size=4),\n    ... Permutation(size=4)])\n    True\n\n    Watch out for entering the range *inside* a set of brackets (which is\n    cycle notation):\n\n    >>> I == Permutation([range(4)])\n    False\n\n\n    Permutation Printing\n    ====================\n\n    There are a few things to note about how Permutations are printed.\n\n    1) If you prefer one form (array or cycle) over another, you can set that\n    with the print_cyclic flag.\n\n    >>> Permutation(1, 2)(4, 5)(3, 4)\n    Permutation([0, 2, 1, 4, 5, 3])\n    >>> p = _\n\n    >>> Permutation.print_cyclic = True\n    >>> p\n    (1 2)(3 4 5)\n    >>> Permutation.print_cyclic = False\n\n    2) Regardless of the setting, a list of elements in the array for cyclic\n    form can be obtained and either of those can be copied and supplied as\n    the argument to Permutation:\n\n    >>> p.array_form\n    [0, 2, 1, 4, 5, 3]\n    >>> p.cyclic_form\n    [[1, 2], [3, 4, 5]]\n    >>> Permutation(_) == p\n    True\n\n    3) Printing is economical in that as little as possible is printed while\n    retaining all information about the size of the permutation:\n\n    >>> Permutation([1, 0, 2, 3])\n    Permutation([1, 0, 2, 3])\n    >>> Permutation([1, 0, 2, 3], size=20)\n    Permutation([1, 0], size=20)\n    >>> Permutation([1, 0, 2, 4, 3, 5, 6], size=20)\n    Permutation([1, 0, 2, 4, 3], size=20)\n\n    >>> p = Permutation([1, 0, 2, 3])\n    >>> Permutation.print_cyclic = True\n    >>> p\n    (3)(0 1)\n    >>> Permutation.print_cyclic = False\n\n    The 2 was not printed but it is still there as can be seen with the\n    array_form and size methods:\n\n    >>> p.array_form\n    [1, 0, 2, 3]\n    >>> p.size\n    4\n\n    Short introduction to other methods\n    ===================================\n\n    The permutation can act as a bijective function, telling what element is\n    located at a given position\n\n    >>> q = Permutation([5, 2, 3, 4, 1, 0])\n    >>> q.array_form[1] # the hard way\n    2\n    >>> q(1) # the easy way\n    2\n    >>> {i: q(i) for i in range(q.size)} # showing the bijection\n    {0: 5, 1: 2, 2: 3, 3: 4, 4: 1, 5: 0}\n\n    The full cyclic form (including singletons) can be obtained:\n\n    >>> p.full_cyclic_form\n    [[0, 1], [2], [3]]\n\n    Any permutation can be factored into transpositions of pairs of elements:\n\n    >>> Permutation([[1, 2], [3, 4, 5]]).transpositions()\n    [(1, 2), (3, 5), (3, 4)]\n    >>> Permutation.rmul(*[Permutation([ti], size=6) for ti in _]).cyclic_form\n    [[1, 2], [3, 4, 5]]\n\n    The number of permutations on a set of n elements is given by n! and is\n    called the cardinality.\n\n    >>> p.size\n    4\n    >>> p.cardinality\n    24\n\n    A given permutation has a rank among all the possible permutations of the\n    same elements, but what that rank is depends on how the permutations are\n    enumerated. (There are a number of different methods of doing so.) The\n    lexicographic rank is given by the rank method and this rank is used to\n    increment a permutation with addition/subtraction:\n\n    >>> p.rank()\n    6\n    >>> p + 1\n    Permutation([1, 0, 3, 2])\n    >>> p.next_lex()\n    Permutation([1, 0, 3, 2])\n    >>> _.rank()\n    7\n    >>> p.unrank_lex(p.size, rank=7)\n    Permutation([1, 0, 3, 2])\n\n    The product of two permutations p and q is defined as their composition as\n    functions, (p*q)(i) = q(p(i)) [6]_.\n\n    >>> p = Permutation([1, 0, 2, 3])\n    >>> q = Permutation([2, 3, 1, 0])\n    >>> list(q*p)\n    [2, 3, 0, 1]\n    >>> list(p*q)\n    [3, 2, 1, 0]\n    >>> [q(p(i)) for i in range(p.size)]\n    [3, 2, 1, 0]\n\n    The permutation can be 'applied' to any list-like object, not only\n    Permutations:\n\n    >>> p(['zero', 'one', 'four', 'two'])\n     ['one', 'zero', 'four', 'two']\n    >>> p('zo42')\n    ['o', 'z', '4', '2']\n\n    If you have a list of arbitrary elements, the corresponding permutation\n    can be found with the from_sequence method:\n\n    >>> Permutation.from_sequence('SymPy')\n    Permutation([1, 3, 2, 0, 4])\n\n    See Also\n    ========\n\n    Cycle\n\n    References\n    ==========\n\n    .. [1] Skiena, S. 'Permutations.' 1.1 in Implementing Discrete Mathematics\n           Combinatorics and Graph Theory with Mathematica.  Reading, MA:\n           Addison-Wesley, pp. 3-16, 1990.\n\n    .. [2] Knuth, D. E. The Art of Computer Programming, Vol. 4: Combinatorial\n           Algorithms, 1st ed. Reading, MA: Addison-Wesley, 2011.\n\n    .. [3] Wendy Myrvold and Frank Ruskey. 2001. Ranking and unranking\n           permutations in linear time. Inf. Process. Lett. 79, 6 (September 2001),\n           281-284. DOI=10.1016/S0020-0190(01)00141-7\n\n    .. [4] D. L. Kreher, D. R. Stinson 'Combinatorial Algorithms'\n           CRC Press, 1999\n\n    .. [5] Graham, R. L.; Knuth, D. E.; and Patashnik, O.\n           Concrete Mathematics: A Foundation for Computer Science, 2nd ed.\n           Reading, MA: Addison-Wesley, 1994.\n\n    .. [6] http://en.wikipedia.org/wiki/Permutation#Product_and_inverse\n\n    .. [7] http://en.wikipedia.org/wiki/Lehmer_code\n\n    \"\"\"\n\n    is_Permutation = True\n\n    _array_form = None\n    _cyclic_form = None\n    _cycle_structure = None\n    _size = None\n    _rank = None\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"\n        Constructor for the Permutation object from a list or a\n        list of lists in which all elements of the permutation may\n        appear only once.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n\n        Permutations entered in array-form are left unaltered:\n\n        >>> Permutation([0, 2, 1])\n        Permutation([0, 2, 1])\n\n        Permutations entered in cyclic form are converted to array form;\n        singletons need not be entered, but can be entered to indicate the\n        largest element:\n\n        >>> Permutation([[4, 5, 6], [0, 1]])\n        Permutation([1, 0, 2, 3, 5, 6, 4])\n        >>> Permutation([[4, 5, 6], [0, 1], [19]])\n        Permutation([1, 0, 2, 3, 5, 6, 4], size=20)\n\n        All manipulation of permutations assumes that the smallest element\n        is 0 (in keeping with 0-based indexing in Python) so if the 0 is\n        missing when entering a permutation in array form, an error will be\n        raised:\n\n        >>> Permutation([2, 1])\n        Traceback (most recent call last):\n        ...\n        ValueError: Integers 0 through 2 must be present.\n\n        If a permutation is entered in cyclic form, it can be entered without\n        singletons and the ``size`` specified so those values can be filled\n        in, otherwise the array form will only extend to the maximum value\n        in the cycles:\n\n        >>> Permutation([[1, 4], [3, 5, 2]], size=10)\n        Permutation([0, 4, 3, 5, 1, 2], size=10)\n        >>> _.array_form\n        [0, 4, 3, 5, 1, 2, 6, 7, 8, 9]\n        \"\"\"\n        size = kwargs.pop('size', None)\n        if size is not None:\n            size = int(size)\n\n        #a) ()\n        #b) (1) = identity\n        #c) (1, 2) = cycle\n        #d) ([1, 2, 3]) = array form\n        #e) ([[1, 2]]) = cyclic form\n        #f) (Cycle) = conversion to permutation\n        #g) (Permutation) = adjust size or return copy\n        ok = True\n        if not args:  # a\n            return _af_new(list(range(size or 0)))\n        elif len(args) > 1:  # c\n            return _af_new(Cycle(*args).list(size))\n        if len(args) == 1:\n            a = args[0]\n            if isinstance(a, Perm):  # g\n                if size is None or size == a.size:\n                    return a\n                return Perm(a.array_form, size=size)\n            if isinstance(a, Cycle):  # f\n                return _af_new(a.list(size))\n            if not is_sequence(a):  # b\n                return _af_new(list(range(a + 1)))\n            if has_variety(is_sequence(ai) for ai in a):\n                ok = False\n        else:\n            ok = False\n        if not ok:\n            raise ValueError(\"Permutation argument must be a list of ints, \"\n                             \"a list of lists, Permutation or Cycle.\")\n\n\n        # safe to assume args are valid; this also makes a copy\n        # of the args\n        args = list(args[0])\n\n        is_cycle = args and is_sequence(args[0])\n        if is_cycle:  # e\n            args = [[int(i) for i in c] for c in args]\n        else:  # d\n            args = [int(i) for i in args]\n\n        # if there are n elements present, 0, 1, ..., n-1 should be present\n        # unless a cycle notation has been provided. A 0 will be added\n        # for convenience in case one wants to enter permutations where\n        # counting starts from 1.\n\n        temp = flatten(args)\n        if has_dups(temp) and not is_cycle:\n            raise ValueError('there were repeated elements.')\n        temp = set(temp)\n\n        if not is_cycle and \\\n                any(i not in temp for i in range(len(temp))):\n            raise ValueError(\"Integers 0 through %s must be present.\" %\n                             max(temp))\n\n        if is_cycle:\n            # it's not necessarily canonical so we won't store\n            # it -- use the array form instead\n            c = Cycle()\n            for ci in args:\n                c = c(*ci)\n            aform = c.list()\n        else:\n            aform = list(args)\n        if size and size > len(aform):\n            # don't allow for truncation of permutation which\n            # might split a cycle and lead to an invalid aform\n            # but do allow the permutation size to be increased\n            aform.extend(list(range(len(aform), size)))\n        size = len(aform)\n        obj = Basic.__new__(cls, aform)\n        obj._array_form = aform\n        obj._size = size\n        return obj\n\n    @staticmethod\n    def _af_new(perm):\n        \"\"\"A method to produce a Permutation object from a list;\n        the list is bound to the _array_form attribute, so it must\n        not be modified; this method is meant for internal use only;\n        the list ``a`` is supposed to be generated as a temporary value\n        in a method, so p = Perm._af_new(a) is the only object\n        to hold a reference to ``a``::\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Perm\n        >>> Perm.print_cyclic = False\n        >>> a = [2,1,3,0]\n        >>> p = Perm._af_new(a)\n        >>> p\n        Permutation([2, 1, 3, 0])\n\n        \"\"\"\n        p = Basic.__new__(Perm, perm)\n        p._array_form = perm\n        p._size = len(perm)\n        return p\n\n    def _hashable_content(self):\n        # the array_form (a list) is the Permutation arg, so we need to\n        # return a tuple, instead\n        return tuple(self.array_form)\n\n    @property\n    def array_form(self):\n        \"\"\"\n        Return a copy of the attribute _array_form\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> p = Permutation([[2, 0], [3, 1]])\n        >>> p.array_form\n        [2, 3, 0, 1]\n        >>> Permutation([[2, 0, 3, 1]]).array_form\n        [3, 2, 0, 1]\n        >>> Permutation([2, 0, 3, 1]).array_form\n        [2, 0, 3, 1]\n        >>> Permutation([[1, 2], [4, 5]]).array_form\n        [0, 2, 1, 3, 5, 4]\n        \"\"\"\n        return self._array_form[:]\n\n    def __repr__(self):\n        from sympy.combinatorics.permutations import Permutation, Cycle\n        if Permutation.print_cyclic:\n            if not self.size:\n                return 'Permutation()'\n            # before taking Cycle notation, see if the last element is\n            # a singleton and move it to the head of the string\n            s = Cycle(self)(self.size - 1).__repr__()[len('Cycle'):]\n            last = s.rfind('(')\n            if not last == 0 and ',' not in s[last:]:\n                s = s[last:] + s[:last]\n            return 'Permutation%s' %s\n        else:\n            s = self.support()\n            if not s:\n                if self.size < 5:\n                    return 'Permutation(%s)' % str(self.array_form)\n                return 'Permutation([], size=%s)' % self.size\n            trim = str(self.array_form[:s[-1] + 1]) + ', size=%s' % self.size\n            use = full = str(self.array_form)\n            if len(trim) < len(full):\n                use = trim\n            return 'Permutation(%s)' % use\n\n    def list(self, size=None):\n        \"\"\"Return the permutation as an explicit list, possibly\n        trimming unmoved elements if size is less than the maximum\n        element in the permutation; if this is desired, setting\n        ``size=-1`` will guarantee such trimming.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> p = Permutation(2, 3)(4, 5)\n        >>> p.list()\n        [0, 1, 3, 2, 5, 4]\n        >>> p.list(10)\n        [0, 1, 3, 2, 5, 4, 6, 7, 8, 9]\n\n        Passing a length too small will trim trailing, unchanged elements\n        in the permutation:\n\n        >>> Permutation(2, 4)(1, 2, 4).list(-1)\n        [0, 2, 1]\n        >>> Permutation(3).list(-1)\n        []\n        \"\"\"\n        if not self and size is None:\n            raise ValueError('must give size for empty Cycle')\n        rv = self.array_form\n        if size is not None:\n            if size > self.size:\n                rv.extend(list(range(self.size, size)))\n            else:\n                # find first value from rhs where rv[i] != i\n                i = self.size - 1\n                while rv:\n                    if rv[-1] != i:\n                        break\n                    rv.pop()\n                    i -= 1\n        return rv\n\n    @property\n    def cyclic_form(self):\n        \"\"\"\n        This is used to convert to the cyclic notation\n        from the canonical notation. Singletons are omitted.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> p = Permutation([0, 3, 1, 2])\n        >>> p.cyclic_form\n        [[1, 3, 2]]\n        >>> Permutation([1, 0, 2, 4, 3, 5]).cyclic_form\n        [[0, 1], [3, 4]]\n\n        See Also\n        ========\n\n        array_form, full_cyclic_form\n        \"\"\"\n        if self._cyclic_form is not None:\n            return list(self._cyclic_form)\n        array_form = self.array_form\n        unchecked = [True] * len(array_form)\n        cyclic_form = []\n        for i in range(len(array_form)):\n            if unchecked[i]:\n                cycle = []\n                cycle.append(i)\n                unchecked[i] = False\n                j = i\n                while unchecked[array_form[j]]:\n                    j = array_form[j]\n                    cycle.append(j)\n                    unchecked[j] = False\n                if len(cycle) > 1:\n                    cyclic_form.append(cycle)\n                    assert cycle == list(minlex(cycle, is_set=True))\n        cyclic_form.sort()\n        self._cyclic_form = cyclic_form[:]\n        return cyclic_form\n\n    @property\n    def full_cyclic_form(self):\n        \"\"\"Return permutation in cyclic form including singletons.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation([0, 2, 1]).full_cyclic_form\n        [[0], [1, 2]]\n        \"\"\"\n        need = set(range(self.size)) - set(flatten(self.cyclic_form))\n        rv = self.cyclic_form\n        rv.extend([[i] for i in need])\n        rv.sort()\n        return rv\n\n    @property\n    def size(self):\n        \"\"\"\n        Returns the number of elements in the permutation.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> Permutation([[3, 2], [0, 1]]).size\n        4\n\n        See Also\n        ========\n\n        cardinality, length, order, rank\n        \"\"\"\n        return self._size\n\n    def support(self):\n        \"\"\"Return the elements in permutation, P, for which P[i] != i.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> p = Permutation([[3, 2], [0, 1], [4]])\n        >>> p.array_form\n        [1, 0, 3, 2, 4]\n        >>> p.support()\n        [0, 1, 2, 3]\n        \"\"\"\n        a = self.array_form\n        return [i for i, e in enumerate(a) if a[i] != i]\n\n    def __add__(self, other):\n        \"\"\"Return permutation that is other higher in rank than self.\n\n        The rank is the lexicographical rank, with the identity permutation\n        having rank of 0.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> I = Permutation([0, 1, 2, 3])\n        >>> a = Permutation([2, 1, 3, 0])\n        >>> I + a.rank() == a\n        True\n\n        See Also\n        ========\n\n        __sub__, inversion_vector\n\n        \"\"\"\n        rank = (self.rank() + other) % self.cardinality\n        rv = Perm.unrank_lex(self.size, rank)\n        rv._rank = rank\n        return rv\n\n    def __sub__(self, other):\n        \"\"\"Return the permutation that is other lower in rank than self.\n\n        See Also\n        ========\n\n        __add__\n        \"\"\"\n        return self.__add__(-other)\n\n    @staticmethod\n    def rmul(*args):\n        \"\"\"\n        Return product of Permutations [a, b, c, ...] as the Permutation whose\n        ith value is a(b(c(i))).\n\n        a, b, c, ... can be Permutation objects or tuples.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import _af_rmul, Permutation\n        >>> Permutation.print_cyclic = False\n\n        >>> a, b = [1, 0, 2], [0, 2, 1]\n        >>> a = Permutation(a); b = Permutation(b)\n        >>> list(Permutation.rmul(a, b))\n        [1, 2, 0]\n        >>> [a(b(i)) for i in range(3)]\n        [1, 2, 0]\n\n        This handles the operands in reverse order compared to the ``*`` operator:\n\n        >>> a = Permutation(a); b = Permutation(b)\n        >>> list(a*b)\n        [2, 0, 1]\n        >>> [b(a(i)) for i in range(3)]\n        [2, 0, 1]\n\n        Notes\n        =====\n\n        All items in the sequence will be parsed by Permutation as\n        necessary as long as the first item is a Permutation:\n\n        >>> Permutation.rmul(a, [0, 2, 1]) == Permutation.rmul(a, b)\n        True\n\n        The reverse order of arguments will raise a TypeError.\n\n        \"\"\"\n        rv = args[0]\n        for i in range(1, len(args)):\n            rv = args[i]*rv\n        return rv\n\n    @staticmethod\n    def rmul_with_af(*args):\n        \"\"\"\n        same as rmul, but the elements of args are Permutation objects\n        which have _array_form\n        \"\"\"\n        a = [x._array_form for x in args]\n        rv = _af_new(_af_rmuln(*a))\n        return rv\n\n    def mul_inv(self, other):\n        \"\"\"\n        other*~self, self and other have _array_form\n        \"\"\"\n        a = _af_invert(self._array_form)\n        b = other._array_form\n        return _af_new(_af_rmul(a, b))\n\n    def __rmul__(self, other):\n        \"\"\"This is needed to coerse other to Permutation in rmul.\"\"\"\n        return Perm(other)*self\n\n    def __mul__(self, other):\n        \"\"\"\n        Return the product a*b as a Permutation; the ith value is b(a(i)).\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import _af_rmul, Permutation\n        >>> Permutation.print_cyclic = False\n\n        >>> a, b = [1, 0, 2], [0, 2, 1]\n        >>> a = Permutation(a); b = Permutation(b)\n        >>> list(a*b)\n        [2, 0, 1]\n        >>> [b(a(i)) for i in range(3)]\n        [2, 0, 1]\n\n        This handles operands in reverse order compared to _af_rmul and rmul:\n\n        >>> al = list(a); bl = list(b)\n        >>> _af_rmul(al, bl)\n        [1, 2, 0]\n        >>> [al[bl[i]] for i in range(3)]\n        [1, 2, 0]\n\n        It is acceptable for the arrays to have different lengths; the shorter\n        one will be padded to match the longer one:\n\n        >>> b*Permutation([1, 0])\n        Permutation([1, 2, 0])\n        >>> Permutation([1, 0])*b\n        Permutation([2, 0, 1])\n\n        It is also acceptable to allow coercion to handle conversion of a\n        single list to the left of a Permutation:\n\n        >>> [0, 1]*a # no change: 2-element identity\n        Permutation([1, 0, 2])\n        >>> [[0, 1]]*a # exchange first two elements\n        Permutation([0, 1, 2])\n\n        You cannot use more than 1 cycle notation in a product of cycles\n        since coercion can only handle one argument to the left. To handle\n        multiple cycles it is convenient to use Cycle instead of Permutation:\n\n        >>> [[1, 2]]*[[2, 3]]*Permutation([]) # doctest: +SKIP\n        >>> from sympy.combinatorics.permutations import Cycle\n        >>> Cycle(1, 2)(2, 3)\n        (1 3 2)\n\n        \"\"\"\n        a = self.array_form\n        # __rmul__ makes sure the other is a Permutation\n        b = other.array_form\n        if not b:\n            perm = a\n        else:\n            b.extend(list(range(len(b), len(a))))\n            perm = [b[i] for i in a] + b[len(a):]\n        return _af_new(perm)\n\n    def commutes_with(self, other):\n        \"\"\"\n        Checks if the elements are commuting.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> a = Permutation([1, 4, 3, 0, 2, 5])\n        >>> b = Permutation([0, 1, 2, 3, 4, 5])\n        >>> a.commutes_with(b)\n        True\n        >>> b = Permutation([2, 3, 5, 4, 1, 0])\n        >>> a.commutes_with(b)\n        False\n        \"\"\"\n        a = self.array_form\n        b = other.array_form\n        return _af_commutes_with(a, b)\n\n    def __pow__(self, n):\n        \"\"\"\n        Routine for finding powers of a permutation.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> p = Permutation([2,0,3,1])\n        >>> p.order()\n        4\n        >>> p**4\n        Permutation([0, 1, 2, 3])\n        \"\"\"\n        if type(n) == Perm:\n            raise NotImplementedError(\n                'p**p is not defined; do you mean p^p (conjugate)?')\n        n = int(n)\n        return _af_new(_af_pow(self.array_form, n))\n\n    def __rxor__(self, i):\n        \"\"\"Return self(i) when ``i`` is an int.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> p = Permutation(1, 2, 9)\n        >>> 2^p == p(2) == 9\n        True\n        \"\"\"\n        if int(i) == i:\n            return self(i)\n        else:\n            raise NotImplementedError(\n                \"i^p = p(i) when i is an integer, not %s.\" % i)\n\n    def __xor__(self, h):\n        \"\"\"Return the conjugate permutation ``~h*self*h` `.\n\n        If ``a`` and ``b`` are conjugates, ``a = h*b*~h`` and\n        ``b = ~h*a*h`` and both have the same cycle structure.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = True\n        >>> p = Permutation(1, 2, 9)\n        >>> q = Permutation(6, 9, 8)\n        >>> p*q != q*p\n        True\n\n        Calculate and check properties of the conjugate:\n\n        >>> c = p^q\n        >>> c == ~q*p*q and p == q*c*~q\n        True\n\n        The expression q^p^r is equivalent to q^(p*r):\n\n        >>> r = Permutation(9)(4, 6, 8)\n        >>> q^p^r == q^(p*r)\n        True\n\n        If the term to the left of the conjugate operator, i, is an integer\n        then this is interpreted as selecting the ith element from the\n        permutation to the right:\n\n        >>> all(i^p == p(i) for i in range(p.size))\n        True\n\n        Note that the * operator as higher precedence than the ^ operator:\n\n        >>> q^r*p^r == q^(r*p)^r == Permutation(9)(1, 6, 4)\n        True\n\n        Notes\n        =====\n\n        In Python the precedence rule is p^q^r = (p^q)^r which differs\n        in general from p^(q^r)\n\n        >>> q^p^r\n        (9)(1 4 8)\n        >>> q^(p^r)\n        (9)(1 8 6)\n\n        For a given r and p, both of the following are conjugates of p:\n        ~r*p*r and r*p*~r. But these are not necessarily the same:\n\n        >>> ~r*p*r == r*p*~r\n        True\n\n        >>> p = Permutation(1, 2, 9)(5, 6)\n        >>> ~r*p*r == r*p*~r\n        False\n\n        The conjugate ~r*p*r was chosen so that ``p^q^r`` would be equivalent\n        to ``p^(q*r)`` rather than ``p^(r*q)``. To obtain r*p*~r, pass ~r to\n        this method:\n\n        >>> p^~r == r*p*~r\n        True\n        \"\"\"\n\n        if self.size != h.size:\n            raise ValueError(\"The permutations must be of equal size.\")\n        a = [None]*self.size\n        h = h._array_form\n        p = self._array_form\n        for i in range(self.size):\n            a[h[i]] = h[p[i]]\n        return _af_new(a)\n\n    def transpositions(self):\n        \"\"\"\n        Return the permutation decomposed into a list of transpositions.\n\n        It is always possible to express a permutation as the product of\n        transpositions, see [1]\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([[1, 2, 3], [0, 4, 5, 6, 7]])\n        >>> t = p.transpositions()\n        >>> t\n        [(0, 7), (0, 6), (0, 5), (0, 4), (1, 3), (1, 2)]\n        >>> print(''.join(str(c) for c in t))\n        (0, 7)(0, 6)(0, 5)(0, 4)(1, 3)(1, 2)\n        >>> Permutation.rmul(*[Permutation([ti], size=p.size) for ti in t]) == p\n        True\n\n        References\n        ==========\n\n        1. http://en.wikipedia.org/wiki/Transposition_%28mathematics%29#Properties\n\n        \"\"\"\n        a = self.cyclic_form\n        res = []\n        for x in a:\n            nx = len(x)\n            if nx == 2:\n                res.append(tuple(x))\n            elif nx > 2:\n                first = x[0]\n                for y in x[nx - 1:0:-1]:\n                    res.append((first, y))\n        return res\n\n    @classmethod\n    def from_sequence(self, i, key=None):\n        \"\"\"Return the permutation needed to obtain ``i`` from the sorted\n        elements of ``i``. If custom sorting is desired, a key can be given.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> Permutation.print_cyclic = True\n\n        >>> Permutation.from_sequence('SymPy')\n        (4)(0 1 3)\n        >>> _(sorted(\"SymPy\"))\n        ['S', 'y', 'm', 'P', 'y']\n        >>> Permutation.from_sequence('SymPy', key=lambda x: x.lower())\n        (4)(0 2)(1 3)\n        \"\"\"\n        ic = list(zip(i, list(range(len(i)))))\n        if key:\n            ic.sort(key=lambda x: key(x[0]))\n        else:\n            ic.sort()\n        return ~Permutation([i[1] for i in ic])\n\n    def __invert__(self):\n        \"\"\"\n        Return the inverse of the permutation.\n\n        A permutation multiplied by its inverse is the identity permutation.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([[2,0], [3,1]])\n        >>> ~p\n        Permutation([2, 3, 0, 1])\n        >>> _ == p**-1\n        True\n        >>> p*~p == ~p*p == Permutation([0, 1, 2, 3])\n        True\n        \"\"\"\n        return _af_new(_af_invert(self._array_form))\n\n    def __iter__(self):\n        \"\"\"Yield elements from array form.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> list(Permutation(range(3)))\n        [0, 1, 2]\n        \"\"\"\n        for i in self.array_form:\n            yield i\n\n    def __call__(self, *i):\n        \"\"\"\n        Allows applying a permutation instance as a bijective function.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([[2, 0], [3, 1]])\n        >>> p.array_form\n        [2, 3, 0, 1]\n        >>> [p(i) for i in range(4)]\n        [2, 3, 0, 1]\n\n        If an array is given then the permutation selects the items\n        from the array (i.e. the permutation is applied to the array):\n\n        >>> from sympy.abc import x\n        >>> p([x, 1, 0, x**2])\n        [0, x**2, x, 1]\n        \"\"\"\n        # list indices can be Integer or int; leave this\n        # as it is (don't test or convert it) because this\n        # gets called a lot and should be fast\n        if len(i) == 1:\n            i = i[0]\n            try:\n                # P(1)\n                return self._array_form[i]\n            except TypeError:\n                try:\n                    # P([a, b, c])\n                    return [i[j] for j in self._array_form]\n                except Exception:\n                    raise TypeError('unrecognized argument')\n        else:\n            # P(1, 2, 3)\n            return self*Permutation(Cycle(*i), size=self.size)\n\n    def atoms(self):\n        \"\"\"\n        Returns all the elements of a permutation\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> Permutation([0, 1, 2, 3, 4, 5]).atoms()\n        {0, 1, 2, 3, 4, 5}\n        >>> Permutation([[0, 1], [2, 3], [4, 5]]).atoms()\n        {0, 1, 2, 3, 4, 5}\n        \"\"\"\n        return set(self.array_form)\n\n    def next_lex(self):\n        \"\"\"\n        Returns the next permutation in lexicographical order.\n        If self is the last permutation in lexicographical order\n        it returns None.\n        See [4] section 2.4.\n\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([2, 3, 1, 0])\n        >>> p = Permutation([2, 3, 1, 0]); p.rank()\n        17\n        >>> p = p.next_lex(); p.rank()\n        18\n\n        See Also\n        ========\n\n        rank, unrank_lex\n        \"\"\"\n        perm = self.array_form[:]\n        n = len(perm)\n        i = n - 2\n        while perm[i + 1] < perm[i]:\n            i -= 1\n        if i == -1:\n            return None\n        else:\n            j = n - 1\n            while perm[j] < perm[i]:\n                j -= 1\n            perm[j], perm[i] = perm[i], perm[j]\n            i += 1\n            j = n - 1\n            while i < j:\n                perm[j], perm[i] = perm[i], perm[j]\n                i += 1\n                j -= 1\n        return _af_new(perm)\n\n    @classmethod\n    def unrank_nonlex(self, n, r):\n        \"\"\"\n        This is a linear time unranking algorithm that does not\n        respect lexicographic order [3].\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> Permutation.unrank_nonlex(4, 5)\n        Permutation([2, 0, 3, 1])\n        >>> Permutation.unrank_nonlex(4, -1)\n        Permutation([0, 1, 2, 3])\n\n        See Also\n        ========\n\n        next_nonlex, rank_nonlex\n        \"\"\"\n        def _unrank1(n, r, a):\n            if n > 0:\n                a[n - 1], a[r % n] = a[r % n], a[n - 1]\n                _unrank1(n - 1, r//n, a)\n\n        id_perm = list(range(n))\n        n = int(n)\n        r = r % ifac(n)\n        _unrank1(n, r, id_perm)\n        return _af_new(id_perm)\n\n    def rank_nonlex(self, inv_perm=None):\n        \"\"\"\n        This is a linear time ranking algorithm that does not\n        enforce lexicographic order [3].\n\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 1, 2, 3])\n        >>> p.rank_nonlex()\n        23\n\n        See Also\n        ========\n\n        next_nonlex, unrank_nonlex\n        \"\"\"\n        def _rank1(n, perm, inv_perm):\n            if n == 1:\n                return 0\n            s = perm[n - 1]\n            t = inv_perm[n - 1]\n            perm[n - 1], perm[t] = perm[t], s\n            inv_perm[n - 1], inv_perm[s] = inv_perm[s], t\n            return s + n*_rank1(n - 1, perm, inv_perm)\n\n        if inv_perm is None:\n            inv_perm = (~self).array_form\n        if not inv_perm:\n            return 0\n        perm = self.array_form[:]\n        r = _rank1(len(perm), perm, inv_perm)\n        return r\n\n    def next_nonlex(self):\n        \"\"\"\n        Returns the next permutation in nonlex order [3].\n        If self is the last permutation in this order it returns None.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> p = Permutation([2, 0, 3, 1]); p.rank_nonlex()\n        5\n        >>> p = p.next_nonlex(); p\n        Permutation([3, 0, 1, 2])\n        >>> p.rank_nonlex()\n        6\n\n        See Also\n        ========\n\n        rank_nonlex, unrank_nonlex\n        \"\"\"\n        r = self.rank_nonlex()\n        if r == ifac(self.size) - 1:\n            return None\n        return Perm.unrank_nonlex(self.size, r + 1)\n\n    def rank(self):\n        \"\"\"\n        Returns the lexicographic rank of the permutation.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 1, 2, 3])\n        >>> p.rank()\n        0\n        >>> p = Permutation([3, 2, 1, 0])\n        >>> p.rank()\n        23\n\n        See Also\n        ========\n\n        next_lex, unrank_lex, cardinality, length, order, size\n        \"\"\"\n        if not self._rank is None:\n            return self._rank\n        rank = 0\n        rho = self.array_form[:]\n        n = self.size - 1\n        size = n + 1\n        psize = int(ifac(n))\n        for j in range(size - 1):\n            rank += rho[j]*psize\n            for i in range(j + 1, size):\n                if rho[i] > rho[j]:\n                    rho[i] -= 1\n            psize //= n\n            n -= 1\n        self._rank = rank\n        return rank\n\n    @property\n    def cardinality(self):\n        \"\"\"\n        Returns the number of all possible permutations.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 1, 2, 3])\n        >>> p.cardinality\n        24\n\n        See Also\n        ========\n\n        length, order, rank, size\n        \"\"\"\n        return int(ifac(self.size))\n\n    def parity(self):\n        \"\"\"\n        Computes the parity of a permutation.\n\n        The parity of a permutation reflects the parity of the\n        number of inversions in the permutation, i.e., the\n        number of pairs of x and y such that ``x > y`` but ``p[x] < p[y]``.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 1, 2, 3])\n        >>> p.parity()\n        0\n        >>> p = Permutation([3, 2, 0, 1])\n        >>> p.parity()\n        1\n\n        See Also\n        ========\n\n        _af_parity\n        \"\"\"\n        if self._cyclic_form is not None:\n            return (self.size - self.cycles) % 2\n\n        return _af_parity(self.array_form)\n\n    @property\n    def is_even(self):\n        \"\"\"\n        Checks if a permutation is even.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 1, 2, 3])\n        >>> p.is_even\n        True\n        >>> p = Permutation([3, 2, 1, 0])\n        >>> p.is_even\n        True\n\n        See Also\n        ========\n\n        is_odd\n        \"\"\"\n        return not self.is_odd\n\n    @property\n    def is_odd(self):\n        \"\"\"\n        Checks if a permutation is odd.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 1, 2, 3])\n        >>> p.is_odd\n        False\n        >>> p = Permutation([3, 2, 0, 1])\n        >>> p.is_odd\n        True\n\n        See Also\n        ========\n\n        is_even\n        \"\"\"\n        return bool(self.parity() % 2)\n\n    @property\n    def is_Singleton(self):\n        \"\"\"\n        Checks to see if the permutation contains only one number and is\n        thus the only possible permutation of this set of numbers\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> Permutation([0]).is_Singleton\n        True\n        >>> Permutation([0, 1]).is_Singleton\n        False\n\n        See Also\n        ========\n\n        is_Empty\n        \"\"\"\n        return self.size == 1\n\n    @property\n    def is_Empty(self):\n        \"\"\"\n        Checks to see if the permutation is a set with zero elements\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> Permutation([]).is_Empty\n        True\n        >>> Permutation([0]).is_Empty\n        False\n\n        See Also\n        ========\n\n        is_Singleton\n        \"\"\"\n        return self.size == 0\n\n    @property\n    def is_Identity(self):\n        \"\"\"\n        Returns True if the Permutation is an identity permutation.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([])\n        >>> p.is_Identity\n        True\n        >>> p = Permutation([[0], [1], [2]])\n        >>> p.is_Identity\n        True\n        >>> p = Permutation([0, 1, 2])\n        >>> p.is_Identity\n        True\n        >>> p = Permutation([0, 2, 1])\n        >>> p.is_Identity\n        False\n\n        See Also\n        ========\n\n        order\n        \"\"\"\n        af = self.array_form\n        return not af or all(i == af[i] for i in range(self.size))\n\n    def ascents(self):\n        \"\"\"\n        Returns the positions of ascents in a permutation, ie, the location\n        where p[i] < p[i+1]\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([4, 0, 1, 3, 2])\n        >>> p.ascents()\n        [1, 2]\n\n        See Also\n        ========\n\n        descents, inversions, min, max\n        \"\"\"\n        a = self.array_form\n        pos = [i for i in range(len(a) - 1) if a[i] < a[i + 1]]\n        return pos\n\n    def descents(self):\n        \"\"\"\n        Returns the positions of descents in a permutation, ie, the location\n        where p[i] > p[i+1]\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([4, 0, 1, 3, 2])\n        >>> p.descents()\n        [0, 3]\n\n        See Also\n        ========\n\n        ascents, inversions, min, max\n        \"\"\"\n        a = self.array_form\n        pos = [i for i in range(len(a) - 1) if a[i] > a[i + 1]]\n        return pos\n\n    def max(self):\n        \"\"\"\n        The maximum element moved by the permutation.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([1, 0, 2, 3, 4])\n        >>> p.max()\n        1\n\n        See Also\n        ========\n\n        min, descents, ascents, inversions\n        \"\"\"\n        max = 0\n        a = self.array_form\n        for i in range(len(a)):\n            if a[i] != i and a[i] > max:\n                max = a[i]\n        return max\n\n    def min(self):\n        \"\"\"\n        The minimum element moved by the permutation.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 1, 4, 3, 2])\n        >>> p.min()\n        2\n\n        See Also\n        ========\n\n        max, descents, ascents, inversions\n        \"\"\"\n        a = self.array_form\n        min = len(a)\n        for i in range(len(a)):\n            if a[i] != i and a[i] < min:\n                min = a[i]\n        return min\n\n    def inversions(self):\n        \"\"\"\n        Computes the number of inversions of a permutation.\n\n        An inversion is where i > j but p[i] < p[j].\n\n        For small length of p, it iterates over all i and j\n        values and calculates the number of inversions.\n        For large length of p, it uses a variation of merge\n        sort to calculate the number of inversions.\n\n        References\n        ==========\n\n        [1] http://www.cp.eng.chula.ac.th/~piak/teaching/algo/algo2008/count-inv.htm\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 1, 2, 3, 4, 5])\n        >>> p.inversions()\n        0\n        >>> Permutation([3, 2, 1, 0]).inversions()\n        6\n\n        See Also\n        ========\n\n        descents, ascents, min, max\n        \"\"\"\n        inversions = 0\n        a = self.array_form\n        n = len(a)\n        if n < 130:\n            for i in range(n - 1):\n                b = a[i]\n                for c in a[i + 1:]:\n                    if b > c:\n                        inversions += 1\n        else:\n            k = 1\n            right = 0\n            arr = a[:]\n            temp = a[:]\n            while k < n:\n                i = 0\n                while i + k < n:\n                    right = i + k * 2 - 1\n                    if right >= n:\n                        right = n - 1\n                    inversions += _merge(arr, temp, i, i + k, right)\n                    i = i + k * 2\n                k = k * 2\n        return inversions\n\n    def commutator(self, x):\n        \"\"\"Return the commutator of self and x: ``~x*~self*x*self``\n\n        If f and g are part of a group, G, then the commutator of f and g\n        is the group identity iff f and g commute, i.e. fg == gf.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> p = Permutation([0, 2, 3, 1])\n        >>> x = Permutation([2, 0, 3, 1])\n        >>> c = p.commutator(x); c\n        Permutation([2, 1, 3, 0])\n        >>> c == ~x*~p*x*p\n        True\n\n        >>> I = Permutation(3)\n        >>> p = [I + i for i in range(6)]\n        >>> for i in range(len(p)):\n        ...     for j in range(len(p)):\n        ...         c = p[i].commutator(p[j])\n        ...         if p[i]*p[j] == p[j]*p[i]:\n        ...             assert c == I\n        ...         else:\n        ...             assert c != I\n        ...\n\n        References\n        ==========\n\n        http://en.wikipedia.org/wiki/Commutator\n        \"\"\"\n\n        a = self.array_form\n        b = x.array_form\n        n = len(a)\n        if len(b) != n:\n            raise ValueError(\"The permutations must be of equal size.\")\n        inva = [None]*n\n        for i in range(n):\n            inva[a[i]] = i\n        invb = [None]*n\n        for i in range(n):\n            invb[b[i]] = i\n        return _af_new([a[b[inva[i]]] for i in invb])\n\n    def signature(self):\n        \"\"\"\n        Gives the signature of the permutation needed to place the\n        elements of the permutation in canonical order.\n\n        The signature is calculated as (-1)^<number of inversions>\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 1, 2])\n        >>> p.inversions()\n        0\n        >>> p.signature()\n        1\n        >>> q = Permutation([0,2,1])\n        >>> q.inversions()\n        1\n        >>> q.signature()\n        -1\n\n        See Also\n        ========\n\n        inversions\n        \"\"\"\n        if self.is_even:\n            return 1\n        return -1\n\n    def order(self):\n        \"\"\"\n        Computes the order of a permutation.\n\n        When the permutation is raised to the power of its\n        order it equals the identity permutation.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> p = Permutation([3, 1, 5, 2, 4, 0])\n        >>> p.order()\n        4\n        >>> (p**(p.order()))\n        Permutation([], size=6)\n\n        See Also\n        ========\n\n        identity, cardinality, length, rank, size\n        \"\"\"\n\n        return reduce(lcm, [len(cycle) for cycle in self.cyclic_form], 1)\n\n    def length(self):\n        \"\"\"\n        Returns the number of integers moved by a permutation.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> Permutation([0, 3, 2, 1]).length()\n        2\n        >>> Permutation([[0, 1], [2, 3]]).length()\n        4\n\n        See Also\n        ========\n\n        min, max, support, cardinality, order, rank, size\n        \"\"\"\n\n        return len(self.support())\n\n    @property\n    def cycle_structure(self):\n        \"\"\"Return the cycle structure of the permutation as a dictionary\n        indicating the multiplicity of each cycle length.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> Permutation.print_cyclic = True\n        >>> Permutation(3).cycle_structure\n        {1: 4}\n        >>> Permutation(0, 4, 3)(1, 2)(5, 6).cycle_structure\n        {2: 2, 3: 1}\n        \"\"\"\n        if self._cycle_structure:\n            rv = self._cycle_structure\n        else:\n            rv = defaultdict(int)\n            singletons = self.size\n            for c in self.cyclic_form:\n                rv[len(c)] += 1\n                singletons -= len(c)\n            if singletons:\n                rv[1] = singletons\n            self._cycle_structure = rv\n        return dict(rv)  # make a copy\n\n    @property\n    def cycles(self):\n        \"\"\"\n        Returns the number of cycles contained in the permutation\n        (including singletons).\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> Permutation([0, 1, 2]).cycles\n        3\n        >>> Permutation([0, 1, 2]).full_cyclic_form\n        [[0], [1], [2]]\n        >>> Permutation(0, 1)(2, 3).cycles\n        2\n\n        See Also\n        ========\n        sympy.functions.combinatorial.numbers.stirling\n        \"\"\"\n        return len(self.full_cyclic_form)\n\n    def index(self):\n        \"\"\"\n        Returns the index of a permutation.\n\n        The index of a permutation is the sum of all subscripts j such\n        that p[j] is greater than p[j+1].\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([3, 0, 2, 1, 4])\n        >>> p.index()\n        2\n        \"\"\"\n        a = self.array_form\n\n        return sum([j for j in range(len(a) - 1) if a[j] > a[j + 1]])\n\n    def runs(self):\n        \"\"\"\n        Returns the runs of a permutation.\n\n        An ascending sequence in a permutation is called a run [5].\n\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([2, 5, 7, 3, 6, 0, 1, 4, 8])\n        >>> p.runs()\n        [[2, 5, 7], [3, 6], [0, 1, 4, 8]]\n        >>> q = Permutation([1,3,2,0])\n        >>> q.runs()\n        [[1, 3], [2], [0]]\n        \"\"\"\n        return runs(self.array_form)\n\n    def inversion_vector(self):\n        \"\"\"Return the inversion vector of the permutation.\n\n        The inversion vector consists of elements whose value\n        indicates the number of elements in the permutation\n        that are lesser than it and lie on its right hand side.\n\n        The inversion vector is the same as the Lehmer encoding of a\n        permutation.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([4, 8, 0, 7, 1, 5, 3, 6, 2])\n        >>> p.inversion_vector()\n        [4, 7, 0, 5, 0, 2, 1, 1]\n        >>> p = Permutation([3, 2, 1, 0])\n        >>> p.inversion_vector()\n        [3, 2, 1]\n\n        The inversion vector increases lexicographically with the rank\n        of the permutation, the -ith element cycling through 0..i.\n\n        >>> p = Permutation(2)\n        >>> while p:\n        ...     print('%s %s %s' % (p, p.inversion_vector(), p.rank()))\n        ...     p = p.next_lex()\n        ...\n        Permutation([0, 1, 2]) [0, 0] 0\n        Permutation([0, 2, 1]) [0, 1] 1\n        Permutation([1, 0, 2]) [1, 0] 2\n        Permutation([1, 2, 0]) [1, 1] 3\n        Permutation([2, 0, 1]) [2, 0] 4\n        Permutation([2, 1, 0]) [2, 1] 5\n\n        See Also\n        ========\n        from_inversion_vector\n        \"\"\"\n        self_array_form = self.array_form\n        n = len(self_array_form)\n        inversion_vector = [0] * (n - 1)\n\n        for i in range(n - 1):\n            val = 0\n            for j in range(i + 1, n):\n                if self_array_form[j] < self_array_form[i]:\n                    val += 1\n            inversion_vector[i] = val\n        return inversion_vector\n\n    def rank_trotterjohnson(self):\n        \"\"\"\n        Returns the Trotter Johnson rank, which we get from the minimal\n        change algorithm. See [4] section 2.4.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 1, 2, 3])\n        >>> p.rank_trotterjohnson()\n        0\n        >>> p = Permutation([0, 2, 1, 3])\n        >>> p.rank_trotterjohnson()\n        7\n\n        See Also\n        ========\n\n        unrank_trotterjohnson, next_trotterjohnson\n        \"\"\"\n        if self.array_form == [] or self.is_Identity:\n            return 0\n        if self.array_form == [1, 0]:\n            return 1\n        perm = self.array_form\n        n = self.size\n        rank = 0\n        for j in range(1, n):\n            k = 1\n            i = 0\n            while perm[i] != j:\n                if perm[i] < j:\n                    k += 1\n                i += 1\n            j1 = j + 1\n            if rank % 2 == 0:\n                rank = j1*rank + j1 - k\n            else:\n                rank = j1*rank + k - 1\n        return rank\n\n    @classmethod\n    def unrank_trotterjohnson(self, size, rank):\n        \"\"\"\n        Trotter Johnson permutation unranking. See [4] section 2.4.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.unrank_trotterjohnson(5, 10)\n        Permutation([0, 3, 1, 2, 4])\n\n        See Also\n        ========\n\n        rank_trotterjohnson, next_trotterjohnson\n        \"\"\"\n        perm = [0]*size\n        r2 = 0\n        n = ifac(size)\n        pj = 1\n        for j in range(2, size + 1):\n            pj *= j\n            r1 = (rank * pj) // n\n            k = r1 - j*r2\n            if r2 % 2 == 0:\n                for i in range(j - 1, j - k - 1, -1):\n                    perm[i] = perm[i - 1]\n                perm[j - k - 1] = j - 1\n            else:\n                for i in range(j - 1, k, -1):\n                    perm[i] = perm[i - 1]\n                perm[k] = j - 1\n            r2 = r1\n        return _af_new(perm)\n\n    def next_trotterjohnson(self):\n        \"\"\"\n        Returns the next permutation in Trotter-Johnson order.\n        If self is the last permutation it returns None.\n        See [4] section 2.4. If it is desired to generate all such\n        permutations, they can be generated in order more quickly\n        with the ``generate_bell`` function.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> p = Permutation([3, 0, 2, 1])\n        >>> p.rank_trotterjohnson()\n        4\n        >>> p = p.next_trotterjohnson(); p\n        Permutation([0, 3, 2, 1])\n        >>> p.rank_trotterjohnson()\n        5\n\n        See Also\n        ========\n\n        rank_trotterjohnson, unrank_trotterjohnson, sympy.utilities.iterables.generate_bell\n        \"\"\"\n        pi = self.array_form[:]\n        n = len(pi)\n        st = 0\n        rho = pi[:]\n        done = False\n        m = n-1\n        while m > 0 and not done:\n            d = rho.index(m)\n            for i in range(d, m):\n                rho[i] = rho[i + 1]\n            par = _af_parity(rho[:m])\n            if par == 1:\n                if d == m:\n                    m -= 1\n                else:\n                    pi[st + d], pi[st + d + 1] = pi[st + d + 1], pi[st + d]\n                    done = True\n            else:\n                if d == 0:\n                    m -= 1\n                    st += 1\n                else:\n                    pi[st + d], pi[st + d - 1] = pi[st + d - 1], pi[st + d]\n                    done = True\n        if m == 0:\n            return None\n        return _af_new(pi)\n\n    def get_precedence_matrix(self):\n        \"\"\"\n        Gets the precedence matrix. This is used for computing the\n        distance between two permutations.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation.josephus(3, 6, 1)\n        >>> p\n        Permutation([2, 5, 3, 1, 4, 0])\n        >>> p.get_precedence_matrix()\n        Matrix([\n        [0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 1, 0],\n        [1, 1, 0, 1, 1, 1],\n        [1, 1, 0, 0, 1, 0],\n        [1, 0, 0, 0, 0, 0],\n        [1, 1, 0, 1, 1, 0]])\n\n        See Also\n        ========\n\n        get_precedence_distance, get_adjacency_matrix, get_adjacency_distance\n        \"\"\"\n        m = zeros(self.size)\n        perm = self.array_form\n        for i in range(m.rows):\n            for j in range(i + 1, m.cols):\n                m[perm[i], perm[j]] = 1\n        return m\n\n    def get_precedence_distance(self, other):\n        \"\"\"\n        Computes the precedence distance between two permutations.\n\n        Suppose p and p' represent n jobs. The precedence metric\n        counts the number of times a job j is preceded by job i\n        in both p and p'. This metric is commutative.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([2, 0, 4, 3, 1])\n        >>> q = Permutation([3, 1, 2, 4, 0])\n        >>> p.get_precedence_distance(q)\n        7\n        >>> q.get_precedence_distance(p)\n        7\n\n        See Also\n        ========\n\n        get_precedence_matrix, get_adjacency_matrix, get_adjacency_distance\n        \"\"\"\n        if self.size != other.size:\n            raise ValueError(\"The permutations must be of equal size.\")\n        self_prec_mat = self.get_precedence_matrix()\n        other_prec_mat = other.get_precedence_matrix()\n        n_prec = 0\n        for i in range(self.size):\n            for j in range(self.size):\n                if i == j:\n                    continue\n                if self_prec_mat[i, j] * other_prec_mat[i, j] == 1:\n                    n_prec += 1\n        d = self.size * (self.size - 1)//2 - n_prec\n        return d\n\n    def get_adjacency_matrix(self):\n        \"\"\"\n        Computes the adjacency matrix of a permutation.\n\n        If job i is adjacent to job j in a permutation p\n        then we set m[i, j] = 1 where m is the adjacency\n        matrix of p.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation.josephus(3, 6, 1)\n        >>> p.get_adjacency_matrix()\n        Matrix([\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 1],\n        [0, 1, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0]])\n        >>> q = Permutation([0, 1, 2, 3])\n        >>> q.get_adjacency_matrix()\n        Matrix([\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1],\n        [0, 0, 0, 0]])\n\n        See Also\n        ========\n\n        get_precedence_matrix, get_precedence_distance, get_adjacency_distance\n        \"\"\"\n        m = zeros(self.size)\n        perm = self.array_form\n        for i in range(self.size - 1):\n            m[perm[i], perm[i + 1]] = 1\n        return m\n\n    def get_adjacency_distance(self, other):\n        \"\"\"\n        Computes the adjacency distance between two permutations.\n\n        This metric counts the number of times a pair i,j of jobs is\n        adjacent in both p and p'. If n_adj is this quantity then\n        the adjacency distance is n - n_adj - 1 [1]\n\n        [1] Reeves, Colin R. Landscapes, Operators and Heuristic search, Annals\n        of Operational Research, 86, pp 473-490. (1999)\n\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 3, 1, 2, 4])\n        >>> q = Permutation.josephus(4, 5, 2)\n        >>> p.get_adjacency_distance(q)\n        3\n        >>> r = Permutation([0, 2, 1, 4, 3])\n        >>> p.get_adjacency_distance(r)\n        4\n\n        See Also\n        ========\n\n        get_precedence_matrix, get_precedence_distance, get_adjacency_matrix\n        \"\"\"\n        if self.size != other.size:\n            raise ValueError(\"The permutations must be of the same size.\")\n        self_adj_mat = self.get_adjacency_matrix()\n        other_adj_mat = other.get_adjacency_matrix()\n        n_adj = 0\n        for i in range(self.size):\n            for j in range(self.size):\n                if i == j:\n                    continue\n                if self_adj_mat[i, j] * other_adj_mat[i, j] == 1:\n                    n_adj += 1\n        d = self.size - n_adj - 1\n        return d\n\n    def get_positional_distance(self, other):\n        \"\"\"\n        Computes the positional distance between two permutations.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> p = Permutation([0, 3, 1, 2, 4])\n        >>> q = Permutation.josephus(4, 5, 2)\n        >>> r = Permutation([3, 1, 4, 0, 2])\n        >>> p.get_positional_distance(q)\n        12\n        >>> p.get_positional_distance(r)\n        12\n\n        See Also\n        ========\n\n        get_precedence_distance, get_adjacency_distance\n        \"\"\"\n        a = self.array_form\n        b = other.array_form\n        if len(a) != len(b):\n            raise ValueError(\"The permutations must be of the same size.\")\n        return sum([abs(a[i] - b[i]) for i in range(len(a))])\n\n    @classmethod\n    def josephus(self, m, n, s=1):\n        \"\"\"Return as a permutation the shuffling of range(n) using the Josephus\n        scheme in which every m-th item is selected until all have been chosen.\n        The returned permutation has elements listed by the order in which they\n        were selected.\n\n        The parameter ``s`` stops the selection process when there are ``s``\n        items remaining and these are selected by continuing the selection,\n        counting by 1 rather than by ``m``.\n\n        Consider selecting every 3rd item from 6 until only 2 remain::\n\n            choices    chosen\n            ========   ======\n              012345\n              01 345   2\n              01 34    25\n              01  4    253\n              0   4    2531\n              0        25314\n                       253140\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics import Permutation\n        >>> Permutation.josephus(3, 6, 2).array_form\n        [2, 5, 3, 1, 4, 0]\n\n        References\n        ==========\n\n        1. http://en.wikipedia.org/wiki/Flavius_Josephus\n        2. http://en.wikipedia.org/wiki/Josephus_problem\n        3. http://www.wou.edu/~burtonl/josephus.html\n\n        \"\"\"\n        from collections import deque\n        m -= 1\n        Q = deque(list(range(n)))\n        perm = []\n        while len(Q) > max(s, 1):\n            for dp in range(m):\n                Q.append(Q.popleft())\n            perm.append(Q.popleft())\n        perm.extend(list(Q))\n        return Perm(perm)\n\n    @classmethod\n    def from_inversion_vector(self, inversion):\n        \"\"\"\n        Calculates the permutation from the inversion vector.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> Permutation.from_inversion_vector([3, 2, 1, 0, 0])\n        Permutation([3, 2, 1, 0, 4, 5])\n\n        \"\"\"\n        size = len(inversion)\n        N = list(range(size + 1))\n        perm = []\n        try:\n            for k in range(size):\n                val = N[inversion[k]]\n                perm.append(val)\n                N.remove(val)\n        except IndexError:\n            raise ValueError(\"The inversion vector is not valid.\")\n        perm.extend(N)\n        return _af_new(perm)\n\n    @classmethod\n    def random(self, n):\n        \"\"\"\n        Generates a random permutation of length ``n``.\n\n        Uses the underlying Python pseudo-random number generator.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.random(2) in (Permutation([1, 0]), Permutation([0, 1]))\n        True\n\n        \"\"\"\n        perm_array = list(range(n))\n        random.shuffle(perm_array)\n        return _af_new(perm_array)\n\n    @classmethod\n    def unrank_lex(self, size, rank):\n        \"\"\"\n        Lexicographic permutation unranking.\n\n        Examples\n        ========\n\n        >>> from sympy.combinatorics.permutations import Permutation\n        >>> Permutation.print_cyclic = False\n        >>> a = Permutation.unrank_lex(5, 10)\n        >>> a.rank()\n        10\n        >>> a\n        Permutation([0, 2, 4, 1, 3])\n\n        See Also\n        ========\n\n        rank, next_lex\n        \"\"\"\n        perm_array = [0] * size\n        psize = 1\n        for i in range(size):\n            new_psize = psize*(i + 1)\n            d = (rank % new_psize) // psize\n            rank -= d*psize\n            perm_array[size - i - 1] = d\n            for j in range(size - i, size):\n                if perm_array[j] > d - 1:\n                    perm_array[j] += 1\n            psize = new_psize\n        return _af_new(perm_array)\n\n    # global flag to control how permutations are printed\n    # when True, Permutation([0, 2, 1, 3]) -> Cycle(1, 2)\n    # when False, Permutation([0, 2, 1, 3]) -> Permutation([0, 2, 1])\n    print_cyclic = True\n\n\ndef _merge(arr, temp, left, mid, right):\n    \"\"\"\n    Merges two sorted arrays and calculates the inversion count.\n\n    Helper function for calculating inversions. This method is\n    for internal use only.\n    \"\"\"\n    i = k = left\n    j = mid\n    inv_count = 0\n    while i < mid and j <= right:\n        if arr[i] < arr[j]:\n            temp[k] = arr[i]\n            k += 1\n            i += 1\n        else:\n            temp[k] = arr[j]\n            k += 1\n            j += 1\n            inv_count += (mid -i)\n    while i < mid:\n        temp[k] = arr[i]\n        k += 1\n        i += 1\n    if j <= right:\n        k += right - j + 1\n        j += right - j + 1\n        arr[left:k + 1] = temp[left:k + 1]\n    else:\n        arr[left:right + 1] = temp[left:right + 1]\n    return inv_count\n\nPerm = Permutation\n_af_new = Perm._af_new"}
{"model_name_or_path": "agentless", "instance_id": "sympy__sympy-13031", "model_patch": "diff --git a/sympy/matrices/expressions/blockmatrix.py b/sympy/matrices/expressions/blockmatrix.py\nindex a79ab56..a920d3b 100644\n--- a/sympy/matrices/expressions/blockmatrix.py\n+++ b/sympy/matrices/expressions/blockmatrix.py\n@@ -227,7 +227,7 @@ class BlockDiagMatrix(BlockMatrix):\n     @property\n     def shape(self):\n         return (sum(block.rows for block in self.args),\n-                sum(block.cols for block in self.args))\n+                sum(block.cols for block in self.args if block.rows > 0))\n \n     @property\n     def blockshape(self):\n", "raw_model_patch": "diff --git a/sympy/matrices/expressions/blockmatrix.py b/sympy/matrices/expressions/blockmatrix.py\nindex a79ab56..a920d3b 100644\n--- a/sympy/matrices/expressions/blockmatrix.py\n+++ b/sympy/matrices/expressions/blockmatrix.py\n@@ -227,7 +227,7 @@ class BlockDiagMatrix(BlockMatrix):\n     @property\n     def shape(self):\n         return (sum(block.rows for block in self.args),\n-                sum(block.cols for block in self.args))\n+                sum(block.cols for block in self.args if block.rows > 0))\n \n     @property\n     def blockshape(self):\n", "original_file_content": "from __future__ import print_function, division\n\nfrom sympy import ask, Q\nfrom sympy.core import Basic, Add, sympify\nfrom sympy.core.compatibility import range\nfrom sympy.strategies import typed, exhaust, condition, do_one, unpack\nfrom sympy.strategies.traverse import bottom_up\nfrom sympy.utilities import sift\n\nfrom sympy.matrices.expressions.matexpr import MatrixExpr, ZeroMatrix, Identity\nfrom sympy.matrices.expressions.matmul import MatMul\nfrom sympy.matrices.expressions.matadd import MatAdd\nfrom sympy.matrices.expressions.transpose import Transpose, transpose\nfrom sympy.matrices.expressions.trace import Trace\nfrom sympy.matrices.expressions.determinant import det, Determinant\nfrom sympy.matrices.expressions.slice import MatrixSlice\nfrom sympy.matrices.expressions.inverse import Inverse\nfrom sympy.matrices import Matrix, ShapeError\nfrom sympy.functions.elementary.complexes import re, im\n\nclass BlockMatrix(MatrixExpr):\n    \"\"\"A BlockMatrix is a Matrix composed of other smaller, submatrices\n\n    The submatrices are stored in a SymPy Matrix object but accessed as part of\n    a Matrix Expression\n\n    >>> from sympy import (MatrixSymbol, BlockMatrix, symbols,\n    ...     Identity, ZeroMatrix, block_collapse)\n    >>> n,m,l = symbols('n m l')\n    >>> X = MatrixSymbol('X', n, n)\n    >>> Y = MatrixSymbol('Y', m ,m)\n    >>> Z = MatrixSymbol('Z', n, m)\n    >>> B = BlockMatrix([[X, Z], [ZeroMatrix(m,n), Y]])\n    >>> print(B)\n    Matrix([\n    [X, Z],\n    [0, Y]])\n\n    >>> C = BlockMatrix([[Identity(n), Z]])\n    >>> print(C)\n    Matrix([[I, Z]])\n\n    >>> print(block_collapse(C*B))\n    Matrix([[X, Z*Y + Z]])\n\n    \"\"\"\n    def __new__(cls, *args):\n        from sympy.matrices.immutable import ImmutableDenseMatrix\n        args = map(sympify, args)\n        mat = ImmutableDenseMatrix(*args)\n\n        obj = Basic.__new__(cls, mat)\n        return obj\n\n    @property\n    def shape(self):\n        numrows = numcols = 0\n        M = self.blocks\n        for i in range(M.shape[0]):\n            numrows += M[i, 0].shape[0]\n        for i in range(M.shape[1]):\n            numcols += M[0, i].shape[1]\n        return (numrows, numcols)\n\n    @property\n    def blockshape(self):\n        return self.blocks.shape\n\n    @property\n    def blocks(self):\n        return self.args[0]\n\n    @property\n    def rowblocksizes(self):\n        return [self.blocks[i, 0].rows for i in range(self.blockshape[0])]\n\n    @property\n    def colblocksizes(self):\n        return [self.blocks[0, i].cols for i in range(self.blockshape[1])]\n\n    def structurally_equal(self, other):\n        return (isinstance(other, BlockMatrix)\n            and self.shape == other.shape\n            and self.blockshape == other.blockshape\n            and self.rowblocksizes == other.rowblocksizes\n            and self.colblocksizes == other.colblocksizes)\n\n    def _blockmul(self, other):\n        if (isinstance(other, BlockMatrix) and\n                self.colblocksizes == other.rowblocksizes):\n            return BlockMatrix(self.blocks*other.blocks)\n\n        return self * other\n\n    def _blockadd(self, other):\n        if (isinstance(other, BlockMatrix)\n                and self.structurally_equal(other)):\n            return BlockMatrix(self.blocks + other.blocks)\n\n        return self + other\n\n    def _eval_transpose(self):\n        # Flip all the individual matrices\n        matrices = [transpose(matrix) for matrix in self.blocks]\n        # Make a copy\n        M = Matrix(self.blockshape[0], self.blockshape[1], matrices)\n        # Transpose the block structure\n        M = M.transpose()\n        return BlockMatrix(M)\n\n    def _eval_trace(self):\n        if self.rowblocksizes == self.colblocksizes:\n            return Add(*[Trace(self.blocks[i, i])\n                        for i in range(self.blockshape[0])])\n        raise NotImplementedError(\n            \"Can't perform trace of irregular blockshape\")\n\n    def _eval_determinant(self):\n        if self.blockshape == (2, 2):\n            [[A, B],\n             [C, D]] = self.blocks.tolist()\n            if ask(Q.invertible(A)):\n                return det(A)*det(D - C*A.I*B)\n            elif ask(Q.invertible(D)):\n                return det(D)*det(A - B*D.I*C)\n        return Determinant(self)\n\n    def as_real_imag(self):\n        real_matrices = [re(matrix) for matrix in self.blocks]\n        real_matrices = Matrix(self.blockshape[0], self.blockshape[1], real_matrices)\n\n        im_matrices = [im(matrix) for matrix in self.blocks]\n        im_matrices = Matrix(self.blockshape[0], self.blockshape[1], im_matrices)\n\n        return (real_matrices, im_matrices)\n\n    def transpose(self):\n        \"\"\"Return transpose of matrix.\n\n        Examples\n        ========\n\n        >>> from sympy import MatrixSymbol, BlockMatrix, ZeroMatrix\n        >>> from sympy.abc import l, m, n\n        >>> X = MatrixSymbol('X', n, n)\n        >>> Y = MatrixSymbol('Y', m ,m)\n        >>> Z = MatrixSymbol('Z', n, m)\n        >>> B = BlockMatrix([[X, Z], [ZeroMatrix(m,n), Y]])\n        >>> B.transpose()\n        Matrix([\n        [X.T,  0],\n        [Z.T, Y.T]])\n        >>> _.transpose()\n        Matrix([\n        [X, Z],\n        [0, Y]])\n        \"\"\"\n        return self._eval_transpose()\n\n    def _entry(self, i, j):\n        # Find row entry\n        for row_block, numrows in enumerate(self.rowblocksizes):\n            if (i < numrows) != False:\n                break\n            else:\n                i -= numrows\n        for col_block, numcols in enumerate(self.colblocksizes):\n            if (j < numcols) != False:\n                break\n            else:\n                j -= numcols\n        return self.blocks[row_block, col_block][i, j]\n\n    @property\n    def is_Identity(self):\n        if self.blockshape[0] != self.blockshape[1]:\n            return False\n        for i in range(self.blockshape[0]):\n            for j in range(self.blockshape[1]):\n                if i==j and not self.blocks[i, j].is_Identity:\n                    return False\n                if i!=j and not self.blocks[i, j].is_ZeroMatrix:\n                    return False\n        return True\n\n    @property\n    def is_structurally_symmetric(self):\n        return self.rowblocksizes == self.colblocksizes\n\n    def equals(self, other):\n        if self == other:\n            return True\n        if (isinstance(other, BlockMatrix) and self.blocks == other.blocks):\n            return True\n        return super(BlockMatrix, self).equals(other)\n\nclass BlockDiagMatrix(BlockMatrix):\n    \"\"\"\n    A BlockDiagMatrix is a BlockMatrix with matrices only along the diagonal\n\n    >>> from sympy import MatrixSymbol, BlockDiagMatrix, symbols, Identity\n    >>> n,m,l = symbols('n m l')\n    >>> X = MatrixSymbol('X', n, n)\n    >>> Y = MatrixSymbol('Y', m ,m)\n    >>> BlockDiagMatrix(X, Y)\n    Matrix([\n    [X, 0],\n    [0, Y]])\n\n    \"\"\"\n    def __new__(cls, *mats):\n        return Basic.__new__(BlockDiagMatrix, *mats)\n\n    @property\n    def diag(self):\n        return self.args\n\n    @property\n    def blocks(self):\n        from sympy.matrices.immutable import ImmutableDenseMatrix\n        mats = self.args\n        data = [[mats[i] if i == j else ZeroMatrix(mats[i].rows, mats[j].cols)\n                        for j in range(len(mats))]\n                        for i in range(len(mats))]\n        return ImmutableDenseMatrix(data)\n\n    @property\n    def shape(self):\n        return (sum(block.rows for block in self.args),\n                sum(block.cols for block in self.args))\n\n    @property\n    def blockshape(self):\n        n = len(self.args)\n        return (n, n)\n\n    @property\n    def rowblocksizes(self):\n        return [block.rows for block in self.args]\n\n    @property\n    def colblocksizes(self):\n        return [block.cols for block in self.args]\n\n    def _eval_inverse(self, expand='ignored'):\n        return BlockDiagMatrix(*[mat.inverse() for mat in self.args])\n\n    def _blockmul(self, other):\n        if (isinstance(other, BlockDiagMatrix) and\n                self.colblocksizes == other.rowblocksizes):\n            return BlockDiagMatrix(*[a*b for a, b in zip(self.args, other.args)])\n        else:\n            return BlockMatrix._blockmul(self, other)\n\n    def _blockadd(self, other):\n        if (isinstance(other, BlockDiagMatrix) and\n                self.blockshape == other.blockshape and\n                self.rowblocksizes == other.rowblocksizes and\n                self.colblocksizes == other.colblocksizes):\n            return BlockDiagMatrix(*[a + b for a, b in zip(self.args, other.args)])\n        else:\n            return BlockMatrix._blockadd(self, other)\n\n\ndef block_collapse(expr):\n    \"\"\"Evaluates a block matrix expression\n\n    >>> from sympy import MatrixSymbol, BlockMatrix, symbols, \\\n                          Identity, Matrix, ZeroMatrix, block_collapse\n    >>> n,m,l = symbols('n m l')\n    >>> X = MatrixSymbol('X', n, n)\n    >>> Y = MatrixSymbol('Y', m ,m)\n    >>> Z = MatrixSymbol('Z', n, m)\n    >>> B = BlockMatrix([[X, Z], [ZeroMatrix(m, n), Y]])\n    >>> print(B)\n    Matrix([\n    [X, Z],\n    [0, Y]])\n\n    >>> C = BlockMatrix([[Identity(n), Z]])\n    >>> print(C)\n    Matrix([[I, Z]])\n\n    >>> print(block_collapse(C*B))\n    Matrix([[X, Z*Y + Z]])\n    \"\"\"\n    hasbm = lambda expr: isinstance(expr, MatrixExpr) and expr.has(BlockMatrix)\n    rule = exhaust(\n        bottom_up(exhaust(condition(hasbm, typed(\n            {MatAdd: do_one(bc_matadd, bc_block_plus_ident),\n             MatMul: do_one(bc_matmul, bc_dist),\n             Transpose: bc_transpose,\n             Inverse: bc_inverse,\n             BlockMatrix: do_one(bc_unpack, deblock)})))))\n    result = rule(expr)\n    try:\n        return result.doit()\n    except AttributeError:\n        return result\n\ndef bc_unpack(expr):\n    if expr.blockshape == (1, 1):\n        return expr.blocks[0, 0]\n    return expr\n\ndef bc_matadd(expr):\n    args = sift(expr.args, lambda M: isinstance(M, BlockMatrix))\n    blocks = args[True]\n    if not blocks:\n        return expr\n\n    nonblocks = args[False]\n    block = blocks[0]\n    for b in blocks[1:]:\n        block = block._blockadd(b)\n    if nonblocks:\n        return MatAdd(*nonblocks) + block\n    else:\n        return block\n\ndef bc_block_plus_ident(expr):\n    idents = [arg for arg in expr.args if arg.is_Identity]\n    if not idents:\n        return expr\n\n    blocks = [arg for arg in expr.args if isinstance(arg, BlockMatrix)]\n    if (blocks and all(b.structurally_equal(blocks[0]) for b in blocks)\n               and blocks[0].is_structurally_symmetric):\n        block_id = BlockDiagMatrix(*[Identity(k)\n                                        for k in blocks[0].rowblocksizes])\n        return MatAdd(block_id * len(idents), *blocks).doit()\n\n    return expr\n\ndef bc_dist(expr):\n    \"\"\" Turn  a*[X, Y] into [a*X, a*Y] \"\"\"\n    factor, mat = expr.as_coeff_mmul()\n    if factor != 1 and isinstance(unpack(mat), BlockMatrix):\n        B = unpack(mat).blocks\n        return BlockMatrix([[factor * B[i, j] for j in range(B.cols)]\n                                              for i in range(B.rows)])\n    return expr\n\n\ndef bc_matmul(expr):\n    factor, matrices = expr.as_coeff_matrices()\n\n    i = 0\n    while (i+1 < len(matrices)):\n        A, B = matrices[i:i+2]\n        if isinstance(A, BlockMatrix) and isinstance(B, BlockMatrix):\n            matrices[i] = A._blockmul(B)\n            matrices.pop(i+1)\n        elif isinstance(A, BlockMatrix):\n            matrices[i] = A._blockmul(BlockMatrix([[B]]))\n            matrices.pop(i+1)\n        elif isinstance(B, BlockMatrix):\n            matrices[i] = BlockMatrix([[A]])._blockmul(B)\n            matrices.pop(i+1)\n        else:\n            i+=1\n    return MatMul(factor, *matrices).doit()\n\ndef bc_transpose(expr):\n    return BlockMatrix(block_collapse(expr.arg).blocks.applyfunc(transpose).T)\n\n\ndef bc_inverse(expr):\n    expr2 = blockinverse_1x1(expr)\n    if expr != expr2:\n        return expr2\n    return blockinverse_2x2(Inverse(reblock_2x2(expr.arg)))\n\ndef blockinverse_1x1(expr):\n    if isinstance(expr.arg, BlockMatrix) and expr.arg.blockshape == (1, 1):\n        mat = Matrix([[expr.arg.blocks[0].inverse()]])\n        return BlockMatrix(mat)\n    return expr\n\ndef blockinverse_2x2(expr):\n    if isinstance(expr.arg, BlockMatrix) and expr.arg.blockshape == (2, 2):\n        # Cite: The Matrix Cookbook Section 9.1.3\n        [[A, B],\n         [C, D]] = expr.arg.blocks.tolist()\n\n        return BlockMatrix([[ (A - B*D.I*C).I,  (-A).I*B*(D - C*A.I*B).I],\n                            [-(D - C*A.I*B).I*C*A.I,     (D - C*A.I*B).I]])\n    else:\n        return expr\n\ndef deblock(B):\n    \"\"\" Flatten a BlockMatrix of BlockMatrices \"\"\"\n    if not isinstance(B, BlockMatrix) or not B.blocks.has(BlockMatrix):\n        return B\n    wrap = lambda x: x if isinstance(x, BlockMatrix) else BlockMatrix([[x]])\n    bb = B.blocks.applyfunc(wrap)  # everything is a block\n\n    from sympy import Matrix\n    try:\n        MM = Matrix(0, sum(bb[0, i].blocks.shape[1] for i in range(bb.shape[1])), [])\n        for row in range(0, bb.shape[0]):\n            M = Matrix(bb[row, 0].blocks)\n            for col in range(1, bb.shape[1]):\n                M = M.row_join(bb[row, col].blocks)\n            MM = MM.col_join(M)\n\n        return BlockMatrix(MM)\n    except ShapeError:\n        return B\n\n\n\ndef reblock_2x2(B):\n    \"\"\" Reblock a BlockMatrix so that it has 2x2 blocks of block matrices \"\"\"\n    if not isinstance(B, BlockMatrix) or not all(d > 2 for d in B.blocks.shape):\n        return B\n\n    BM = BlockMatrix  # for brevity's sake\n    return BM([[   B.blocks[0,  0],  BM(B.blocks[0,  1:])],\n               [BM(B.blocks[1:, 0]), BM(B.blocks[1:, 1:])]])\n\n\ndef bounds(sizes):\n    \"\"\" Convert sequence of numbers into pairs of low-high pairs\n\n    >>> from sympy.matrices.expressions.blockmatrix import bounds\n    >>> bounds((1, 10, 50))\n    [(0, 1), (1, 11), (11, 61)]\n    \"\"\"\n    low = 0\n    rv = []\n    for size in sizes:\n        rv.append((low, low + size))\n        low += size\n    return rv\n\ndef blockcut(expr, rowsizes, colsizes):\n    \"\"\" Cut a matrix expression into Blocks\n\n    >>> from sympy import ImmutableMatrix, blockcut\n    >>> M = ImmutableMatrix(4, 4, range(16))\n    >>> B = blockcut(M, (1, 3), (1, 3))\n    >>> type(B).__name__\n    'BlockMatrix'\n    >>> ImmutableMatrix(B.blocks[0, 1])\n    Matrix([[1, 2, 3]])\n    \"\"\"\n\n    rowbounds = bounds(rowsizes)\n    colbounds = bounds(colsizes)\n    return BlockMatrix([[MatrixSlice(expr, rowbound, colbound)\n                         for colbound in colbounds]\n                         for rowbound in rowbounds])"}
{"model_name_or_path": "agentless", "instance_id": "sympy__sympy-16450", "model_patch": "diff --git a/sympy/core/symbol.py b/sympy/core/symbol.py\nindex 9a1c686..15549bc 100644\n--- a/sympy/core/symbol.py\n+++ b/sympy/core/symbol.py\n@@ -232,6 +232,7 @@ class Symbol(AtomicExpr, Boolean):\n         # be strict about commutativity\n         is_commutative = fuzzy_bool(assumptions.get('commutative', True))\n         assumptions['commutative'] = is_commutative\n+        assumptions['finite'] = assumptions.get('finite', True)  # Preserve the finite assumption\n         obj._assumptions = StdFactKB(assumptions)\n         obj._assumptions._generator = tmp_asm_copy  # Issue #8873\n         return obj\n", "raw_model_patch": "diff --git a/sympy/core/symbol.py b/sympy/core/symbol.py\nindex 9a1c686..15549bc 100644\n--- a/sympy/core/symbol.py\n+++ b/sympy/core/symbol.py\n@@ -232,6 +232,7 @@ class Symbol(AtomicExpr, Boolean):\n         # be strict about commutativity\n         is_commutative = fuzzy_bool(assumptions.get('commutative', True))\n         assumptions['commutative'] = is_commutative\n+        assumptions['finite'] = assumptions.get('finite', True)  # Preserve the finite assumption\n         obj._assumptions = StdFactKB(assumptions)\n         obj._assumptions._generator = tmp_asm_copy  # Issue #8873\n         return obj\n", "original_file_content": "from __future__ import print_function, division\n\nfrom sympy.core.assumptions import StdFactKB\nfrom sympy.core.compatibility import (string_types, range, is_sequence,\n    ordered)\nfrom .basic import Basic\nfrom .sympify import sympify\nfrom .singleton import S\nfrom .expr import Expr, AtomicExpr\nfrom .cache import cacheit\nfrom .function import FunctionClass\nfrom sympy.core.logic import fuzzy_bool\nfrom sympy.logic.boolalg import Boolean\nfrom sympy.utilities.iterables import cartes\nfrom sympy.core.containers import Tuple\n\nimport string\nimport re as _re\nimport random\n\n\ndef _symbol(s, matching_symbol=None, **assumptions):\n    \"\"\"Return s if s is a Symbol, else if s is a string, return either\n    the matching_symbol if the names are the same or else a new symbol\n    with the same assumptions as the matching symbol (or the\n    assumptions as provided).\n\n    Examples\n    ========\n\n    >>> from sympy import Symbol, Dummy\n    >>> from sympy.core.symbol import _symbol\n    >>> _symbol('y')\n    y\n    >>> _.is_real is None\n    True\n    >>> _symbol('y', real=True).is_real\n    True\n\n    >>> x = Symbol('x')\n    >>> _symbol(x, real=True)\n    x\n    >>> _.is_real is None  # ignore attribute if s is a Symbol\n    True\n\n    Below, the variable sym has the name 'foo':\n\n    >>> sym = Symbol('foo', real=True)\n\n    Since 'x' is not the same as sym's name, a new symbol is created:\n\n    >>> _symbol('x', sym).name\n    'x'\n\n    It will acquire any assumptions give:\n\n    >>> _symbol('x', sym, real=False).is_real\n    False\n\n    Since 'foo' is the same as sym's name, sym is returned\n\n    >>> _symbol('foo', sym)\n    foo\n\n    Any assumptions given are ignored:\n\n    >>> _symbol('foo', sym, real=False).is_real\n    True\n\n    NB: the symbol here may not be the same as a symbol with the same\n    name defined elsewhere as a result of different assumptions.\n\n    See Also\n    ========\n\n    sympy.core.symbol.Symbol\n\n    \"\"\"\n    if isinstance(s, string_types):\n        if matching_symbol and matching_symbol.name == s:\n            return matching_symbol\n        return Symbol(s, **assumptions)\n    elif isinstance(s, Symbol):\n        return s\n    else:\n        raise ValueError('symbol must be string for symbol name or Symbol')\n\n\ndef _uniquely_named_symbol(xname, exprs=(), compare=str, modify=None, **assumptions):\n    \"\"\"Return a symbol which, when printed, will have a name unique\n    from any other already in the expressions given. The name is made\n    unique by prepending underscores (default) but this can be\n    customized with the keyword 'modify'.\n\n    Parameters\n    ==========\n\n        xname : a string or a Symbol (when symbol xname <- str(xname))\n        compare : a single arg function that takes a symbol and returns\n            a string to be compared with xname (the default is the str\n            function which indicates how the name will look when it\n            is printed, e.g. this includes underscores that appear on\n            Dummy symbols)\n        modify : a single arg function that changes its string argument\n            in some way (the default is to preppend underscores)\n\n    Examples\n    ========\n\n    >>> from sympy.core.symbol import _uniquely_named_symbol as usym, Dummy\n    >>> from sympy.abc import x\n    >>> usym('x', x)\n    _x\n    \"\"\"\n    default = None\n    if is_sequence(xname):\n        xname, default = xname\n    x = str(xname)\n    if not exprs:\n        return _symbol(x, default, **assumptions)\n    if not is_sequence(exprs):\n        exprs = [exprs]\n    syms = set().union(*[e.free_symbols for e in exprs])\n    if modify is None:\n        modify = lambda s: '_' + s\n    while any(x == compare(s) for s in syms):\n        x = modify(x)\n    return _symbol(x, default, **assumptions)\n\n\nclass Symbol(AtomicExpr, Boolean):\n    \"\"\"\n    Assumptions:\n       commutative = True\n\n    You can override the default assumptions in the constructor:\n\n    >>> from sympy import symbols\n    >>> A,B = symbols('A,B', commutative = False)\n    >>> bool(A*B != B*A)\n    True\n    >>> bool(A*B*2 == 2*A*B) == True # multiplication by scalars is commutative\n    True\n\n    \"\"\"\n\n    is_comparable = False\n\n    __slots__ = ['name']\n\n    is_Symbol = True\n    is_symbol = True\n\n    @property\n    def _diff_wrt(self):\n        \"\"\"Allow derivatives wrt Symbols.\n\n        Examples\n        ========\n\n            >>> from sympy import Symbol\n            >>> x = Symbol('x')\n            >>> x._diff_wrt\n            True\n        \"\"\"\n        return True\n\n    @staticmethod\n    def _sanitize(assumptions, obj=None):\n        \"\"\"Remove None, covert values to bool, check commutativity *in place*.\n        \"\"\"\n\n        # be strict about commutativity: cannot be None\n        is_commutative = fuzzy_bool(assumptions.get('commutative', True))\n        if is_commutative is None:\n            whose = '%s ' % obj.__name__ if obj else ''\n            raise ValueError(\n                '%scommutativity must be True or False.' % whose)\n\n        # sanitize other assumptions so 1 -> True and 0 -> False\n        for key in list(assumptions.keys()):\n            from collections import defaultdict\n            from sympy.utilities.exceptions import SymPyDeprecationWarning\n            keymap = defaultdict(lambda: None)\n            keymap.update({'bounded': 'finite', 'unbounded': 'infinite', 'infinitesimal': 'zero'})\n            if keymap[key]:\n                SymPyDeprecationWarning(\n                    feature=\"%s assumption\" % key,\n                    useinstead=\"%s\" % keymap[key],\n                    issue=8071,\n                    deprecated_since_version=\"0.7.6\").warn()\n                assumptions[keymap[key]] = assumptions[key]\n                assumptions.pop(key)\n                key = keymap[key]\n\n            v = assumptions[key]\n            if v is None:\n                assumptions.pop(key)\n                continue\n            assumptions[key] = bool(v)\n\n    def __new__(cls, name, **assumptions):\n        \"\"\"Symbols are identified by name and assumptions::\n\n        >>> from sympy import Symbol\n        >>> Symbol(\"x\") == Symbol(\"x\")\n        True\n        >>> Symbol(\"x\", real=True) == Symbol(\"x\", real=False)\n        False\n\n        \"\"\"\n        cls._sanitize(assumptions, cls)\n        return Symbol.__xnew_cached_(cls, name, **assumptions)\n\n    def __new_stage2__(cls, name, **assumptions):\n        if not isinstance(name, string_types):\n            raise TypeError(\"name should be a string, not %s\" % repr(type(name)))\n\n        obj = Expr.__new__(cls)\n        obj.name = name\n\n        # TODO: Issue #8873: Forcing the commutative assumption here means\n        # later code such as ``srepr()`` cannot tell whether the user\n        # specified ``commutative=True`` or omitted it.  To workaround this,\n        # we keep a copy of the assumptions dict, then create the StdFactKB,\n        # and finally overwrite its ``._generator`` with the dict copy.  This\n        # is a bit of a hack because we assume StdFactKB merely copies the\n        # given dict as ``._generator``, but future modification might, e.g.,\n        # compute a minimal equivalent assumption set.\n        tmp_asm_copy = assumptions.copy()\n\n        # be strict about commutativity\n        is_commutative = fuzzy_bool(assumptions.get('commutative', True))\n        assumptions['commutative'] = is_commutative\n        obj._assumptions = StdFactKB(assumptions)\n        obj._assumptions._generator = tmp_asm_copy  # Issue #8873\n        return obj\n\n    __xnew__ = staticmethod(\n        __new_stage2__)            # never cached (e.g. dummy)\n    __xnew_cached_ = staticmethod(\n        cacheit(__new_stage2__))   # symbols are always cached\n\n    def __getnewargs__(self):\n        return (self.name,)\n\n    def __getstate__(self):\n        return {'_assumptions': self._assumptions}\n\n    def _hashable_content(self):\n        # Note: user-specified assumptions not hashed, just derived ones\n        return (self.name,) + tuple(sorted(self.assumptions0.items()))\n\n    def _eval_subs(self, old, new):\n        from sympy.core.power import Pow\n        if old.is_Pow:\n            return Pow(self, S.One, evaluate=False)._eval_subs(old, new)\n\n    @property\n    def assumptions0(self):\n        return dict((key, value) for key, value\n                in self._assumptions.items() if value is not None)\n\n    @cacheit\n    def sort_key(self, order=None):\n        return self.class_key(), (1, (str(self),)), S.One.sort_key(), S.One\n\n    def as_dummy(self):\n        return Dummy(self.name)\n\n    def as_real_imag(self, deep=True, **hints):\n        from sympy import im, re\n        if hints.get('ignore') == self:\n            return None\n        else:\n            return (re(self), im(self))\n\n    def _sage_(self):\n        import sage.all as sage\n        return sage.var(self.name)\n\n    def is_constant(self, *wrt, **flags):\n        if not wrt:\n            return False\n        return not self in wrt\n\n    @property\n    def free_symbols(self):\n        return {self}\n\n    binary_symbols = free_symbols  # in this case, not always\n\n    def as_set(self):\n        return S.UniversalSet\n\n\nclass Dummy(Symbol):\n    \"\"\"Dummy symbols are each unique, even if they have the same name:\n\n    >>> from sympy import Dummy\n    >>> Dummy(\"x\") == Dummy(\"x\")\n    False\n\n    If a name is not supplied then a string value of an internal count will be\n    used. This is useful when a temporary variable is needed and the name\n    of the variable used in the expression is not important.\n\n    >>> Dummy() #doctest: +SKIP\n    _Dummy_10\n\n    \"\"\"\n\n    # In the rare event that a Dummy object needs to be recreated, both the\n    # `name` and `dummy_index` should be passed.  This is used by `srepr` for\n    # example:\n    # >>> d1 = Dummy()\n    # >>> d2 = eval(srepr(d1))\n    # >>> d2 == d1\n    # True\n    #\n    # If a new session is started between `srepr` and `eval`, there is a very\n    # small chance that `d2` will be equal to a previously-created Dummy.\n\n    _count = 0\n    _prng = random.Random()\n    _base_dummy_index = _prng.randint(10**6, 9*10**6)\n\n    __slots__ = ['dummy_index']\n\n    is_Dummy = True\n\n    def __new__(cls, name=None, dummy_index=None, **assumptions):\n        if dummy_index is not None:\n            assert name is not None, \"If you specify a dummy_index, you must also provide a name\"\n\n        if name is None:\n            name = \"Dummy_\" + str(Dummy._count)\n\n        if dummy_index is None:\n            dummy_index = Dummy._base_dummy_index + Dummy._count\n            Dummy._count += 1\n\n        cls._sanitize(assumptions, cls)\n        obj = Symbol.__xnew__(cls, name, **assumptions)\n\n        obj.dummy_index = dummy_index\n\n        return obj\n\n    def __getstate__(self):\n        return {'_assumptions': self._assumptions, 'dummy_index': self.dummy_index}\n\n    @cacheit\n    def sort_key(self, order=None):\n        return self.class_key(), (\n            2, (str(self), self.dummy_index)), S.One.sort_key(), S.One\n\n    def _hashable_content(self):\n        return Symbol._hashable_content(self) + (self.dummy_index,)\n\n\nclass Wild(Symbol):\n    \"\"\"\n    A Wild symbol matches anything, or anything\n    without whatever is explicitly excluded.\n\n    Parameters\n    ==========\n\n    name : str\n        Name of the Wild instance.\n    exclude : iterable, optional\n        Instances in ``exclude`` will not be matched.\n    properties : iterable of functions, optional\n        Functions, each taking an expressions as input\n        and returns a ``bool``. All functions in ``properties``\n        need to return ``True`` in order for the Wild instance\n        to match the expression.\n\n    Examples\n    ========\n\n    >>> from sympy import Wild, WildFunction, cos, pi\n    >>> from sympy.abc import x, y, z\n    >>> a = Wild('a')\n    >>> x.match(a)\n    {a_: x}\n    >>> pi.match(a)\n    {a_: pi}\n    >>> (3*x**2).match(a*x)\n    {a_: 3*x}\n    >>> cos(x).match(a)\n    {a_: cos(x)}\n    >>> b = Wild('b', exclude=[x])\n    >>> (3*x**2).match(b*x)\n    >>> b.match(a)\n    {a_: b_}\n    >>> A = WildFunction('A')\n    >>> A.match(a)\n    {a_: A_}\n\n    Tips\n    ====\n\n    When using Wild, be sure to use the exclude\n    keyword to make the pattern more precise.\n    Without the exclude pattern, you may get matches\n    that are technically correct, but not what you\n    wanted. For example, using the above without\n    exclude:\n\n    >>> from sympy import symbols\n    >>> a, b = symbols('a b', cls=Wild)\n    >>> (2 + 3*y).match(a*x + b*y)\n    {a_: 2/x, b_: 3}\n\n    This is technically correct, because\n    (2/x)*x + 3*y == 2 + 3*y, but you probably\n    wanted it to not match at all. The issue is that\n    you really didn't want a and b to include x and y,\n    and the exclude parameter lets you specify exactly\n    this.  With the exclude parameter, the pattern will\n    not match.\n\n    >>> a = Wild('a', exclude=[x, y])\n    >>> b = Wild('b', exclude=[x, y])\n    >>> (2 + 3*y).match(a*x + b*y)\n\n    Exclude also helps remove ambiguity from matches.\n\n    >>> E = 2*x**3*y*z\n    >>> a, b = symbols('a b', cls=Wild)\n    >>> E.match(a*b)\n    {a_: 2*y*z, b_: x**3}\n    >>> a = Wild('a', exclude=[x, y])\n    >>> E.match(a*b)\n    {a_: z, b_: 2*x**3*y}\n    >>> a = Wild('a', exclude=[x, y, z])\n    >>> E.match(a*b)\n    {a_: 2, b_: x**3*y*z}\n\n    Wild also accepts a ``properties`` parameter:\n\n    >>> a = Wild('a', properties=[lambda k: k.is_Integer])\n    >>> E.match(a*b)\n    {a_: 2, b_: x**3*y*z}\n\n    \"\"\"\n    is_Wild = True\n\n    __slots__ = ['exclude', 'properties']\n\n    def __new__(cls, name, exclude=(), properties=(), **assumptions):\n        exclude = tuple([sympify(x) for x in exclude])\n        properties = tuple(properties)\n        cls._sanitize(assumptions, cls)\n        return Wild.__xnew__(cls, name, exclude, properties, **assumptions)\n\n    def __getnewargs__(self):\n        return (self.name, self.exclude, self.properties)\n\n    @staticmethod\n    @cacheit\n    def __xnew__(cls, name, exclude, properties, **assumptions):\n        obj = Symbol.__xnew__(cls, name, **assumptions)\n        obj.exclude = exclude\n        obj.properties = properties\n        return obj\n\n    def _hashable_content(self):\n        return super(Wild, self)._hashable_content() + (self.exclude, self.properties)\n\n    # TODO add check against another Wild\n    def matches(self, expr, repl_dict={}, old=False):\n        if any(expr.has(x) for x in self.exclude):\n            return None\n        if any(not f(expr) for f in self.properties):\n            return None\n        repl_dict = repl_dict.copy()\n        repl_dict[self] = expr\n        return repl_dict\n\n\n_range = _re.compile('([0-9]*:[0-9]+|[a-zA-Z]?:[a-zA-Z])')\n\ndef symbols(names, **args):\n    r\"\"\"\n    Transform strings into instances of :class:`Symbol` class.\n\n    :func:`symbols` function returns a sequence of symbols with names taken\n    from ``names`` argument, which can be a comma or whitespace delimited\n    string, or a sequence of strings::\n\n        >>> from sympy import symbols, Function\n\n        >>> x, y, z = symbols('x,y,z')\n        >>> a, b, c = symbols('a b c')\n\n    The type of output is dependent on the properties of input arguments::\n\n        >>> symbols('x')\n        x\n        >>> symbols('x,')\n        (x,)\n        >>> symbols('x,y')\n        (x, y)\n        >>> symbols(('a', 'b', 'c'))\n        (a, b, c)\n        >>> symbols(['a', 'b', 'c'])\n        [a, b, c]\n        >>> symbols({'a', 'b', 'c'})\n        {a, b, c}\n\n    If an iterable container is needed for a single symbol, set the ``seq``\n    argument to ``True`` or terminate the symbol name with a comma::\n\n        >>> symbols('x', seq=True)\n        (x,)\n\n    To reduce typing, range syntax is supported to create indexed symbols.\n    Ranges are indicated by a colon and the type of range is determined by\n    the character to the right of the colon. If the character is a digit\n    then all contiguous digits to the left are taken as the nonnegative\n    starting value (or 0 if there is no digit left of the colon) and all\n    contiguous digits to the right are taken as 1 greater than the ending\n    value::\n\n        >>> symbols('x:10')\n        (x0, x1, x2, x3, x4, x5, x6, x7, x8, x9)\n\n        >>> symbols('x5:10')\n        (x5, x6, x7, x8, x9)\n        >>> symbols('x5(:2)')\n        (x50, x51)\n\n        >>> symbols('x5:10,y:5')\n        (x5, x6, x7, x8, x9, y0, y1, y2, y3, y4)\n\n        >>> symbols(('x5:10', 'y:5'))\n        ((x5, x6, x7, x8, x9), (y0, y1, y2, y3, y4))\n\n    If the character to the right of the colon is a letter, then the single\n    letter to the left (or 'a' if there is none) is taken as the start\n    and all characters in the lexicographic range *through* the letter to\n    the right are used as the range::\n\n        >>> symbols('x:z')\n        (x, y, z)\n        >>> symbols('x:c')  # null range\n        ()\n        >>> symbols('x(:c)')\n        (xa, xb, xc)\n\n        >>> symbols(':c')\n        (a, b, c)\n\n        >>> symbols('a:d, x:z')\n        (a, b, c, d, x, y, z)\n\n        >>> symbols(('a:d', 'x:z'))\n        ((a, b, c, d), (x, y, z))\n\n    Multiple ranges are supported; contiguous numerical ranges should be\n    separated by parentheses to disambiguate the ending number of one\n    range from the starting number of the next::\n\n        >>> symbols('x:2(1:3)')\n        (x01, x02, x11, x12)\n        >>> symbols(':3:2')  # parsing is from left to right\n        (00, 01, 10, 11, 20, 21)\n\n    Only one pair of parentheses surrounding ranges are removed, so to\n    include parentheses around ranges, double them. And to include spaces,\n    commas, or colons, escape them with a backslash::\n\n        >>> symbols('x((a:b))')\n        (x(a), x(b))\n        >>> symbols(r'x(:1\\,:2)')  # or r'x((:1)\\,(:2))'\n        (x(0,0), x(0,1))\n\n    All newly created symbols have assumptions set according to ``args``::\n\n        >>> a = symbols('a', integer=True)\n        >>> a.is_integer\n        True\n\n        >>> x, y, z = symbols('x,y,z', real=True)\n        >>> x.is_real and y.is_real and z.is_real\n        True\n\n    Despite its name, :func:`symbols` can create symbol-like objects like\n    instances of Function or Wild classes. To achieve this, set ``cls``\n    keyword argument to the desired type::\n\n        >>> symbols('f,g,h', cls=Function)\n        (f, g, h)\n\n        >>> type(_[0])\n        <class 'sympy.core.function.UndefinedFunction'>\n\n    \"\"\"\n    result = []\n\n    if isinstance(names, string_types):\n        marker = 0\n        literals = [r'\\,', r'\\:', r'\\ ']\n        for i in range(len(literals)):\n            lit = literals.pop(0)\n            if lit in names:\n                while chr(marker) in names:\n                    marker += 1\n                lit_char = chr(marker)\n                marker += 1\n                names = names.replace(lit, lit_char)\n                literals.append((lit_char, lit[1:]))\n        def literal(s):\n            if literals:\n                for c, l in literals:\n                    s = s.replace(c, l)\n            return s\n\n        names = names.strip()\n        as_seq = names.endswith(',')\n        if as_seq:\n            names = names[:-1].rstrip()\n        if not names:\n            raise ValueError('no symbols given')\n\n        # split on commas\n        names = [n.strip() for n in names.split(',')]\n        if not all(n for n in names):\n            raise ValueError('missing symbol between commas')\n        # split on spaces\n        for i in range(len(names) - 1, -1, -1):\n            names[i: i + 1] = names[i].split()\n\n        cls = args.pop('cls', Symbol)\n        seq = args.pop('seq', as_seq)\n\n        for name in names:\n            if not name:\n                raise ValueError('missing symbol')\n\n            if ':' not in name:\n                symbol = cls(literal(name), **args)\n                result.append(symbol)\n                continue\n\n            split = _range.split(name)\n            # remove 1 layer of bounding parentheses around ranges\n            for i in range(len(split) - 1):\n                if i and ':' in split[i] and split[i] != ':' and \\\n                        split[i - 1].endswith('(') and \\\n                        split[i + 1].startswith(')'):\n                    split[i - 1] = split[i - 1][:-1]\n                    split[i + 1] = split[i + 1][1:]\n            for i, s in enumerate(split):\n                if ':' in s:\n                    if s[-1].endswith(':'):\n                        raise ValueError('missing end range')\n                    a, b = s.split(':')\n                    if b[-1] in string.digits:\n                        a = 0 if not a else int(a)\n                        b = int(b)\n                        split[i] = [str(c) for c in range(a, b)]\n                    else:\n                        a = a or 'a'\n                        split[i] = [string.ascii_letters[c] for c in range(\n                            string.ascii_letters.index(a),\n                            string.ascii_letters.index(b) + 1)]  # inclusive\n                    if not split[i]:\n                        break\n                else:\n                    split[i] = [s]\n            else:\n                seq = True\n                if len(split) == 1:\n                    names = split[0]\n                else:\n                    names = [''.join(s) for s in cartes(*split)]\n                if literals:\n                    result.extend([cls(literal(s), **args) for s in names])\n                else:\n                    result.extend([cls(s, **args) for s in names])\n\n        if not seq and len(result) <= 1:\n            if not result:\n                return ()\n            return result[0]\n\n        return tuple(result)\n    else:\n        for name in names:\n            result.append(symbols(name, **args))\n\n        return type(names)(result)\n\n\ndef var(names, **args):\n    \"\"\"\n    Create symbols and inject them into the global namespace.\n\n    This calls :func:`symbols` with the same arguments and puts the results\n    into the *global* namespace. It's recommended not to use :func:`var` in\n    library code, where :func:`symbols` has to be used::\n\n    Examples\n    ========\n\n    >>> from sympy import var\n\n    >>> var('x')\n    x\n    >>> x\n    x\n\n    >>> var('a,ab,abc')\n    (a, ab, abc)\n    >>> abc\n    abc\n\n    >>> var('x,y', real=True)\n    (x, y)\n    >>> x.is_real and y.is_real\n    True\n\n    See :func:`symbol` documentation for more details on what kinds of\n    arguments can be passed to :func:`var`.\n\n    \"\"\"\n    def traverse(symbols, frame):\n        \"\"\"Recursively inject symbols to the global namespace. \"\"\"\n        for symbol in symbols:\n            if isinstance(symbol, Basic):\n                frame.f_globals[symbol.name] = symbol\n            elif isinstance(symbol, FunctionClass):\n                frame.f_globals[symbol.__name__] = symbol\n            else:\n                traverse(symbol, frame)\n\n    from inspect import currentframe\n    frame = currentframe().f_back\n\n    try:\n        syms = symbols(names, **args)\n\n        if syms is not None:\n            if isinstance(syms, Basic):\n                frame.f_globals[syms.name] = syms\n            elif isinstance(syms, FunctionClass):\n                frame.f_globals[syms.__name__] = syms\n            else:\n                traverse(syms, frame)\n    finally:\n        del frame  # break cyclic dependencies as stated in inspect docs\n\n    return syms\n\ndef disambiguate(*iter):\n    \"\"\"\n    Return a Tuple containing the passed expressions with symbols\n    that appear the same when printed replaced with numerically\n    subscripted symbols, and all Dummy symbols replaced with Symbols.\n\n    Parameters\n    ==========\n\n    iter: list of symbols or expressions.\n\n    Examples\n    ========\n\n    >>> from sympy.core.symbol import disambiguate\n    >>> from sympy import Dummy, Symbol, Tuple\n    >>> from sympy.abc import y\n\n    >>> tup = Symbol('_x'), Dummy('x'), Dummy('x')\n    >>> disambiguate(*tup)\n    (x_2, x, x_1)\n\n    >>> eqs = Tuple(Symbol('x')/y, Dummy('x')/y)\n    >>> disambiguate(*eqs)\n    (x_1/y, x/y)\n\n    >>> ix = Symbol('x', integer=True)\n    >>> vx = Symbol('x')\n    >>> disambiguate(vx + ix)\n    (x + x_1,)\n\n    To make your own mapping of symbols to use, pass only the free symbols\n    of the expressions and create a dictionary:\n\n    >>> free = eqs.free_symbols\n    >>> mapping = dict(zip(free, disambiguate(*free)))\n    >>> eqs.xreplace(mapping)\n    (x_1/y, x/y)\n\n    \"\"\"\n    new_iter = Tuple(*iter)\n    key = lambda x:tuple(sorted(x.assumptions0.items()))\n    syms = ordered(new_iter.free_symbols, keys=key)\n    mapping = {}\n    for s in syms:\n        mapping.setdefault(str(s).lstrip('_'), []).append(s)\n    reps = {}\n    for k in mapping:\n        # the first or only symbol doesn't get subscripted but make\n        # sure that it's a Symbol, not a Dummy\n        k0 = Symbol(\"%s\" % (k), **mapping[k][0].assumptions0)\n        if k != k0:\n            reps[mapping[k][0]] = k0\n        # the others get subscripts (and are made into Symbols)\n        skip = 0\n        for i in range(1, len(mapping[k])):\n            while True:\n                name = \"%s_%i\" % (k, i + skip)\n                if name not in mapping:\n                    break\n                skip += 1\n            ki = mapping[k][i]\n            reps[ki] = Symbol(name, **ki.assumptions0)\n    return new_iter.xreplace(reps)"}
{"model_name_or_path": "agentless", "instance_id": "sympy__sympy-17139", "model_patch": "diff --git a/sympy/simplify/fu.py b/sympy/simplify/fu.py\nindex 0cdedc1..fd5c4d8 100644\n--- a/sympy/simplify/fu.py\n+++ b/sympy/simplify/fu.py\n@@ -501,7 +501,7 @@ def _TR56(rv, f, g, h, max, pow):\n         if not (rv.is_Pow and rv.base.func == f):\n             return rv\n \n-        if (rv.exp < 0) == True:\n+        if rv.exp.is_real and (rv.exp < 0) == True:\n             return rv\n         if (rv.exp > max) == True:\n             return rv\n", "raw_model_patch": "diff --git a/sympy/simplify/fu.py b/sympy/simplify/fu.py\nindex 0cdedc1..fd5c4d8 100644\n--- a/sympy/simplify/fu.py\n+++ b/sympy/simplify/fu.py\n@@ -501,7 +501,7 @@ def _TR56(rv, f, g, h, max, pow):\n         if not (rv.is_Pow and rv.base.func == f):\n             return rv\n \n-        if (rv.exp < 0) == True:\n+        if rv.exp.is_real and (rv.exp < 0) == True:\n             return rv\n         if (rv.exp > max) == True:\n             return rv\n", "original_file_content": "\"\"\"\nImplementation of the trigsimp algorithm by Fu et al.\n\nThe idea behind the ``fu`` algorithm is to use a sequence of rules, applied\nin what is heuristically known to be a smart order, to select a simpler\nexpression that is equivalent to the input.\n\nThere are transform rules in which a single rule is applied to the\nexpression tree. The following are just mnemonic in nature; see the\ndocstrings for examples.\n\n    TR0 - simplify expression\n    TR1 - sec-csc to cos-sin\n    TR2 - tan-cot to sin-cos ratio\n    TR2i - sin-cos ratio to tan\n    TR3 - angle canonicalization\n    TR4 - functions at special angles\n    TR5 - powers of sin to powers of cos\n    TR6 - powers of cos to powers of sin\n    TR7 - reduce cos power (increase angle)\n    TR8 - expand products of sin-cos to sums\n    TR9 - contract sums of sin-cos to products\n    TR10 - separate sin-cos arguments\n    TR10i - collect sin-cos arguments\n    TR11 - reduce double angles\n    TR12 - separate tan arguments\n    TR12i - collect tan arguments\n    TR13 - expand product of tan-cot\n    TRmorrie - prod(cos(x*2**i), (i, 0, k - 1)) -> sin(2**k*x)/(2**k*sin(x))\n    TR14 - factored powers of sin or cos to cos or sin power\n    TR15 - negative powers of sin to cot power\n    TR16 - negative powers of cos to tan power\n    TR22 - tan-cot powers to negative powers of sec-csc functions\n    TR111 - negative sin-cos-tan powers to csc-sec-cot\n\nThere are 4 combination transforms (CTR1 - CTR4) in which a sequence of\ntransformations are applied and the simplest expression is selected from\na few options.\n\nFinally, there are the 2 rule lists (RL1 and RL2), which apply a\nsequence of transformations and combined transformations, and the ``fu``\nalgorithm itself, which applies rules and rule lists and selects the\nbest expressions. There is also a function ``L`` which counts the number\nof trigonometric functions that appear in the expression.\n\nOther than TR0, re-writing of expressions is not done by the transformations.\ne.g. TR10i finds pairs of terms in a sum that are in the form like\n``cos(x)*cos(y) + sin(x)*sin(y)``. Such expression are targeted in a bottom-up\ntraversal of the expression, but no manipulation to make them appear is\nattempted. For example,\n\n    Set-up for examples below:\n\n    >>> from sympy.simplify.fu import fu, L, TR9, TR10i, TR11\n    >>> from sympy import factor, sin, cos, powsimp\n    >>> from sympy.abc import x, y, z, a\n    >>> from time import time\n\n>>> eq = cos(x + y)/cos(x)\n>>> TR10i(eq.expand(trig=True))\n-sin(x)*sin(y)/cos(x) + cos(y)\n\nIf the expression is put in \"normal\" form (with a common denominator) then\nthe transformation is successful:\n\n>>> TR10i(_.normal())\ncos(x + y)/cos(x)\n\nTR11's behavior is similar. It rewrites double angles as smaller angles but\ndoesn't do any simplification of the result.\n\n>>> TR11(sin(2)**a*cos(1)**(-a), 1)\n(2*sin(1)*cos(1))**a*cos(1)**(-a)\n>>> powsimp(_)\n(2*sin(1))**a\n\nThe temptation is to try make these TR rules \"smarter\" but that should really\nbe done at a higher level; the TR rules should try maintain the \"do one thing\nwell\" principle.  There is one exception, however. In TR10i and TR9 terms are\nrecognized even when they are each multiplied by a common factor:\n\n>>> fu(a*cos(x)*cos(y) + a*sin(x)*sin(y))\na*cos(x - y)\n\nFactoring with ``factor_terms`` is used but it it \"JIT\"-like, being delayed\nuntil it is deemed necessary. Furthermore, if the factoring does not\nhelp with the simplification, it is not retained, so\n``a*cos(x)*cos(y) + a*sin(x)*sin(z)`` does not become the factored\n(but unsimplified in the trigonometric sense) expression:\n\n>>> fu(a*cos(x)*cos(y) + a*sin(x)*sin(z))\na*sin(x)*sin(z) + a*cos(x)*cos(y)\n\nIn some cases factoring might be a good idea, but the user is left\nto make that decision. For example:\n\n>>> expr=((15*sin(2*x) + 19*sin(x + y) + 17*sin(x + z) + 19*cos(x - z) +\n... 25)*(20*sin(2*x) + 15*sin(x + y) + sin(y + z) + 14*cos(x - z) +\n... 14*cos(y - z))*(9*sin(2*y) + 12*sin(y + z) + 10*cos(x - y) + 2*cos(y -\n... z) + 18)).expand(trig=True).expand()\n\nIn the expanded state, there are nearly 1000 trig functions:\n\n>>> L(expr)\n932\n\nIf the expression where factored first, this would take time but the\nresulting expression would be transformed very quickly:\n\n>>> def clock(f, n=2):\n...    t=time(); f(); return round(time()-t, n)\n...\n>>> clock(lambda: factor(expr))  # doctest: +SKIP\n0.86\n>>> clock(lambda: TR10i(expr), 3)  # doctest: +SKIP\n0.016\n\nIf the unexpanded expression is used, the transformation takes longer but\nnot as long as it took to factor it and then transform it:\n\n>>> clock(lambda: TR10i(expr), 2)  # doctest: +SKIP\n0.28\n\nSo neither expansion nor factoring is used in ``TR10i``: if the\nexpression is already factored (or partially factored) then expansion\nwith ``trig=True`` would destroy what is already known and take\nlonger; if the expression is expanded, factoring may take longer than\nsimply applying the transformation itself.\n\nAlthough the algorithms should be canonical, always giving the same\nresult, they may not yield the best result. This, in general, is\nthe nature of simplification where searching all possible transformation\npaths is very expensive. Here is a simple example. There are 6 terms\nin the following sum:\n\n>>> expr = (sin(x)**2*cos(y)*cos(z) + sin(x)*sin(y)*cos(x)*cos(z) +\n... sin(x)*sin(z)*cos(x)*cos(y) + sin(y)*sin(z)*cos(x)**2 + sin(y)*sin(z) +\n... cos(y)*cos(z))\n>>> args = expr.args\n\nSerendipitously, fu gives the best result:\n\n>>> fu(expr)\n3*cos(y - z)/2 - cos(2*x + y + z)/2\n\nBut if different terms were combined, a less-optimal result might be\nobtained, requiring some additional work to get better simplification,\nbut still less than optimal. The following shows an alternative form\nof ``expr`` that resists optimal simplification once a given step\nis taken since it leads to a dead end:\n\n>>> TR9(-cos(x)**2*cos(y + z) + 3*cos(y - z)/2 +\n...     cos(y + z)/2 + cos(-2*x + y + z)/4 - cos(2*x + y + z)/4)\nsin(2*x)*sin(y + z)/2 - cos(x)**2*cos(y + z) + 3*cos(y - z)/2 + cos(y + z)/2\n\nHere is a smaller expression that exhibits the same behavior:\n\n>>> a = sin(x)*sin(z)*cos(x)*cos(y) + sin(x)*sin(y)*cos(x)*cos(z)\n>>> TR10i(a)\nsin(x)*sin(y + z)*cos(x)\n>>> newa = _\n>>> TR10i(expr - a)  # this combines two more of the remaining terms\nsin(x)**2*cos(y)*cos(z) + sin(y)*sin(z)*cos(x)**2 + cos(y - z)\n>>> TR10i(_ + newa) == _ + newa  # but now there is no more simplification\nTrue\n\nWithout getting lucky or trying all possible pairings of arguments, the\nfinal result may be less than optimal and impossible to find without\nbetter heuristics or brute force trial of all possibilities.\n\nNotes\n=====\n\nThis work was started by Dimitar Vlahovski at the Technological School\n\"Electronic systems\" (30.11.2011).\n\nReferences\n==========\n\nFu, Hongguang, Xiuqin Zhong, and Zhenbing Zeng. \"Automated and readable\nsimplification of trigonometric expressions.\" Mathematical and computer\nmodelling 44.11 (2006): 1169-1177.\nhttp://rfdz.ph-noe.ac.at/fileadmin/Mathematik_Uploads/ACDCA/DESTIME2006/DES_contribs/Fu/simplification.pdf\n\nhttp://www.sosmath.com/trig/Trig5/trig5/pdf/pdf.html gives a formula sheet.\n\n\"\"\"\n\nfrom __future__ import print_function, division\n\nfrom collections import defaultdict\n\nfrom sympy.core.add import Add\nfrom sympy.core.basic import S\nfrom sympy.core.compatibility import ordered, range\nfrom sympy.core.expr import Expr\nfrom sympy.core.exprtools import Factors, gcd_terms, factor_terms\nfrom sympy.core.function import expand_mul\nfrom sympy.core.mul import Mul\nfrom sympy.core.numbers import pi, I\nfrom sympy.core.power import Pow\nfrom sympy.core.symbol import Dummy\nfrom sympy.core.sympify import sympify\nfrom sympy.functions.combinatorial.factorials import binomial\nfrom sympy.functions.elementary.hyperbolic import (\n    cosh, sinh, tanh, coth, sech, csch, HyperbolicFunction)\nfrom sympy.functions.elementary.trigonometric import (\n    cos, sin, tan, cot, sec, csc, sqrt, TrigonometricFunction)\nfrom sympy.ntheory.factor_ import perfect_power\nfrom sympy.polys.polytools import factor\nfrom sympy.simplify.simplify import bottom_up\nfrom sympy.strategies.tree import greedy\nfrom sympy.strategies.core import identity, debug\n\nfrom sympy import SYMPY_DEBUG\n\n\n# ================== Fu-like tools ===========================\n\n\ndef TR0(rv):\n    \"\"\"Simplification of rational polynomials, trying to simplify\n    the expression, e.g. combine things like 3*x + 2*x, etc....\n    \"\"\"\n    # although it would be nice to use cancel, it doesn't work\n    # with noncommutatives\n    return rv.normal().factor().expand()\n\n\ndef TR1(rv):\n    \"\"\"Replace sec, csc with 1/cos, 1/sin\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR1, sec, csc\n    >>> from sympy.abc import x\n    >>> TR1(2*csc(x) + sec(x))\n    1/cos(x) + 2/sin(x)\n    \"\"\"\n\n    def f(rv):\n        if isinstance(rv, sec):\n            a = rv.args[0]\n            return S.One/cos(a)\n        elif isinstance(rv, csc):\n            a = rv.args[0]\n            return S.One/sin(a)\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR2(rv):\n    \"\"\"Replace tan and cot with sin/cos and cos/sin\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR2\n    >>> from sympy.abc import x\n    >>> from sympy import tan, cot, sin, cos\n    >>> TR2(tan(x))\n    sin(x)/cos(x)\n    >>> TR2(cot(x))\n    cos(x)/sin(x)\n    >>> TR2(tan(tan(x) - sin(x)/cos(x)))\n    0\n\n    \"\"\"\n\n    def f(rv):\n        if isinstance(rv, tan):\n            a = rv.args[0]\n            return sin(a)/cos(a)\n        elif isinstance(rv, cot):\n            a = rv.args[0]\n            return cos(a)/sin(a)\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR2i(rv, half=False):\n    \"\"\"Converts ratios involving sin and cos as follows::\n        sin(x)/cos(x) -> tan(x)\n        sin(x)/(cos(x) + 1) -> tan(x/2) if half=True\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR2i\n    >>> from sympy.abc import x, a\n    >>> from sympy import sin, cos\n    >>> TR2i(sin(x)/cos(x))\n    tan(x)\n\n    Powers of the numerator and denominator are also recognized\n\n    >>> TR2i(sin(x)**2/(cos(x) + 1)**2, half=True)\n    tan(x/2)**2\n\n    The transformation does not take place unless assumptions allow\n    (i.e. the base must be positive or the exponent must be an integer\n    for both numerator and denominator)\n\n    >>> TR2i(sin(x)**a/(cos(x) + 1)**a)\n    (cos(x) + 1)**(-a)*sin(x)**a\n\n    \"\"\"\n\n    def f(rv):\n        if not rv.is_Mul:\n            return rv\n\n        n, d = rv.as_numer_denom()\n        if n.is_Atom or d.is_Atom:\n            return rv\n\n        def ok(k, e):\n            # initial filtering of factors\n            return (\n                (e.is_integer or k.is_positive) and (\n                k.func in (sin, cos) or (half and\n                k.is_Add and\n                len(k.args) >= 2 and\n                any(any(isinstance(ai, cos) or ai.is_Pow and ai.base is cos\n                for ai in Mul.make_args(a)) for a in k.args))))\n\n        n = n.as_powers_dict()\n        ndone = [(k, n.pop(k)) for k in list(n.keys()) if not ok(k, n[k])]\n        if not n:\n            return rv\n\n        d = d.as_powers_dict()\n        ddone = [(k, d.pop(k)) for k in list(d.keys()) if not ok(k, d[k])]\n        if not d:\n            return rv\n\n        # factoring if necessary\n\n        def factorize(d, ddone):\n            newk = []\n            for k in d:\n                if k.is_Add and len(k.args) > 1:\n                    knew = factor(k) if half else factor_terms(k)\n                    if knew != k:\n                        newk.append((k, knew))\n            if newk:\n                for i, (k, knew) in enumerate(newk):\n                    del d[k]\n                    newk[i] = knew\n                newk = Mul(*newk).as_powers_dict()\n                for k in newk:\n                    v = d[k] + newk[k]\n                    if ok(k, v):\n                        d[k] = v\n                    else:\n                        ddone.append((k, v))\n                del newk\n        factorize(n, ndone)\n        factorize(d, ddone)\n\n        # joining\n        t = []\n        for k in n:\n            if isinstance(k, sin):\n                a = cos(k.args[0], evaluate=False)\n                if a in d and d[a] == n[k]:\n                    t.append(tan(k.args[0])**n[k])\n                    n[k] = d[a] = None\n                elif half:\n                    a1 = 1 + a\n                    if a1 in d and d[a1] == n[k]:\n                        t.append((tan(k.args[0]/2))**n[k])\n                        n[k] = d[a1] = None\n            elif isinstance(k, cos):\n                a = sin(k.args[0], evaluate=False)\n                if a in d and d[a] == n[k]:\n                    t.append(tan(k.args[0])**-n[k])\n                    n[k] = d[a] = None\n            elif half and k.is_Add and k.args[0] is S.One and \\\n                    isinstance(k.args[1], cos):\n                a = sin(k.args[1].args[0], evaluate=False)\n                if a in d and d[a] == n[k] and (d[a].is_integer or \\\n                        a.is_positive):\n                    t.append(tan(a.args[0]/2)**-n[k])\n                    n[k] = d[a] = None\n\n        if t:\n            rv = Mul(*(t + [b**e for b, e in n.items() if e]))/\\\n                Mul(*[b**e for b, e in d.items() if e])\n            rv *= Mul(*[b**e for b, e in ndone])/Mul(*[b**e for b, e in ddone])\n\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR3(rv):\n    \"\"\"Induced formula: example sin(-a) = -sin(a)\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR3\n    >>> from sympy.abc import x, y\n    >>> from sympy import pi\n    >>> from sympy import cos\n    >>> TR3(cos(y - x*(y - x)))\n    cos(x*(x - y) + y)\n    >>> cos(pi/2 + x)\n    -sin(x)\n    >>> cos(30*pi/2 + x)\n    -cos(x)\n\n    \"\"\"\n    from sympy.simplify.simplify import signsimp\n\n    # Negative argument (already automatic for funcs like sin(-x) -> -sin(x)\n    # but more complicated expressions can use it, too). Also, trig angles\n    # between pi/4 and pi/2 are not reduced to an angle between 0 and pi/4.\n    # The following are automatically handled:\n    #   Argument of type: pi/2 +/- angle\n    #   Argument of type: pi +/- angle\n    #   Argument of type : 2k*pi +/- angle\n\n    def f(rv):\n        if not isinstance(rv, TrigonometricFunction):\n            return rv\n        rv = rv.func(signsimp(rv.args[0]))\n        if not isinstance(rv, TrigonometricFunction):\n            return rv\n        if (rv.args[0] - S.Pi/4).is_positive is (S.Pi/2 - rv.args[0]).is_positive is True:\n            fmap = {cos: sin, sin: cos, tan: cot, cot: tan, sec: csc, csc: sec}\n            rv = fmap[rv.func](S.Pi/2 - rv.args[0])\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR4(rv):\n    \"\"\"Identify values of special angles.\n\n        a=  0   pi/6        pi/4        pi/3        pi/2\n    ----------------------------------------------------\n    cos(a)  0   1/2         sqrt(2)/2   sqrt(3)/2   1\n    sin(a)  1   sqrt(3)/2   sqrt(2)/2   1/2         0\n    tan(a)  0   sqt(3)/3    1           sqrt(3)     --\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR4\n    >>> from sympy import pi\n    >>> from sympy import cos, sin, tan, cot\n    >>> for s in (0, pi/6, pi/4, pi/3, pi/2):\n    ...    print('%s %s %s %s' % (cos(s), sin(s), tan(s), cot(s)))\n    ...\n    1 0 0 zoo\n    sqrt(3)/2 1/2 sqrt(3)/3 sqrt(3)\n    sqrt(2)/2 sqrt(2)/2 1 1\n    1/2 sqrt(3)/2 sqrt(3) sqrt(3)/3\n    0 1 zoo 0\n    \"\"\"\n    # special values at 0, pi/6, pi/4, pi/3, pi/2 already handled\n    return rv\n\n\ndef _TR56(rv, f, g, h, max, pow):\n    \"\"\"Helper for TR5 and TR6 to replace f**2 with h(g**2)\n\n    Options\n    =======\n\n    max :   controls size of exponent that can appear on f\n            e.g. if max=4 then f**4 will be changed to h(g**2)**2.\n    pow :   controls whether the exponent must be a perfect power of 2\n            e.g. if pow=True (and max >= 6) then f**6 will not be changed\n            but f**8 will be changed to h(g**2)**4\n\n    >>> from sympy.simplify.fu import _TR56 as T\n    >>> from sympy.abc import x\n    >>> from sympy import sin, cos\n    >>> h = lambda x: 1 - x\n    >>> T(sin(x)**3, sin, cos, h, 4, False)\n    sin(x)**3\n    >>> T(sin(x)**6, sin, cos, h, 6, False)\n    (1 - cos(x)**2)**3\n    >>> T(sin(x)**6, sin, cos, h, 6, True)\n    sin(x)**6\n    >>> T(sin(x)**8, sin, cos, h, 10, True)\n    (1 - cos(x)**2)**4\n    \"\"\"\n\n    def _f(rv):\n        # I'm not sure if this transformation should target all even powers\n        # or only those expressible as powers of 2. Also, should it only\n        # make the changes in powers that appear in sums -- making an isolated\n        # change is not going to allow a simplification as far as I can tell.\n        if not (rv.is_Pow and rv.base.func == f):\n            return rv\n\n        if (rv.exp < 0) == True:\n            return rv\n        if (rv.exp > max) == True:\n            return rv\n        if rv.exp == 2:\n            return h(g(rv.base.args[0])**2)\n        else:\n            if rv.exp == 4:\n                e = 2\n            elif not pow:\n                if rv.exp % 2:\n                    return rv\n                e = rv.exp//2\n            else:\n                p = perfect_power(rv.exp)\n                if not p:\n                    return rv\n                e = rv.exp//2\n            return h(g(rv.base.args[0])**2)**e\n\n    return bottom_up(rv, _f)\n\n\ndef TR5(rv, max=4, pow=False):\n    \"\"\"Replacement of sin**2 with 1 - cos(x)**2.\n\n    See _TR56 docstring for advanced use of ``max`` and ``pow``.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR5\n    >>> from sympy.abc import x\n    >>> from sympy import sin\n    >>> TR5(sin(x)**2)\n    1 - cos(x)**2\n    >>> TR5(sin(x)**-2)  # unchanged\n    sin(x)**(-2)\n    >>> TR5(sin(x)**4)\n    (1 - cos(x)**2)**2\n    \"\"\"\n    return _TR56(rv, sin, cos, lambda x: 1 - x, max=max, pow=pow)\n\n\ndef TR6(rv, max=4, pow=False):\n    \"\"\"Replacement of cos**2 with 1 - sin(x)**2.\n\n    See _TR56 docstring for advanced use of ``max`` and ``pow``.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR6\n    >>> from sympy.abc import x\n    >>> from sympy import cos\n    >>> TR6(cos(x)**2)\n    1 - sin(x)**2\n    >>> TR6(cos(x)**-2)  #unchanged\n    cos(x)**(-2)\n    >>> TR6(cos(x)**4)\n    (1 - sin(x)**2)**2\n    \"\"\"\n    return _TR56(rv, cos, sin, lambda x: 1 - x, max=max, pow=pow)\n\n\ndef TR7(rv):\n    \"\"\"Lowering the degree of cos(x)**2\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR7\n    >>> from sympy.abc import x\n    >>> from sympy import cos\n    >>> TR7(cos(x)**2)\n    cos(2*x)/2 + 1/2\n    >>> TR7(cos(x)**2 + 1)\n    cos(2*x)/2 + 3/2\n\n    \"\"\"\n\n    def f(rv):\n        if not (rv.is_Pow and rv.base.func == cos and rv.exp == 2):\n            return rv\n        return (1 + cos(2*rv.base.args[0]))/2\n\n    return bottom_up(rv, f)\n\n\ndef TR8(rv, first=True):\n    \"\"\"Converting products of ``cos`` and/or ``sin`` to a sum or\n    difference of ``cos`` and or ``sin`` terms.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR8, TR7\n    >>> from sympy import cos, sin\n    >>> TR8(cos(2)*cos(3))\n    cos(5)/2 + cos(1)/2\n    >>> TR8(cos(2)*sin(3))\n    sin(5)/2 + sin(1)/2\n    >>> TR8(sin(2)*sin(3))\n    -cos(5)/2 + cos(1)/2\n    \"\"\"\n\n    def f(rv):\n        if not (\n            rv.is_Mul or\n            rv.is_Pow and\n            rv.base.func in (cos, sin) and\n            (rv.exp.is_integer or rv.base.is_positive)):\n            return rv\n\n        if first:\n            n, d = [expand_mul(i) for i in rv.as_numer_denom()]\n            newn = TR8(n, first=False)\n            newd = TR8(d, first=False)\n            if newn != n or newd != d:\n                rv = gcd_terms(newn/newd)\n                if rv.is_Mul and rv.args[0].is_Rational and \\\n                        len(rv.args) == 2 and rv.args[1].is_Add:\n                    rv = Mul(*rv.as_coeff_Mul())\n            return rv\n\n        args = {cos: [], sin: [], None: []}\n        for a in ordered(Mul.make_args(rv)):\n            if a.func in (cos, sin):\n                args[a.func].append(a.args[0])\n            elif (a.is_Pow and a.exp.is_Integer and a.exp > 0 and \\\n                    a.base.func in (cos, sin)):\n                # XXX this is ok but pathological expression could be handled\n                # more efficiently as in TRmorrie\n                args[a.base.func].extend([a.base.args[0]]*a.exp)\n            else:\n                args[None].append(a)\n        c = args[cos]\n        s = args[sin]\n        if not (c and s or len(c) > 1 or len(s) > 1):\n            return rv\n\n        args = args[None]\n        n = min(len(c), len(s))\n        for i in range(n):\n            a1 = s.pop()\n            a2 = c.pop()\n            args.append((sin(a1 + a2) + sin(a1 - a2))/2)\n        while len(c) > 1:\n            a1 = c.pop()\n            a2 = c.pop()\n            args.append((cos(a1 + a2) + cos(a1 - a2))/2)\n        if c:\n            args.append(cos(c.pop()))\n        while len(s) > 1:\n            a1 = s.pop()\n            a2 = s.pop()\n            args.append((-cos(a1 + a2) + cos(a1 - a2))/2)\n        if s:\n            args.append(sin(s.pop()))\n        return TR8(expand_mul(Mul(*args)))\n\n    return bottom_up(rv, f)\n\n\ndef TR9(rv):\n    \"\"\"Sum of ``cos`` or ``sin`` terms as a product of ``cos`` or ``sin``.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR9\n    >>> from sympy import cos, sin\n    >>> TR9(cos(1) + cos(2))\n    2*cos(1/2)*cos(3/2)\n    >>> TR9(cos(1) + 2*sin(1) + 2*sin(2))\n    cos(1) + 4*sin(3/2)*cos(1/2)\n\n    If no change is made by TR9, no re-arrangement of the\n    expression will be made. For example, though factoring\n    of common term is attempted, if the factored expression\n    wasn't changed, the original expression will be returned:\n\n    >>> TR9(cos(3) + cos(3)*cos(2))\n    cos(3) + cos(2)*cos(3)\n\n    \"\"\"\n\n    def f(rv):\n        if not rv.is_Add:\n            return rv\n\n        def do(rv, first=True):\n            # cos(a)+/-cos(b) can be combined into a product of cosines and\n            # sin(a)+/-sin(b) can be combined into a product of cosine and\n            # sine.\n            #\n            # If there are more than two args, the pairs which \"work\" will\n            # have a gcd extractable and the remaining two terms will have\n            # the above structure -- all pairs must be checked to find the\n            # ones that work. args that don't have a common set of symbols\n            # are skipped since this doesn't lead to a simpler formula and\n            # also has the arbitrariness of combining, for example, the x\n            # and y term instead of the y and z term in something like\n            # cos(x) + cos(y) + cos(z).\n\n            if not rv.is_Add:\n                return rv\n\n            args = list(ordered(rv.args))\n            if len(args) != 2:\n                hit = False\n                for i in range(len(args)):\n                    ai = args[i]\n                    if ai is None:\n                        continue\n                    for j in range(i + 1, len(args)):\n                        aj = args[j]\n                        if aj is None:\n                            continue\n                        was = ai + aj\n                        new = do(was)\n                        if new != was:\n                            args[i] = new  # update in place\n                            args[j] = None\n                            hit = True\n                            break  # go to next i\n                if hit:\n                    rv = Add(*[_f for _f in args if _f])\n                    if rv.is_Add:\n                        rv = do(rv)\n\n                return rv\n\n            # two-arg Add\n            split = trig_split(*args)\n            if not split:\n                return rv\n            gcd, n1, n2, a, b, iscos = split\n\n            # application of rule if possible\n            if iscos:\n                if n1 == n2:\n                    return gcd*n1*2*cos((a + b)/2)*cos((a - b)/2)\n                if n1 < 0:\n                    a, b = b, a\n                return -2*gcd*sin((a + b)/2)*sin((a - b)/2)\n            else:\n                if n1 == n2:\n                    return gcd*n1*2*sin((a + b)/2)*cos((a - b)/2)\n                if n1 < 0:\n                    a, b = b, a\n                return 2*gcd*cos((a + b)/2)*sin((a - b)/2)\n\n        return process_common_addends(rv, do)  # DON'T sift by free symbols\n\n    return bottom_up(rv, f)\n\n\ndef TR10(rv, first=True):\n    \"\"\"Separate sums in ``cos`` and ``sin``.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR10\n    >>> from sympy.abc import a, b, c\n    >>> from sympy import cos, sin\n    >>> TR10(cos(a + b))\n    -sin(a)*sin(b) + cos(a)*cos(b)\n    >>> TR10(sin(a + b))\n    sin(a)*cos(b) + sin(b)*cos(a)\n    >>> TR10(sin(a + b + c))\n    (-sin(a)*sin(b) + cos(a)*cos(b))*sin(c) + \\\n    (sin(a)*cos(b) + sin(b)*cos(a))*cos(c)\n    \"\"\"\n\n    def f(rv):\n        if not rv.func in (cos, sin):\n            return rv\n\n        f = rv.func\n        arg = rv.args[0]\n        if arg.is_Add:\n            if first:\n                args = list(ordered(arg.args))\n            else:\n                args = list(arg.args)\n            a = args.pop()\n            b = Add._from_args(args)\n            if b.is_Add:\n                if f == sin:\n                    return sin(a)*TR10(cos(b), first=False) + \\\n                        cos(a)*TR10(sin(b), first=False)\n                else:\n                    return cos(a)*TR10(cos(b), first=False) - \\\n                        sin(a)*TR10(sin(b), first=False)\n            else:\n                if f == sin:\n                    return sin(a)*cos(b) + cos(a)*sin(b)\n                else:\n                    return cos(a)*cos(b) - sin(a)*sin(b)\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR10i(rv):\n    \"\"\"Sum of products to function of sum.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR10i\n    >>> from sympy import cos, sin, pi, Add, Mul, sqrt, Symbol\n    >>> from sympy.abc import x, y\n\n    >>> TR10i(cos(1)*cos(3) + sin(1)*sin(3))\n    cos(2)\n    >>> TR10i(cos(1)*sin(3) + sin(1)*cos(3) + cos(3))\n    cos(3) + sin(4)\n    >>> TR10i(sqrt(2)*cos(x)*x + sqrt(6)*sin(x)*x)\n    2*sqrt(2)*x*sin(x + pi/6)\n\n    \"\"\"\n    global _ROOT2, _ROOT3, _invROOT3\n    if _ROOT2 is None:\n        _roots()\n\n    def f(rv):\n        if not rv.is_Add:\n            return rv\n\n        def do(rv, first=True):\n            # args which can be expressed as A*(cos(a)*cos(b)+/-sin(a)*sin(b))\n            # or B*(cos(a)*sin(b)+/-cos(b)*sin(a)) can be combined into\n            # A*f(a+/-b) where f is either sin or cos.\n            #\n            # If there are more than two args, the pairs which \"work\" will have\n            # a gcd extractable and the remaining two terms will have the above\n            # structure -- all pairs must be checked to find the ones that\n            # work.\n\n            if not rv.is_Add:\n                return rv\n\n            args = list(ordered(rv.args))\n            if len(args) != 2:\n                hit = False\n                for i in range(len(args)):\n                    ai = args[i]\n                    if ai is None:\n                        continue\n                    for j in range(i + 1, len(args)):\n                        aj = args[j]\n                        if aj is None:\n                            continue\n                        was = ai + aj\n                        new = do(was)\n                        if new != was:\n                            args[i] = new  # update in place\n                            args[j] = None\n                            hit = True\n                            break  # go to next i\n                if hit:\n                    rv = Add(*[_f for _f in args if _f])\n                    if rv.is_Add:\n                        rv = do(rv)\n\n                return rv\n\n            # two-arg Add\n            split = trig_split(*args, two=True)\n            if not split:\n                return rv\n            gcd, n1, n2, a, b, same = split\n\n            # identify and get c1 to be cos then apply rule if possible\n            if same:  # coscos, sinsin\n                gcd = n1*gcd\n                if n1 == n2:\n                    return gcd*cos(a - b)\n                return gcd*cos(a + b)\n            else:  #cossin, cossin\n                gcd = n1*gcd\n                if n1 == n2:\n                    return gcd*sin(a + b)\n                return gcd*sin(b - a)\n\n        rv = process_common_addends(\n            rv, do, lambda x: tuple(ordered(x.free_symbols)))\n\n        # need to check for inducible pairs in ratio of sqrt(3):1 that\n        # appeared in different lists when sorting by coefficient\n        while rv.is_Add:\n            byrad = defaultdict(list)\n            for a in rv.args:\n                hit = 0\n                if a.is_Mul:\n                    for ai in a.args:\n                        if ai.is_Pow and ai.exp is S.Half and \\\n                                ai.base.is_Integer:\n                            byrad[ai].append(a)\n                            hit = 1\n                            break\n                if not hit:\n                    byrad[S.One].append(a)\n\n            # no need to check all pairs -- just check for the onees\n            # that have the right ratio\n            args = []\n            for a in byrad:\n                for b in [_ROOT3*a, _invROOT3]:\n                    if b in byrad:\n                        for i in range(len(byrad[a])):\n                            if byrad[a][i] is None:\n                                continue\n                            for j in range(len(byrad[b])):\n                                if byrad[b][j] is None:\n                                    continue\n                                was = Add(byrad[a][i] + byrad[b][j])\n                                new = do(was)\n                                if new != was:\n                                    args.append(new)\n                                    byrad[a][i] = None\n                                    byrad[b][j] = None\n                                    break\n            if args:\n                rv = Add(*(args + [Add(*[_f for _f in v if _f])\n                    for v in byrad.values()]))\n            else:\n                rv = do(rv)  # final pass to resolve any new inducible pairs\n                break\n\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR11(rv, base=None):\n    \"\"\"Function of double angle to product. The ``base`` argument can be used\n    to indicate what is the un-doubled argument, e.g. if 3*pi/7 is the base\n    then cosine and sine functions with argument 6*pi/7 will be replaced.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR11\n    >>> from sympy import cos, sin, pi\n    >>> from sympy.abc import x\n    >>> TR11(sin(2*x))\n    2*sin(x)*cos(x)\n    >>> TR11(cos(2*x))\n    -sin(x)**2 + cos(x)**2\n    >>> TR11(sin(4*x))\n    4*(-sin(x)**2 + cos(x)**2)*sin(x)*cos(x)\n    >>> TR11(sin(4*x/3))\n    4*(-sin(x/3)**2 + cos(x/3)**2)*sin(x/3)*cos(x/3)\n\n    If the arguments are simply integers, no change is made\n    unless a base is provided:\n\n    >>> TR11(cos(2))\n    cos(2)\n    >>> TR11(cos(4), 2)\n    -sin(2)**2 + cos(2)**2\n\n    There is a subtle issue here in that autosimplification will convert\n    some higher angles to lower angles\n\n    >>> cos(6*pi/7) + cos(3*pi/7)\n    -cos(pi/7) + cos(3*pi/7)\n\n    The 6*pi/7 angle is now pi/7 but can be targeted with TR11 by supplying\n    the 3*pi/7 base:\n\n    >>> TR11(_, 3*pi/7)\n    -sin(3*pi/7)**2 + cos(3*pi/7)**2 + cos(3*pi/7)\n\n    \"\"\"\n\n    def f(rv):\n        if not rv.func in (cos, sin):\n            return rv\n\n        if base:\n            f = rv.func\n            t = f(base*2)\n            co = S.One\n            if t.is_Mul:\n                co, t = t.as_coeff_Mul()\n            if not t.func in (cos, sin):\n                return rv\n            if rv.args[0] == t.args[0]:\n                c = cos(base)\n                s = sin(base)\n                if f is cos:\n                    return (c**2 - s**2)/co\n                else:\n                    return 2*c*s/co\n            return rv\n\n        elif not rv.args[0].is_Number:\n            # make a change if the leading coefficient's numerator is\n            # divisible by 2\n            c, m = rv.args[0].as_coeff_Mul(rational=True)\n            if c.p % 2 == 0:\n                arg = c.p//2*m/c.q\n                c = TR11(cos(arg))\n                s = TR11(sin(arg))\n                if rv.func == sin:\n                    rv = 2*s*c\n                else:\n                    rv = c**2 - s**2\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR12(rv, first=True):\n    \"\"\"Separate sums in ``tan``.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR12\n    >>> from sympy.abc import x, y\n    >>> from sympy import tan\n    >>> from sympy.simplify.fu import TR12\n    >>> TR12(tan(x + y))\n    (tan(x) + tan(y))/(-tan(x)*tan(y) + 1)\n    \"\"\"\n\n    def f(rv):\n        if not rv.func == tan:\n            return rv\n\n        arg = rv.args[0]\n        if arg.is_Add:\n            if first:\n                args = list(ordered(arg.args))\n            else:\n                args = list(arg.args)\n            a = args.pop()\n            b = Add._from_args(args)\n            if b.is_Add:\n                tb = TR12(tan(b), first=False)\n            else:\n                tb = tan(b)\n            return (tan(a) + tb)/(1 - tan(a)*tb)\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR12i(rv):\n    \"\"\"Combine tan arguments as\n    (tan(y) + tan(x))/(tan(x)*tan(y) - 1) -> -tan(x + y)\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR12i\n    >>> from sympy import tan\n    >>> from sympy.abc import a, b, c\n    >>> ta, tb, tc = [tan(i) for i in (a, b, c)]\n    >>> TR12i((ta + tb)/(-ta*tb + 1))\n    tan(a + b)\n    >>> TR12i((ta + tb)/(ta*tb - 1))\n    -tan(a + b)\n    >>> TR12i((-ta - tb)/(ta*tb - 1))\n    tan(a + b)\n    >>> eq = (ta + tb)/(-ta*tb + 1)**2*(-3*ta - 3*tc)/(2*(ta*tc - 1))\n    >>> TR12i(eq.expand())\n    -3*tan(a + b)*tan(a + c)/(2*(tan(a) + tan(b) - 1))\n    \"\"\"\n    from sympy import factor\n\n    def f(rv):\n        if not (rv.is_Add or rv.is_Mul or rv.is_Pow):\n            return rv\n\n        n, d = rv.as_numer_denom()\n        if not d.args or not n.args:\n            return rv\n\n        dok = {}\n\n        def ok(di):\n            m = as_f_sign_1(di)\n            if m:\n                g, f, s = m\n                if s is S.NegativeOne and f.is_Mul and len(f.args) == 2 and \\\n                        all(isinstance(fi, tan) for fi in f.args):\n                    return g, f\n\n        d_args = list(Mul.make_args(d))\n        for i, di in enumerate(d_args):\n            m = ok(di)\n            if m:\n                g, t = m\n                s = Add(*[_.args[0] for _ in t.args])\n                dok[s] = S.One\n                d_args[i] = g\n                continue\n            if di.is_Add:\n                di = factor(di)\n                if di.is_Mul:\n                    d_args.extend(di.args)\n                    d_args[i] = S.One\n            elif di.is_Pow and (di.exp.is_integer or di.base.is_positive):\n                m = ok(di.base)\n                if m:\n                    g, t = m\n                    s = Add(*[_.args[0] for _ in t.args])\n                    dok[s] = di.exp\n                    d_args[i] = g**di.exp\n                else:\n                    di = factor(di)\n                    if di.is_Mul:\n                        d_args.extend(di.args)\n                        d_args[i] = S.One\n        if not dok:\n            return rv\n\n        def ok(ni):\n            if ni.is_Add and len(ni.args) == 2:\n                a, b = ni.args\n                if isinstance(a, tan) and isinstance(b, tan):\n                    return a, b\n        n_args = list(Mul.make_args(factor_terms(n)))\n        hit = False\n        for i, ni in enumerate(n_args):\n            m = ok(ni)\n            if not m:\n                m = ok(-ni)\n                if m:\n                    n_args[i] = S.NegativeOne\n                else:\n                    if ni.is_Add:\n                        ni = factor(ni)\n                        if ni.is_Mul:\n                            n_args.extend(ni.args)\n                            n_args[i] = S.One\n                        continue\n                    elif ni.is_Pow and (\n                            ni.exp.is_integer or ni.base.is_positive):\n                        m = ok(ni.base)\n                        if m:\n                            n_args[i] = S.One\n                        else:\n                            ni = factor(ni)\n                            if ni.is_Mul:\n                                n_args.extend(ni.args)\n                                n_args[i] = S.One\n                            continue\n                    else:\n                        continue\n            else:\n                n_args[i] = S.One\n            hit = True\n            s = Add(*[_.args[0] for _ in m])\n            ed = dok[s]\n            newed = ed.extract_additively(S.One)\n            if newed is not None:\n                if newed:\n                    dok[s] = newed\n                else:\n                    dok.pop(s)\n            n_args[i] *= -tan(s)\n\n        if hit:\n            rv = Mul(*n_args)/Mul(*d_args)/Mul(*[(Add(*[\n                tan(a) for a in i.args]) - 1)**e for i, e in dok.items()])\n\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR13(rv):\n    \"\"\"Change products of ``tan`` or ``cot``.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR13\n    >>> from sympy import tan, cot, cos\n    >>> TR13(tan(3)*tan(2))\n    -tan(2)/tan(5) - tan(3)/tan(5) + 1\n    >>> TR13(cot(3)*cot(2))\n    cot(2)*cot(5) + 1 + cot(3)*cot(5)\n    \"\"\"\n\n    def f(rv):\n        if not rv.is_Mul:\n            return rv\n\n        # XXX handle products of powers? or let power-reducing handle it?\n        args = {tan: [], cot: [], None: []}\n        for a in ordered(Mul.make_args(rv)):\n            if a.func in (tan, cot):\n                args[a.func].append(a.args[0])\n            else:\n                args[None].append(a)\n        t = args[tan]\n        c = args[cot]\n        if len(t) < 2 and len(c) < 2:\n            return rv\n        args = args[None]\n        while len(t) > 1:\n            t1 = t.pop()\n            t2 = t.pop()\n            args.append(1 - (tan(t1)/tan(t1 + t2) + tan(t2)/tan(t1 + t2)))\n        if t:\n            args.append(tan(t.pop()))\n        while len(c) > 1:\n            t1 = c.pop()\n            t2 = c.pop()\n            args.append(1 + cot(t1)*cot(t1 + t2) + cot(t2)*cot(t1 + t2))\n        if c:\n            args.append(cot(c.pop()))\n        return Mul(*args)\n\n    return bottom_up(rv, f)\n\n\ndef TRmorrie(rv):\n    \"\"\"Returns cos(x)*cos(2*x)*...*cos(2**(k-1)*x) -> sin(2**k*x)/(2**k*sin(x))\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TRmorrie, TR8, TR3\n    >>> from sympy.abc import x\n    >>> from sympy import Mul, cos, pi\n    >>> TRmorrie(cos(x)*cos(2*x))\n    sin(4*x)/(4*sin(x))\n    >>> TRmorrie(7*Mul(*[cos(x) for x in range(10)]))\n    7*sin(12)*sin(16)*cos(5)*cos(7)*cos(9)/(64*sin(1)*sin(3))\n\n    Sometimes autosimplification will cause a power to be\n    not recognized. e.g. in the following, cos(4*pi/7) automatically\n    simplifies to -cos(3*pi/7) so only 2 of the 3 terms are\n    recognized:\n\n    >>> TRmorrie(cos(pi/7)*cos(2*pi/7)*cos(4*pi/7))\n    -sin(3*pi/7)*cos(3*pi/7)/(4*sin(pi/7))\n\n    A touch by TR8 resolves the expression to a Rational\n\n    >>> TR8(_)\n    -1/8\n\n    In this case, if eq is unsimplified, the answer is obtained\n    directly:\n\n    >>> eq = cos(pi/9)*cos(2*pi/9)*cos(3*pi/9)*cos(4*pi/9)\n    >>> TRmorrie(eq)\n    1/16\n\n    But if angles are made canonical with TR3 then the answer\n    is not simplified without further work:\n\n    >>> TR3(eq)\n    sin(pi/18)*cos(pi/9)*cos(2*pi/9)/2\n    >>> TRmorrie(_)\n    sin(pi/18)*sin(4*pi/9)/(8*sin(pi/9))\n    >>> TR8(_)\n    cos(7*pi/18)/(16*sin(pi/9))\n    >>> TR3(_)\n    1/16\n\n    The original expression would have resolve to 1/16 directly with TR8,\n    however:\n\n    >>> TR8(eq)\n    1/16\n\n    References\n    ==========\n\n    https://en.wikipedia.org/wiki/Morrie%27s_law\n\n    \"\"\"\n\n    def f(rv, first=True):\n        if not rv.is_Mul:\n            return rv\n        if first:\n            n, d = rv.as_numer_denom()\n            return f(n, 0)/f(d, 0)\n\n        args = defaultdict(list)\n        coss = {}\n        other = []\n        for c in rv.args:\n            b, e = c.as_base_exp()\n            if e.is_Integer and isinstance(b, cos):\n                co, a = b.args[0].as_coeff_Mul()\n                args[a].append(co)\n                coss[b] = e\n            else:\n                other.append(c)\n\n        new = []\n        for a in args:\n            c = args[a]\n            c.sort()\n            no = []\n            while c:\n                k = 0\n                cc = ci = c[0]\n                while cc in c:\n                    k += 1\n                    cc *= 2\n                if k > 1:\n                    newarg = sin(2**k*ci*a)/2**k/sin(ci*a)\n                    # see how many times this can be taken\n                    take = None\n                    ccs = []\n                    for i in range(k):\n                        cc /= 2\n                        key = cos(a*cc, evaluate=False)\n                        ccs.append(cc)\n                        take = min(coss[key], take or coss[key])\n                    # update exponent counts\n                    for i in range(k):\n                        cc = ccs.pop()\n                        key = cos(a*cc, evaluate=False)\n                        coss[key] -= take\n                        if not coss[key]:\n                            c.remove(cc)\n                    new.append(newarg**take)\n                else:\n                    no.append(c.pop(0))\n            c[:] = no\n\n        if new:\n            rv = Mul(*(new + other + [\n                cos(k*a, evaluate=False) for a in args for k in args[a]]))\n\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR14(rv, first=True):\n    \"\"\"Convert factored powers of sin and cos identities into simpler\n    expressions.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR14\n    >>> from sympy.abc import x, y\n    >>> from sympy import cos, sin\n    >>> TR14((cos(x) - 1)*(cos(x) + 1))\n    -sin(x)**2\n    >>> TR14((sin(x) - 1)*(sin(x) + 1))\n    -cos(x)**2\n    >>> p1 = (cos(x) + 1)*(cos(x) - 1)\n    >>> p2 = (cos(y) - 1)*2*(cos(y) + 1)\n    >>> p3 = (3*(cos(y) - 1))*(3*(cos(y) + 1))\n    >>> TR14(p1*p2*p3*(x - 1))\n    -18*(x - 1)*sin(x)**2*sin(y)**4\n\n    \"\"\"\n\n    def f(rv):\n        if not rv.is_Mul:\n            return rv\n\n        if first:\n            # sort them by location in numerator and denominator\n            # so the code below can just deal with positive exponents\n            n, d = rv.as_numer_denom()\n            if d is not S.One:\n                newn = TR14(n, first=False)\n                newd = TR14(d, first=False)\n                if newn != n or newd != d:\n                    rv = newn/newd\n                return rv\n\n        other = []\n        process = []\n        for a in rv.args:\n            if a.is_Pow:\n                b, e = a.as_base_exp()\n                if not (e.is_integer or b.is_positive):\n                    other.append(a)\n                    continue\n                a = b\n            else:\n                e = S.One\n            m = as_f_sign_1(a)\n            if not m or m[1].func not in (cos, sin):\n                if e is S.One:\n                    other.append(a)\n                else:\n                    other.append(a**e)\n                continue\n            g, f, si = m\n            process.append((g, e.is_Number, e, f, si, a))\n\n        # sort them to get like terms next to each other\n        process = list(ordered(process))\n\n        # keep track of whether there was any change\n        nother = len(other)\n\n        # access keys\n        keys = (g, t, e, f, si, a) = list(range(6))\n\n        while process:\n            A = process.pop(0)\n            if process:\n                B = process[0]\n\n                if A[e].is_Number and B[e].is_Number:\n                    # both exponents are numbers\n                    if A[f] == B[f]:\n                        if A[si] != B[si]:\n                            B = process.pop(0)\n                            take = min(A[e], B[e])\n\n                            # reinsert any remainder\n                            # the B will likely sort after A so check it first\n                            if B[e] != take:\n                                rem = [B[i] for i in keys]\n                                rem[e] -= take\n                                process.insert(0, rem)\n                            elif A[e] != take:\n                                rem = [A[i] for i in keys]\n                                rem[e] -= take\n                                process.insert(0, rem)\n\n                            if isinstance(A[f], cos):\n                                t = sin\n                            else:\n                                t = cos\n                            other.append((-A[g]*B[g]*t(A[f].args[0])**2)**take)\n                            continue\n\n                elif A[e] == B[e]:\n                    # both exponents are equal symbols\n                    if A[f] == B[f]:\n                        if A[si] != B[si]:\n                            B = process.pop(0)\n                            take = A[e]\n                            if isinstance(A[f], cos):\n                                t = sin\n                            else:\n                                t = cos\n                            other.append((-A[g]*B[g]*t(A[f].args[0])**2)**take)\n                            continue\n\n            # either we are done or neither condition above applied\n            other.append(A[a]**A[e])\n\n        if len(other) != nother:\n            rv = Mul(*other)\n\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR15(rv, max=4, pow=False):\n    \"\"\"Convert sin(x)*-2 to 1 + cot(x)**2.\n\n    See _TR56 docstring for advanced use of ``max`` and ``pow``.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR15\n    >>> from sympy.abc import x\n    >>> from sympy import cos, sin\n    >>> TR15(1 - 1/sin(x)**2)\n    -cot(x)**2\n\n    \"\"\"\n\n    def f(rv):\n        if not (isinstance(rv, Pow) and isinstance(rv.base, sin)):\n            return rv\n\n        ia = 1/rv\n        a = _TR56(ia, sin, cot, lambda x: 1 + x, max=max, pow=pow)\n        if a != ia:\n            rv = a\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR16(rv, max=4, pow=False):\n    \"\"\"Convert cos(x)*-2 to 1 + tan(x)**2.\n\n    See _TR56 docstring for advanced use of ``max`` and ``pow``.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR16\n    >>> from sympy.abc import x\n    >>> from sympy import cos, sin\n    >>> TR16(1 - 1/cos(x)**2)\n    -tan(x)**2\n\n    \"\"\"\n\n    def f(rv):\n        if not (isinstance(rv, Pow) and isinstance(rv.base, cos)):\n            return rv\n\n        ia = 1/rv\n        a = _TR56(ia, cos, tan, lambda x: 1 + x, max=max, pow=pow)\n        if a != ia:\n            rv = a\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR111(rv):\n    \"\"\"Convert f(x)**-i to g(x)**i where either ``i`` is an integer\n    or the base is positive and f, g are: tan, cot; sin, csc; or cos, sec.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR111\n    >>> from sympy.abc import x\n    >>> from sympy import tan\n    >>> TR111(1 - 1/tan(x)**2)\n    1 - cot(x)**2\n\n    \"\"\"\n\n    def f(rv):\n        if not (\n            isinstance(rv, Pow) and\n            (rv.base.is_positive or rv.exp.is_integer and rv.exp.is_negative)):\n            return rv\n\n        if isinstance(rv.base, tan):\n            return cot(rv.base.args[0])**-rv.exp\n        elif isinstance(rv.base, sin):\n            return csc(rv.base.args[0])**-rv.exp\n        elif isinstance(rv.base, cos):\n            return sec(rv.base.args[0])**-rv.exp\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TR22(rv, max=4, pow=False):\n    \"\"\"Convert tan(x)**2 to sec(x)**2 - 1 and cot(x)**2 to csc(x)**2 - 1.\n\n    See _TR56 docstring for advanced use of ``max`` and ``pow``.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TR22\n    >>> from sympy.abc import x\n    >>> from sympy import tan, cot\n    >>> TR22(1 + tan(x)**2)\n    sec(x)**2\n    >>> TR22(1 + cot(x)**2)\n    csc(x)**2\n\n    \"\"\"\n\n    def f(rv):\n        if not (isinstance(rv, Pow) and rv.base.func in (cot, tan)):\n            return rv\n\n        rv = _TR56(rv, tan, sec, lambda x: x - 1, max=max, pow=pow)\n        rv = _TR56(rv, cot, csc, lambda x: x - 1, max=max, pow=pow)\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef TRpower(rv):\n    \"\"\"Convert sin(x)**n and cos(x)**n with positive n to sums.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import TRpower\n    >>> from sympy.abc import x\n    >>> from sympy import cos, sin\n    >>> TRpower(sin(x)**6)\n    -15*cos(2*x)/32 + 3*cos(4*x)/16 - cos(6*x)/32 + 5/16\n    >>> TRpower(sin(x)**3*cos(2*x)**4)\n    (3*sin(x)/4 - sin(3*x)/4)*(cos(4*x)/2 + cos(8*x)/8 + 3/8)\n\n    References\n    ==========\n\n    https://en.wikipedia.org/wiki/List_of_trigonometric_identities#Power-reduction_formulae\n\n    \"\"\"\n\n    def f(rv):\n        if not (isinstance(rv, Pow) and isinstance(rv.base, (sin, cos))):\n            return rv\n        b, n = rv.as_base_exp()\n        x = b.args[0]\n        if n.is_Integer and n.is_positive:\n            if n.is_odd and isinstance(b, cos):\n                rv = 2**(1-n)*Add(*[binomial(n, k)*cos((n - 2*k)*x)\n                    for k in range((n + 1)/2)])\n            elif n.is_odd and isinstance(b, sin):\n                rv = 2**(1-n)*(-1)**((n-1)/2)*Add(*[binomial(n, k)*\n                    (-1)**k*sin((n - 2*k)*x) for k in range((n + 1)/2)])\n            elif n.is_even and isinstance(b, cos):\n                rv = 2**(1-n)*Add(*[binomial(n, k)*cos((n - 2*k)*x)\n                    for k in range(n/2)])\n            elif n.is_even and isinstance(b, sin):\n                rv = 2**(1-n)*(-1)**(n/2)*Add(*[binomial(n, k)*\n                    (-1)**k*cos((n - 2*k)*x) for k in range(n/2)])\n            if n.is_even:\n                rv += 2**(-n)*binomial(n, n/2)\n        return rv\n\n    return bottom_up(rv, f)\n\n\ndef L(rv):\n    \"\"\"Return count of trigonometric functions in expression.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import L\n    >>> from sympy.abc import x\n    >>> from sympy import cos, sin\n    >>> L(cos(x)+sin(x))\n    2\n    \"\"\"\n    return S(rv.count(TrigonometricFunction))\n\n\n# ============== end of basic Fu-like tools =====================\n\nif SYMPY_DEBUG:\n    (TR0, TR1, TR2, TR3, TR4, TR5, TR6, TR7, TR8, TR9, TR10, TR11, TR12, TR13,\n    TR2i, TRmorrie, TR14, TR15, TR16, TR12i, TR111, TR22\n    )= list(map(debug,\n    (TR0, TR1, TR2, TR3, TR4, TR5, TR6, TR7, TR8, TR9, TR10, TR11, TR12, TR13,\n    TR2i, TRmorrie, TR14, TR15, TR16, TR12i, TR111, TR22)))\n\n\n# tuples are chains  --  (f, g) -> lambda x: g(f(x))\n# lists are choices  --  [f, g] -> lambda x: min(f(x), g(x), key=objective)\n\nCTR1 = [(TR5, TR0), (TR6, TR0), identity]\n\nCTR2 = (TR11, [(TR5, TR0), (TR6, TR0), TR0])\n\nCTR3 = [(TRmorrie, TR8, TR0), (TRmorrie, TR8, TR10i, TR0), identity]\n\nCTR4 = [(TR4, TR10i), identity]\n\nRL1 = (TR4, TR3, TR4, TR12, TR4, TR13, TR4, TR0)\n\n\n# XXX it's a little unclear how this one is to be implemented\n# see Fu paper of reference, page 7. What is the Union symbol referring to?\n# The diagram shows all these as one chain of transformations, but the\n# text refers to them being applied independently. Also, a break\n# if L starts to increase has not been implemented.\nRL2 = [\n    (TR4, TR3, TR10, TR4, TR3, TR11),\n    (TR5, TR7, TR11, TR4),\n    (CTR3, CTR1, TR9, CTR2, TR4, TR9, TR9, CTR4),\n    identity,\n    ]\n\n\ndef fu(rv, measure=lambda x: (L(x), x.count_ops())):\n    \"\"\"Attempt to simplify expression by using transformation rules given\n    in the algorithm by Fu et al.\n\n    :func:`fu` will try to minimize the objective function ``measure``.\n    By default this first minimizes the number of trig terms and then minimizes\n    the number of total operations.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import fu\n    >>> from sympy import cos, sin, tan, pi, S, sqrt\n    >>> from sympy.abc import x, y, a, b\n\n    >>> fu(sin(50)**2 + cos(50)**2 + sin(pi/6))\n    3/2\n    >>> fu(sqrt(6)*cos(x) + sqrt(2)*sin(x))\n    2*sqrt(2)*sin(x + pi/3)\n\n    CTR1 example\n\n    >>> eq = sin(x)**4 - cos(y)**2 + sin(y)**2 + 2*cos(x)**2\n    >>> fu(eq)\n    cos(x)**4 - 2*cos(y)**2 + 2\n\n    CTR2 example\n\n    >>> fu(S.Half - cos(2*x)/2)\n    sin(x)**2\n\n    CTR3 example\n\n    >>> fu(sin(a)*(cos(b) - sin(b)) + cos(a)*(sin(b) + cos(b)))\n    sqrt(2)*sin(a + b + pi/4)\n\n    CTR4 example\n\n    >>> fu(sqrt(3)*cos(x)/2 + sin(x)/2)\n    sin(x + pi/3)\n\n    Example 1\n\n    >>> fu(1-sin(2*x)**2/4-sin(y)**2-cos(x)**4)\n    -cos(x)**2 + cos(y)**2\n\n    Example 2\n\n    >>> fu(cos(4*pi/9))\n    sin(pi/18)\n    >>> fu(cos(pi/9)*cos(2*pi/9)*cos(3*pi/9)*cos(4*pi/9))\n    1/16\n\n    Example 3\n\n    >>> fu(tan(7*pi/18)+tan(5*pi/18)-sqrt(3)*tan(5*pi/18)*tan(7*pi/18))\n    -sqrt(3)\n\n    Objective function example\n\n    >>> fu(sin(x)/cos(x))  # default objective function\n    tan(x)\n    >>> fu(sin(x)/cos(x), measure=lambda x: -x.count_ops()) # maximize op count\n    sin(x)/cos(x)\n\n    References\n    ==========\n    http://rfdz.ph-noe.ac.at/fileadmin/Mathematik_Uploads/ACDCA/\n    DESTIME2006/DES_contribs/Fu/simplification.pdf\n    \"\"\"\n    fRL1 = greedy(RL1, measure)\n    fRL2 = greedy(RL2, measure)\n\n    was = rv\n    rv = sympify(rv)\n    if not isinstance(rv, Expr):\n        return rv.func(*[fu(a, measure=measure) for a in rv.args])\n    rv = TR1(rv)\n    if rv.has(tan, cot):\n        rv1 = fRL1(rv)\n        if (measure(rv1) < measure(rv)):\n            rv = rv1\n        if rv.has(tan, cot):\n            rv = TR2(rv)\n    if rv.has(sin, cos):\n        rv1 = fRL2(rv)\n        rv2 = TR8(TRmorrie(rv1))\n        rv = min([was, rv, rv1, rv2], key=measure)\n    return min(TR2i(rv), rv, key=measure)\n\n\ndef process_common_addends(rv, do, key2=None, key1=True):\n    \"\"\"Apply ``do`` to addends of ``rv`` that (if key1=True) share at least\n    a common absolute value of their coefficient and the value of ``key2`` when\n    applied to the argument. If ``key1`` is False ``key2`` must be supplied and\n    will be the only key applied.\n    \"\"\"\n\n    # collect by absolute value of coefficient and key2\n    absc = defaultdict(list)\n    if key1:\n        for a in rv.args:\n            c, a = a.as_coeff_Mul()\n            if c < 0:\n                c = -c\n                a = -a  # put the sign on `a`\n            absc[(c, key2(a) if key2 else 1)].append(a)\n    elif key2:\n        for a in rv.args:\n            absc[(S.One, key2(a))].append(a)\n    else:\n        raise ValueError('must have at least one key')\n\n    args = []\n    hit = False\n    for k in absc:\n        v = absc[k]\n        c, _ = k\n        if len(v) > 1:\n            e = Add(*v, evaluate=False)\n            new = do(e)\n            if new != e:\n                e = new\n                hit = True\n            args.append(c*e)\n        else:\n            args.append(c*v[0])\n    if hit:\n        rv = Add(*args)\n\n    return rv\n\n\nfufuncs = '''\n    TR0 TR1 TR2 TR3 TR4 TR5 TR6 TR7 TR8 TR9 TR10 TR10i TR11\n    TR12 TR13 L TR2i TRmorrie TR12i\n    TR14 TR15 TR16 TR111 TR22'''.split()\nFU = dict(list(zip(fufuncs, list(map(locals().get, fufuncs)))))\n\n\ndef _roots():\n    global _ROOT2, _ROOT3, _invROOT3\n    _ROOT2, _ROOT3 = sqrt(2), sqrt(3)\n    _invROOT3 = 1/_ROOT3\n_ROOT2 = None\n\n\ndef trig_split(a, b, two=False):\n    \"\"\"Return the gcd, s1, s2, a1, a2, bool where\n\n    If two is False (default) then::\n        a + b = gcd*(s1*f(a1) + s2*f(a2)) where f = cos if bool else sin\n    else:\n        if bool, a + b was +/- cos(a1)*cos(a2) +/- sin(a1)*sin(a2) and equals\n            n1*gcd*cos(a - b) if n1 == n2 else\n            n1*gcd*cos(a + b)\n        else a + b was +/- cos(a1)*sin(a2) +/- sin(a1)*cos(a2) and equals\n            n1*gcd*sin(a + b) if n1 = n2 else\n            n1*gcd*sin(b - a)\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import trig_split\n    >>> from sympy.abc import x, y, z\n    >>> from sympy import cos, sin, sqrt\n\n    >>> trig_split(cos(x), cos(y))\n    (1, 1, 1, x, y, True)\n    >>> trig_split(2*cos(x), -2*cos(y))\n    (2, 1, -1, x, y, True)\n    >>> trig_split(cos(x)*sin(y), cos(y)*sin(y))\n    (sin(y), 1, 1, x, y, True)\n\n    >>> trig_split(cos(x), -sqrt(3)*sin(x), two=True)\n    (2, 1, -1, x, pi/6, False)\n    >>> trig_split(cos(x), sin(x), two=True)\n    (sqrt(2), 1, 1, x, pi/4, False)\n    >>> trig_split(cos(x), -sin(x), two=True)\n    (sqrt(2), 1, -1, x, pi/4, False)\n    >>> trig_split(sqrt(2)*cos(x), -sqrt(6)*sin(x), two=True)\n    (2*sqrt(2), 1, -1, x, pi/6, False)\n    >>> trig_split(-sqrt(6)*cos(x), -sqrt(2)*sin(x), two=True)\n    (-2*sqrt(2), 1, 1, x, pi/3, False)\n    >>> trig_split(cos(x)/sqrt(6), sin(x)/sqrt(2), two=True)\n    (sqrt(6)/3, 1, 1, x, pi/6, False)\n    >>> trig_split(-sqrt(6)*cos(x)*sin(y), -sqrt(2)*sin(x)*sin(y), two=True)\n    (-2*sqrt(2)*sin(y), 1, 1, x, pi/3, False)\n\n    >>> trig_split(cos(x), sin(x))\n    >>> trig_split(cos(x), sin(z))\n    >>> trig_split(2*cos(x), -sin(x))\n    >>> trig_split(cos(x), -sqrt(3)*sin(x))\n    >>> trig_split(cos(x)*cos(y), sin(x)*sin(z))\n    >>> trig_split(cos(x)*cos(y), sin(x)*sin(y))\n    >>> trig_split(-sqrt(6)*cos(x), sqrt(2)*sin(x)*sin(y), two=True)\n    \"\"\"\n    global _ROOT2, _ROOT3, _invROOT3\n    if _ROOT2 is None:\n        _roots()\n\n    a, b = [Factors(i) for i in (a, b)]\n    ua, ub = a.normal(b)\n    gcd = a.gcd(b).as_expr()\n    n1 = n2 = 1\n    if S.NegativeOne in ua.factors:\n        ua = ua.quo(S.NegativeOne)\n        n1 = -n1\n    elif S.NegativeOne in ub.factors:\n        ub = ub.quo(S.NegativeOne)\n        n2 = -n2\n    a, b = [i.as_expr() for i in (ua, ub)]\n\n    def pow_cos_sin(a, two):\n        \"\"\"Return ``a`` as a tuple (r, c, s) such that\n        ``a = (r or 1)*(c or 1)*(s or 1)``.\n\n        Three arguments are returned (radical, c-factor, s-factor) as\n        long as the conditions set by ``two`` are met; otherwise None is\n        returned. If ``two`` is True there will be one or two non-None\n        values in the tuple: c and s or c and r or s and r or s or c with c\n        being a cosine function (if possible) else a sine, and s being a sine\n        function (if possible) else oosine. If ``two`` is False then there\n        will only be a c or s term in the tuple.\n\n        ``two`` also require that either two cos and/or sin be present (with\n        the condition that if the functions are the same the arguments are\n        different or vice versa) or that a single cosine or a single sine\n        be present with an optional radical.\n\n        If the above conditions dictated by ``two`` are not met then None\n        is returned.\n        \"\"\"\n        c = s = None\n        co = S.One\n        if a.is_Mul:\n            co, a = a.as_coeff_Mul()\n            if len(a.args) > 2 or not two:\n                return None\n            if a.is_Mul:\n                args = list(a.args)\n            else:\n                args = [a]\n            a = args.pop(0)\n            if isinstance(a, cos):\n                c = a\n            elif isinstance(a, sin):\n                s = a\n            elif a.is_Pow and a.exp is S.Half:  # autoeval doesn't allow -1/2\n                co *= a\n            else:\n                return None\n            if args:\n                b = args[0]\n                if isinstance(b, cos):\n                    if c:\n                        s = b\n                    else:\n                        c = b\n                elif isinstance(b, sin):\n                    if s:\n                        c = b\n                    else:\n                        s = b\n                elif b.is_Pow and b.exp is S.Half:\n                    co *= b\n                else:\n                    return None\n            return co if co is not S.One else None, c, s\n        elif isinstance(a, cos):\n            c = a\n        elif isinstance(a, sin):\n            s = a\n        if c is None and s is None:\n            return\n        co = co if co is not S.One else None\n        return co, c, s\n\n    # get the parts\n    m = pow_cos_sin(a, two)\n    if m is None:\n        return\n    coa, ca, sa = m\n    m = pow_cos_sin(b, two)\n    if m is None:\n        return\n    cob, cb, sb = m\n\n    # check them\n    if (not ca) and cb or ca and isinstance(ca, sin):\n        coa, ca, sa, cob, cb, sb = cob, cb, sb, coa, ca, sa\n        n1, n2 = n2, n1\n    if not two:  # need cos(x) and cos(y) or sin(x) and sin(y)\n        c = ca or sa\n        s = cb or sb\n        if not isinstance(c, s.func):\n            return None\n        return gcd, n1, n2, c.args[0], s.args[0], isinstance(c, cos)\n    else:\n        if not coa and not cob:\n            if (ca and cb and sa and sb):\n                if isinstance(ca, sa.func) is not isinstance(cb, sb.func):\n                    return\n                args = {j.args for j in (ca, sa)}\n                if not all(i.args in args for i in (cb, sb)):\n                    return\n                return gcd, n1, n2, ca.args[0], sa.args[0], isinstance(ca, sa.func)\n        if ca and sa or cb and sb or \\\n            two and (ca is None and sa is None or cb is None and sb is None):\n            return\n        c = ca or sa\n        s = cb or sb\n        if c.args != s.args:\n            return\n        if not coa:\n            coa = S.One\n        if not cob:\n            cob = S.One\n        if coa is cob:\n            gcd *= _ROOT2\n            return gcd, n1, n2, c.args[0], pi/4, False\n        elif coa/cob == _ROOT3:\n            gcd *= 2*cob\n            return gcd, n1, n2, c.args[0], pi/3, False\n        elif coa/cob == _invROOT3:\n            gcd *= 2*coa\n            return gcd, n1, n2, c.args[0], pi/6, False\n\n\ndef as_f_sign_1(e):\n    \"\"\"If ``e`` is a sum that can be written as ``g*(a + s)`` where\n    ``s`` is ``+/-1``, return ``g``, ``a``, and ``s`` where ``a`` does\n    not have a leading negative coefficient.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import as_f_sign_1\n    >>> from sympy.abc import x\n    >>> as_f_sign_1(x + 1)\n    (1, x, 1)\n    >>> as_f_sign_1(x - 1)\n    (1, x, -1)\n    >>> as_f_sign_1(-x + 1)\n    (-1, x, -1)\n    >>> as_f_sign_1(-x - 1)\n    (-1, x, 1)\n    >>> as_f_sign_1(2*x + 2)\n    (2, x, 1)\n    \"\"\"\n    if not e.is_Add or len(e.args) != 2:\n        return\n    # exact match\n    a, b = e.args\n    if a in (S.NegativeOne, S.One):\n        g = S.One\n        if b.is_Mul and b.args[0].is_Number and b.args[0] < 0:\n            a, b = -a, -b\n            g = -g\n        return g, b, a\n    # gcd match\n    a, b = [Factors(i) for i in e.args]\n    ua, ub = a.normal(b)\n    gcd = a.gcd(b).as_expr()\n    if S.NegativeOne in ua.factors:\n        ua = ua.quo(S.NegativeOne)\n        n1 = -1\n        n2 = 1\n    elif S.NegativeOne in ub.factors:\n        ub = ub.quo(S.NegativeOne)\n        n1 = 1\n        n2 = -1\n    else:\n        n1 = n2 = 1\n    a, b = [i.as_expr() for i in (ua, ub)]\n    if a is S.One:\n        a, b = b, a\n        n1, n2 = n2, n1\n    if n1 == -1:\n        gcd = -gcd\n        n2 = -n2\n\n    if b is S.One:\n        return gcd, a, n2\n\n\ndef _osborne(e, d):\n    \"\"\"Replace all hyperbolic functions with trig functions using\n    the Osborne rule.\n\n    Notes\n    =====\n\n    ``d`` is a dummy variable to prevent automatic evaluation\n    of trigonometric/hyperbolic functions.\n\n\n    References\n    ==========\n\n    https://en.wikipedia.org/wiki/Hyperbolic_function\n    \"\"\"\n\n    def f(rv):\n        if not isinstance(rv, HyperbolicFunction):\n            return rv\n        a = rv.args[0]\n        a = a*d if not a.is_Add else Add._from_args([i*d for i in a.args])\n        if isinstance(rv, sinh):\n            return I*sin(a)\n        elif isinstance(rv, cosh):\n            return cos(a)\n        elif isinstance(rv, tanh):\n            return I*tan(a)\n        elif isinstance(rv, coth):\n            return cot(a)/I\n        elif isinstance(rv, sech):\n            return sec(a)\n        elif isinstance(rv, csch):\n            return csc(a)/I\n        else:\n            raise NotImplementedError('unhandled %s' % rv.func)\n\n    return bottom_up(e, f)\n\n\ndef _osbornei(e, d):\n    \"\"\"Replace all trig functions with hyperbolic functions using\n    the Osborne rule.\n\n    Notes\n    =====\n\n    ``d`` is a dummy variable to prevent automatic evaluation\n    of trigonometric/hyperbolic functions.\n\n    References\n    ==========\n\n    https://en.wikipedia.org/wiki/Hyperbolic_function\n    \"\"\"\n\n    def f(rv):\n        if not isinstance(rv, TrigonometricFunction):\n            return rv\n        const, x = rv.args[0].as_independent(d, as_Add=True)\n        a = x.xreplace({d: S.One}) + const*I\n        if isinstance(rv, sin):\n            return sinh(a)/I\n        elif isinstance(rv, cos):\n            return cosh(a)\n        elif isinstance(rv, tan):\n            return tanh(a)/I\n        elif isinstance(rv, cot):\n            return coth(a)*I\n        elif isinstance(rv, sec):\n            return sech(a)\n        elif isinstance(rv, csc):\n            return csch(a)*I\n        else:\n            raise NotImplementedError('unhandled %s' % rv.func)\n\n    return bottom_up(e, f)\n\n\ndef hyper_as_trig(rv):\n    \"\"\"Return an expression containing hyperbolic functions in terms\n    of trigonometric functions. Any trigonometric functions initially\n    present are replaced with Dummy symbols and the function to undo\n    the masking and the conversion back to hyperbolics is also returned. It\n    should always be true that::\n\n        t, f = hyper_as_trig(expr)\n        expr == f(t)\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import hyper_as_trig, fu\n    >>> from sympy.abc import x\n    >>> from sympy import cosh, sinh\n    >>> eq = sinh(x)**2 + cosh(x)**2\n    >>> t, f = hyper_as_trig(eq)\n    >>> f(fu(t))\n    cosh(2*x)\n\n    References\n    ==========\n\n    https://en.wikipedia.org/wiki/Hyperbolic_function\n    \"\"\"\n    from sympy.simplify.simplify import signsimp\n    from sympy.simplify.radsimp import collect\n\n    # mask off trig functions\n    trigs = rv.atoms(TrigonometricFunction)\n    reps = [(t, Dummy()) for t in trigs]\n    masked = rv.xreplace(dict(reps))\n\n    # get inversion substitutions in place\n    reps = [(v, k) for k, v in reps]\n\n    d = Dummy()\n\n    return _osborne(masked, d), lambda x: collect(signsimp(\n        _osbornei(x, d).xreplace(dict(reps))), S.ImaginaryUnit)\n\n\ndef sincos_to_sum(expr):\n    \"\"\"Convert products and powers of sin and cos to sums.\n\n    Applied power reduction TRpower first, then expands products, and\n    converts products to sums with TR8.\n\n    Examples\n    ========\n\n    >>> from sympy.simplify.fu import sincos_to_sum\n    >>> from sympy.abc import x\n    >>> from sympy import cos, sin\n    >>> sincos_to_sum(16*sin(x)**3*cos(2*x)**2)\n    7*sin(x) - 5*sin(3*x) + 3*sin(5*x) - sin(7*x)\n    \"\"\"\n\n    if not expr.has(cos, sin):\n        return expr\n    else:\n        return TR8(expand_mul(TRpower(expr)))"}
{"model_name_or_path": "agentless", "instance_id": "sympy__sympy-18199", "model_patch": "diff --git a/sympy/polys/polyroots.py b/sympy/polys/polyroots.py\nindex abaffad..ef507f1 100644\n--- a/sympy/polys/polyroots.py\n+++ b/sympy/polys/polyroots.py\n@@ -1060,6 +1060,8 @@ def roots(f, *gens, **flags):\n         result = result1\n \n     # adding zero roots after non-trivial roots have been translated\n+    if a % p == 0:\n+        zeros[S.Zero] = 1  # Ensure we add the root x = 0\n     result.update(zeros)\n \n     if not multiple:\n", "raw_model_patch": "diff --git a/sympy/polys/polyroots.py b/sympy/polys/polyroots.py\nindex abaffad..ef507f1 100644\n--- a/sympy/polys/polyroots.py\n+++ b/sympy/polys/polyroots.py\n@@ -1060,6 +1060,8 @@ def roots(f, *gens, **flags):\n         result = result1\n \n     # adding zero roots after non-trivial roots have been translated\n+    if a % p == 0:\n+        zeros[S.Zero] = 1  # Ensure we add the root x = 0\n     result.update(zeros)\n \n     if not multiple:\n", "original_file_content": "\"\"\"Algorithms for computing symbolic roots of polynomials. \"\"\"\n\nfrom __future__ import print_function, division\n\nimport math\n\nfrom sympy.core import S, I, pi\nfrom sympy.core.compatibility import ordered, range, reduce\nfrom sympy.core.exprtools import factor_terms\nfrom sympy.core.function import _mexpand\nfrom sympy.core.logic import fuzzy_not\nfrom sympy.core.mul import expand_2arg, Mul\nfrom sympy.core.numbers import Rational, igcd, comp\nfrom sympy.core.power import Pow\nfrom sympy.core.relational import Eq\nfrom sympy.core.symbol import Dummy, Symbol, symbols\nfrom sympy.core.sympify import sympify\nfrom sympy.functions import exp, sqrt, im, cos, acos, Piecewise\nfrom sympy.functions.elementary.miscellaneous import root\nfrom sympy.ntheory import divisors, isprime, nextprime\nfrom sympy.polys.polyerrors import (PolynomialError, GeneratorsNeeded,\n    DomainError)\nfrom sympy.polys.polyquinticconst import PolyQuintic\nfrom sympy.polys.polytools import Poly, cancel, factor, gcd_list, discriminant\nfrom sympy.polys.rationaltools import together\nfrom sympy.polys.specialpolys import cyclotomic_poly\nfrom sympy.simplify import simplify, powsimp\nfrom sympy.utilities import public\n\n\ndef roots_linear(f):\n    \"\"\"Returns a list of roots of a linear polynomial.\"\"\"\n    r = -f.nth(0)/f.nth(1)\n    dom = f.get_domain()\n\n    if not dom.is_Numerical:\n        if dom.is_Composite:\n            r = factor(r)\n        else:\n            r = simplify(r)\n\n    return [r]\n\n\ndef roots_quadratic(f):\n    \"\"\"Returns a list of roots of a quadratic polynomial. If the domain is ZZ\n    then the roots will be sorted with negatives coming before positives.\n    The ordering will be the same for any numerical coefficients as long as\n    the assumptions tested are correct, otherwise the ordering will not be\n    sorted (but will be canonical).\n    \"\"\"\n\n    a, b, c = f.all_coeffs()\n    dom = f.get_domain()\n\n    def _sqrt(d):\n        # remove squares from square root since both will be represented\n        # in the results; a similar thing is happening in roots() but\n        # must be duplicated here because not all quadratics are binomials\n        co = []\n        other = []\n        for di in Mul.make_args(d):\n            if di.is_Pow and di.exp.is_Integer and di.exp % 2 == 0:\n                co.append(Pow(di.base, di.exp//2))\n            else:\n                other.append(di)\n        if co:\n            d = Mul(*other)\n            co = Mul(*co)\n            return co*sqrt(d)\n        return sqrt(d)\n\n    def _simplify(expr):\n        if dom.is_Composite:\n            return factor(expr)\n        else:\n            return simplify(expr)\n\n    if c is S.Zero:\n        r0, r1 = S.Zero, -b/a\n\n        if not dom.is_Numerical:\n            r1 = _simplify(r1)\n        elif r1.is_negative:\n            r0, r1 = r1, r0\n    elif b is S.Zero:\n        r = -c/a\n        if not dom.is_Numerical:\n            r = _simplify(r)\n\n        R = _sqrt(r)\n        r0 = -R\n        r1 = R\n    else:\n        d = b**2 - 4*a*c\n        A = 2*a\n        B = -b/A\n\n        if not dom.is_Numerical:\n            d = _simplify(d)\n            B = _simplify(B)\n\n        D = factor_terms(_sqrt(d)/A)\n        r0 = B - D\n        r1 = B + D\n        if a.is_negative:\n            r0, r1 = r1, r0\n        elif not dom.is_Numerical:\n            r0, r1 = [expand_2arg(i) for i in (r0, r1)]\n\n    return [r0, r1]\n\n\ndef roots_cubic(f, trig=False):\n    \"\"\"Returns a list of roots of a cubic polynomial.\n\n    References\n    ==========\n    [1] https://en.wikipedia.org/wiki/Cubic_function, General formula for roots,\n    (accessed November 17, 2014).\n    \"\"\"\n    if trig:\n        a, b, c, d = f.all_coeffs()\n        p = (3*a*c - b**2)/3/a**2\n        q = (2*b**3 - 9*a*b*c + 27*a**2*d)/(27*a**3)\n        D = 18*a*b*c*d - 4*b**3*d + b**2*c**2 - 4*a*c**3 - 27*a**2*d**2\n        if (D > 0) == True:\n            rv = []\n            for k in range(3):\n                rv.append(2*sqrt(-p/3)*cos(acos(q/p*sqrt(-3/p)*Rational(3, 2))/3 - k*pi*Rational(2, 3)))\n            return [i - b/3/a for i in rv]\n\n    _, a, b, c = f.monic().all_coeffs()\n\n    if c is S.Zero:\n        x1, x2 = roots([1, a, b], multiple=True)\n        return [x1, S.Zero, x2]\n\n    p = b - a**2/3\n    q = c - a*b/3 + 2*a**3/27\n\n    pon3 = p/3\n    aon3 = a/3\n\n    u1 = None\n    if p is S.Zero:\n        if q is S.Zero:\n            return [-aon3]*3\n        if q.is_real:\n            if q.is_positive:\n                u1 = -root(q, 3)\n            elif q.is_negative:\n                u1 = root(-q, 3)\n    elif q is S.Zero:\n        y1, y2 = roots([1, 0, p], multiple=True)\n        return [tmp - aon3 for tmp in [y1, S.Zero, y2]]\n    elif q.is_real and q.is_negative:\n        u1 = -root(-q/2 + sqrt(q**2/4 + pon3**3), 3)\n\n    coeff = I*sqrt(3)/2\n    if u1 is None:\n        u1 = S.One\n        u2 = Rational(-1, 2) + coeff\n        u3 = Rational(-1, 2) - coeff\n        a, b, c, d = S(1), a, b, c\n        D0 = b**2 - 3*a*c\n        D1 = 2*b**3 - 9*a*b*c + 27*a**2*d\n        C = root((D1 + sqrt(D1**2 - 4*D0**3))/2, 3)\n        return [-(b + uk*C + D0/C/uk)/3/a for uk in [u1, u2, u3]]\n\n    u2 = u1*(Rational(-1, 2) + coeff)\n    u3 = u1*(Rational(-1, 2) - coeff)\n\n    if p is S.Zero:\n        return [u1 - aon3, u2 - aon3, u3 - aon3]\n\n    soln = [\n        -u1 + pon3/u1 - aon3,\n        -u2 + pon3/u2 - aon3,\n        -u3 + pon3/u3 - aon3\n    ]\n\n    return soln\n\ndef _roots_quartic_euler(p, q, r, a):\n    \"\"\"\n    Descartes-Euler solution of the quartic equation\n\n    Parameters\n    ==========\n\n    p, q, r: coefficients of ``x**4 + p*x**2 + q*x + r``\n    a: shift of the roots\n\n    Notes\n    =====\n\n    This is a helper function for ``roots_quartic``.\n\n    Look for solutions of the form ::\n\n      ``x1 = sqrt(R) - sqrt(A + B*sqrt(R))``\n      ``x2 = -sqrt(R) - sqrt(A - B*sqrt(R))``\n      ``x3 = -sqrt(R) + sqrt(A - B*sqrt(R))``\n      ``x4 = sqrt(R) + sqrt(A + B*sqrt(R))``\n\n    To satisfy the quartic equation one must have\n    ``p = -2*(R + A); q = -4*B*R; r = (R - A)**2 - B**2*R``\n    so that ``R`` must satisfy the Descartes-Euler resolvent equation\n    ``64*R**3 + 32*p*R**2 + (4*p**2 - 16*r)*R - q**2 = 0``\n\n    If the resolvent does not have a rational solution, return None;\n    in that case it is likely that the Ferrari method gives a simpler\n    solution.\n\n    Examples\n    ========\n\n    >>> from sympy import S\n    >>> from sympy.polys.polyroots import _roots_quartic_euler\n    >>> p, q, r = -S(64)/5, -S(512)/125, -S(1024)/3125\n    >>> _roots_quartic_euler(p, q, r, S(0))[0]\n    -sqrt(32*sqrt(5)/125 + 16/5) + 4*sqrt(5)/5\n    \"\"\"\n    # solve the resolvent equation\n    x = Dummy('x')\n    eq = 64*x**3 + 32*p*x**2 + (4*p**2 - 16*r)*x - q**2\n    xsols = list(roots(Poly(eq, x), cubics=False).keys())\n    xsols = [sol for sol in xsols if sol.is_rational and sol.is_nonzero]\n    if not xsols:\n        return None\n    R = max(xsols)\n    c1 = sqrt(R)\n    B = -q*c1/(4*R)\n    A = -R - p/2\n    c2 = sqrt(A + B)\n    c3 = sqrt(A - B)\n    return [c1 - c2 - a, -c1 - c3 - a, -c1 + c3 - a, c1 + c2 - a]\n\n\ndef roots_quartic(f):\n    r\"\"\"\n    Returns a list of roots of a quartic polynomial.\n\n    There are many references for solving quartic expressions available [1-5].\n    This reviewer has found that many of them require one to select from among\n    2 or more possible sets of solutions and that some solutions work when one\n    is searching for real roots but don't work when searching for complex roots\n    (though this is not always stated clearly). The following routine has been\n    tested and found to be correct for 0, 2 or 4 complex roots.\n\n    The quasisymmetric case solution [6] looks for quartics that have the form\n    `x**4 + A*x**3 + B*x**2 + C*x + D = 0` where `(C/A)**2 = D`.\n\n    Although no general solution that is always applicable for all\n    coefficients is known to this reviewer, certain conditions are tested\n    to determine the simplest 4 expressions that can be returned:\n\n      1) `f = c + a*(a**2/8 - b/2) == 0`\n      2) `g = d - a*(a*(3*a**2/256 - b/16) + c/4) = 0`\n      3) if `f != 0` and `g != 0` and `p = -d + a*c/4 - b**2/12` then\n        a) `p == 0`\n        b) `p != 0`\n\n    Examples\n    ========\n\n        >>> from sympy import Poly, symbols, I\n        >>> from sympy.polys.polyroots import roots_quartic\n\n        >>> r = roots_quartic(Poly('x**4-6*x**3+17*x**2-26*x+20'))\n\n        >>> # 4 complex roots: 1+-I*sqrt(3), 2+-I\n        >>> sorted(str(tmp.evalf(n=2)) for tmp in r)\n        ['1.0 + 1.7*I', '1.0 - 1.7*I', '2.0 + 1.0*I', '2.0 - 1.0*I']\n\n    References\n    ==========\n\n    1. http://mathforum.org/dr.math/faq/faq.cubic.equations.html\n    2. https://en.wikipedia.org/wiki/Quartic_function#Summary_of_Ferrari.27s_method\n    3. http://planetmath.org/encyclopedia/GaloisTheoreticDerivationOfTheQuarticFormula.html\n    4. http://staff.bath.ac.uk/masjhd/JHD-CA.pdf\n    5. http://www.albmath.org/files/Math_5713.pdf\n    6. http://www.statemaster.com/encyclopedia/Quartic-equation\n    7. eqworld.ipmnet.ru/en/solutions/ae/ae0108.pdf\n    \"\"\"\n    _, a, b, c, d = f.monic().all_coeffs()\n\n    if not d:\n        return [S.Zero] + roots([1, a, b, c], multiple=True)\n    elif (c/a)**2 == d:\n        x, m = f.gen, c/a\n\n        g = Poly(x**2 + a*x + b - 2*m, x)\n\n        z1, z2 = roots_quadratic(g)\n\n        h1 = Poly(x**2 - z1*x + m, x)\n        h2 = Poly(x**2 - z2*x + m, x)\n\n        r1 = roots_quadratic(h1)\n        r2 = roots_quadratic(h2)\n\n        return r1 + r2\n    else:\n        a2 = a**2\n        e = b - 3*a2/8\n        f = _mexpand(c + a*(a2/8 - b/2))\n        g = _mexpand(d - a*(a*(3*a2/256 - b/16) + c/4))\n        aon4 = a/4\n\n        if f is S.Zero:\n            y1, y2 = [sqrt(tmp) for tmp in\n                      roots([1, e, g], multiple=True)]\n            return [tmp - aon4 for tmp in [-y1, -y2, y1, y2]]\n        if g is S.Zero:\n            y = [S.Zero] + roots([1, 0, e, f], multiple=True)\n            return [tmp - aon4 for tmp in y]\n        else:\n            # Descartes-Euler method, see [7]\n            sols = _roots_quartic_euler(e, f, g, aon4)\n            if sols:\n                return sols\n            # Ferrari method, see [1, 2]\n            a2 = a**2\n            e = b - 3*a2/8\n            f = c + a*(a2/8 - b/2)\n            g = d - a*(a*(3*a2/256 - b/16) + c/4)\n            p = -e**2/12 - g\n            q = -e**3/108 + e*g/3 - f**2/8\n            TH = Rational(1, 3)\n\n            def _ans(y):\n                w = sqrt(e + 2*y)\n                arg1 = 3*e + 2*y\n                arg2 = 2*f/w\n                ans = []\n                for s in [-1, 1]:\n                    root = sqrt(-(arg1 + s*arg2))\n                    for t in [-1, 1]:\n                        ans.append((s*w - t*root)/2 - aon4)\n                return ans\n\n            # p == 0 case\n            y1 = e*Rational(-5, 6) - q**TH\n            if p.is_zero:\n                return _ans(y1)\n\n            # if p != 0 then u below is not 0\n            root = sqrt(q**2/4 + p**3/27)\n            r = -q/2 + root  # or -q/2 - root\n            u = r**TH  # primary root of solve(x**3 - r, x)\n            y2 = e*Rational(-5, 6) + u - p/u/3\n            if fuzzy_not(p.is_zero):\n                return _ans(y2)\n\n            # sort it out once they know the values of the coefficients\n            return [Piecewise((a1, Eq(p, 0)), (a2, True))\n                for a1, a2 in zip(_ans(y1), _ans(y2))]\n\n\ndef roots_binomial(f):\n    \"\"\"Returns a list of roots of a binomial polynomial. If the domain is ZZ\n    then the roots will be sorted with negatives coming before positives.\n    The ordering will be the same for any numerical coefficients as long as\n    the assumptions tested are correct, otherwise the ordering will not be\n    sorted (but will be canonical).\n    \"\"\"\n    n = f.degree()\n\n    a, b = f.nth(n), f.nth(0)\n    base = -cancel(b/a)\n    alpha = root(base, n)\n\n    if alpha.is_number:\n        alpha = alpha.expand(complex=True)\n\n    # define some parameters that will allow us to order the roots.\n    # If the domain is ZZ this is guaranteed to return roots sorted\n    # with reals before non-real roots and non-real sorted according\n    # to real part and imaginary part, e.g. -1, 1, -1 + I, 2 - I\n    neg = base.is_negative\n    even = n % 2 == 0\n    if neg:\n        if even == True and (base + 1).is_positive:\n            big = True\n        else:\n            big = False\n\n    # get the indices in the right order so the computed\n    # roots will be sorted when the domain is ZZ\n    ks = []\n    imax = n//2\n    if even:\n        ks.append(imax)\n        imax -= 1\n    if not neg:\n        ks.append(0)\n    for i in range(imax, 0, -1):\n        if neg:\n            ks.extend([i, -i])\n        else:\n            ks.extend([-i, i])\n    if neg:\n        ks.append(0)\n        if big:\n            for i in range(0, len(ks), 2):\n                pair = ks[i: i + 2]\n                pair = list(reversed(pair))\n\n    # compute the roots\n    roots, d = [], 2*I*pi/n\n    for k in ks:\n        zeta = exp(k*d).expand(complex=True)\n        roots.append((alpha*zeta).expand(power_base=False))\n\n    return roots\n\n\ndef _inv_totient_estimate(m):\n    \"\"\"\n    Find ``(L, U)`` such that ``L <= phi^-1(m) <= U``.\n\n    Examples\n    ========\n\n    >>> from sympy.polys.polyroots import _inv_totient_estimate\n\n    >>> _inv_totient_estimate(192)\n    (192, 840)\n    >>> _inv_totient_estimate(400)\n    (400, 1750)\n\n    \"\"\"\n    primes = [ d + 1 for d in divisors(m) if isprime(d + 1) ]\n\n    a, b = 1, 1\n\n    for p in primes:\n        a *= p\n        b *= p - 1\n\n    L = m\n    U = int(math.ceil(m*(float(a)/b)))\n\n    P = p = 2\n    primes = []\n\n    while P <= U:\n        p = nextprime(p)\n        primes.append(p)\n        P *= p\n\n    P //= p\n    b = 1\n\n    for p in primes[:-1]:\n        b *= p - 1\n\n    U = int(math.ceil(m*(float(P)/b)))\n\n    return L, U\n\n\ndef roots_cyclotomic(f, factor=False):\n    \"\"\"Compute roots of cyclotomic polynomials. \"\"\"\n    L, U = _inv_totient_estimate(f.degree())\n\n    for n in range(L, U + 1):\n        g = cyclotomic_poly(n, f.gen, polys=True)\n\n        if f == g:\n            break\n    else:  # pragma: no cover\n        raise RuntimeError(\"failed to find index of a cyclotomic polynomial\")\n\n    roots = []\n\n    if not factor:\n        # get the indices in the right order so the computed\n        # roots will be sorted\n        h = n//2\n        ks = [i for i in range(1, n + 1) if igcd(i, n) == 1]\n        ks.sort(key=lambda x: (x, -1) if x <= h else (abs(x - n), 1))\n        d = 2*I*pi/n\n        for k in reversed(ks):\n            roots.append(exp(k*d).expand(complex=True))\n    else:\n        g = Poly(f, extension=root(-1, n))\n\n        for h, _ in ordered(g.factor_list()[1]):\n            roots.append(-h.TC())\n\n    return roots\n\n\ndef roots_quintic(f):\n    \"\"\"\n    Calculate exact roots of a solvable quintic\n    \"\"\"\n    result = []\n    coeff_5, coeff_4, p, q, r, s = f.all_coeffs()\n\n    # Eqn must be of the form x^5 + px^3 + qx^2 + rx + s\n    if coeff_4:\n        return result\n\n    if coeff_5 != 1:\n        l = [p/coeff_5, q/coeff_5, r/coeff_5, s/coeff_5]\n        if not all(coeff.is_Rational for coeff in l):\n            return result\n        f = Poly(f/coeff_5)\n    quintic = PolyQuintic(f)\n\n    # Eqn standardized. Algo for solving starts here\n    if not f.is_irreducible:\n        return result\n\n    f20 = quintic.f20\n    # Check if f20 has linear factors over domain Z\n    if f20.is_irreducible:\n        return result\n\n    # Now, we know that f is solvable\n    for _factor in f20.factor_list()[1]:\n        if _factor[0].is_linear:\n            theta = _factor[0].root(0)\n            break\n    d = discriminant(f)\n    delta = sqrt(d)\n    # zeta = a fifth root of unity\n    zeta1, zeta2, zeta3, zeta4 = quintic.zeta\n    T = quintic.T(theta, d)\n    tol = S(1e-10)\n    alpha = T[1] + T[2]*delta\n    alpha_bar = T[1] - T[2]*delta\n    beta = T[3] + T[4]*delta\n    beta_bar = T[3] - T[4]*delta\n\n    disc = alpha**2 - 4*beta\n    disc_bar = alpha_bar**2 - 4*beta_bar\n\n    l0 = quintic.l0(theta)\n\n    l1 = _quintic_simplify((-alpha + sqrt(disc)) / S(2))\n    l4 = _quintic_simplify((-alpha - sqrt(disc)) / S(2))\n\n    l2 = _quintic_simplify((-alpha_bar + sqrt(disc_bar)) / S(2))\n    l3 = _quintic_simplify((-alpha_bar - sqrt(disc_bar)) / S(2))\n\n    order = quintic.order(theta, d)\n    test = (order*delta.n()) - ( (l1.n() - l4.n())*(l2.n() - l3.n()) )\n    # Comparing floats\n    if not comp(test, 0, tol):\n        l2, l3 = l3, l2\n\n    # Now we have correct order of l's\n    R1 = l0 + l1*zeta1 + l2*zeta2 + l3*zeta3 + l4*zeta4\n    R2 = l0 + l3*zeta1 + l1*zeta2 + l4*zeta3 + l2*zeta4\n    R3 = l0 + l2*zeta1 + l4*zeta2 + l1*zeta3 + l3*zeta4\n    R4 = l0 + l4*zeta1 + l3*zeta2 + l2*zeta3 + l1*zeta4\n\n    Res = [None, [None]*5, [None]*5, [None]*5, [None]*5]\n    Res_n = [None, [None]*5, [None]*5, [None]*5, [None]*5]\n    sol = Symbol('sol')\n\n    # Simplifying improves performance a lot for exact expressions\n    R1 = _quintic_simplify(R1)\n    R2 = _quintic_simplify(R2)\n    R3 = _quintic_simplify(R3)\n    R4 = _quintic_simplify(R4)\n\n    # Solve imported here. Causing problems if imported as 'solve'\n    # and hence the changed name\n    from sympy.solvers.solvers import solve as _solve\n    a, b = symbols('a b', cls=Dummy)\n    _sol = _solve( sol**5 - a - I*b, sol)\n    for i in range(5):\n        _sol[i] = factor(_sol[i])\n    R1 = R1.as_real_imag()\n    R2 = R2.as_real_imag()\n    R3 = R3.as_real_imag()\n    R4 = R4.as_real_imag()\n\n    for i, currentroot in enumerate(_sol):\n        Res[1][i] = _quintic_simplify(currentroot.subs({ a: R1[0], b: R1[1] }))\n        Res[2][i] = _quintic_simplify(currentroot.subs({ a: R2[0], b: R2[1] }))\n        Res[3][i] = _quintic_simplify(currentroot.subs({ a: R3[0], b: R3[1] }))\n        Res[4][i] = _quintic_simplify(currentroot.subs({ a: R4[0], b: R4[1] }))\n\n    for i in range(1, 5):\n        for j in range(5):\n            Res_n[i][j] = Res[i][j].n()\n            Res[i][j] = _quintic_simplify(Res[i][j])\n    r1 = Res[1][0]\n    r1_n = Res_n[1][0]\n\n    for i in range(5):\n        if comp(im(r1_n*Res_n[4][i]), 0, tol):\n            r4 = Res[4][i]\n            break\n\n    # Now we have various Res values. Each will be a list of five\n    # values. We have to pick one r value from those five for each Res\n    u, v = quintic.uv(theta, d)\n    testplus = (u + v*delta*sqrt(5)).n()\n    testminus = (u - v*delta*sqrt(5)).n()\n\n    # Evaluated numbers suffixed with _n\n    # We will use evaluated numbers for calculation. Much faster.\n    r4_n = r4.n()\n    r2 = r3 = None\n\n    for i in range(5):\n        r2temp_n = Res_n[2][i]\n        for j in range(5):\n            # Again storing away the exact number and using\n            # evaluated numbers in computations\n            r3temp_n = Res_n[3][j]\n            if (comp((r1_n*r2temp_n**2 + r4_n*r3temp_n**2 - testplus).n(), 0, tol) and\n                comp((r3temp_n*r1_n**2 + r2temp_n*r4_n**2 - testminus).n(), 0, tol)):\n                r2 = Res[2][i]\n                r3 = Res[3][j]\n                break\n        if r2:\n            break\n\n    # Now, we have r's so we can get roots\n    x1 = (r1 + r2 + r3 + r4)/5\n    x2 = (r1*zeta4 + r2*zeta3 + r3*zeta2 + r4*zeta1)/5\n    x3 = (r1*zeta3 + r2*zeta1 + r3*zeta4 + r4*zeta2)/5\n    x4 = (r1*zeta2 + r2*zeta4 + r3*zeta1 + r4*zeta3)/5\n    x5 = (r1*zeta1 + r2*zeta2 + r3*zeta3 + r4*zeta4)/5\n    result = [x1, x2, x3, x4, x5]\n\n    # Now check if solutions are distinct\n\n    saw = set()\n    for r in result:\n        r = r.n(2)\n        if r in saw:\n            # Roots were identical. Abort, return []\n            # and fall back to usual solve\n            return []\n        saw.add(r)\n    return result\n\n\ndef _quintic_simplify(expr):\n    expr = powsimp(expr)\n    expr = cancel(expr)\n    return together(expr)\n\n\ndef _integer_basis(poly):\n    \"\"\"Compute coefficient basis for a polynomial over integers.\n\n    Returns the integer ``div`` such that substituting ``x = div*y``\n    ``p(x) = m*q(y)`` where the coefficients of ``q`` are smaller\n    than those of ``p``.\n\n    For example ``x**5 + 512*x + 1024 = 0``\n    with ``div = 4`` becomes ``y**5 + 2*y + 1 = 0``\n\n    Returns the integer ``div`` or ``None`` if there is no possible scaling.\n\n    Examples\n    ========\n\n    >>> from sympy.polys import Poly\n    >>> from sympy.abc import x\n    >>> from sympy.polys.polyroots import _integer_basis\n    >>> p = Poly(x**5 + 512*x + 1024, x, domain='ZZ')\n    >>> _integer_basis(p)\n    4\n    \"\"\"\n    monoms, coeffs = list(zip(*poly.terms()))\n\n    monoms, = list(zip(*monoms))\n    coeffs = list(map(abs, coeffs))\n\n    if coeffs[0] < coeffs[-1]:\n        coeffs = list(reversed(coeffs))\n        n = monoms[0]\n        monoms = [n - i for i in reversed(monoms)]\n    else:\n        return None\n\n    monoms = monoms[:-1]\n    coeffs = coeffs[:-1]\n\n    divs = reversed(divisors(gcd_list(coeffs))[1:])\n\n    try:\n        div = next(divs)\n    except StopIteration:\n        return None\n\n    while True:\n        for monom, coeff in zip(monoms, coeffs):\n            if coeff % div**monom != 0:\n                try:\n                    div = next(divs)\n                except StopIteration:\n                    return None\n                else:\n                    break\n        else:\n            return div\n\n\ndef preprocess_roots(poly):\n    \"\"\"Try to get rid of symbolic coefficients from ``poly``. \"\"\"\n    coeff = S.One\n\n    poly_func = poly.func\n    try:\n        _, poly = poly.clear_denoms(convert=True)\n    except DomainError:\n        return coeff, poly\n\n    poly = poly.primitive()[1]\n    poly = poly.retract()\n\n    # TODO: This is fragile. Figure out how to make this independent of construct_domain().\n    if poly.get_domain().is_Poly and all(c.is_term for c in poly.rep.coeffs()):\n        poly = poly.inject()\n\n        strips = list(zip(*poly.monoms()))\n        gens = list(poly.gens[1:])\n\n        base, strips = strips[0], strips[1:]\n\n        for gen, strip in zip(list(gens), strips):\n            reverse = False\n\n            if strip[0] < strip[-1]:\n                strip = reversed(strip)\n                reverse = True\n\n            ratio = None\n\n            for a, b in zip(base, strip):\n                if not a and not b:\n                    continue\n                elif not a or not b:\n                    break\n                elif b % a != 0:\n                    break\n                else:\n                    _ratio = b // a\n\n                    if ratio is None:\n                        ratio = _ratio\n                    elif ratio != _ratio:\n                        break\n            else:\n                if reverse:\n                    ratio = -ratio\n\n                poly = poly.eval(gen, 1)\n                coeff *= gen**(-ratio)\n                gens.remove(gen)\n\n        if gens:\n            poly = poly.eject(*gens)\n\n    if poly.is_univariate and poly.get_domain().is_ZZ:\n        basis = _integer_basis(poly)\n\n        if basis is not None:\n            n = poly.degree()\n\n            def func(k, coeff):\n                return coeff//basis**(n - k[0])\n\n            poly = poly.termwise(func)\n            coeff *= basis\n\n    if not isinstance(poly, poly_func):\n        poly = poly_func(poly)\n    return coeff, poly\n\n\n@public\ndef roots(f, *gens, **flags):\n    \"\"\"\n    Computes symbolic roots of a univariate polynomial.\n\n    Given a univariate polynomial f with symbolic coefficients (or\n    a list of the polynomial's coefficients), returns a dictionary\n    with its roots and their multiplicities.\n\n    Only roots expressible via radicals will be returned.  To get\n    a complete set of roots use RootOf class or numerical methods\n    instead. By default cubic and quartic formulas are used in\n    the algorithm. To disable them because of unreadable output\n    set ``cubics=False`` or ``quartics=False`` respectively. If cubic\n    roots are real but are expressed in terms of complex numbers\n    (casus irreducibilis [1]) the ``trig`` flag can be set to True to\n    have the solutions returned in terms of cosine and inverse cosine\n    functions.\n\n    To get roots from a specific domain set the ``filter`` flag with\n    one of the following specifiers: Z, Q, R, I, C. By default all\n    roots are returned (this is equivalent to setting ``filter='C'``).\n\n    By default a dictionary is returned giving a compact result in\n    case of multiple roots.  However to get a list containing all\n    those roots set the ``multiple`` flag to True; the list will\n    have identical roots appearing next to each other in the result.\n    (For a given Poly, the all_roots method will give the roots in\n    sorted numerical order.)\n\n    Examples\n    ========\n\n    >>> from sympy import Poly, roots\n    >>> from sympy.abc import x, y\n\n    >>> roots(x**2 - 1, x)\n    {-1: 1, 1: 1}\n\n    >>> p = Poly(x**2-1, x)\n    >>> roots(p)\n    {-1: 1, 1: 1}\n\n    >>> p = Poly(x**2-y, x, y)\n\n    >>> roots(Poly(p, x))\n    {-sqrt(y): 1, sqrt(y): 1}\n\n    >>> roots(x**2 - y, x)\n    {-sqrt(y): 1, sqrt(y): 1}\n\n    >>> roots([1, 0, -1])\n    {-1: 1, 1: 1}\n\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Cubic_function#Trigonometric_.28and_hyperbolic.29_method\n\n    \"\"\"\n    from sympy.polys.polytools import to_rational_coeffs\n    flags = dict(flags)\n\n    auto = flags.pop('auto', True)\n    cubics = flags.pop('cubics', True)\n    trig = flags.pop('trig', False)\n    quartics = flags.pop('quartics', True)\n    quintics = flags.pop('quintics', False)\n    multiple = flags.pop('multiple', False)\n    filter = flags.pop('filter', None)\n    predicate = flags.pop('predicate', None)\n\n    if isinstance(f, list):\n        if gens:\n            raise ValueError('redundant generators given')\n\n        x = Dummy('x')\n\n        poly, i = {}, len(f) - 1\n\n        for coeff in f:\n            poly[i], i = sympify(coeff), i - 1\n\n        f = Poly(poly, x, field=True)\n    else:\n        try:\n            f = Poly(f, *gens, **flags)\n            if f.length == 2 and f.degree() != 1:\n                # check for foo**n factors in the constant\n                n = f.degree()\n                npow_bases = []\n                others = []\n                expr = f.as_expr()\n                con = expr.as_independent(*gens)[0]\n                for p in Mul.make_args(con):\n                    if p.is_Pow and not p.exp % n:\n                        npow_bases.append(p.base**(p.exp/n))\n                    else:\n                        others.append(p)\n                    if npow_bases:\n                        b = Mul(*npow_bases)\n                        B = Dummy()\n                        d = roots(Poly(expr - con + B**n*Mul(*others), *gens,\n                            **flags), *gens, **flags)\n                        rv = {}\n                        for k, v in d.items():\n                            rv[k.subs(B, b)] = v\n                        return rv\n\n        except GeneratorsNeeded:\n            if multiple:\n                return []\n            else:\n                return {}\n\n        if f.is_multivariate:\n            raise PolynomialError('multivariate polynomials are not supported')\n\n    def _update_dict(result, currentroot, k):\n        if currentroot in result:\n            result[currentroot] += k\n        else:\n            result[currentroot] = k\n\n    def _try_decompose(f):\n        \"\"\"Find roots using functional decomposition. \"\"\"\n        factors, roots = f.decompose(), []\n\n        for currentroot in _try_heuristics(factors[0]):\n            roots.append(currentroot)\n\n        for currentfactor in factors[1:]:\n            previous, roots = list(roots), []\n\n            for currentroot in previous:\n                g = currentfactor - Poly(currentroot, f.gen)\n\n                for currentroot in _try_heuristics(g):\n                    roots.append(currentroot)\n\n        return roots\n\n    def _try_heuristics(f):\n        \"\"\"Find roots using formulas and some tricks. \"\"\"\n        if f.is_ground:\n            return []\n        if f.is_monomial:\n            return [S.Zero]*f.degree()\n\n        if f.length() == 2:\n            if f.degree() == 1:\n                return list(map(cancel, roots_linear(f)))\n            else:\n                return roots_binomial(f)\n\n        result = []\n\n        for i in [-1, 1]:\n            if not f.eval(i):\n                f = f.quo(Poly(f.gen - i, f.gen))\n                result.append(i)\n                break\n\n        n = f.degree()\n\n        if n == 1:\n            result += list(map(cancel, roots_linear(f)))\n        elif n == 2:\n            result += list(map(cancel, roots_quadratic(f)))\n        elif f.is_cyclotomic:\n            result += roots_cyclotomic(f)\n        elif n == 3 and cubics:\n            result += roots_cubic(f, trig=trig)\n        elif n == 4 and quartics:\n            result += roots_quartic(f)\n        elif n == 5 and quintics:\n            result += roots_quintic(f)\n\n        return result\n\n    (k,), f = f.terms_gcd()\n\n    if not k:\n        zeros = {}\n    else:\n        zeros = {S.Zero: k}\n\n    coeff, f = preprocess_roots(f)\n\n    if auto and f.get_domain().is_Ring:\n        f = f.to_field()\n\n    rescale_x = None\n    translate_x = None\n\n    result = {}\n\n    if not f.is_ground:\n        dom = f.get_domain()\n        if not dom.is_Exact and dom.is_Numerical:\n            for r in f.nroots():\n                _update_dict(result, r, 1)\n        elif f.degree() == 1:\n            result[roots_linear(f)[0]] = 1\n        elif f.length() == 2:\n            roots_fun = roots_quadratic if f.degree() == 2 else roots_binomial\n            for r in roots_fun(f):\n                _update_dict(result, r, 1)\n        else:\n            _, factors = Poly(f.as_expr()).factor_list()\n            if len(factors) == 1 and f.degree() == 2:\n                for r in roots_quadratic(f):\n                    _update_dict(result, r, 1)\n            else:\n                if len(factors) == 1 and factors[0][1] == 1:\n                    if f.get_domain().is_EX:\n                        res = to_rational_coeffs(f)\n                        if res:\n                            if res[0] is None:\n                                translate_x, f = res[2:]\n                            else:\n                                rescale_x, f = res[1], res[-1]\n                            result = roots(f)\n                            if not result:\n                                for currentroot in _try_decompose(f):\n                                    _update_dict(result, currentroot, 1)\n                        else:\n                            for r in _try_heuristics(f):\n                                _update_dict(result, r, 1)\n                    else:\n                        for currentroot in _try_decompose(f):\n                            _update_dict(result, currentroot, 1)\n                else:\n                    for currentfactor, k in factors:\n                        for r in _try_heuristics(Poly(currentfactor, f.gen, field=True)):\n                            _update_dict(result, r, k)\n\n    if coeff is not S.One:\n        _result, result, = result, {}\n\n        for currentroot, k in _result.items():\n            result[coeff*currentroot] = k\n\n    if filter not in [None, 'C']:\n        handlers = {\n            'Z': lambda r: r.is_Integer,\n            'Q': lambda r: r.is_Rational,\n            'R': lambda r: all(a.is_real for a in r.as_numer_denom()),\n            'I': lambda r: r.is_imaginary,\n        }\n\n        try:\n            query = handlers[filter]\n        except KeyError:\n            raise ValueError(\"Invalid filter: %s\" % filter)\n\n        for zero in dict(result).keys():\n            if not query(zero):\n                del result[zero]\n\n    if predicate is not None:\n        for zero in dict(result).keys():\n            if not predicate(zero):\n                del result[zero]\n    if rescale_x:\n        result1 = {}\n        for k, v in result.items():\n            result1[k*rescale_x] = v\n        result = result1\n    if translate_x:\n        result1 = {}\n        for k, v in result.items():\n            result1[k + translate_x] = v\n        result = result1\n\n    # adding zero roots after non-trivial roots have been translated\n    result.update(zeros)\n\n    if not multiple:\n        return result\n    else:\n        zeros = []\n\n        for zero in ordered(result):\n            zeros.extend([zero]*result[zero])\n\n        return zeros\n\n\ndef root_factors(f, *gens, **args):\n    \"\"\"\n    Returns all factors of a univariate polynomial.\n\n    Examples\n    ========\n\n    >>> from sympy.abc import x, y\n    >>> from sympy.polys.polyroots import root_factors\n\n    >>> root_factors(x**2 - y, x)\n    [x - sqrt(y), x + sqrt(y)]\n\n    \"\"\"\n    args = dict(args)\n    filter = args.pop('filter', None)\n\n    F = Poly(f, *gens, **args)\n\n    if not F.is_Poly:\n        return [f]\n\n    if F.is_multivariate:\n        raise ValueError('multivariate polynomials are not supported')\n\n    x = F.gens[0]\n\n    zeros = roots(F, filter=filter)\n\n    if not zeros:\n        factors = [F]\n    else:\n        factors, N = [], 0\n\n        for r, n in ordered(zeros.items()):\n            factors, N = factors + [Poly(x - r, x)]*n, N + n\n\n        if N < F.degree():\n            G = reduce(lambda p, q: p*q, factors)\n            factors.append(F.quo(G))\n\n    if not isinstance(f, Poly):\n        factors = [ f.as_expr() for f in factors ]\n\n    return factors"}
{"model_name_or_path": "agentless", "instance_id": "sympy__sympy-20916", "model_patch": "", "raw_model_patch": "", "original_file_content": "\"\"\"Symbolic primitives + unicode/ASCII abstraction for pretty.py\"\"\"\n\nimport sys\nimport warnings\nfrom string import ascii_lowercase, ascii_uppercase\nimport unicodedata\n\nunicode_warnings = ''\n\ndef U(name):\n    \"\"\"\n    Get a unicode character by name or, None if not found.\n\n    This exists because older versions of python use older unicode databases.\n    \"\"\"\n    try:\n        return unicodedata.lookup(name)\n    except KeyError:\n        global unicode_warnings\n        unicode_warnings += 'No \\'%s\\' in unicodedata\\n' % name\n        return None\n\nfrom sympy.printing.conventions import split_super_sub\nfrom sympy.core.alphabets import greeks\nfrom sympy.utilities.exceptions import SymPyDeprecationWarning\n\n# prefix conventions when constructing tables\n# L   - LATIN     i\n# G   - GREEK     beta\n# D   - DIGIT     0\n# S   - SYMBOL    +\n\n\n__all__ = ['greek_unicode', 'sub', 'sup', 'xsym', 'vobj', 'hobj', 'pretty_symbol',\n           'annotated']\n\n\n_use_unicode = False\n\n\ndef pretty_use_unicode(flag=None):\n    \"\"\"Set whether pretty-printer should use unicode by default\"\"\"\n    global _use_unicode\n    global unicode_warnings\n    if flag is None:\n        return _use_unicode\n\n    if flag and unicode_warnings:\n        # print warnings (if any) on first unicode usage\n        warnings.warn(unicode_warnings)\n        unicode_warnings = ''\n\n    use_unicode_prev = _use_unicode\n    _use_unicode = flag\n    return use_unicode_prev\n\n\ndef pretty_try_use_unicode():\n    \"\"\"See if unicode output is available and leverage it if possible\"\"\"\n\n    encoding = getattr(sys.stdout, 'encoding', None)\n\n    # this happens when e.g. stdout is redirected through a pipe, or is\n    # e.g. a cStringIO.StringO\n    if encoding is None:\n        return  # sys.stdout has no encoding\n\n    symbols = []\n\n    # see if we can represent greek alphabet\n    symbols += greek_unicode.values()\n\n    # and atoms\n    symbols += atoms_table.values()\n\n    for s in symbols:\n        if s is None:\n            return  # common symbols not present!\n\n        try:\n            s.encode(encoding)\n        except UnicodeEncodeError:\n            return\n\n    # all the characters were present and encodable\n    pretty_use_unicode(True)\n\n\ndef xstr(*args):\n    SymPyDeprecationWarning(\n        feature=\"``xstr`` function\",\n        useinstead=\"``str``\",\n        deprecated_since_version=\"1.7\").warn()\n    return str(*args)\n\n# GREEK\ng = lambda l: U('GREEK SMALL LETTER %s' % l.upper())\nG = lambda l: U('GREEK CAPITAL LETTER %s' % l.upper())\n\ngreek_letters = list(greeks) # make a copy\n# deal with Unicode's funny spelling of lambda\ngreek_letters[greek_letters.index('lambda')] = 'lamda'\n\n# {}  greek letter -> (g,G)\ngreek_unicode = {L: g(L) for L in greek_letters}\ngreek_unicode.update((L[0].upper() + L[1:], G(L)) for L in greek_letters)\n\n# aliases\ngreek_unicode['lambda'] = greek_unicode['lamda']\ngreek_unicode['Lambda'] = greek_unicode['Lamda']\ngreek_unicode['varsigma'] = '\\N{GREEK SMALL LETTER FINAL SIGMA}'\n\n# BOLD\nb = lambda l: U('MATHEMATICAL BOLD SMALL %s' % l.upper())\nB = lambda l: U('MATHEMATICAL BOLD CAPITAL %s' % l.upper())\n\nbold_unicode = {l: b(l) for l in ascii_lowercase}\nbold_unicode.update((L, B(L)) for L in ascii_uppercase)\n\n# GREEK BOLD\ngb = lambda l: U('MATHEMATICAL BOLD SMALL %s' % l.upper())\nGB = lambda l: U('MATHEMATICAL BOLD CAPITAL  %s' % l.upper())\n\ngreek_bold_letters = list(greeks) # make a copy, not strictly required here\n# deal with Unicode's funny spelling of lambda\ngreek_bold_letters[greek_bold_letters.index('lambda')] = 'lamda'\n\n# {}  greek letter -> (g,G)\ngreek_bold_unicode = {L: g(L) for L in greek_bold_letters}\ngreek_bold_unicode.update((L[0].upper() + L[1:], G(L)) for L in greek_bold_letters)\ngreek_bold_unicode['lambda'] = greek_unicode['lamda']\ngreek_bold_unicode['Lambda'] = greek_unicode['Lamda']\ngreek_bold_unicode['varsigma'] = '\\N{MATHEMATICAL BOLD SMALL FINAL SIGMA}'\n\ndigit_2txt = {\n    '0':    'ZERO',\n    '1':    'ONE',\n    '2':    'TWO',\n    '3':    'THREE',\n    '4':    'FOUR',\n    '5':    'FIVE',\n    '6':    'SIX',\n    '7':    'SEVEN',\n    '8':    'EIGHT',\n    '9':    'NINE',\n}\n\nsymb_2txt = {\n    '+':    'PLUS SIGN',\n    '-':    'MINUS',\n    '=':    'EQUALS SIGN',\n    '(':    'LEFT PARENTHESIS',\n    ')':    'RIGHT PARENTHESIS',\n    '[':    'LEFT SQUARE BRACKET',\n    ']':    'RIGHT SQUARE BRACKET',\n    '{':    'LEFT CURLY BRACKET',\n    '}':    'RIGHT CURLY BRACKET',\n\n    # non-std\n    '{}':   'CURLY BRACKET',\n    'sum':  'SUMMATION',\n    'int':  'INTEGRAL',\n}\n\n# SUBSCRIPT & SUPERSCRIPT\nLSUB = lambda letter: U('LATIN SUBSCRIPT SMALL LETTER %s' % letter.upper())\nGSUB = lambda letter: U('GREEK SUBSCRIPT SMALL LETTER %s' % letter.upper())\nDSUB = lambda digit:  U('SUBSCRIPT %s' % digit_2txt[digit])\nSSUB = lambda symb:   U('SUBSCRIPT %s' % symb_2txt[symb])\n\nLSUP = lambda letter: U('SUPERSCRIPT LATIN SMALL LETTER %s' % letter.upper())\nDSUP = lambda digit:  U('SUPERSCRIPT %s' % digit_2txt[digit])\nSSUP = lambda symb:   U('SUPERSCRIPT %s' % symb_2txt[symb])\n\nsub = {}    # symb -> subscript symbol\nsup = {}    # symb -> superscript symbol\n\n# latin subscripts\nfor l in 'aeioruvxhklmnpst':\n    sub[l] = LSUB(l)\n\nfor l in 'in':\n    sup[l] = LSUP(l)\n\nfor gl in ['beta', 'gamma', 'rho', 'phi', 'chi']:\n    sub[gl] = GSUB(gl)\n\nfor d in [str(i) for i in range(10)]:\n    sub[d] = DSUB(d)\n    sup[d] = DSUP(d)\n\nfor s in '+-=()':\n    sub[s] = SSUB(s)\n    sup[s] = SSUP(s)\n\n# Variable modifiers\n# TODO: Make brackets adjust to height of contents\nmodifier_dict = {\n    # Accents\n    'mathring': lambda s: center_accent(s, '\\N{COMBINING RING ABOVE}'),\n    'ddddot': lambda s: center_accent(s, '\\N{COMBINING FOUR DOTS ABOVE}'),\n    'dddot': lambda s: center_accent(s, '\\N{COMBINING THREE DOTS ABOVE}'),\n    'ddot': lambda s: center_accent(s, '\\N{COMBINING DIAERESIS}'),\n    'dot': lambda s: center_accent(s, '\\N{COMBINING DOT ABOVE}'),\n    'check': lambda s: center_accent(s, '\\N{COMBINING CARON}'),\n    'breve': lambda s: center_accent(s, '\\N{COMBINING BREVE}'),\n    'acute': lambda s: center_accent(s, '\\N{COMBINING ACUTE ACCENT}'),\n    'grave': lambda s: center_accent(s, '\\N{COMBINING GRAVE ACCENT}'),\n    'tilde': lambda s: center_accent(s, '\\N{COMBINING TILDE}'),\n    'hat': lambda s: center_accent(s, '\\N{COMBINING CIRCUMFLEX ACCENT}'),\n    'bar': lambda s: center_accent(s, '\\N{COMBINING OVERLINE}'),\n    'vec': lambda s: center_accent(s, '\\N{COMBINING RIGHT ARROW ABOVE}'),\n    'prime': lambda s: s+'\\N{PRIME}',\n    'prm': lambda s: s+'\\N{PRIME}',\n    # # Faces -- these are here for some compatibility with latex printing\n    # 'bold': lambda s: s,\n    # 'bm': lambda s: s,\n    # 'cal': lambda s: s,\n    # 'scr': lambda s: s,\n    # 'frak': lambda s: s,\n    # Brackets\n    'norm': lambda s: '\\N{DOUBLE VERTICAL LINE}'+s+'\\N{DOUBLE VERTICAL LINE}',\n    'avg': lambda s: '\\N{MATHEMATICAL LEFT ANGLE BRACKET}'+s+'\\N{MATHEMATICAL RIGHT ANGLE BRACKET}',\n    'abs': lambda s: '\\N{VERTICAL LINE}'+s+'\\N{VERTICAL LINE}',\n    'mag': lambda s: '\\N{VERTICAL LINE}'+s+'\\N{VERTICAL LINE}',\n}\n\n# VERTICAL OBJECTS\nHUP = lambda symb: U('%s UPPER HOOK' % symb_2txt[symb])\nCUP = lambda symb: U('%s UPPER CORNER' % symb_2txt[symb])\nMID = lambda symb: U('%s MIDDLE PIECE' % symb_2txt[symb])\nEXT = lambda symb: U('%s EXTENSION' % symb_2txt[symb])\nHLO = lambda symb: U('%s LOWER HOOK' % symb_2txt[symb])\nCLO = lambda symb: U('%s LOWER CORNER' % symb_2txt[symb])\nTOP = lambda symb: U('%s TOP' % symb_2txt[symb])\nBOT = lambda symb: U('%s BOTTOM' % symb_2txt[symb])\n\n# {} '('  ->  (extension, start, end, middle) 1-character\n_xobj_unicode = {\n\n    # vertical symbols\n    #       (( ext, top, bot, mid ), c1)\n    '(':    (( EXT('('), HUP('('), HLO('(') ), '('),\n    ')':    (( EXT(')'), HUP(')'), HLO(')') ), ')'),\n    '[':    (( EXT('['), CUP('['), CLO('[') ), '['),\n    ']':    (( EXT(']'), CUP(']'), CLO(']') ), ']'),\n    '{':    (( EXT('{}'), HUP('{'), HLO('{'), MID('{') ), '{'),\n    '}':    (( EXT('{}'), HUP('}'), HLO('}'), MID('}') ), '}'),\n    '|':    U('BOX DRAWINGS LIGHT VERTICAL'),\n\n    '<':    ((U('BOX DRAWINGS LIGHT VERTICAL'),\n              U('BOX DRAWINGS LIGHT DIAGONAL UPPER RIGHT TO LOWER LEFT'),\n              U('BOX DRAWINGS LIGHT DIAGONAL UPPER LEFT TO LOWER RIGHT')), '<'),\n\n    '>':    ((U('BOX DRAWINGS LIGHT VERTICAL'),\n              U('BOX DRAWINGS LIGHT DIAGONAL UPPER LEFT TO LOWER RIGHT'),\n              U('BOX DRAWINGS LIGHT DIAGONAL UPPER RIGHT TO LOWER LEFT')), '>'),\n\n    'lfloor': (( EXT('['), EXT('['), CLO('[') ), U('LEFT FLOOR')),\n    'rfloor': (( EXT(']'), EXT(']'), CLO(']') ), U('RIGHT FLOOR')),\n    'lceil':  (( EXT('['), CUP('['), EXT('[') ), U('LEFT CEILING')),\n    'rceil':  (( EXT(']'), CUP(']'), EXT(']') ), U('RIGHT CEILING')),\n\n    'int':  (( EXT('int'), U('TOP HALF INTEGRAL'), U('BOTTOM HALF INTEGRAL') ), U('INTEGRAL')),\n    'sum':  (( U('BOX DRAWINGS LIGHT DIAGONAL UPPER LEFT TO LOWER RIGHT'), '_', U('OVERLINE'), U('BOX DRAWINGS LIGHT DIAGONAL UPPER RIGHT TO LOWER LEFT')), U('N-ARY SUMMATION')),\n\n    # horizontal objects\n    #'-':   '-',\n    '-':    U('BOX DRAWINGS LIGHT HORIZONTAL'),\n    '_':    U('LOW LINE'),\n    # We used to use this, but LOW LINE looks better for roots, as it's a\n    # little lower (i.e., it lines up with the / perfectly.  But perhaps this\n    # one would still be wanted for some cases?\n    # '_':    U('HORIZONTAL SCAN LINE-9'),\n\n    # diagonal objects '\\' & '/' ?\n    '/':    U('BOX DRAWINGS LIGHT DIAGONAL UPPER RIGHT TO LOWER LEFT'),\n    '\\\\':   U('BOX DRAWINGS LIGHT DIAGONAL UPPER LEFT TO LOWER RIGHT'),\n}\n\n_xobj_ascii = {\n    # vertical symbols\n    #       (( ext, top, bot, mid ), c1)\n    '(':    (( '|', '/', '\\\\' ), '('),\n    ')':    (( '|', '\\\\', '/' ), ')'),\n\n# XXX this looks ugly\n#   '[':    (( '|', '-', '-' ), '['),\n#   ']':    (( '|', '-', '-' ), ']'),\n# XXX not so ugly :(\n    '[':    (( '[', '[', '[' ), '['),\n    ']':    (( ']', ']', ']' ), ']'),\n\n    '{':    (( '|', '/', '\\\\', '<' ), '{'),\n    '}':    (( '|', '\\\\', '/', '>' ), '}'),\n    '|':    '|',\n\n    '<':    (( '|', '/', '\\\\' ), '<'),\n    '>':    (( '|', '\\\\', '/' ), '>'),\n\n    'int':  ( ' | ', '  /', '/  ' ),\n\n    # horizontal objects\n    '-':    '-',\n    '_':    '_',\n\n    # diagonal objects '\\' & '/' ?\n    '/':    '/',\n    '\\\\':   '\\\\',\n}\n\n\ndef xobj(symb, length):\n    \"\"\"Construct spatial object of given length.\n\n    return: [] of equal-length strings\n    \"\"\"\n\n    if length <= 0:\n        raise ValueError(\"Length should be greater than 0\")\n\n    # TODO robustify when no unicodedat available\n    if _use_unicode:\n        _xobj = _xobj_unicode\n    else:\n        _xobj = _xobj_ascii\n\n    vinfo = _xobj[symb]\n\n    c1 = top = bot = mid = None\n\n    if not isinstance(vinfo, tuple):        # 1 entry\n        ext = vinfo\n    else:\n        if isinstance(vinfo[0], tuple):     # (vlong), c1\n            vlong = vinfo[0]\n            c1 = vinfo[1]\n        else:                               # (vlong), c1\n            vlong = vinfo\n\n        ext = vlong[0]\n\n        try:\n            top = vlong[1]\n            bot = vlong[2]\n            mid = vlong[3]\n        except IndexError:\n            pass\n\n    if c1 is None:\n        c1 = ext\n    if top is None:\n        top = ext\n    if bot is None:\n        bot = ext\n    if mid is not None:\n        if (length % 2) == 0:\n            # even height, but we have to print it somehow anyway...\n            # XXX is it ok?\n            length += 1\n\n    else:\n        mid = ext\n\n    if length == 1:\n        return c1\n\n    res = []\n    next = (length - 2)//2\n    nmid = (length - 2) - next*2\n\n    res += [top]\n    res += [ext]*next\n    res += [mid]*nmid\n    res += [ext]*next\n    res += [bot]\n\n    return res\n\n\ndef vobj(symb, height):\n    \"\"\"Construct vertical object of a given height\n\n       see: xobj\n    \"\"\"\n    return '\\n'.join( xobj(symb, height) )\n\n\ndef hobj(symb, width):\n    \"\"\"Construct horizontal object of a given width\n\n       see: xobj\n    \"\"\"\n    return ''.join( xobj(symb, width) )\n\n# RADICAL\n# n -> symbol\nroot = {\n    2: U('SQUARE ROOT'),   # U('RADICAL SYMBOL BOTTOM')\n    3: U('CUBE ROOT'),\n    4: U('FOURTH ROOT'),\n}\n\n\n# RATIONAL\nVF = lambda txt: U('VULGAR FRACTION %s' % txt)\n\n# (p,q) -> symbol\nfrac = {\n    (1, 2): VF('ONE HALF'),\n    (1, 3): VF('ONE THIRD'),\n    (2, 3): VF('TWO THIRDS'),\n    (1, 4): VF('ONE QUARTER'),\n    (3, 4): VF('THREE QUARTERS'),\n    (1, 5): VF('ONE FIFTH'),\n    (2, 5): VF('TWO FIFTHS'),\n    (3, 5): VF('THREE FIFTHS'),\n    (4, 5): VF('FOUR FIFTHS'),\n    (1, 6): VF('ONE SIXTH'),\n    (5, 6): VF('FIVE SIXTHS'),\n    (1, 8): VF('ONE EIGHTH'),\n    (3, 8): VF('THREE EIGHTHS'),\n    (5, 8): VF('FIVE EIGHTHS'),\n    (7, 8): VF('SEVEN EIGHTHS'),\n}\n\n\n# atom symbols\n_xsym = {\n    '==':  ('=', '='),\n    '<':   ('<', '<'),\n    '>':   ('>', '>'),\n    '<=':  ('<=', U('LESS-THAN OR EQUAL TO')),\n    '>=':  ('>=', U('GREATER-THAN OR EQUAL TO')),\n    '!=':  ('!=', U('NOT EQUAL TO')),\n    ':=':  (':=', ':='),\n    '+=':  ('+=', '+='),\n    '-=':  ('-=', '-='),\n    '*=':  ('*=', '*='),\n    '/=':  ('/=', '/='),\n    '%=':  ('%=', '%='),\n    '*':   ('*', U('DOT OPERATOR')),\n    '-->': ('-->', U('EM DASH') + U('EM DASH') +\n            U('BLACK RIGHT-POINTING TRIANGLE') if U('EM DASH')\n            and U('BLACK RIGHT-POINTING TRIANGLE') else None),\n    '==>': ('==>', U('BOX DRAWINGS DOUBLE HORIZONTAL') +\n            U('BOX DRAWINGS DOUBLE HORIZONTAL') +\n            U('BLACK RIGHT-POINTING TRIANGLE') if\n            U('BOX DRAWINGS DOUBLE HORIZONTAL') and\n            U('BOX DRAWINGS DOUBLE HORIZONTAL') and\n            U('BLACK RIGHT-POINTING TRIANGLE') else None),\n    '.':   ('*', U('RING OPERATOR')),\n}\n\n\ndef xsym(sym):\n    \"\"\"get symbology for a 'character'\"\"\"\n    op = _xsym[sym]\n\n    if _use_unicode:\n        return op[1]\n    else:\n        return op[0]\n\n\n# SYMBOLS\n\natoms_table = {\n    # class                    how-to-display\n    'Exp1':                    U('SCRIPT SMALL E'),\n    'Pi':                      U('GREEK SMALL LETTER PI'),\n    'Infinity':                U('INFINITY'),\n    'NegativeInfinity':        U('INFINITY') and ('-' + U('INFINITY')),  # XXX what to do here\n    #'ImaginaryUnit':          U('GREEK SMALL LETTER IOTA'),\n    #'ImaginaryUnit':          U('MATHEMATICAL ITALIC SMALL I'),\n    'ImaginaryUnit':           U('DOUBLE-STRUCK ITALIC SMALL I'),\n    'EmptySet':                U('EMPTY SET'),\n    'Naturals':                U('DOUBLE-STRUCK CAPITAL N'),\n    'Naturals0':               (U('DOUBLE-STRUCK CAPITAL N') and\n                                (U('DOUBLE-STRUCK CAPITAL N') +\n                                 U('SUBSCRIPT ZERO'))),\n    'Integers':                U('DOUBLE-STRUCK CAPITAL Z'),\n    'Rationals':               U('DOUBLE-STRUCK CAPITAL Q'),\n    'Reals':                   U('DOUBLE-STRUCK CAPITAL R'),\n    'Complexes':               U('DOUBLE-STRUCK CAPITAL C'),\n    'Union':                   U('UNION'),\n    'SymmetricDifference':     U('INCREMENT'),\n    'Intersection':            U('INTERSECTION'),\n    'Ring':                    U('RING OPERATOR'),\n    'Modifier Letter Low Ring':U('Modifier Letter Low Ring'),\n    'EmptySequence':           'EmptySequence',\n}\n\n\ndef pretty_atom(atom_name, default=None, printer=None):\n    \"\"\"return pretty representation of an atom\"\"\"\n    if _use_unicode:\n        if printer is not None and atom_name == 'ImaginaryUnit' and printer._settings['imaginary_unit'] == 'j':\n            return U('DOUBLE-STRUCK ITALIC SMALL J')\n        else:\n            return atoms_table[atom_name]\n    else:\n        if default is not None:\n            return default\n\n        raise KeyError('only unicode')  # send it default printer\n\n\ndef pretty_symbol(symb_name, bold_name=False):\n    \"\"\"return pretty representation of a symbol\"\"\"\n    # let's split symb_name into symbol + index\n    # UC: beta1\n    # UC: f_beta\n\n    if not _use_unicode:\n        return symb_name\n\n    name, sups, subs = split_super_sub(symb_name)\n\n    def translate(s, bold_name) :\n        if bold_name:\n            gG = greek_bold_unicode.get(s)\n        else:\n            gG = greek_unicode.get(s)\n        if gG is not None:\n            return gG\n        for key in sorted(modifier_dict.keys(), key=lambda k:len(k), reverse=True) :\n            if s.lower().endswith(key) and len(s)>len(key):\n                return modifier_dict[key](translate(s[:-len(key)], bold_name))\n        if bold_name:\n            return ''.join([bold_unicode[c] for c in s])\n        return s\n\n    name = translate(name, bold_name)\n\n    # Let's prettify sups/subs. If it fails at one of them, pretty sups/subs are\n    # not used at all.\n    def pretty_list(l, mapping):\n        result = []\n        for s in l:\n            pretty = mapping.get(s)\n            if pretty is None:\n                try:  # match by separate characters\n                    pretty = ''.join([mapping[c] for c in s])\n                except (TypeError, KeyError):\n                    return None\n            result.append(pretty)\n        return result\n\n    pretty_sups = pretty_list(sups, sup)\n    if pretty_sups is not None:\n        pretty_subs = pretty_list(subs, sub)\n    else:\n        pretty_subs = None\n\n    # glue the results into one string\n    if pretty_subs is None:  # nice formatting of sups/subs did not work\n        if subs:\n            name += '_'+'_'.join([translate(s, bold_name) for s in subs])\n        if sups:\n            name += '__'+'__'.join([translate(s, bold_name) for s in sups])\n        return name\n    else:\n        sups_result = ' '.join(pretty_sups)\n        subs_result = ' '.join(pretty_subs)\n\n    return ''.join([name, sups_result, subs_result])\n\n\ndef annotated(letter):\n    \"\"\"\n    Return a stylised drawing of the letter ``letter``, together with\n    information on how to put annotations (super- and subscripts to the\n    left and to the right) on it.\n\n    See pretty.py functions _print_meijerg, _print_hyper on how to use this\n    information.\n    \"\"\"\n    ucode_pics = {\n        'F': (2, 0, 2, 0, '\\N{BOX DRAWINGS LIGHT DOWN AND RIGHT}\\N{BOX DRAWINGS LIGHT HORIZONTAL}\\n'\n                          '\\N{BOX DRAWINGS LIGHT VERTICAL AND RIGHT}\\N{BOX DRAWINGS LIGHT HORIZONTAL}\\n'\n                          '\\N{BOX DRAWINGS LIGHT UP}'),\n        'G': (3, 0, 3, 1, '\\N{BOX DRAWINGS LIGHT ARC DOWN AND RIGHT}\\N{BOX DRAWINGS LIGHT HORIZONTAL}\\N{BOX DRAWINGS LIGHT ARC DOWN AND LEFT}\\n'\n                          '\\N{BOX DRAWINGS LIGHT VERTICAL}\\N{BOX DRAWINGS LIGHT RIGHT}\\N{BOX DRAWINGS LIGHT DOWN AND LEFT}\\n'\n                          '\\N{BOX DRAWINGS LIGHT ARC UP AND RIGHT}\\N{BOX DRAWINGS LIGHT HORIZONTAL}\\N{BOX DRAWINGS LIGHT ARC UP AND LEFT}')\n    }\n    ascii_pics = {\n        'F': (3, 0, 3, 0, ' _\\n|_\\n|\\n'),\n        'G': (3, 0, 3, 1, ' __\\n/__\\n\\\\_|')\n    }\n\n    if _use_unicode:\n        return ucode_pics[letter]\n    else:\n        return ascii_pics[letter]\n\n_remove_combining = dict.fromkeys(list(range(ord('\\N{COMBINING GRAVE ACCENT}'), ord('\\N{COMBINING LATIN SMALL LETTER X}')))\n                            + list(range(ord('\\N{COMBINING LEFT HARPOON ABOVE}'), ord('\\N{COMBINING ASTERISK ABOVE}'))))\n\ndef is_combining(sym):\n    \"\"\"Check whether symbol is a unicode modifier. \"\"\"\n\n    return ord(sym) in _remove_combining\n\n\ndef center_accent(string, accent):\n    \"\"\"\n    Returns a string with accent inserted on the middle character. Useful to\n    put combining accents on symbol names, including multi-character names.\n\n    Parameters\n    ==========\n\n    string : string\n        The string to place the accent in.\n    accent : string\n        The combining accent to insert\n\n    References\n    ==========\n\n    .. [1] https://en.wikipedia.org/wiki/Combining_character\n    .. [2] https://en.wikipedia.org/wiki/Combining_Diacritical_Marks\n\n    \"\"\"\n\n    # Accent is placed on the previous character, although it may not always look\n    # like that depending on console\n    midpoint = len(string) // 2 + 1\n    firstpart = string[:midpoint]\n    secondpart = string[midpoint:]\n    return firstpart + accent + secondpart\n\n\ndef line_width(line):\n    \"\"\"Unicode combining symbols (modifiers) are not ever displayed as\n    separate symbols and thus shouldn't be counted\n    \"\"\"\n    return len(line.translate(_remove_combining))"}
{"model_name_or_path": "agentless", "instance_id": "sympy__sympy-24213", "model_patch": "diff --git a/sympy/physics/units/unitsystem.py b/sympy/physics/units/unitsystem.py\nindex f6b73be..57a3e00 100644\n--- a/sympy/physics/units/unitsystem.py\n+++ b/sympy/physics/units/unitsystem.py\n@@ -175,7 +175,7 @@ class UnitSystem(_QuantityMapper):\n             for addend in expr.args[1:]:\n                 addend_factor, addend_dim = \\\n                     self._collect_factor_and_dimension(addend)\n-                if dim != addend_dim:\n+                if not self.get_dimension_system().is_compatible(dim, addend_dim):\n                     raise ValueError(\n                         'Dimension of \"{}\" is {}, '\n                         'but it should be {}'.format(\n", "raw_model_patch": "diff --git a/sympy/physics/units/unitsystem.py b/sympy/physics/units/unitsystem.py\nindex f6b73be..57a3e00 100644\n--- a/sympy/physics/units/unitsystem.py\n+++ b/sympy/physics/units/unitsystem.py\n@@ -175,7 +175,7 @@ class UnitSystem(_QuantityMapper):\n             for addend in expr.args[1:]:\n                 addend_factor, addend_dim = \\\n                     self._collect_factor_and_dimension(addend)\n-                if dim != addend_dim:\n+                if not self.get_dimension_system().is_compatible(dim, addend_dim):\n                     raise ValueError(\n                         'Dimension of \"{}\" is {}, '\n                         'but it should be {}'.format(\n", "original_file_content": "\"\"\"\nUnit system for physical quantities; include definition of constants.\n\"\"\"\n\nfrom typing import Dict as tDict, Set as tSet\n\nfrom sympy.core.add import Add\nfrom sympy.core.function import (Derivative, Function)\nfrom sympy.core.mul import Mul\nfrom sympy.core.power import Pow\nfrom sympy.core.singleton import S\nfrom sympy.physics.units.dimensions import _QuantityMapper\nfrom sympy.physics.units.quantities import Quantity\n\nfrom .dimensions import Dimension\n\n\nclass UnitSystem(_QuantityMapper):\n    \"\"\"\n    UnitSystem represents a coherent set of units.\n\n    A unit system is basically a dimension system with notions of scales. Many\n    of the methods are defined in the same way.\n\n    It is much better if all base units have a symbol.\n    \"\"\"\n\n    _unit_systems = {}  # type: tDict[str, UnitSystem]\n\n    def __init__(self, base_units, units=(), name=\"\", descr=\"\", dimension_system=None, derived_units: tDict[Dimension, Quantity]={}):\n\n        UnitSystem._unit_systems[name] = self\n\n        self.name = name\n        self.descr = descr\n\n        self._base_units = base_units\n        self._dimension_system = dimension_system\n        self._units = tuple(set(base_units) | set(units))\n        self._base_units = tuple(base_units)\n        self._derived_units = derived_units\n\n        super().__init__()\n\n    def __str__(self):\n        \"\"\"\n        Return the name of the system.\n\n        If it does not exist, then it makes a list of symbols (or names) of\n        the base dimensions.\n        \"\"\"\n\n        if self.name != \"\":\n            return self.name\n        else:\n            return \"UnitSystem((%s))\" % \", \".join(\n                str(d) for d in self._base_units)\n\n    def __repr__(self):\n        return '<UnitSystem: %s>' % repr(self._base_units)\n\n    def extend(self, base, units=(), name=\"\", description=\"\", dimension_system=None, derived_units: tDict[Dimension, Quantity]={}):\n        \"\"\"Extend the current system into a new one.\n\n        Take the base and normal units of the current system to merge\n        them to the base and normal units given in argument.\n        If not provided, name and description are overridden by empty strings.\n        \"\"\"\n\n        base = self._base_units + tuple(base)\n        units = self._units + tuple(units)\n\n        return UnitSystem(base, units, name, description, dimension_system, {**self._derived_units, **derived_units})\n\n    def get_dimension_system(self):\n        return self._dimension_system\n\n    def get_quantity_dimension(self, unit):\n        qdm = self.get_dimension_system()._quantity_dimension_map\n        if unit in qdm:\n            return qdm[unit]\n        return super().get_quantity_dimension(unit)\n\n    def get_quantity_scale_factor(self, unit):\n        qsfm = self.get_dimension_system()._quantity_scale_factors\n        if unit in qsfm:\n            return qsfm[unit]\n        return super().get_quantity_scale_factor(unit)\n\n    @staticmethod\n    def get_unit_system(unit_system):\n        if isinstance(unit_system, UnitSystem):\n            return unit_system\n\n        if unit_system not in UnitSystem._unit_systems:\n            raise ValueError(\n                \"Unit system is not supported. Currently\"\n                \"supported unit systems are {}\".format(\n                    \", \".join(sorted(UnitSystem._unit_systems))\n                )\n            )\n\n        return UnitSystem._unit_systems[unit_system]\n\n    @staticmethod\n    def get_default_unit_system():\n        return UnitSystem._unit_systems[\"SI\"]\n\n    @property\n    def dim(self):\n        \"\"\"\n        Give the dimension of the system.\n\n        That is return the number of units forming the basis.\n        \"\"\"\n        return len(self._base_units)\n\n    @property\n    def is_consistent(self):\n        \"\"\"\n        Check if the underlying dimension system is consistent.\n        \"\"\"\n        # test is performed in DimensionSystem\n        return self.get_dimension_system().is_consistent\n\n    @property\n    def derived_units(self) -> tDict[Dimension, Quantity]:\n        return self._derived_units\n\n    def get_dimensional_expr(self, expr):\n        from sympy.physics.units import Quantity\n        if isinstance(expr, Mul):\n            return Mul(*[self.get_dimensional_expr(i) for i in expr.args])\n        elif isinstance(expr, Pow):\n            return self.get_dimensional_expr(expr.base) ** expr.exp\n        elif isinstance(expr, Add):\n            return self.get_dimensional_expr(expr.args[0])\n        elif isinstance(expr, Derivative):\n            dim = self.get_dimensional_expr(expr.expr)\n            for independent, count in expr.variable_count:\n                dim /= self.get_dimensional_expr(independent)**count\n            return dim\n        elif isinstance(expr, Function):\n            args = [self.get_dimensional_expr(arg) for arg in expr.args]\n            if all(i == 1 for i in args):\n                return S.One\n            return expr.func(*args)\n        elif isinstance(expr, Quantity):\n            return self.get_quantity_dimension(expr).name\n        return S.One\n\n    def _collect_factor_and_dimension(self, expr):\n        \"\"\"\n        Return tuple with scale factor expression and dimension expression.\n        \"\"\"\n        from sympy.physics.units import Quantity\n        if isinstance(expr, Quantity):\n            return expr.scale_factor, expr.dimension\n        elif isinstance(expr, Mul):\n            factor = 1\n            dimension = Dimension(1)\n            for arg in expr.args:\n                arg_factor, arg_dim = self._collect_factor_and_dimension(arg)\n                factor *= arg_factor\n                dimension *= arg_dim\n            return factor, dimension\n        elif isinstance(expr, Pow):\n            factor, dim = self._collect_factor_and_dimension(expr.base)\n            exp_factor, exp_dim = self._collect_factor_and_dimension(expr.exp)\n            if self.get_dimension_system().is_dimensionless(exp_dim):\n                exp_dim = 1\n            return factor ** exp_factor, dim ** (exp_factor * exp_dim)\n        elif isinstance(expr, Add):\n            factor, dim = self._collect_factor_and_dimension(expr.args[0])\n            for addend in expr.args[1:]:\n                addend_factor, addend_dim = \\\n                    self._collect_factor_and_dimension(addend)\n                if dim != addend_dim:\n                    raise ValueError(\n                        'Dimension of \"{}\" is {}, '\n                        'but it should be {}'.format(\n                            addend, addend_dim, dim))\n                factor += addend_factor\n            return factor, dim\n        elif isinstance(expr, Derivative):\n            factor, dim = self._collect_factor_and_dimension(expr.args[0])\n            for independent, count in expr.variable_count:\n                ifactor, idim = self._collect_factor_and_dimension(independent)\n                factor /= ifactor**count\n                dim /= idim**count\n            return factor, dim\n        elif isinstance(expr, Function):\n            fds = [self._collect_factor_and_dimension(arg) for arg in expr.args]\n            dims = [Dimension(1) if self.get_dimension_system().is_dimensionless(d[1]) else d[1] for d in fds]\n            return (expr.func(*(f[0] for f in fds)), *dims)\n        elif isinstance(expr, Dimension):\n            return S.One, expr\n        else:\n            return expr, Dimension(1)\n\n    def get_units_non_prefixed(self) -> tSet[Quantity]:\n        \"\"\"\n        Return the units of the system that do not have a prefix.\n        \"\"\"\n        return set(filter(lambda u: not u.is_prefixed and not u.is_physical_constant, self._units))"}
